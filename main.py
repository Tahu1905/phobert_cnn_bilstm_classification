import torch
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

from Model import *
from utils import *
from train_eval import train, init_network


def prepare_data_in_ram(config):
    """
    HÃ m nÃ y cháº¡y 1 láº§n: Äá»c file -> Extract Feature -> Tráº£ vá» Tensor
    """
    print(">>> 1. Äang Ä‘á»c vÃ  xá»­ lÃ½ dá»¯ liá»‡u (Extract Features)...")
    phobert = PhoBert()

    # Äá»c file TSV (HÃ£y cháº¯c cháº¯n file 'train_new.tsv' náº±m cÃ¹ng thÆ° má»¥c)
    try:
        df = pd.read_csv('train_new.tsv', sep='\t', encoding='utf-8-sig').dropna()
    except FileNotFoundError:
        print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file 'train_new.tsv'. HÃ£y kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n!")
        exit()

    label_map = {'Human': 0, 'AI': 1, 0: 0, 1: 1, '0': 0, '1': 1}
    all_features = []
    all_labels = []

    print(f"ðŸ”„ Äang trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng cho {len(df)} máº«u...")
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        text = str(row[0]).strip()
        label_raw = row[1]
        if label_raw not in label_map: continue
        label = label_map[label_raw]

        try:
            # Extract vÃ  chuyá»ƒn vá» CPU ngay
            feature = phobert.extract_feature(text).squeeze(0).cpu()
            all_features.append(feature)
            all_labels.append(label)
        except Exception as e:
            print(f"Lá»—i dÃ²ng {idx}: {e}")

    return all_features, all_labels

def main():
    config = Config()

    # Setup Seed
    np.random.seed(1)
    torch.manual_seed(1)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(1)

    # --- BÆ¯á»šC 1: Xá»­ lÃ½ dá»¯ liá»‡u thÃ´ thÃ nh Vector (Náº·ng nháº¥t) ---
    # List cÃ¡c tensor: [Tensor1, Tensor2,...]
    X_list, y_list = prepare_data_in_ram(config)

    # --- BÆ¯á»šC 2: Chia dá»¯ liá»‡u (Stratified Split) ---
    print(">>> 2. Äang chia táº­p dá»¯ liá»‡u (Stratified Split)...")

    # VÃ¬ X_list lÃ  list cÃ¡c tensor cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau, ta chia index
    indices = np.arange(len(X_list))

    # TÃ¡ch Train (80%) - Temp (20%)
    train_idx, temp_idx, y_train, y_temp = train_test_split(
        indices, y_list, test_size=0.2, stratify=y_list, random_state=42
    )

    # TÃ¡ch Dev (10%) - Test (10%)
    dev_idx, test_idx, y_dev, y_test = train_test_split(
        temp_idx, y_temp, test_size=0.5, stratify=y_temp, random_state=42
    )

    print(f"   + Train: {len(train_idx)} | Dev: {len(dev_idx)} | Test: {len(test_idx)}")


    from torch.nn.utils.rnn import pad_sequence



    # Helper function táº¡o dataset tá»« index
    def create_dataset(indices, all_X, all_Y):
        data = []
        for i in indices:
            data.append((all_X[i], all_Y[i]))
        return data  # Tráº£ vá» list tuple Ä‘á»ƒ Ä‘Æ°a vÃ o DataLoader

    train_data = create_dataset(train_idx, X_list, y_list)
    dev_data = create_dataset(dev_idx, X_list, y_list)
    test_data = create_dataset(test_idx, X_list, y_list)

    train_iter = DataLoader(train_data, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)
    dev_iter = DataLoader(dev_data, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)
    test_iter = DataLoader(test_data, batch_size=config.batch_size, shuffle=False,
                           collate_fn=collate_fn)  # Sá»­a test_dataset -> test_data

    # --- BÆ¯á»šC 4: Train ---
    print(">>> 4. Khá»Ÿi táº¡o Model & Train...")
    model = CNN_BiLSTM(config).to(config.device)
    init_network(model)

    train(config, model, train_iter, dev_iter, test_iter)


if __name__ == "__main__":
    main()