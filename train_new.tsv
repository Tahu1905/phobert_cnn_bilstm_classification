label	sentence
0	Chiều dài nước nhảy là một đặc trưng quan trọng cần được tính toán chính xác vì nó ảnh hưởng trực tiếp đến chiều dài bể tiêu năng.  Vì vậy, mục đích của nghiên cứu này là phát triển và đánh giá sáu mô hình học máy, gồm có: Cây quyết định (Decision Tree – DT), Rừng cây ngẫu nhiên (Random Forest - RT), Tăng cường thích ứng (Adaptive Boosting – Ada), Tăng cường độ dốc (Gradient Boosting - GB), Cây bổ sung (Extra Trees - ET) và Máy Vector hỗ trợ (Support Vector Machine – SVM). Nghiên cứu này đã sử dụng Định lý $\pi$-Buckingham để tìm năm tham số không thứ nguyên phục vụ cho các mô hình học máy và ứng dụng các mô hình này để đánh giá mức độ ảnh hưởng của các biến độc lập đến biến mục tiêu.
0	Phương pháp học máy cho thấy hiệu quả vượt trội so với phương pháp công thức kinh nghiệm. Các mô hình học máy có xét đến ảnh hưởng của độ nhám và chiều rộng lòng dẫn, tính nhớt của chất lỏng, có sai số dự báo nhỏ hơn so với các công thức kinh nghiệm. Mô hình ET cho kết quả tốt nhất với hệ số Nash đạt 0.99, sau đó là Ada, RF, GB, DT, SVR, theo thứ tự giảm dần. Kết quả nghiên cứu cho thấy mô hình ET có thể thay thế các công thức kinh nghiệm trong việc tính toán chiều dài nước nhảy trong kênh lăng trụ đáy bằng có mặt cắt chữ nhật.
0	Cho đến nay, chiều dài nước nhảy được tính toán bằng các công thức kinh nghiệm, không có phương trình thuần túy lý thuyết cho việc này. Các công thức kinh nghiệm có ưu điểm là đơn giản, dễ sử dụng. Chỉ cần biết độ sâu và vận tốc trước nước nhảy hoặc hai độ sâu nước nhảy là tính được chiều dài của nó. Các nhà khoa học như Chertausov (1935), Pikalov (1954), Silvester (1964), Hager (1992) đã đề xuất các công thức tính tỷ số chiều dài với độ sâu trước nước nhảy, gọi là chiều dài tương đối của nước nhảy, theo số Froude trước nước nhảy trong kênh chữ nhật nằm ngang (Hager, 1992; Mammadov, 2017; Silvester R., 1964) (Brakeni et al., 2021) [5; 12; 16; 3].
0	Các công thức này không cần độ sâu sau nước nhảy, giúp cho việc tính toán đơn giản mà vẫn đảm bảo độ chính xác, vì độ sâu sau nước nhảy có thể được tính từ độ sâu và số Froude trước nước nhảy. Tuy nhiên, các công thức kinh nghiệm có hạn chế là: không đồng nhất nên dẫn đến các kết quả khác nhau; một số trường hợp có sai số lớn với sai số trung bình lên đến 27% (xem Bảng 5); không xét đến ảnh hưởng của chiều rộng và độ nhám lòng dẫn, tính nhớt của chất lỏng.
0	Vì vậy, cần có một phương pháp khác để khắc phục những hạn chế trên và tính toán chính xác hơn chiều dài nước nhảy trong kênh chữ nhật nằm ngang. Hình 1 minh họa các đặc trưng hình học của nước nhảy, trong đó: $L_r$ là chiều dài khu xoáy; $L_j$ là chiều dài nước nhảy; $h_1$ là độ sâu trước nước nhảy; $h_2$ là độ sâu sau nước nhảy.  Hiện nay, các thuật toán học máy (Machine Learning – ML) đã và đang được ứng dụng rộng rãi trong nhiều lĩnh vực khác nhau, bao gồm tài nguyên nước nói chung và thủy lực nói riêng (Ho et al., 2022; Truong et al., 2021) [7; 17].
0	Các mô hình ML thuộc nhóm các mô hình dựa trên cơ sở dữ liệu, đã được áp dụng để nghiên cứu các thông số của nước nhảy từ năm 2012 (Abbaspour et al., 2013; Naseri & Othman, 2012) [1; 13]. Những mô hình này sử dụng mối quan hệ thống kê giữa dữ liệu đầu vào và đầu ra để đưa ra dự báo. Việc ứng dụng các mô hình ML cho hiệu quả tốt trong nghiên cứu các vấn đề của cơ học chất lỏng và thủy lực, hỗ trợ các mô hình vật lý để giải quyết các bài toán thực tế (Brunton et al., 2020) [4].
0	Vì những nguyên nhân kể trên, mục đích của nghiên cứu này là phát triển và đánh giá khả năng dự báo của 6 mô hình ML , gồm Cây quyết định (Decision Tree – DT), Rừng cây ngẫu nhiên (Random Forest - RT), Tăng cường thích ứng (Adaptive Boosting – Ada), Tăng cường độ dốc (Gradient Boosting - GB), Cây bổ sung (Extra Trees - ET) và Máy Vector hỗ trợ (Support Vector Machine – SVM). Kết quả dự báo của sáu mô hình này sẽ được so sánh với bốn công thức kinh nghiệm nhằm tìm ra mô hình hiệu quả nhất cho việc tính toán chiều dài nước nhảy trong kênh lăng trụ đáy bằng có mặt cắt chữ nhật.
0	Các máng A, B, C, D, E tạo ra nước nhảy sau chân dốc của đập tràn. Trong khi đó, máng F tạo nước nhảy sau cửa cống phẳng, đáy cống nằm ngang.  Các kích cỡ và cách sắp xếp máng khác nhau giúp xác định ảnh hưởng của chiều rộng máng (b) và góc dòng chảy đi vào nước nhảy. Các thí nghiệm có nhiều thông số được liệt kê trong Bảng 1, cho phép quan sát nước nhảy với các kích cỡ khác nhau. Các máng có tường bên làm bằng kính để tiện theo dõi thí nghiệm.
0	Tổng cộng 120 mẫu kết quả thí nghiệm đã được sử dụng cho nghiên cứu này. Bộ dữ liệu này được chia làm hai phần để phục vụ các mô hình ML , phần thứ nhất gồm 96 mẫu (80% số liệu) nhằm mục đích huấn luyện mô hình (training), phần thứ hai gồm 24 mẫu (20% số liệu) để kiểm định mô hình (testing). Thuật toán ML sẽ chọn ngẫu nhiên 24 số liệu kiểm định dùng chung cho tất cả các mô hình nhằm đảm bảo tính khách quan, không phụ thuộc vào ý muốn của người sử dụng mô hình.
0	Mô hình cây quyết định (DT) là một mô hình được sử dụng khá phổ biến và hiệu quả trong bài toán dự báo của học máy có giám sát. Khác với những thuật toán khác trong học có giám sát, mô hình cây quyết định không tồn tại phương trình dự báo. Chúng ta cần tìm ra một cây quyết định dự báo tốt trên tập huấn luyện và sử dụng cây quyết định này dự báo trên tập kiểm tra. Các tiêu chí để lựa chọn biến phù hợp là các độ đo như entropy, Gini đo lường mức độ tinh khiết (purity) và vẩn đục (impurity) của một biến nào đó. Chỉ số Gini được sử dụng trong thuật toán CART (Classification And Regression Tree) của sklearn.
0	Dù có độ chính xác khá cao nhưng thuật toán cây quyết định (DT) tồn tại những hạn chế lớn. Sức mạnh của một cây quyết định là không cao thì hợp sức của nhiều cây sẽ trở nên mạnh mẽ hơn. Đó chính là mô hình rừng cây ngẫu nhiên (RT) . Vì có độ chính xác cao, giảm thiểu hiện tượng quá khớp (overfitting) nên mô hình RT được sử dụng rộng rãi trong cả hai bài toán phân loại và dự báo của học có giám sát. Mô hình RT được huấn luyện dựa trên sự phối hợp giữa quá trình kết hợp (ensembling) và lấy mẫu tái lặp (boostrapping).
0	Mô hình này tạo ra nhiều DT (Cây Quyết định) mà mỗi DT được huấn luyện dựa trên nhiều mẫu con khác nhau và kết quả dự báo là giá trị trung bình thu được từ toàn bộ những DT. Do đó, một kết quả dự báo được tổng hợp từ nhiều mô hình sẽ không bị sai lệch do các DT đều sử dụng bộ dữ liệu huấn luyện chung. Ngoài ra, tập hợp kết quả dự báo từ nhiều mô hình sẽ có phương sai nhỏ hơn và ít bị ảnh hưởng bởi nhiễu so với chỉ từ một mô hình. Trong mô hình RT (Rừng Cây Ngẫu nhiên), những DT là hoàn toàn độc lập với nhau.
0	"Thuật toán Ada, viết tắt của ""Adaptive Boosting - Tăng cường thích ứng"", là một phương pháp tổng hợp lặp đi lặp lại, chủ yếu được sử dụng để tăng hiệu suất của các mô hình phân loại yếu (weak classifiers). Một mô hình phân loại yếu có tỷ lệ dự báo sai lớn và giả định nó chỉ tốt hơn so với phân loại ngẫu nhiên một chút. Nguyên tắc cốt lõi của mô hình Ada là cân nhắc từng mẫu trong tập dữ liệu đầu vào dựa trên các lỗi của lần lặp trước đó. Mô hình Ada áp dụng liên tiếp các mô hình phân loại yếu để điều chỉnh lại trọng số cho các quan sát. Việc điều chỉnh trọng số của mỗi lần lặp nhằm đảm bảo rằng bộ học yếu (weak learner) tiếp theo tập trung nhiều hơn vào các mẫu bị phân loại sai trước đó."
0	Việc điều chỉnh này tiếp tục lặp lại cho đến khi sai số hội tụ về một giá trị nhỏ nhất hoặc đạt được một số cây (DT) nhất định. Như vậy, Ada là một mô hình dự báo được kết hợp từ các mô hình phân loại yếu trong chuỗi. Do tính chất thích ứng của mình, mô hình Ada có hiệu quả tốt trong các dự báo có ranh giới phức tạp giữa các lớp hoặc các bài toán hồi quy phi tuyến. Tiềm năng của mô hình Ada trong việc xác định các mối tương quan phi tuyến phức tạp giữa các yếu tố đầu vào và đầu ra có thể đóng vai trò then chốt trong việc dự báo chính xác. Phương trình hồi quy của Ada có thể được biểu diễn dưới dạng (11) .
0	Thuật toán GB (Gradient Boosting) là một thuật toán hiện đại được xây dựng dựa trên Ada. Cũng tương tự như Ada, nó huấn luyện liên tiếp các mô hình yếu. Thuật toán GB kết hợp các DT (Cây Quyết định) nhưng các cây không hoàn toàn độc lập mà chúng có sự phụ thuộc theo chuỗi. Tức là một DT được phát triển từ việc sử dụng thông tin được dự báo từ những DT được huấn luyện trước đó. Mô hình GB không sử dụng mẫu tái lặp (bootstrapping) để tạo dữ liệu huấn luyện mà mô hình được huấn luyện ngay trên dữ liệu gốc.
0	Điểm đặc biệt của mô hình này là thay vì cố gắng khớp giá trị biến mục tiêu, nó sẽ tìm cách khớp giá trị sai số của mô hình trước đó. Sau đó mô hình huấn luyện sẽ được đưa thêm vào hàm dự báo để cập nhật dần phần dư. Thuật toán sẽ dừng cập nhật khi số lượng DT (Cây Quyết định) đạt ngưỡng tối đa $K$, hoặc toàn bộ các quan sát trên tập huấn luyện được dự báo đúng.  Bằng cách khớp trên những DT có kích thước rất nhỏ trên những phần dư, hàm dự báo sẽ từ từ được cải thiện trong vùng mà nó không dự báo tốt. Giống như phương pháp kết hợp, kết quả dự báo từ chuỗi mô hình sẽ là kết hợp của các mô hình con, theo phương trình (12).
0	Từ quan điểm phương sai, yếu tố căn bản của phương pháp ET (Extra Trees) là sự ngẫu nhiên rõ ràng của điểm giới hạn và thuộc tính kết hợp với tính trung bình tổng thể có thể làm giảm phương sai mạnh hơn so với các sơ đồ ngẫu nhiên yếu hơn được sử dụng trong các phương pháp khác. Việc sử dụng mẫu huấn luyện ban đầu đầy đủ thay vì bản sao mẫu tái lặp được thúc đẩy để làm giảm thiểu sai lệch. Tuy nhiên, do tính đơn giản của quy trình tách nút, hy vọng rằng, hệ số không đổi sẽ nhỏ hơn nhiều so với các phương pháp tổng hợp khác nhằm tối ưu hóa cục bộ các điểm giới hạn.
0	SVM (Support Vector Machine) là một thuật toán khá hiệu quả trong việc phân loại nhị phân và dự báo của học máy có giám sát. Thuật toán này có ưu điểm là hoạt động tốt đối với những mẫu dữ liệu có kích thước lớn và thường mang lại kết quả vượt trội so với các thuật toán khác trong học có giám sát. Nó tiêu tốn ít bộ nhớ vì chỉ sử dụng các điểm trong tập hỗ trợ (support vectors) để dự báo trong hàm quyết định được tạo ra từ những hàm kernel khác nhau. Việc sử dụng đúng hàm kernel có thể giúp cải thiện đáng kể kết quả dự báo của thuật toán.
0	Những hàm kernel phổ biến đã được tích hợp bên trong sklearn gồm có: Kernel RBF (Radial Basis Function) dựa trên hàm Gaussian RBF biến đổi phi tuyến; Kernel tuyến tính (linear): đây là tích vô hướng giữa hai véc tơ; Kernel đa thức (poly) tạo ra một đa thức bậc cao kết hợp giữa hai véc tơ; Kernel Sigmoid dựa trên kernel về đa thức, chuyển tiếp qua hàm tanh và có thể biểu diễn theo hàm sigmoid. Trong bài toán dự báo, thuật toán có tên là SVR (Support Vector Regression), kết quả dự báo được thể hiện trong phương trình (14).
0	Trong nghiên cứu này, sáu mô hình ML đã được lập trình nhờ ứng dụng thư viện phần mềm mã nguồn mở Keras và Scikit-learn cũng như API cấp cao của TensorFlow 2. Hơn nữa, ngôn ngữ lập trình Python 3.7 và một số thư viện nhằm minh họa và quản lý dữ liệu như Numpy, Pandas và Matplotlib, đã được sử dụng. Chiến lược tìm kiếm bằng lưới (grid search) kết hợp với phương pháp thử dần đã được sử dụng trong nghiên cứu này để điều chỉnh các siêu tham số, đây là một phương pháp được sử dụng rộng rãi nhằm cải thiện độ chính xác và độ tin cậy của kết quả dự báo.
0	Hiện nay, có hai phương pháp để xác định mức độ ảnh hưởng hay điểm quan trọng của các biến độc lập, bao gồm phương pháp thống kê (đơn giản nhất) và phương pháp tầm quan trọng. Trong nghiên cứu này, phương pháp tầm quan trọng của đặc trưng (feature importance) được sử dụng để tính điểm quan trọng (xem Bảng 3). Điểm quan trọng này được tính toán cho từng thuộc tính trong tập dữ liệu đầu vào, nhờ đó các thuộc tính được xếp hạng và so sánh với nhau. Điểm của từng thuộc tính cho biết mức độ hữu ích hoặc có giá trị của nó trong việc xây dựng cây quyết định của mô hình.
0	Nghiên cứu này sử dụng năm thuật toán ML, bao gồm DT, RF, ET, Ada và GB, để đánh giá mức độ tác động của các yếu tố thủy lực đến độ dài tương đối của nước nhảy. Kết quả tính toán trong Bảng 3 cho thấy, cả năm thuật toán đều đánh giá số Fr1 (Số Froude trước nước nhảy) có ảnh hưởng nhiều nhất, vượt trội so với các yếu tố khác. Bốn thuật toán cho rằng số Re1(Số Reynolds) có ảnh hưởng thứ hai sau số Fr1, nhưng ảnh hưởng này không lớn. Chiều rộng và độ nhám tương đối có ảnh hưởng ít nhất.  Kết quả này đã giải thích vì sao các công thức thực nghiệm tính chiều dài nước nhảy tương đối chỉ phụ thuộc vào số Fr1.
0	Mô hình SVR có kết quả huấn luyện tốt nhất, với hệ số Nash đạt gần 0.99 và sai số tương đối trung bình MAPE dưới 6 %, sau đó đến mô hình ET với sai số trên 6 %. Mô hình DT có hệ số Nash thấp nhất và sai số tương đối MAPE cao nhất, trên 9 %. Điều này phản ánh đúng tính chất của thuật toán DT. Các thuật toán khác, trừ SVR, đều được cải tiến dựa trên DT nên có kết quả tốt hơn DT. Ba mô hình RF, Ada, GB có kết quả xấp xỉ nhau, với hệ số Nash gần 0.98 và sai số tương đối trên 7 %. Các sai số MAE, RMSE cũng có chung quy luật như sai số tương đối MAPE.
0	Kết thúc giai đoạn huấn luyện, các mô hình ML được chuyển sang giai đoạn kiểm định. Hình 2 so sánh kết quả huấn luyện và kiểm định mô hình , cho thấy trong giai đoạn kiểm định, mô hình SVR có kết quả dự báo kém chính xác nhất. Kết quả kiểm định mô hình này có các chỉ số thống kê đều thua kém kết quả huấn luyện. Năm mô hình còn lại cho kết quả dự báo trong giai đoạn kiểm định chính xác hơn khi huấn luyện. Việc này cho thấy hiệu suất cao của năm mô hình DT, RF, ET, Ada và GB trong dự báo chiều dài nước nhảy tương đối.
0	Mặc dù có kết quả huấn luyện cao nhất, nhưng mô hình SVR lại có kết quả kiểm định thấp nhất so với năm mô hình ML khác. Điều này hoàn toàn trái ngược với các nhận xét trước đây về thuật toán SVR. Như vậy, có thể thấy rằng kết quả của nghiên cứu này là rất cần thiết cho việc lựa chọn mô hình ML để dự báo chiều dài nước nhảy. Hình 3 so sánh kết quả dự báo của sáu mô hình ML với số liệu thực đo trong thí nghiệm mô hình vật lý. Các điểm trên Hình 3 đều bám sát đường phân giác của góc 90° , cho thấy độ chính xác của kết quả dự báo và hiệu suất mô hình rất cao.
0	Bảng 5 thống kê kết quả kiểm định sáu mô hình ML và bốn công thức kinh nghiệm. Điều đáng mừng là tất cả các mô hình toán và công thức kinh nghiệm đều có hệ số Nash (NSE) từ 0.98 trở lên, chứng tỏ kết quả tính có độ chính xác rất cao. Chính xác nhất là kết quả của mô hình ET (Extra Trees) với hệ số NSE đạt 0.99 và sai số tương đối MAPE khoảng 5%.  Tiếp theo sau ET là Ada, RF, GB, DT, SVR theo thứ tự độ chính xác giảm dần. Mô hình DT có sai số MAE và MAPE cao nhất trong số các mô hình ML.
0	Tuy vậy, tất cả các mô hình ML đều cho kết quả kiểm định chính xác hơn các công thức kinh nghiệm. Các mô hình ML có trị số RMSE và MAPE thấp hơn, và hệ số Nash cao hơn so với các công thức kinh nghiệm. Cá biệt, các công thức Pikalov và Chertausov có sai số MAPE lên trên 20%, Silvester trên 10%. Công thức Hager có độ chính xác cao hơn ba công thức kể trên. Kết quả kiểm định được thể hiện bằng hình ảnh trong Hình 4 . Rõ ràng là các mô hình ML đã làm lu mờ các công thức kinh nghiệm bởi độ chính xác và tính hiệu quả trong dự báo.
0	Từ kết quả kiểm định mô hình có thể nhận thấy các ưu điểm của mô hình ML là: có độ chính xác và hiệu suất dự báo cao, xét đến nhiều yếu tố ảnh hưởng, có thể dễ dàng cập nhật khi có thêm dữ liệu mới, dễ tiếp cận, tốc độ tính toán nhanh. Trong các ưu điểm trên thì độ chính xác cao và tính dễ cập nhật là ưu điểm vượt trội của mô hình ML so với các công thức kinh nghiệm. Rõ ràng, các công thức kinh nghiệm không thể cập nhật và phạm vi ứng dụng phụ thuộc vào điều kiện thí nghiệm trên mô hình vật lý.
0	Tuy nhiên, mô hình ML cũng có hạn chế, đó là sự không kiên định trong các lần dự báo, phụ thuộc vào người sử dụng mô hình. Nghĩa là mỗi lần chạy có thể cho kết quả khác nhau, độ chính xác có thể tăng nhưng cũng có thể giảm trong lần chạy sau. Khi dự báo mức độ quan trọng của các biến độc lập, các mô hình có thể cho ra những xu hướng khác nhau trong kết quả tính. Những hạn chế này có thể được khắc phục bằng phương pháp thử dần, mỗi mô hình cần chạy nhiều lần để tìm ra kết quả tốt nhất. Trong số sáu mô hình ML, SVR (Support Vector Regression) có sự kiên định cao nhất, kết quả dự báo của nó không đổi sau các lần chạy. Để làm rõ hơn về vấn đề này cần tiếp tục nghiên cứu sâu hơn.
0	Bài báo này trình bày một phương pháp nghiên cứu mới để tính toán chiều dài nước nhảy tương đối trong kênh chữ nhật đáy bằng. Trong nghiên cứu này, Định lý $\pi$-Buckingham đã được sử dụng để tìm các tham số không thứ nguyên làm đầu vào và đầu ra của mô hình toán. Không chỉ số Froude trước nước nhảy, độ nhám và chiều rộng kênh cũng như độ nhớt của chất lỏng đã được xem xét khi tính chiều dài nước nhảy. Nghiên cứu này đã thiết lập sáu mô hình ML để đánh giá tầm quan trọng của các biến đầu vào, sau đó sử dụng các mô hình này để dự báo chiều dài nước nhảy tương đối. Ngoài ra, các ưu điểm và hạn chế của mô hình ML cũng được chỉ ra trong bài báo này.
0	Với tầm quan trọng của việc dự báo và cảnh báo lũ, các nghiên cứu tập trung vào ứng dụng các mô hình học máy vào các bài toán dự báo lũ đang ngày càng được quan tâm. Trong bài báo này, chúng tôi tổng quan lại các bài nghiên cứu gần đây về ứng dụng của học máy trong lĩnh vực về dự báo lũ, dự đoán mực nước, lưu lượng, độ sâu ngập,… cùng với đó là các chỉ số thường dùng để đánh giá độ tin cậy của các mô hình học máy. Các nghiên cứu đã cho thấy, khác với các mô hình toán, mô hình học máy cần ít thông số đầu vào, tốn ít thời gian để mô phỏng và không đòi hỏi nhiều kiến thức về mô phỏng ngập lụt mà vẫn đưa ra kết quả dự đoán có độ chính xác tốt.
0	Lũ lụt là một hiện tượng thiên nhiên hằng năm gây ra nhiều thiệt hại nặng nề đối với cơ sở hạ tầng, hoa màu cũng như nền kinh tế của các nước trên toàn thế giới. Nguyên nhân chủ yếu gây ra lũ lụt là do các tác động của biến đổi khí hậu, do tác động của con người, do mưa lớn làm mực nước trên sông tăng nhanh, khiến nước không kịp thoát [1]. Trong hơn 27 năm qua, lũ lụt là nguyên nhân gây ra cái chết cho hơn 175.000 người, và gây ảnh hưởng nặng nề về kinh tế ước tính lên đến 2,2 tỉ đô trên toàn cầu [2].
0	Do đó việc dự báo lũ, mực nước và lưu lượng trên sông đặc biệt trên các sông chưa có hoặc có ít trạm quan trắc thuỷ văn, rất quan trọng trong việc cảnh báo lũ cho người dân và chính quyền địa phương. Đến nay đã có nhiều dự án cũng như nghiên cứu của các nhà khoa học trong và ngoài nước ứng dụng các mô hình dựa trên tính chất vật lý (physically-based model) có độ chính xác cao vào việc dự đoán mực nước, lưu lượng hay dòng chảy đến trên các con sông, hồ chứa, … như mô hình MIKE, HYDRO River, HEC-HMS, SOBEK, EFDC, …
0	Hơn thế, để thiết lập, mô phỏng và phân tích các kết quả đầu ra của các mô hình dựa trên tính chất vật lý đòi hỏi sự tham gia của các chuyên gia trong lĩnh vực thuỷ văn, thuỷ lực,… Do đó tính ứng dụng thực tế của các mô hình này vào việc cảnh báo lũ theo thời gian chưa cao. Hướng phát triển trong tương lai của ngành thuỷ văn và quản lý tài nguyên nước đó là tìm ra phương pháp để tích hợp quản lý tài nguyên nước dựa trên các mô hình toán truyền thống vào các mô hình học máy để trực tiếp xử lý, phân tích và lấy thông tin từ các nguồn dữ liệu lớn [4].
0	Ngày nay, với sự phát triển của công nghệ thông tin thì các thuật ngữ học máy hay học sâu (deep learning) không còn quá xa lạ với chúng ta. Học máy được ứng dụng vào nhiều ngành nghề, lĩnh vực khác nhau của xã hội trong đó có lĩnh vực quản lý tài nguyên nước. Trong Bảng 2 liệt kê một số bài báo tổng quan về ứng dụng phương pháp học máy trong lĩnh vực quản lý tài nguyên nước nói chung và dự báo lũ nói riêng. Các bài tổng quan đã cho thấy sự phát triển nghiên cứu và ứng dụng học máy vào các bài toán trong lĩnh vực quản lý tài nguyên nước và quản lý rủi ro thiên tai.
0	Phương pháp học máy (Machine Learning) là một lĩnh vực của trí tuệ nhân tạo (Artificial Intelligence) và tập trung vào việc xây dựng các mô hình và thuật toán có khả năng học hỏi và tự điều chỉnh dữ liệu. Mục tiêu chính của học máy là cho máy tính tự động “học hỏi” từ những dữ liệu mà không cần phải được lập trình một cách cụ thể. Các thuật toán học máy có thể được chia thành 3 nhóm chính gồm học có giám sát (Supervised Learning), học không giám sát (Unsupervised Learning) và học tăng cường (Reinforcement Learning) [9] (Hình 1). Học có giám sát là nhóm phổ biến nhất trong các thuật toán học máy.
0	Các thuật toán học có giám sát được phân ra thành hai loại chính: i) phân lớp (classification) – khi các nhãn của dữ liệu đầu vào được chia thành một số hữu hạn lớp (miền là giá trị rời rạc); và ii) hồi quy (regression) – khi nhãn không được chia thành các nhóm mà là một giá trị thực cụ thể (miền là giá trị liên tục) [13]. Trái với học có giám sát, học không giám sát không yêu cầu dữ liệu đã được gán nhãn. Mô hình được huấn luyện để tìm hiểu cấu trúc, mẫu hoặc nhóm trong dữ liệu mà không có đầu ra tương ứng. Các thuật toán học không có giám sát thường được sử dụng để phân cụm (clustering) dữ liệu, phân tích thành phần chính (principal component analysis), hoặc khám phá cấu trúc dữ liệu [14].
0	Học tăng cường là một hình thức học máy nơi mô hình học thông qua tương tác liên tục với một môi trường và nhận điểm thưởng (thu được trong quá trình tương tác của mô hình với môi trường để khích lệ mô hình thực hiện các hành động có lợi để đạt được mục tiêu – reward) dựa trên hành động của nó. Mục tiêu của phương pháp này là tìm ra cách tối đa hóa điểm thưởng tích lũy trong thời gian dài thông qua việc tìm hiểu và tối ưu hóa chính sách hành động (quy tắc chiến lược xác định cách mà mô hình chọn hành động trong một tình huống cụ thể - action policy) [15]. Bảng 3 trình bày một cách tổng quan về sự khác nhau giữa ba phương pháp chính của học máy.
0	Học có giám sát là quá trình xây dựng một mô hình dự đoán hoặc phân loại dựa trên tập dữ liệu huấn luyện đã được gán nhãn. Trong học có giám sát, chúng ta có một tập dữ liệu huấn luyện bao gồm các mẫu dữ liệu đã được gán nhãn với kết quả mong muốn. Mục tiêu là tìm mối quan hệ giữa biến độc lập (các đặc trưng) và biến phụ thuộc (kết quả dự đoán hoặc nhãn), để sau đó có thể ước lượng hoặc dự đoán giá trị kỳ vọng của biến phụ thuộc cho các dữ liệu chưa được gán nhãn. Một số phương pháp học có giám sát thường được sử dụng trong bài toán dự báo lũ được miêu tả dưới đây.
0	Hồi quy tuyến tính (Linear Regression - LR) là một trong những phương pháp quan trọng và phổ biến nhất trong học máy. Mục tiêu của hồi quy tuyến tính là xây dựng một mô hình dự đoán có thể tìm ra các hệ số tối ưu để dự đoán giá trị biến mục tiêu dựa trên giá trị biến đầu vào [19]. Trong bài toán dự báo chuỗi thời gian, hồi quy tuyến tính có thể được áp dụng bằng cách sử dụng các đặc trưng (features) của chuỗi thời gian để dự đoán giá trị tương lai của chuỗi thời gian. Ví dụ như trong bài toán về dự báo mực nước, mô hình sẽ học từ các đặc trưng của chuỗi số liệu mực nước, lượng mưa, bốc hơi hay lưu lượng đến, … để đưa ra dự đoán về mực nước tại vị trí và thời điểm muốn dự đoán.
0	RFR là một phương pháp học máy có giám sát được sử dụng trong bài toán hồi quy. Nó là một thuật toán tổng hợp dựa trên việc kết hợp nhiều cây quyết định để tạo ra một mô hình dự báo chính xác và ổn định. RFR hoạt động bằng cách xây dựng một tập hợp các cây quyết định đơn lẻ. Mỗi cây trong RFR được xây dựng bằng cách lấy một mẫu ngẫu nhiên từ tập dữ liệu huấn luyện và sử dụng một phương pháp gọi là “bagging” để lựa chọn một tập con của các thuộc tính tại mỗi nút của cây. Quá trình này giúp tạo ra sự đa dạng và khả năng tổng hợp thông tin từ các cây con. Khi có một dữ liệu mới cần dự báo, RFR sử dụng các cây quyết định để đưa ra dự đoán riêng lẻ.
0	LGBMR là phương pháp được phát triển dựa trên kỹ thuật “boosting” và sử dụng thuật toán gradient boosting. Mô hình được cho là có hiệu quả tốt hơn, thời gian xử lý chạy mô hình nhanh hơn và tốn ít bộ nhớ hơn do nó sử dụng phương pháp phát triển theo lá cây (leaf-wise tree growth) [28] (Hình 2). SVM là một kỹ thuật phi tuyến tính trong lĩnh vực học máy và có thể hoạt động tốt trong các bài toán về phân loại, hồi quy và dự báo chuỗi thời gian [29]. Phương pháp SVM được sử dụng rộng rãi trong lĩnh vực về dự báo lũ và thuỷ văn vì chúng phù hợp cho cả các vấn đề về tuyến tính và phi tuyến tính, và cũng nổi tiếng với khả năng tổng quát hoá mạnh mẽ.
0	Mô hình học sâu (Deep Learning) hay mô hình các mạng nơron, là một nhánh của học máy được phát triển dựa trên hệ thống nơron trong não bộ của con người. Do đó các mô hình học sâu có thể xử lý và hiểu sự tương tác phức tạp hơn của các dữ liệu đầu vào. Về cấu trúc, các mô hình học sâu sẽ gồm ba lớp chính, cụ thể là lớp đầu vào (input layer) để mô hình nhận dữ liệu, các lớp ẩn (hidden layers) để thực hiện các phép tính và biến đổi dữ liệu thông qua các nơron nhân tạo, và lớp đầu ra (output layer) để sản xuất và đưa ra kết quả dự đoán của mô hình (Hình 3).
0	"RNN là một loại mạng thần kinh nhân tạo được sử dụng trong học sâu để xử lý dữ liệu chuỗi hoặc dữ liệu có tính tuần tự [31]. RNN có khả năng mô hình hóa và hiểu các mối quan hệ phụ thuộc thời gian trong dữ liệu. Một đặc điểm quan trọng của RNN là khả năng lưu trữ thông tin từ các bước trước đó và sử dụng nó để ảnh hưởng đến các bước kế tiếp. Điều này giúp RNN xử lý các chuỗi dữ liệu có độ dài thay đổi và phụ thuộc vào ngữ cảnh. Cấu trúc cơ bản của một RNN bao gồm một chuỗi các ""đơn vị"" (units) nằm trong các lớp. Mỗi đơn vị RNN nhận đầu vào từ bước thời gian hiện tại và trạng thái ẩn từ bước thời gian trước đó, sau đó tính toán trạng thái ẩn mới.
0	LSTM là một biến thể của RNN đang được sử dụng rộng rãi trong các bài toán về dự báo chuỗi thời gian [32]. Bằng cách sử dụng các cơ chế cổng (gate mechanism) để kiểm soát luồng thông tin trong quá trình tính toán trạng thái ẩn đã giúp LSTM có thể tăng khả năng mô hình học và hiểu các phụ thuộc dài hạn trong chuỗi dữ liệu hơn nhiều so với phương pháp RNN. Các cổng này gồm: Cổng quên (Forget gate) để quên thông tin không cần thiết từ trạng thái ẩn trước đó, cổng đầu vào (Input gate) để quyết định thông tin mới nào sẽ được lưu trữ vào trạng thái ẩn và cổng đầu ra (Output gate) để quyết định phần nào của trạng thái ẩn sẽ được chọn làm đầu ra (Hình 4).
0	GRU là một loại kiến trúc RNN được đề xuất nhằm giải quyết vấn đề mất mát thông tin xa (long-term information loss) trong các mạng RNN truyền thống. GRU có khả năng học các phụ thuộc dài hạn trong dữ liệu chuỗi thời gian mà không bị ảnh hưởng nhiều bởi vấn đề “gradient vụn”. Cấu trúc của GRU bao gồm các cổng để kiểm soát thông lượng thông tin trong mạng (Hình 5). Các cổng này bao gồm: Cổng cập nhật (Update gate) để xác định mức độ cập nhật thông tin mới vào trạng thái ẩn của GRU. Cổng đặt lại (Reset gate) để quyết định xem thông tin trong quá khứ có cần được đặt lại hay không. GRU có thể lưu giữ thông tin quan trọng từ quá khứ và sử dụng nó để ảnh hưởng đến trạng thái hiện tại.
0	CNN là một kiến trúc mạng nơron nhân tạo được sử dụng chủ yếu trong xử lý dữ liệu không gian như hình ảnh và video. Tuy nhiên, mô hình CNN cho bài toán xây dựng bản đồ ngập lụt, dữ liệu được chuyển thành một ma trận đầu vào 2 chiều, trong đó trục thứ nhất đại diện cho thời gian và trục thứ hai đại diện cho các biến đặc trưng. Các lớp “Convolutional” trong CNN được sử dụng để trích xuất các đặc trưng cục bộ từ dữ liệu chuỗi thời gian. Các lớp “Pooling” giúp giảm kích thước không gian của đặc trưng trích xuất, trong khi lớp “Activation” tạo tính phi tuyến cho mô hình. Cuối cùng, các lớp “Fully Connected” được sử dụng để dự đoán giá trị tiếp theo dựa trên các đặc trưng đã được trích xuất (Hình 6).
0	Trong các bài toán của học máy nói chung và trong các bài toán về dự báo lũ nói riêng, việc mô phỏng mô hình học máy để dự đoán ra kết quả mong muốn sẽ gồm 4 bước chính (được mô tả ở đây), đó là i) Thu thập và xử lý dữ liệu – các tham số đầu vào sẽ được thu thập và tổng hợp lại, cùng với đó các giá trị ngoại lai (outliers) sẽ được loại bỏ, xử lý; ii) Lựa chọn các tham số đầu vào phù hợp – trong bước này thực hiện chọn các tham số (biến) đầu vào có ảnh hưởng đến kết quả dự báo (outcome); iii) Lựa chọn thuật toán học máy – lựa chọn thuật toán phù hợp cho bài toán;
0	Sau đó tập dữ liệu còn lại sẽ được đưa vào mô hình để tiến hành kiểm tra mô hình dựa vào các chỉ số nhằm đánh giá độ chính xác của giá trị dự đoán với giá trị thực đo; và v) Triển khai và sử dụng mô hình – mô hình đạt được hiệu suất tốt sẽ được đưa vào môi trường thực tế và sử dụng để dự đoán kết quả trên tập dữ liệu mới  (Hình 7). Tính chính xác và độ tin cậy của các mô hình dự báo thường được đánh giá dựa trên các chỉ số đánh giá như hệ số xác định (R^2), sai số trung bình tuyệt đối (MAE), sai số trung bình phương (MSE), sai số trung bình phương căn bậc hai (RMSE), độ lệch chuẩn trung bình hoá (MBE).
0	Việc dự báo và cảnh báo lũ luôn là nhiệm vụ cấp bách cần phải thực hiện để hỗ trợ cho việc đưa ra quyết định của chính quyền địa phương. Tuy nhiên, những hạn chế của các mô hình dựa trên tính chất vật lý truyền thống (như cần nhiều dữ liệu, tham số đầu vào để mô phỏng chi tiết và sát nhất với thực tế, đòi hỏi nhiều thời gian mô phỏng và cũng như các chuyên gia trong lĩnh vực để có thể xây dựng và vận hành mô hình) khiến cho việc dự báo lũ theo thời gian thực chưa khả thi. Do đó, bài báo này đã cung cấp cho người đọc cái nhìn tổng quan về hướng tiếp cận mới và hiệu quả, đó là sử dụng các mô hình định hướng dữ liệu, cụ thể là các mô hình ứng dụng học máy, học sâu vào trong dự báo lũ.
0	Để đánh giá hiệu quả của mô hình, bài báo cũng liệt kê các chỉ số thường được dùng để đánh giá hiệu quả và độ tin cậy của các mô hình. Nhìn chung, các mô hình sử dụng phương pháp hồi quy và học sâu đều đưa ra các dự đoán có độ chính xác cao và hiệu quả. Từ các kết quả của các nghiên cứu thì đây sẽ là một cách tiếp cận mới, hiệu quả trong dự đoán và giảm nhẹ thiệt hại gây ra do lũ lụt. Tuy việc áp dụng các mô hình học máy, học sâu vào bài toán dự báo lũ mang nhiều tiềm năng và cơ hội, nhưng cũng đặt ra một số thách thức như đòi hỏi lượng lớn dữ liệu, chất lượng của dữ liệu, hiệu quả của mô hình, độ chính xác của mô hình, cụ thể như dưới đây:
0	Dữ liệu đầu vào: Một trong những thách thức lớn đó là sự khan hiếm dữ liệu lũ chính xác và đầy đủ. Bên cạnh đó cũng cần phải phát triển thêm các phương pháp xử lý, nội suy hay sinh thêm dữ liệu bị thiếu để có thể đảm bảo tính chính xác và độ tin cậy của mô hình dự báo. Đặc biệt điểm hạn chế ở các mô hình học sâu là phải cần một tập dữ liệu lớn hơn rất nhiều so với hồi quy hay phân lớp để có thể đưa ra những dự báo chính xác; Độ phức tạp của mô hình.
0	Kỹ thuật phản hồi: Trong bài toán dự báo lũ, thông tin thời gian thực và sự phản hồi nhanh chóng là rất quan trọng. Do đó cần phát triển các hệ thống tự động và theo thời gian thực để có thể đưa ra các cảnh báo cho chính quyền và người dân một cách kịp thời nhất;Kết hợp dữ liệu đa nguồn: Kết hợp dữ liệu từ nhiều nguồn khác nhau như mô hình thủy văn, thuỷ lực, radar, và các trạm quan trắc mưa, mực nước tự động có thể cải thiện khả năng dự báo lũ. Nghiên cứu về kỹ thuật tích hợp dữ liệu đa nguồn và kết hợp các phương pháp học máy và học sâu sẽ tạo ra kết quả tốt hơn.
0	Sự phát triển nhanh chóng và mạnh mẽ của ngành khoa học máy tính và khả năng tính toán trong vài thập kỷ gần đây đã thúc đẩy những ứng dụng của các phương pháp phân tích tiên tiến vào các bài toán thiết kế kỹ thuật xây dựng nói chung và thực hành thiết kế khung thép nói riêng. Một trong những hướng khả thi và phổ biến là áp dụng các thuật toán học máy vào dự đoán các ứng xử của kết cấu khung thép trong phân tích phi đàn hồi phi tuyến tính. Điều này cho thấy những ưu điểm rõ ràng như đẩy nhanh được quá trình ra quyết định, giảm tỷ lệ lỗi và tăng hiệu quả tính toán.
0	Hiệu quả khi áp dụng các phương pháp học máy được xem xét qua một ví dụ số khảo sát một khung thép phẳng 5 nhịp 14 tầng. Phân tích phi đàn hồi phi tuyến tính nâng cao được thực hiện cho khung thép nhằm tạo bộ dữ liệu cho huấn luyện để giảm thiểu thời gian phân tích. Các biến đầu vào của bài toán là các đặc điểm hình học của tiết diện thanh dầm cột được chọn từ danh mục có sẵn. Hiệu suất của các thuật toán học máy được đánh giá bằng cách sử dụng các chỉ số về lỗi gồm sai số bình phương trung bình (MSE), hệ số xác định (R^2) và Kết quả cho thấy phương pháp rừng ngẫu nhiên có hiệu quả tốt nhất trong ba phương pháp học máy lựa chọn.
0	Vật liệu thép có những ưu điểm vượt trội so với các vật liệu thông thường đó là tính chất cơ lý tốt, có khả năng chịu được mọi loại ứng suất như kéo, nén, uốn, xoắn... và có thể trải qua biến dạng lớn. Nhờ những đặc điểm này, mặc dù trọng lượng riêng của thép khá lớn so với các loại vật liệu khác nhưng các công trình làm bằng thép lại cho phép giảm trọng lượng bản thân đáng kể, kết cấu thanh mảnh, hình thức đa dạng, dễ tạo hình và có thể vượt được nhịp lớn. Ngoài ra, với những lợi ích vượt trội về đẩy nhanh tiến độ, đảm bảo độ bền vững và tính kỹ thuật cao nên kết cấu khung thép đã và đang được sử dụng rộng rãi trong các công trình xây dựng
0	Các phương pháp phân tích kết cấu thép truyền thống thường xem xét kết cấu làm việc trong giới hạn đàn hồi với mối quan hệ giữa ứng suất và biến dạng là tuyến tính, vật liệu được xem là không chảy dẻo và các tính chất của vật liệu không thay đổi, các phương trình cân bằng được thiết lập dựa trên trường hợp mô hình kết cấu chưa biến dạng. Việc phân tích khung thép được tiến hành trên cơ sở phân tích đàn hồi tuyến tính. Sau đó cấu kiện được thiết kế riêng lẻ dựa vào cường độ trong đó có kể đến yếu tố phi tuyến qua hệ số chiều dài tính toán K cho từng cấu kiện, nhưng cách tính toán hệ số này thường phức tạp, không rõ ràng, gây khó hiểu, không thuận lợi để thiết kế khung thép tự động trên máy tính.
0	Đối với thiết kế kết cấu khung thép, yếu tố đầu tiên luôn phải được xem xét là sự làm việc phi tuyến. Phi tuyến ở đây là nói đến đặc điểm phi tuyến tính hình học của kết cấu do kết cấu thanh mảnh và phi đàn hồi là nói đến đặc điểm làm việc ngoài miền đàn hồi của vật liệu. Do vậy đặt ra nhu cầu cần có phương pháp phân tích phi tuyến tính và phi đàn hồi để đánh giá phản ứng trực tiếp của cả hệ kết cấu, xem xét sự tương tác giữa các cấu kiện trong toàn bộ hệ kết cấu như sự suy giảm cường độ và độ cứng trong quá trình chịu tải và sự phân bố lại nội lực và có thể đánh giá được khả năng chịu tải cực hạn của kết cấu.
0	Tuy nhiên, khi gặp các vấn đề phức tạp như bài toán thiết kế thiết kế tối ưu, phân tích độ tin cậy của kết cấu hay thực hiện đánh giá sự phá hoại của kết cấu [1-3]), các phương pháp này gặp vấn đề về tốn nhiều thời gian do phải xử lý một số lượng lớn các phân tích lặp đi lặp lại để hội tụ đến các lời giải cuối cùng [4]. Trong những năm gần đây sự phát triển mạnh mẽ của ngành khoa học máy tính đã có ảnh hưởng nhiều đến việc áp dụng các phương pháp học máy (ML) vào thực hành thiết kế kết cấu khung thép nhằm đảm bảo kết cấu vẫn làm việc an toàn, hiệu quả nhưng với chi phí xây dựng, các nỗ lực tính toán là tốt nhất và rút ngắn được thời gian thực hiện phân tích.
0	Một số các ứng dụng của ML vào phân tích thiết kế khung thép có thể kể đến như Gonzalez et al. [8] đã trình bày phương pháp xác định phá hoại đối với các kết cấu khung thép chịu mô-men sử dụng mạng nơ ron chuyển tiếp (NNs) và các dạng dao động uốn đầu tiên (các tần số và dạng dạng dao động thu được bằng mô hình phần tử hữu hạn cho tòa nhà văn phòng năm tầng) làm đầu vào cho mạng. Sun và cộng sự. [9] đã khảo sát các ứng dụng ML trong thiết kế tòa nhà và kết cấu.
0	Trong xác định khả năng chịu tải cực hạn của công trình hiện nay, việc thực hiện các thuật toán ML để dự đoán khả năng chịu tải của các kết cấu kỹ thuật đã thu hút sự quan tâm ngày càng tăng của các nhà nghiên cứu. Ví dụ, độ bền của các cột CFST đã được dự đoán bằng cách sử dụng các thuật toán ML khác nhau, chẳng hạn như tăng cường độ dốc cây (GTB), học sâu (DL), SVM và tăng cường độ dốc phân loại (Catboost) [15]. Khả năng chịu tải của dầm bản được gia cường sườn đứng được dự đoán bằng cách sử dụng tăng cường độ dốc cực hạn (XGBoost), mang lại mô tả tốt hơn so với các phương trình hiện có được nêu trong tiêu chuẩn thiết kế Eurocode 3, 2006, BS, 2000. Bên cạnh đó, các thuật toán ML đã được áp dụng tốt để đánh giá tính năng kết cấu và khả năng chịu tải tối đa của các dạng công trình khác nhau (ví dụ: giàn [16-17], khung cứng [18], dầm [19], cột [20]).
0	Hiện nay cách tiếp cận ML theo cách thay thế hiệu quả cho các kỹ thuật lập mô hình cổ điển. Nó cung cấp một số lợi thế khi các vấn đề rất phức tạp liên quan đến sự không chắc chắn được xem xét. ML cũng có thể đẩy nhanh quá trình ra quyết định, giảm tỷ lệ lỗi và tăng hiệu quả tính toán. Vì những lý do này, các phương pháp ML gần đây đã thu hút sự chú ý đáng kể trong bối cảnh ngày càng nhiềuứng dụng trong kỹ thuật kết cấu. Trong nghiên cứu này, thông qua các thuật toán ML nổi tiếng gồm hồi quy tuyến tính, học sâu và rừng ngẫu nhiên, dùng kỹ thuật phân tích nâng cao để dự báo khả năng chịu tải cực hạn của khung thép với nỗ lực tính toán vừa phải. Khung thép phẳng 4 nhịp 15 tầng được khảo sát để xem xét và so sánh hiệu quả của các phương pháp. Các bộ dữ liệu được tạo thông qua việc thực hiện các phân tích phi đàn hồi phi tuyến tính. Trong bộ dữ liệu, đầu vào là các điểm hình học của tiết diện dầm và cột chữ W. Một đầu ra là hệ số tải trọng cực hạn (ULF) của kết cấu. Số lượng dữ liệu học thay đổi từ 1.000 đến 10.000 trong ba thuật toán nghiên cứu. Phần kết luận minh họa và tổng kết.hiệu suất của các phương pháp ML lựa chọn.
0	Hình thành bộ dữ liệu là một bước quan trọng cho các mô hình học máy (training models). Phải đảm bảo tính đầy đủ của dữ liệu về kích thước mẫu, đầu vào và đầu ra. Về hồi quy và phân loại khả năng chịu tải giới hạn của khung thép phi tuyến được trình bày trong nghiên cứu này, đầu vào được giới hạn ở các thuộc tính của mặt cắt ngang của dầm và cột. Tổng cộng có mười sáu đặc điểm của tiết diện hình chữ W được xem xét. Trong nghiên cứu này, do số lượng mẫu trong cơ sở dữ liệu lớn,tập dữ liệu huấn luyện và tập dữ liệu kiểm tra được phát triển từ cơ sở dữ liệu. Số lượng mẫu cho tập dữ liệu kiểm tra được cố định ở mức 5000 và số lượng mẫu cho tập dữ liệu huấn luyện được chọn là 1000, 2000, 5000 và 10000. Tất cả các mẫu trong tập dữ liệu huấn luyện và kiểm tra đều khác nhau. Các đặc điểm này là thông tin đầu vào để thực hiện phân tích nâng cao như được trình bày trong phần trước. Các bài toán hồi quy, đầu ra là ULF của kết cấu, trong khi đối với các bài toán phân loại, nếu ULF của kết cấu nhỏ hơn 1,0, đầu ra bằng không; trường hợp ngược lại nó bằng 1.0.
0	Dự đoán khả năng chịu tải cực hạn của khung thép thông qua hệ số ULF là bài toán hồi quy có giám sát do đã biết đầu vào là tiết diện có sẵn và đầu ra đã biết. Các thuật toán đề xuất để giải quyết vấn đề này có thể được phân loại thành các mô hình tuyến tính và phi tuyến. Trong các mô hình tuyến tính (chẳng hạn như LR), mối quan hệ tuyến tính giữa các biến đầu vào và biến đầu ra duy nhất được sử dụng. Ưu điểm chính của các mô hình lớp LR là tính đơn giản, nỗ lực tính toán tối thiểu và cơ sở cho các thuật toán phức tạp khác. Tuy nhiên, các mô hình hồi quy tuyến tính rất nhạy với các giá trị ngoại lệ và nhiễu. Nhiều ứng dụng thực tế hiếm khi được mô tả bằng các quan hệ dữ liệu tuyến tính và phụ thuộc. Các mô hình hồi quy phi tuyến tính (bao gồm RF và DL) tự nhiên linh hoạt để nắm bắt các mẫu phức tạp hơn và có khả năng ánh xạ các mối quan hệ phi tuyến tính của các biến đầu vào và đầu ra khác nhau. Tuy nhiên, các mô hình phi tuyến bao gồm các kết cấu phức tạp với nhiều tham số và thường đòi hỏi nhiều nỗ lực tính toán hơn so với các thuật toán hồi quy tuyến tính. Bên cạnh đó, không có thuật toán duy nhất nào có thể hoạt động tốt nhất trong mọi vấn đề. Sau đây sẽ xem xét cụ thể nội dung của ba thuật toán lựa chọn là LR, DL và RF.
0	Khung được nghiên cứu là khung thép phẳng hai chiều gồm 5 nhịp và 14 tầng, bằng thép A992 có cường độ chảy 345 MPa và mô đun đàn hồi 200.000 MPa. Theo như hình vẽ sơ đồ khung (hình 2 ) , 174 cấu kiện của khung được phân loại thành 20 nhóm thiết kế bao gồm 12 loại cấu kiện cột và 8 loại cấu kiện dầm. Tải trọng gió theo phương ngang được quy đổi thành tải trọng tập trung tại cao độ sàn với độ lớn cho trong Bảng 5. Tĩnh tải và hoạt tải phân bố đều tác dụng lên tất cả các dầm của hai tầng trên cùng như được trình bày trong Hình 2., các tổ hợp tải trọng về cường độ, sử dụng và ràng buộc lần lượt là 13, 2 và 1. Các thuật toán ML được viết bằng ngôn ngữ Python kết hợp các thư viện phần mềm nguồn mở (Tensorflow, Sklearn và Keras). Khả năng dự đoán của các thuật toán ML được nghiên cứu bằng cách dự đoán hệ số tải cực hạn của khung với tổ hợp tải 1,2DL + 1,6W + 0,5LL. Các tham số trong Bảng 1 được áp dụng cho các thuật toán ML được chọn bằng phương pháp thử và sai và được thay đổi theo cách thủ công để xác định các kết hợp tối ưu liên quan đến MSE. Các thông số khác không có trong Bảng 2 được chọn làm giá trị mặc định của chương trình. Hàm mất mát của mô hình huấn luyện là MSE.
0	Phát triển kinh tế xã hội là một trong những nhân tác thúc đẩy mạnh mẽ và nhanh chóng nhất đến thay đổi sử dụng đất có tác động đến môi trường sống và mẫu dạng cảnh quan. Để dự đoán và giảm thiểu những tác động này, các nhà quy hoạch môi trường và quản lý bảo tồn cần có các công cụ và phương pháp để có thể sử dụng ở giai đoạn lập kế hoạch ban đầu. Nghiên cứu này điều tra và lựa chọn các chỉ số độ đo cảnh quan để đánh giá tác động của dự án phát triển du lịch đến môi trường sống, sự phân mảnh và kết nối sinh thái. Khác với các nghiên cứu trước đây tập trung vào việc sử dụng các chỉ số không gian đơn lẻ để mô tả sự phân mảnh cảnh quan, nghiên cứu này đề xuất Chỉ số lượng hóa phân mảnh cảnh quan tổng hợp (OLFI) để phân tích và đánh giá tính không đồng nhất về không gian và thời gian của sự phân mảnh cảnh quan ở Vườn quốc gia (VQG) Núi Chúa - Khu dự trữ sinh quyển thế giới. Nghiên cứu sử dụng thuật toán học máy để phân loại LULC với độ chính xác tổng thể đạt 92,84% và hệ số Kappa là 0,90.
0	Sự phát triển kinh tế xã hội diễn ra mạnh mẽ ở nhiều nơi trên toàn thế giới trong những thập kỷ vừa qua đã và đang tác động tiêu cực đến môi trường, đa dạng sinh học, thay đổi lớp phủ/sử dụng đất (LULC), cơ sở hạ tầng giao thông hiện đại và phân mảnh môi trường sinh thái của khu vực [1]. Thay đổi sử dụng đất được coi là hoạt động nhân sinh quan trọng, gây tác động sâu sắc đến hệ sinh thái và cảnh quan Di sản thiên nhiên (DSTN). Các loại hình sử dụng đất là đơn vị cơ bản cấu thành nên các loại cảnh quan, được coi là điều kiện tiên quyết cho sự tồn tại hay phát triển của các mô hình cảnh quan [2].Đặc biệt, khuvực có đa dạng sinh học cao như Khu dữtrữsinh quyển thếgiới, áp lực từcác hoạt động của con người càng mạnh mẽvà rộng khắp sẽdẫn đến tình trạng không đồng nhất của cảnh quan [3].Điều này đòi hỏi phải xây dựng một quy trình đánh giá có hệthống vềtác động của sựthay đổi LULCđối với chất lượng môi trường sống thông qua phân tích sựphân mảnh cảnh quan.
0	Trong khoảng hai thập kỷ trở lại đây, việc mở rộng đất xây dựng hay sự gia tăng của bề mặt không thấm đã dẫn đến sự thay đổi thường xuyên nhiều đối tượng LULC, thành các bề mặt nhân tạo [4]. Việc đánh giá tác động của mở rộng đất xây dựng hay bề mặt không thấm đối với môi trường sống tự nhiên và các hệ sinh thái tại DSTN là vấn đề cần thiết, có ý nghĩa lý luận và thực tiễn. Số liệu độ đo cảnh quan dựa trên thông tin của các loại hình LULC cho phép lượng hóa sự phân mảnh cảnh quan, mức độ xáo trộn của mẫu dạng cảnh quan và xu hướng tác động của các dự án đối với rủi ro sinh thái cảnh quan [5]. Sự phát triển của công nghệ viễn thám được đặc trưng bởi ảnh vệ tinh với độ phân giải không gian cao và các thuật toán học máy cho phép phân loạiLULCmột cách nhanh chóng với độ chính xác cao [6].Nghiên cứu này xây dựng bản đồ phân bố không gian về phân mảnh cảnh quan sử dụng nhiều độ đo không gian, cũng như đánh giá và phân tích tính không đồng nhất của sự phân mảnh cảnh quan khi có dự án đầu tư phát triển du lịch đến DSTN.
0	Sự phát triển của công nghệ viễn thám được đặc trưng bởi ảnh vệ tinh với độ phân giải không gian cao và các thuật toán học máy cho phép phân loại LULC một cách nhanh chóng với độ chính xác cao [6]. Nghiên cứu này xây dựng bản đồ phân bố không gian về phân mảnh cảnh quan sử dụng nhiều độ đo không gian, cũng như đánh giá và phân tích tính không đồng nhất của sự phân mảnh cảnh quan khi có dự án đầu tư phát triển du lịch đến DSTN. Để đạt được mục tiêu trên, các nội dung cần thực hiện: (i) Lựa chọn các chỉ số độ đo cảnh quan để đánh giá sự phân mảnh cảnh quan và chỉ ra các vùng có nguy cơ chịu tác động của thay đổi sử dụng đất; (ii) Áp dụng các số liệu, chỉ số độ đo cảnh quan đã chọn cho một bối cảnh cụ thể tại Di sản thiên nhiên của Việt Nam đang phải đối mặt với những thách thức về phân mảnh và kết nối cảnh quan liên quan đến thay đổi sử dụng đất; (iii) Thảo luận về tác động của dự án phát triển du lịch đối với sự phân mảnh cảnh quan
0	VQG Núi Chúa được UNESCO công nhận là Khu dự trữ sinh quyển thế giới thứ 10 ở Việt Nam vào năm 2021. Khu dự trữ sinh quyển Núi Chúa được phân hóa thành: (i) Khu vực rừng tự nhiên; (ii) Khu vực ven biển và (iii) Khu vực bán sa mạc. Đây là nơi có khí hậu khắc nghiệt bậc nhất ở Việt Nam với lượng mưa thấp trung bình năm 1.238 mm và nền nhiệt độ trung bình trong năm 240C. Hệ sinh thái đa dạng thành phần loài cả trên cạn và dưới biển, hệ thực vật trên cạn với diện tích lớn rừng thường xanh cây là rộng xen kẽ cây lá kim; trong khi đó khu vực ven biển với sự đa dạng của tài nguyên biển như rạn san hô gần bờ, thảm cỏ biển và các bãi cát ven biển độc đáo là điểm đến sinh sản của nhiều loài rùa biển quý hiếm. Khu vực VQG Núi Chúa, tỉnh Ninh Thuận (Việt Nam) được chọn nghiên cứu trường hợp vì đây là một trong những khu vực có các dự án phát triển du lịch-nghỉ dưỡng-giải trí đã, đang và sẽ triển khai trong những năm tới.
0	Để xác định sự phân mảnh cảnh quan trong Khu dự trữ sinh quyển thế giới Núi Chúa, dữ liệu không gian chính được sử dụng bao gồm ảnh vệ tinh PlanetScope tổ hợp tháng 12/2022 có độ phủ mây dưới 3% được hiệu chỉnh về hệ tọa độ VN2000 múi 49N và được chuẩn hóa, xử lý lấy mẫu lại với thuật toán nội suy song tuyến về cùng độ phân giải không gian 3m nhằm chuẩn bị cho công đoạn phân loại LULC. Dữ liệu giả định khu du lịch sinh thái nghỉ dưỡng tại Bãi Hõm xã Vĩnh Hải, huyện Ninh Hải có tổng diện tích 48,7 ha được quy hoạch sử dụng đất bao gồm các khu chức năng chính như sau: Khu Biệt thự nghỉ dưỡng; Khu Bungalow; Khu Nhà nghỉ liên kế; Khách sạn; Khu bảo tồn, nghiên cứu, tham quan rùa biển và các công trình phụ trợ khác; Khu sảnh đón khách, gian hàng lưu niệm; Khu vực thể thao; Các Khu phụ trợ và nhà ở nhân viên; Khu Nhà hàng, bar rượu; Trung tâm hội nghị; Các Chòi ngắm biển; Hồ bơi và câu lạc bộ sức khỏe; Khu dã ngoại dưới tán rừng; và Khu máng trượt
0	Kỹ thuật phân loại dựa trên đối tượng địa lý (GEOBIA) cho phép xác định, đưa ra các thông tin đối tượng trên ảnh số với độ chính xác cao [7]. Dựa trên so sánh hiệu suất trong nhiều nghiên cứu [8], [9], thuật toán Random Trees đã chứng minh hiệu quả hơn các thuật toán khác trong phân loại GEOBIA về hiệu quả thời gian huấn luyện, độ chính xác phân lớp và khả năng khớp dữ liệu huấn luyện lớn khi sử dụng dữ liệu ảnh vệ tinh có độ phân giải không gian cao. Trong nghiên cứu này, thuật toán Random Trees được lựa chọn và sử dụng để phân loại LULC, với các đặc điểm và điều kiện tự nhiên của VQG Núi Chúa và vùng lân cận, nghiên cứu đã tổng hợp và phân loại thành sáu loại hình LULC chính thể hiện trong hình 3. Đánh giá độ chính xác đã được thực hiện trên phân loại cuối cùng, một bộ 700 vùng mẫu cho sáu loại hình LULC của khu vực VQG Núi Chúa với 490 vùng mẫu (70%) sử dụng cho đào tạo và 210 vùng mẫu (30%) được sử dụng cho kiểm chứng kết quả phân loại. Độ chính xác của kết quả phân loại được xác định thông qua (i) Độ chính xác tổng thể (OA) và (ii) Hệ số Kappa [8]
0	Số liệu về cảnh quan là các chỉ số định lượng chứa thông tin về mẫu dạng cảnh quan tập trung, đo lường mô hình không gian của cảnh quan về cả thành phần và cấu trúc [2]. Dựa trên các tài liệu trước đây, nghiên cứu đã phân tích và tổng hợp thành 4 nhóm yếu tố (A, B, C, D) để lượng hóa sự phân mảnh cảnh quan trong biến đổi sử dụng đất đối với các khu bảo tồn thiên nhiên hay VQG. Các số liệu về độ đo được lựa chọn đặc trưng và biểu thị chức năng định lượng, cô đọng cao thông tin về mẫu dạng cảnh quan và có tính phản ánh các đặc điểm bởi thành phần và cấu trúc không gian của các mẫu dạng cảnh quan điển hình từ kích thước mảnh rời rạc, hình dạng mảnh rời rạc, mật độ mảnh rời rạc, loại mảnh rời rạc, cụm mảnh rời rạc và khoảng cách mảnh rời rạc để thể hiện toàn diện mức độ phân mảnh cảnh quan. Nhóm A (Độ đo diện tích - Area metrics) thể hiện mức độ ảnh hưởng đến sinh khối, năng suất, dự trữ các chất dinh dưỡng, sự di chuyển của các loài nội sinh và số lượng các loài ngoại lai trên một đơn vị diện tích.
0	Đặc biệt, mảnh rời rạc tự nhiên có diện tích lớn đóng vai trò quan trọng trong việc duy trì sự ổn định của cảnh quan; Nhóm B (Độ đo mật độ - Density metrics) biểu thị mức độ ảnh hưởng đến tốc độ di chuyển của các loài, chất và năng lượng trong cảnh quan. Các thành phần cảnh quan tác động đến di chuyển cảnh quan khác nhau; Nhóm C (Độ đo hình dạng - Shape metrics) thể hiện mức độ ảnh hưởng đến hình dạng, sự mở rộng, co rút và di cư của loài, gây ra tác động đến chất lượng môi trường sống, đa dạng sinh học và ổn định cảnh quan; Nhóm D (Độ đo khả năng kết nối - Connectivity metrics) thể hiện mức độ ảnh hưởng đến sự di cư, mở rộng và nhân giống của các loài. Trong nghiên cứu này, phương pháp tích hợp Fuzzy-AHP được sử dụng để xác định trọng số của bốn nhóm yếu tố (A, B, C và D) và các tiêu chí phụ. Các bước chính của phương pháp Fuzzy-AHP bao gồm xác định các chỉ số phân mảnhcảnh quan, thiết lập các số liệu so sánh theo cặp để lấy trọng sốtiêu chí, tính toán chỉ số nhất quán (CR) của các số liệu so sánh và tính toán các trọng số và thứ hạng tiêu chí tương ứng. Nếu CR≤0,1 thì mức độ nhất quán là phù hợp và nếu CR>0,1 điều này thể hiện mức độ không nhất quán của phán đoán. Các chuyên gia được yêu cầu xem xét và sửa đổi các giá trị ban đầu trong các so sánh theo cặp trong quá trình khảo sát. Chỉ sốlượng hóaphân mảnh cảnh quan tổng thể(OLFI) được xây dựngdựa trên các độđo không gian đã được lựa chọn.
0	Hình 3 thể hiện kết quả phân loại khu vực VQG Núi Chúa và vùng lân cận với kết quả độ chính xác tổng thể (OA) đạt 92,84% và hệ số Kappa là 0,90. Lớp phủ/sử dụng đất là một trong những thành phần cảnh quan đóng vai trò quan trọng trong việc duy trì sự đa dạng sinh học, cung cấp các dịch vụ môi trường và tạo ra một môi trường sống thích hợp cho các hệ sinh thái và kiểm soát biến đổi khí hậu. Các yếu tố chuẩn hóa và trọng số tương ứng của các độ đo không gian được tính toán cho các nhóm mục tiêu và các thành phần phụ của các nhóm mục tiêu (bảng 2). Các nhóm mục tiêu (A, B, C và D) có các trọng số lần lượt được xác định là 0,212 (A); 0,389 (B); 0,284 (C), và 0,115 (D) với CI (0,023) và CR (0,026). Các tiêu chí phụ của mỗi nhóm mục tiêu có các trọng số, đối với nhóm A các hệ số của CI, CR lần lượt là 0,021, 0,034; nhóm B có hệ số CI (0,022) và CR (0,018); nhóm C có các hệ số CI (0,019) và CR (0,024) và đối với nhóm D có hệ số CI (0,011) và CR (0,010). Trong đó, các hệ số CR của tất cả nhóm mục tiêu và tiêu chí phụ đều nhỏ hơn 10% (thỏa mãn điều kiện).
0	Tính phân mảnh cảnh quan theo không gian của các nhóm A, B, C và D được thể hiện trong hình 4 với hai giai đoạn năm 2022 và Quy hoạch (QH). Đối với nhóm A, sự phân bố không gian cho thấy cấu trúc của khu vực bị phân tán, mức độ cách ly của mảnh rời rạc tăng lên. Nhóm B, phân bố không gian đã thể hiện mức độ phân mảnh cảnh quan trong khu vực và ảnh hưởng đến cảnh quan sinh thái khác. Nhóm C, các mảnh rời rạc trong cảnh quan bị tách rời, giảm độ kết tụ và độ lan truyền. Nhóm D, sự phân bố không gian cho thấy khi có các dự án phát triển du lịch thì các mảnh cảnh quan cùng loại có mức độ phân tán hay phân mảnh cảnh quan cao hơn. Ngoài ra, tại khu vực nghiên cứu cho thấy rằng sự phát triển kinh tế nhanh xung quanh VQG Núi Chúa là một trong những nhân tố chính ảnh hưởng trực tiếp đến việc thay đổi mẫu dạng cảnh quan, làm tăng tính phân mảnh và giảm tính tổng hợp.
0	Mức độ phân mảnh cảnh quan tại VQG Núi Chúa và vùng phụ cận được phân thành 5 cấp với các mức là Rất cao (50-62), Cao (45-50), Vừa phải (35-45), Thấp (33-35) và Rất thấp (27-33). Phân tích tác động của dự án du lịch đối với VQG Núi Chúa được thực hiện bằng giá trị của OLFI. Sự phân bố không gian của khu vực bị tác động bởi hoạt động xây dựng phù hợp với không gian phân phối của bốn nhóm định lượng cảnh quan. Bảng 3 cho thấy có sự thay đổi chưa rõ rệt trên toàn bộ diện tích VQG Núi Chúa nhưng mức độ tác động phân mảnh cảnh quan đã cho thấy có sự tăng về diện tích khu vực sau Quy hoạch của vùng bị tác động từ mức độ “rất thấp” lên mức độ “thấp” xấp xỉ là 0,1; từ mức độ “thấp” lên mức độ “vừa phải” với tỷ lệ xấp xỉ 0,17. Sau khi có Quy hoạch, mặc dù không nhiều nhưng khu vực bị tác động của dự án phát triển du lịch, thương mại nhưng kết quả cũng thể hiện được mức độ phân mảnh mở rộng hơn. Rõ ràng nhận thấy khu vực chịu tác động chủ yếu nằm ở khu vực giáp biển gần với khu bãi sinh sản của rùa (hình 5).
0	Việc mở rộng diện tích đất xây dựng trong VQG Núi Chúa để phát triển du lịch đe dọa trực tiếp đến các khu vực nhạy cảm, phân mảnh cảnh quan và làm suy giảm đa dạng sinh học. Thông qua việc vận chuyển vật liệu hoặc thiết bị xây dựng có thể du nhập các loài xâm lấn, các loài không phải bản địa làm phá vỡ cấu trúc của hệ sinh thái. Các dự án xây dựng cũng có thể phân mảnh môi trường sống thành các mảng nhỏ hơn, biệt lập, gây ra tiếng ồn, ô nhiễm ánh sáng và xáo trộn vật lý khiến các loài khó di chuyển trong các môi trường sống, khó tiếp cận thức ăn và nước uống cũng như tìm bạn tình và sinh sản đặc biệt là loài rùa biển ở bãi Thịt - Ninh Hải. Điều này có thể dẫn đến giảm khả năng phục hồi đối với các tác nhân gây ô nhiễm môi trường và biến đổi khí hậu. Trong tương lai, có nhiều dựán đầu tư phát triển xây dựng vào khu bảo tồn thì việc làm xáo trộn LULCsẽdẫn đến gia tăng xói mòn và dòng chảy, điều này có thểtác động tiêu cực đến chất lượng nước và hệsinh thái thủy sinh, đặc biệt là với cảnh quan biển.
0	Điều quan trọng là phải đánh giá cẩn thận và giảm thiểu tác động tiềm ẩn. Đặc biệt đối với khu vực có cảnh quan Di sản thiên nhiên đa dạng, phong phú như VQG Núi Chúa cần được xem xét và quản lý cẩn thận thông qua các biện pháp lập kế hoạch, thiết kế và giám sát hiệu quả, có thể giúp giảm thiểu tác động tiêu cực của dự án phát triển du lịch đến giá trị thẩm mỹ, đa dạng sinh học và cảnh quan của VQG. Phát triển kinh tế và bảo tồn hệ sinh thái là hai mục tiêu quan trọng nhất cần đạt được trong các kế hoạch chuyển đổi sinh thái - kinh tế xã hội theo hướng bền vững. Trong nghiên cứu này, căn cứ vào đặc điểm biến đổi và ý nghĩa sinh thái của các thước đo mô hình cảnh quan từ nhiều nghiên cứu trước, 4 nhóm chỉ số A, B, C, D được lựa chọn tương ứng để biểu thị Độ đo diện tích, Độ đo mật độ, Độ đo hình dạng, Độ đo khả năng kết nối, sau đó OLFI được tính theo Công thức (1). Qua phân tích, tác động khi thực hiện dự án vào cảnh quan khu dự trữ sinh quyển Núi Chúa là chưa lớn, nhưng sự phân mảnh đã thể hiện rõ mức độ thay đổi. Việc sử dụng các thước đo cảnh quan và viễn thám kết hợp với mô hình học máy trong đánh giá tác động của dự án phát triển du lịch đến cảnh quan VQG Núi Chúa đảm bảo độ tin cậy, có thể áp dụng cho các di sản thiên nhiên khác. Các kết quả nghiên cứu đưa ra chỉ báo về tác động của dự án, đồng thời góp phần thiết thực trong việc bảo tồn đa dạng sinh học và quản lý cảnh quan di sản thiên nhiên.
0	Việc nhận diện và phân loại phương tiện giao thông có ý nghĩa quan trong quy hoạch đô thị và quản lý giao thông, ngày càng được nghiên cứu sâu trên thế giới. Giao thông tại Việt Nam có mật độ lưu thông lớn và phương tiện chủ yếu là xe máy nên vấn đề nhận diện phương tiện với độ chính xác cao càng trở nên phức tạp. Bài báo này nghiên cứu phát triển thuật toán mới cho phép nhận diện và phân loại phương tiện trong luồng video trực tiếp, thuật toán được đề xuất sử dụng mạng nơron triển khai trên thuật toán Yolo và Sort ứng dụng trong theo dõi đối tượng. Nghiên cứu được thử nghiệm trên các tệp video trực tiếp của camera giám sát, kết quả thử nghiệm cho thấy thuật toán đề xuất hoạt động ổn định và hiệu quả với độ chính xác cao khoảng 95% với oto và trên 80% với xe máy, kết quả rất tích cực so với các phương pháp khác
0	Tắc nghẽn giao thông đã gia tăng trên toàn cầu trong thập kỷ qua, tình trạng tắc nghẽn giao thông là vấn đề rất trầm trọng [1]. Tắc nghẽn giao thông làm lãng phí thời gian, tiêu hao nhiêu liệu và ô nhiễm môi trường [2]. Theo khảo sát năm 2020 thì chi phí do tắc nghẽn gây ra ở các thành phố của Mỹ là hàng tỉ đôla mỗi năm [3]. Ở Việt Nam, theo tính toán, mỗi năm thành phố Hồ Chí Minh thiệt hại khoảng 1,2 triệu giờ công lao động, 1,3 tỉ USD/năm do ùn tắc giao thông và 2,3 tỉ USD do ô nhiễm môi trường từ các phương tiện cơ giới. Đánh giá từ Viện Chiến lược và phát triển Giao thông vận tải cũng cho biết, ùn tắc gây thiệt hại cho thành phố Hà Nội mỗi năm khoảng 1 - 1,2 tỉ USD [4]. Trước những ảnh hưởng nghiêm trọng của tắc nghẽn giao thông, ngày càng nhiều các biện pháp được nghiên cứu và áp dụng, việc ứng dụng khoa học máy tính càng được chú ý nhiều hơn. Các nhà nghiên cứu tại Phòng thí nghiệm Oak Ridge đã sử dụng trí tuệ nhân tạo (AI) và máy học (Machine Learing) để thiết kế hệ thống thị giác máy tính thu thập và xử lý dữ liệu nhận được từ các camera giao thông giúp tránh xung đột tại các giao lộ, đồng thời giảm thiểu tổng lượng tiêu hao nhiên liệu [5]
0	Trong bài báo này, nhóm nghiên cứu sẽ ứng dụng thị giác máy tính trong việc nhận diện và phân loại phương tiện giao thông tại Việt Nam, từ đó xác định số lượng phương tiện lưu thông trên đường và tính toán mật độ lưu lượng phương tiện tham gia giao thông trong một khoảng thời gian xác định. Kết quả thực nghiệm của bài báo sẽ được so sánh với các phương pháp nghiên cứu khác để đánh giá hiệu quả của thuật toán cũng như khả năng ứng dụng thực tế. Yolo là một mô hình mạng neural tích chập (CNN) dùng cho việc phát hiện, nhận dạng, phân loại đối tượng. Yolo được tạo ra từ việc kết hợp giữa các lớp phức tạp (convolutional layers) cho phép trích xuất ra các đặc tính của ảnh và lớp kết nối (connected layers) dự đoán ra xác suất đó và tọa độ của đối tượng [6]. Yolo phân chia hình ảnh thành một mạng lưới 7x7 ô (grid_size=7x7). Từ đó sẽ dự đoán xem trong mỗi ô liệu có đối tượng (object) mà điểm trung tâm rơi vào ô đó không, dự đoán điểm trung tâm, kích thước của đối tượng và xác xuất là đối tượng nào trong số các đối tượng cần xác định. Mỗi ô này có trách nhiệm dự đoán hai hộp (boxes_number=2) bao quanh, mỗi một hộp mô tả hình chữ nhật bao quanh một đối tượng. Hiện nay phiên bản đang được sử dụng là thế hệ thứ 4, gọi là Yolov4. Hình 2.1 mô tả nguyên lý hình ảnh chia ô xác định đối tượng của thuật toán Yolo.Kiến trúc của Yolov4 phù hợp với các bài toán xác định đối tượng trong đó không yêu cầu phần cứng thiết bị phải có khả năng tính toán mạnh [7]
0	Sort là sự phát triển của khung theo dõi nhiều đối tượng trực quan dựa trên các kỹ thuật ước lượng trạng thái và liên kết dữ liệu thô. Sort là một thuật toán thuộc dạng theo dõi và phát hiện (Tracking-by-detection), được thiết kế cho các ứng dụng theo dõi thời gian thực và phương pháp này tạo ra nhận dạng đối tượng một cách nhanh chóng [8]. Một đặc điểm của lớp các thuật toán Tracking-bydetection là tách đối tượng cần xác định ra như một bài toán riêng biệt và cố gắng tối ưu kết quả trong bài toán này. Công việc sau đó là tìm cách liên kết các hộp giới hạn thu được ở mỗi khung và gán ID cho từng đối tượng. Do đó, thu được một khung quá trình xử lí như sau [9]: o Detect: phát hiện vị trí các đối tượng trong frame o Predict: Dự đoán vị trí mới của các đối tượng dựa vào các frame trước đó o Associate: Liên kết các vị trí detected với các vị trí dự đoán được để gán ID tương ứng. Trên hình 2.2. có thể thấy được tốc độ theo dõi đối tượng và độ chính xác trong thời gian thực của thuật toán Sort vượt trội so với các thuật toán khác. Do vậy, nhóm nghiên cứu lựa chọn thuật toán Sort để theo dõi các phương tiện giao thông, phục vụ quá trình phân loại.
0	Nghiên cứu sẽ dựa trên thuật toán xác định vật thể của Yolo, thuật toán theo dõi vật thể của SORT, từ đó gán địa chỉ nhận dạng ID cho từng phương tiện lưu thông và phân loại chúng, xác định số lượng xe theo từng khoảng thời gian cụ thể. Các phương tiện được gán địa chỉ ở đây là: xe ô tô, xe tải, xe buýt, xe máy và xe đạp. Lưu đồ thuật toán nhận dạng và phân loại phương tiện lưu thông được trình bày trong hình 3.1. Sau quá trình khởi tạo thư viện, khởi tạo các biến và chạy mô hình thuật toán Yolov4, tiến hành phát luồng video trực tiếp từ camera hoặc lựa chọn các tệp video. Từng khung hình sẽ được chụp và kiểm tra theo vòng lặp while, nếu khung đọc được không chính xác, vòng lặp sẽ bị phá vỡ. Do góc nhìn camera ở mỗi đoạn đường được thiết lập là khác nhau nên để dễ dàng cho việc thử nghiệm nhóm nghiên cứu đã tạo ra các điểm chọn thủ công để thiết lập vùng nhận diện nhất định trên toàn bộ khung hình. Việc chỉ xử lý một vùng lựa chọn nhất định cũng giúp cho thuật toán tối ưu hơn, loại bỏ các vùng không chứa phương tiện lưu thông. Thiết lập vùng nhận diện được thể hiện trên hình 3.2.
0	Các phương tiện lúc này đã được gán ID riêng và cố định, nhóm nghiên cứu tiến hành kiểm tra phương tiện qua 5 khung hình liên tiếp. Quá trình được lặp lại cho tới khi kết quả trả về là đúng thì ID của phương tiện sẽ được thêm vào danh mục kết quả. Vì thuật toán Sort chỉ định một ID duy nhất bằng cách lấy thông tin hộp giới hạn làm đầu vào, tức là các nhãn lớp được tạo ra bởi phát hiện đối tượng. Để khắc phục hạn chế này, một danh mục được tạo ra bằng cách duy trì ánh xạ các ID tạo bởi Sort tới các nhãn lớp tương ứng từ Yolov4. Một danh sách các ID duy nhất hiện đang được sử dụng và duy trì. Đối với mỗi khung sẽ tìm ra ID nào đã bị loại bỏ giữa các khung liên tiếp bằng cách so sánh danh sách ID duy nhất hiện tại và trước đó. Sau đó, các ID được tìm thấy ngoài danh sách sẽ được sử dụng để xóa các khóa tương ứng khỏi danh mục kết quả vì chúng không còn phù hợp nữa. Từ đó xác định được số lượng từng loại xe trong khoảng thời gian cụ thể tương với với ID trong danh mục kết quả và nhãn lớp trong danh mục đối tượng. Quá trình được lặp lại cho tới khi chương trình kết thúc.
0	Bài báo hướng đến việc xây dựng thuật toán nhận diện và phân loại các phương tiện lưu thông tại Việt Nam dựa vào mô hình Yolov4 và thuật toán Sort. Quá trình nghiên cứu và thực nghiệm với video thực tế trên các đoạn đường tại các thành phố lớn của Việt Nam như Hà Nội, Hải Phòng, nhóm nghiên cứu nhận thấy khi mật độ lưu thông thấp, thuật toán cho kết quả phân loại và kiểm đếm tương đối chính xác. Với mật độ lưu thông trung bình và cao, kết quả bắt đầu có độ chênh lệnh và mất ổn định hơn ở loại phương tiện là xe đạp và xe máy. Thực nghiệm nhận được kết quả tốt hơn so với một số thuật toán nghiên cứu khác [12,13]. Độ chính xác của nghiên cứu đạt được với ô tô, xe tải và xe buýt là 93.11% trong điều kiện mật độ giao thông cao, các thuật toán nghiên cứu khác đạt được độ chính xác từ 90-92%. Đặc biệt với phân loại xe máy và xe đạp, các nghiên cứu khác chưa đề cập nhiều và chưa có kết quả cụ thể. Để có một kết quả cho độ chính xác cao, nhóm nghiên cứu sẽ tiến hành tập huấn mô hình Yolov4 với lượng dữ liệu đầu vào của xe máy và xe đạp lớn hơn. Với tốc độ xử lý, model Yolo và thuật toán Sort cho tốc độ thời gian thực rất nhanh và ổn định với cấu hình máy chủ phù hợp (Card màn hình 1050Ti hoặc 2060Ti)
0	Kỹ thuật đo lường giáo dục học kết hợp với công nghệ thông tin đang góp phần quan trọng đảm bảo chất lượng đào tạo nói chung và chất lượng khảo thí nói riêng, đặc biệt là trong việc nghiên cứu, phát triển và triển khai vận hành các hệ thống kiểm tra trắc nghiệm [1]. Tại Đại học Y Dược Thành phố Hồ Chí Minh, hình thức trắc nghiệm truyền thống đã được chuyển đổi sang trắc nghiệm trên máy tính (CBT), trong đó trắc nghiệm thích nghi (CAT) đã được triển khai thử nghiệm nhiều năm trên đối tượng sinh viên đại học với hệ thống phần mềm mạng UMPCAT. Trắc nghiệm thích nghi (CAT) là một dạng CBT được xây dựng dựa trên nguyên tắc điều chỉnh linh hoạt độ khó của các câu hỏi theo thời gian thực dựa trên kết quả trả lời của từng thí sinh để cá nhân hóa việc tạo lập, cấp phát và quản lý các bài kiểm tra đánh giá kết quả học tập. CAT phân phát cho mỗi thí sinh một bộ câu hỏi riêng phù hợp với mức trình độ kiến thức. Khi hai thí sinh có cùng số câu trả lời đúng thì thí sinh trả lời đúng nhiều câu khó hơn có đánh giá cao hơn [2,3]. Trắc nghiệm thích nghi bắt đầu từ một câu hỏi có độ khó trung bình sau đó dựa trên kết quả trả lời câu hỏi này để ước lượng của thí sinh, điều chỉnh độ khó của câu hỏi tiếp và lựa chọn từ ngân hàng câu hỏi. Quá trình được tiếp tục lặp lại, mức năng lực người học được ước lượng cập nhật sau mỗi bước, quá trình dừng lại khi sai số ước lượng đủ mức chính xác cần thiết hoặc người học trả lời hết số câu hỏi giới hạn [4].
0	CAT được nghiên cứu từ 1970 và được sử dụng lần đầu năm 1985, trở nên phổ biến từ những năm 1990 trong nhiều lĩnh vực như giáo dục, tuyển dụng, đánh giá tâm lý và sức khỏe [5-7], có đủ tính năng giúp giải quyết các khuyết điểm của trắc nghiệm cố định [8,9]. Tại Việt Nam, CAT được triển khai ứng dụng tại một số cơ sở từ những năm 2000 và được ghi nhận là một phương pháp đánh giá hữu hiệu [10,11]. Tuy nhiên, việc tổ chức thực hiện trên thực tế đòi hỏi sự đầu tư nguồn lực và công nghệ lâu dài để có thể phát triển và khai thác hiệu quả [12]. Ngân hàng câu hỏi và thuật toán trắc nghiệm thích nghi là hai thành phần quan trọng nhất, cần được xây dựng và phát triển đồng bộ để hoạt động tương hỗ chặt chẽ để hệ thống CAT vận hành lượng giá hiệu quả, cá nhân hóa và công bằng. Chất lượng đo lường của CAT phụ thuộc vào chất lượng của tập hợp các câu hỏi trắc nghiệm đã được hiệu chuẩn được đưa vào thực hiện lượng giá. Mô hình Rasch giúp việc đo lường thích nghi vững bền và linh hoạt hơn [13]. Mức độ đạt kết quả học của người học có thể được so sánh với nhau ngay cả khi mỗi người học nhận được một bộ câu hỏi duy nhất, thông qua phương pháp cân bằng đề thi để liên kết các bộ câu hỏi và hiệu chuẩn cân bằng số đo logit của các câu hỏi trên một thang đo duy nhất [14].
0	UMPIteamBank là một thành phần của hệ thống UMPCAT, được thiết kế như một ngân hàng câu hỏi để hỗ trợ lượng giá trắc nghiệm thích nghi. Các chức năng chính của UMPItemBank gồm có: lưu trữ và quản lý câu hỏi trắc nghiệm theo các tiêu chí tổ chức và đo lường khác nhau; hỗ trợ người dùng xác định nội dung, chuẩn đầu ra học phần và tạo ma trận liên kết giữa nội dung và chuẩn đầu ra; hỗ trợ tạo đề thi trắc nghiệm tự động hoặc thủ công từ ngân hàng câu hỏi phù hợp với mục tiêu đánh giá và tùy chọn của người dùng; phân tích thống kê đánh giá chất lượng câu hỏi và hiệu quả của đề thi. Nghiên cứu này tập trung đánh giá hiệu quả ngân hàng câu hỏi UMPItemBank, từ quy trình kiến tạo nội dung, kiến trúc hệ thống, nguyên tắc hoạt động đến hiệu quả vận hành thực tế thông qua trường hợp ứng dụng UMPCAT đánh giá năng lực đầu vào học phần Tin Học Ứng Dụng, nhằm cải tiến hệ thống trắc nghiệm thích nghi UMPCAT.
0	Phần mềm ngân hàng câu hỏi UMPItemBank, cấu trúc và khả năng đáp ứng vận hành hệ thống phần mềm trắc nghiệm thích nghi UMPCAT. Bao gồm các đơn nguyên: trình quản lý ngân hàng câu hỏi, thư viện thông tin câu hỏi, điều hướng và liên kết nội tuyến. Cấu hình máy tính chạy UMPCAT gồm CPU Intel Core i5/i7, AMD Ryzen 5/7; RAM 16GB; hệ điều hành Window 11, dung lượng bộ nhớ SSD trống 250 GB, trình duyệt Chrome. Lý thuyết học tập được sử dụng là thuyết kiến tạo, kiến thức mục tiêu là kiến thức cấu trúc về mạng lưới quan hệ giữa các khái niệm lý thuyết và thao tác trên máy tính. Quy trình biên soạn câu hỏi gồm 05 giai đoạn: định nghĩa mục tiêu học tập, mô tả đặc trưng nội dung học tập, biên soạn câu hỏi, rà soát câu hỏi, tổ chức và vận hành đề thi, đánh giá lại độ phù hợp của câu hỏi và mục tiêu học tập. Bộ câu hỏi sau biên soạn được tổ chức phân loại và lưu trữ theo thư viện, chỉ được đưa vào ngân hàng câu hỏi sau khi được chuẩn hóa tham số theo mô hình Rasch và IRT bằng mô phỏng kết hợp kiểm thử thực địa. Sau khi hình thành nội dung, ngân hàng câu hỏi được kết nối thuật toán thích nghi để thử nghiệm chức năng. Kết quả thử nghiệm được phân tích lại để xác định đặc trưng câu hỏi phù hợp với mô hình đo lường và những điểm cần cải tiến. Quy trình biên soạn câu hỏi được thực hiện định kỳ sau mỗi học kỳ hoặc sau mỗi kỳ thi tùy mục đích sử dụng.
0	Cấu trúc hệ thống đã được triển khai trên nền tảng web, được phát triển bằng ngôn ngữ lập trình PHP với nhiều ưu điểm như chạy nhẹ, linh hoạt và đa nền tảng, có thể được sử dụng cho nhiều hệ điều hành khác nhau. Trường hợp nghiên cứu là ngân hàng câu hỏi học phần Tin Học Ứng Dụng. Sau khi được đánh giá cấu trúc và nội dung, thử nghiệm chức năng được thực hiện trên máy tính, thuật toán chọn lựa câu hỏi được sử dụng với các tham số đề thi số câu hỏi là 45, sai số chuẩn SE của ước lượng năng lực thí sinh là 0,3 logit, thời gian làm bài 60 phút. Các tham số được xác định dựa vào kết quả mô phỏng năng lực thí sinh với phân phối chuẩn trong khoảng -3,0 – 3,0 logit, thời gian làm bài được ước tính dựa vào kinh nghiệm của giảng viên phụ trách học phần. Đánh giá quá trình theo Tessmer M (1993) gồm 4 giai đoạn [15]: đánh giá chuyên gia, đánh giá cá nhân, đánh giá nhóm nhỏ, đánh giá thực nghiệm thực địa. Ngân hàng câu hỏi được đánh giá cấu trúc và đánh giá chức năng thông qua vận hành trong hệ thống UMPCAT. Trường hợp nghiên cứu được thực hiện với ngân hàng câu hỏi của học phần Tin Học Ứng Dụng
0	Đánh giá chuyên gia được thực hiện chuyên gia đo lường giáo dục học và giáo dục học, chuyên gia nội dung lĩnh vực, chuyên gia đảm bảo chất lượng phần mềm. Chuyên gia hiểu rõ về bản chất và mức độ tác động của nghiên cứu, tự nguyện tham gia và nhận được đầy đủ thông tin về nghiên cứu, quyền của người tham gia và bảo mật thông tin cá nhân. Nhóm chuyên gia thực hiện thiết kế mô phỏng kiểm thử, xác định tải trọng người dùng và nút nghẽn luồng, theo dõi quá trình và kết quả kiểm tra thực địa, đánh giá quy trình phát triển, cấu hình và phiên bản, giao diện người dùng, kiểm tra thâm nhập và lỗi xác thực danh tính. Chuyên gia đề ra khuyến nghị cải thiện toàn diện hệ thống UMPCAT trong đó có quản lý và truy xuất cơ sở dữ liệu UMPItemBank. Phương pháp phỏng vấn bán cấu trúc được sử dụng khi tiếp xúc với chuyên gia. - Đánh giá cá nhân kết hợp nhóm nhỏ được thực hiện với nhóm sinh viên tự nguyện. Sinh viên tham gia đánh giá hiểu rõ về bản chất và mức độ tác động của nghiên cứu, tự nguyện tham gia và nhận được đầy đủ thông tin về nghiên cứu, quyền của người tham gia và bảo mật thông tin cá nhân. Ngân hàng câu hỏi Tin Học Ứng Dụng được sử dụng.
0	Sau khi được trải nghiệm cá nhân, sinh viên được làm thử một bài kiểm tra thích nghi. Mỗi sinh viên mô tả trải nghiệm của mình, chia sẻ quan điểm cá nhân về ưu điểm khuyết điểm của các câu hỏi được sử dụng. Sau khi có kết quả đánh giá, sinh viên được phỏng vấn bán cấu trúc về trải nghiệm sử dụng, bao gồm cảm nhận chung về đề kiểm tra, đặc trưng các câu hỏi được sử dụng gồm cảm nhận chủ quan về độ phức tạp và tính liên quan với nội dung học tập, tính hợp lý của chuỗi câu hỏi, tính rõ ràng mạch lạc, tính thực tiễn, tính sáng tạo mức độ thách thức, mức tư duy cần thiết, mức độ căng thẳng. Sinh viên cũng được khuyến khích đưa ra ý kiến nên phát triển bộ câu hỏi theo hướng nào. - Đánh giá thực nghiệm thực địa được thực hiện trong môi trường thực tế giảng dạy, dựa trên đánh giá mức kiến thức đầu vào của sinh viên đăng ký học phần Tin Học Ứng Dụng. Quy trình gồm 6 bước: (1) nhập câu hỏi trắc nghiệm vào phần mềm; (2) xác định tham số thích nghi của đề kiểm tra bao gồm số câu hỏi giới hạn, thời gian làm bài giới hạn, độ khó câu hỏi khởi đầu; (3) khởi động thuật toán lựa chọn câu hỏi thích nghi của phần mềm UMPCAT; (4) quản lý vận hành phần mềm trong thời gian trắc nghiệm; (5) rà soát các trường hợp quá hạn tham số đề; (6) thu thập và phân tích số liệu.
0	Về vận hành, phần mềm chạy trên nền tảng mạng, gọn nhẹ, dễ sử dụng và quản lý. Về giao diện người dùng, bố cục trực quan đơn giản, các yếu tố quan trọng được sắp xếp dễ sử dụng và dễ theo dõi, giao diện có phản hồi nhanh và dễ nhận biết. Thiết kế giao diện phù hợp với mục đích sử dụng phần mềm và nhu cầu khảo thí. Tuy nhiên cần chú ý yếu tố thẩm mỹ và sử dụng màu sắc thu hút thị giác của người dùng. Cấu trúc giao diện cố định, chưa có chức năng tùy chỉnh, do đó tính tùy biến đáp ứng người dùng chưa cao. Về hiệu suất, phần mềm có khả năng chịu tải lượng người dùng ổn định, tuy nhiên có khoảng cách truy xuất không đều, một số trường hợp có mức kiến thức xấp xỉ điểm cực đại thông tin nhưng thời gian ước lượng gần mức cao nhất. Điều cần thiết là cải thiện cả thuật toán chọn câu hỏi, thuật toán ước lượng và truy vấn cơ sở dữ liệu. Về tính năng, phần mềm đáp ứng được các kịch bản sử dụng khác nhau, có tương thích với thuật toán lựa chọn câu hỏi trong hệ thống UMPCAT, kết quả ước lượng mức kiến thức của sinh viên đủ chính xác. Độ chính xác tính toán 15 chữ số thập phân, báo cáo kết quả với 5 chữ số thập phân. Thời gian truy xuất cơ sở dữ liệu xấp xỉ 0,02 giây mỗi câu. Kịch bản tối ưu là 120 câu hỏi và 300 thí sinh, thời gian phân tích kết quả khoảng 19-28 giây theo CTT và khoảng 95-120 giây theo Rasch.
0	Tuy nhiên, với ngưỡng sai số chuẩn SE 0,3 hàm thông tin câu hỏi không đạt đỉnh tại mức trình độ thí sinh, cho thấy ước lượng trình độ thí sinh chưa được đảm bảo mức sai số tối ưu. Với ngưỡng SE 0,2 có thể cải thiện vấn đề, tuy nhiên số câu hỏi kiểm tra sẽ tăng lên, đặt ra vấn đề về kích cỡ ngân hàng câu hỏi khi có số lượng thí sinh trên 1000. Sau khi phát hiện câu hỏi có tần suất chọn lặp cao, cần có biện pháp nhận diện và ngăn chặn lựa chọn lặp lại, giúp đảm bảo tính bảo mật của ngân hàng câu hỏi. Đồng thời, cần chú ý phân tích độ sai biệt vận hành của câu hỏi hoặc nhóm câu hỏi đối với các nhóm thí sinh khác nhau để đảm bảo tính công bằng khảo thí. Trong quá trình kiến tạo nội dung ngân hàng câu hỏi, cần có hệ số định lượng độ phù hợp của câu hỏi với mục tiêu để hỗ trợ người biên soạn. Chức năng phân tích và báo cáo kết quả chi tiết về mức năng lực ước tính, độ khó câu hỏi, độ phân cách, hàm thông tin câu hỏi, hàm sai số ước lượng, trực quan hóa dữ liệu. Tuy nhiên hỗ trợ xuất dữ liệu ở các định dạng khác nhau chưa được thiết kế. Về tổng thể, UMPItemBank vận hành đúng chức năng được thiết kế. Dạng câu hỏi trắc nghiệm đồng nhất không yêu cầu thao tác phản hồi phức tạp. Phần mềm có tiềm năng ứng dụng rất cao trong cả quản lý khảo thí và đánh giá thành quả học tập theo trắc nghiệm thích nghi. Tuy nhiên để bảo đảm UMPItemBank vận hành tốt trong môi trường đánh giá thích nghi của UMPCAT cần chú ý kiểm soát khả năng phơi lộ, ngưỡng sai số chuẩn và kích cỡ ngân hàng.
0	Để nâng cao tính cá nhân hóa, cải thiện hiệu suất với số liệu lớn, và đặc biệt là hỗ trợ biên soạn câu hỏi mới cũng như chỉnh đốn các câu hỏi chưa đạt tính giá trị nội dung hoặc đo lường, nhóm phát triển phần mềm nên xem xét việc tích hợp các tính năng học máy vào UMPItemBank. Trong nghiên cứu này, đánh giá cá nhân được kết hợp với đánh giá nhóm nhỏ. Về ưu điểm, sinh viên tham gia đánh giá hài lòng với mức độ thực tế, rõ ràng, chính xác và cụ thể của câu hỏi, mức độ phức tạp và tính thách thức không quá cao nhưng cũng không hoàn toàn đơn giản, thời gian hiển thị câu hỏi khá nhanh và không gián đoạn, độ dài bài kiểm tra và thời gian làm bài đều giảm đi rõ rệt. Về những hạn chế, sinh viên cho biết giao diện người dùng thiếu yếu tố thẩm mỹ đồ họa. Tính tương tác với thí sinh chưa cao, đánh giá kết quả trả lời mỗi câu hỏi cũng như nội dung câu hỏi tiếp theo không hiển thị ngay sau khi thí sinh đã hoàn thành trả lời mỗi câu hỏi. Các câu trong chuỗi câu hỏi có mối liên quan khá chặt chẽ, bao phủ kiến thức khá đầy đủ nhưng chủ đề chưa được sắp xếp thống nhất.
0	Đánh giá thực địa sử dụng 654 câu hỏi trong ngân hàng để nạp vào hệ thống UMPCAT, số lượng câu hỏi như vậy cao hơn so với số câu hỏi đã thu được bằng mô phỏng. Điều này nhằm thu được tham số độ khó câu hỏi của mẫu tương đồng với toàn bộ ngân hàng câu hỏi, đồng thời tránh hiện tượng chọn lặp câu hỏi có thể xảy ra, cũng như đảm bảo tính liên tục của dải độ khó câu hỏi (Bảng 3). Độ khó cổ điển có phân phối lệch trái rõ rệt, cho thấy tỷ lệ trả lời đúng tính trên mẫu rất lớn các thí sinh là khá cao. Độ khó logit có phân phối đối xứng khá rõ và nhọn so với phân phối chuẩn, cho thấy so với phân phối chuẩn thì tỷ lệ thấp hơn ở các câu hỏi đánh giá mức trình độ kiến thức trung bình trong khi tỷ lệ câu hỏi nhằm đến mức trình độ kiến thức cao và thấp lại cao hơn ở hai biên. Đặc điểm này phù hợp với đánh giá mức trình độ kiến thức đầu vào của sinh viên mới đăng ký học phần lần đầu.
0	Kết quả nghiên cứu cho thấy bằng chứng quan trọng đảm bảo chất lượng và độ tin cậy vận hành phần mềm UMPItemBank. Các bài kiểm tra thể hiện đặc điểm cá nhân hóa rõ rệt, kết quả ước lượng mức kiến thức thí sinh có tính độc lập không có sự ảnh hưởng lẫn nhau. Mô hình đo lường Rasch vừa có tính bền vững vừa tương đối đơn giản để triển khai trong phần mềm máy tính. Mô hình Rasch cũng giúp hiệu chuẩn các câu hỏi kiểm tra theo một thang đo logit chung, kể cả khi ngân hàng câu hỏi được cập nhật liên tục. Những đặc điểm này phù hợp với việc thiết kế và phát triển đo lường khách quan và ứng dụng thuật toán trắc nghiệm thích nghi UMPCAT. Tính khả dụng và tương thích hệ thống cao cho phép phần mềm dễ vận hành, không cạnh tranh với các phần mềm khác. Phần mềm có các chức năng theo dõi đánh giá độ phù hợp của câu hỏi với mô hình đo lường, phạm vi và phân phối độ khó câu hỏi, khả năng phân cách của câu hỏi, tuy tính công bằng của câu mới chỉ phân tích sai biệt vận hành câu hỏi đối với nhóm giới tính, chưa được phân tích trên các yếu tố khác. Chức năng lưu trữ và truy xuất câu hỏi hoạt động hiệu quả và tương thích với thuật toán lựa chọn câu hỏi. Chức năng chỉnh sửa và cập nhật câu hỏi mới dễ sử dụng đề câu, tuy nhiên hỗ trợ nhập và xuất dữ liệu câu hỏi từ các định dạng file khác nhau chưa được quan tâm.
0	Trên nền tảng tương tác với ngân hàng câu hỏi, thuật toán lựa chọn câu hỏi hoạt động hiệu quả, cho phép ước tính mức kiến thức thí sinh với độ chính xác cao tuy chưa đạt mức tối ưu, số lượng câu hỏi cần thiết để đạt độ chính xác cần thiết giảm đi rõ rệt. So với một bài kiểm tra CBT thông thường gồm từ 100 đến 120 câu hỏi, và thường có SE khoảng từ 0,5 đến 0,7, bài kiểm tra thích nghi CAT giảm thời gian làm bài trung bình 20 phút, chỉ còn 40 đến 60 câu đủ đạt kết độ chính xác cao hơn với SE 0,3 logit. Kết quả này phù hợp các nghiên cứu của Wainer H (1999), Callear D và King T (1997), Linacre JM (2000) [3,16,17]. Bài kiểm tra CAT thành công trong việc hạn chế đưa ra câu hỏi quá khó hoặc quá dễ đối với từng cá nhân, đặc biệt giảm mức độ căng thẳng tâm lý trong thi cử là một yếu tố gây nhiễu khi đo lường năng lực thực hiện công việc đã học. Tuy nhiên bài kiểm tra thích nghi có thể gây ra những hiệu ứng tâm lý khác liên quan đến trình tự của dãy câu hỏi và không thể quay lại các câu hỏi đã trả lời trước đó, phù hợp với nghiên cứu của Linacre JM (2000) và Colwell NM (2013) [17,18]. Điều này gợi ra một số cải tiến cần thiết về tổ chức ngân hàng câu hỏi phù hợp cho việc mở rộng kiểm tra thích ứng đa giai đoạn, trong đó thí sinh có thể bỏ qua một số câu hỏi cũng như sửa câu trả lời trong phạm vi nhất định.
0	Bên cạnh đó, nghiên cứu cũng chỉ ra một số hạn chế của ngân hàng câu hỏi, thuật toán lựa chọn câu hỏi, và thuật toán ước lượng. Tính thẩm mỹ, tùy biến của giao diện người dùng và tính tương tác với người dùng còn thấp. Đây cũng là những hạn chế đã được khảo sát trong nghiên cứu của Economides AA và Roupas C (2007), Bridegeman B (2017) [19,20]. Một số hướng tiếp cận mới có thể giúp cải tiến phương pháp thiết kế thuật toán và giải quyết vấn đề chất lượng ứng dụng CAT [13,21-23]. Nghiên cứu đã ghi nhận bằng chứng về tính giá trị, độ tin cậy và tính tương thích của phần mềm ngân hàng câu hỏi UMPItemBank với thuật toán lựa chọn câu hỏi trong hệ thống trắc nghiệm thích nghi UMPCAT. Tuy nhiên phần mềm UMPItemBank có một số hạn chế hiệu suất hoạt động và tính tương tác và phản hồi cần khắc phục, cụ thể gồm có tính tùy biến, thẩm mỹ và nhất quán trong giao diện người dùng để tạo trải nghiệm liền mạch, kiểm soát và hạn chế tỷ lệ phơi lộ câu hỏi, chức năng phân tích độ phù hợp của câu hỏi với mục tiêu, phân tích sai biệt chức năng câu hỏi và tối ưu hóa sai số ước lượng. Việc tích hợp chức năng trí tuệ nhân tạo cũng được đề xuất để phát triển phần mềm.
0	Chống giả mạo khuôn mặt (Face Anti-Spoofing, viết tắt là FAS) là một phương thức quan trọng trong các hệ thống nhận dạng khuôn mặt giúp bảo vệ và nhận dạng đúng người. Những năm gần đây, các thuật toán phát hiện giả mạo khuôn mặt được phát triển mạnh mẽ ngay cả trong các trường hợp chưa đưa vào huấn luyện mô hình thuật toán. Tuy nhiên, các mô hình thuật toán học sâu này còn khá cơ bản nên trong nhiều trường hợp mô hình vẫn chưa phát hiện được sự giả mạo khuôn mặt. Gần đây, một số mô hình đã học tập dựa trên các tín hiệu pixel để xử lý nhiệm vụ FAS.
0	Vì vậy, trong bài báo này, nhóm tác giả đưa ra một phương pháp đánh giá mới với tên gọi FACL (Feature Aggregation and Constrastive Learning) dựa trên các đặc trưng thông tin của ảnh khuôn mặt đầu vào và học tương phản. Các thử nghiệm được xây dựng trên hai bộ dữ liệu: (1) Bộ dữ liệu của PTIT; và (2) Bộ dữ liệu của Zalo mang lại các kết quả tốt và hiệu quả hơn một số phương pháp hiện có. Ngoài ra, các nghiên cứu của tác giả cũng chứng minh mang tính hiệu quả khi mô hình học tập các lớp pixel khác nhau và đồng thời để cung cấp các thông tin chuyên sâu giúp giám sát việc chống giả mạo khuân mặt.
0	Trong thời đại hiện nay, việc sử dụng công nghệ nhận dạng khuôn mặt đã trở nên ngày càng phổ biến. Tuy nhiên, điều này đã mở ra cánh cửa cho các hình thức vi phạm gian lận thông qua giả mạo khuôn mặt với mức độ tinh vi ngày càng cao. Do đó, việc phát triển hệ thống chống giả mạo khuôn mặt trở nên vô cùng cấp bách và có ý nghĩa quan trọng hơn bao giờ hết. Các hệ thống này phải có khả năng xử lý các tình huống phức tạp, bao gồm việc nhận diện ảnh chụp từ các thiết bị khác, ảnh in, video được phát lại, cũng như việc phát hiện mặt nạ 3D và các phương pháp giả mạo khác.
0	Công nghệ nhận dạng khuôn mặt hiện nay đã đạt được sự tiến bộ đáng kể với độ chính xác cao [1]. Điều này được thúc đẩy bởi sự phát triển đáng kể của các bộ dữ liệu, trong đó nhiều nhóm nghiên cứu [2], [3] đã thu thập một lượng lớn thông tin về khuôn mặt con người từ khắp nơi trên thế giới. Sự gia tăng đột biến này về khối lượng dữ liệu đã cung cấp nền tảng cho việc phát triển các thuật toán học sâu trong việc nhận dạng khuôn mặt. Tuy nhiên, nhiều dữ liệu vẫn chưa trải qua quá trình xử lý chất lượng, dẫn đến tăng cường về số lượng dữ liệu nhưng chất lượng chưa được đảm bảo. Do đó, nhiều nhóm nghiên cứu [4] đã sử dụng những dữ liệu này để xây dựng các mô-đun tấn công vào các hệ thống nhận diện khuôn mặt.
0	Các bộ dữ liệu được sử dụng để xây dựng các mô-đun tấn công vào hệ thống nhận diện khuôn mặt thường chủ yếu bao gồm ảnh RGB [5]. Nhược điểm của các bộ dữ liệu này thường xuất phát từ việc hạn chế về số lượng các đối tượng được bao gồm. Ngoài ra, cũng có một số bộ dữ liệu [5] được sử dụng để phát triển các mô hình chống giả mạo. Những bộ dữ liệu này được biết đến với sự đa dạng về kích thước và định dạng, bao gồm cả ảnh RGB, hồng ngoại (IR) và thông tin độ sâu.
0	Trong nghiên cứu này, nhóm tác giả giới thiệu một phương pháp mới nhằm giải quyết thách thức về việc ngăn chặn giả mạo thông qua nhận diện khuôn mặt. Nhóm tác giả đã thay đổi cấu trúc của mô hình, tiến hành xử lý từng phương pháp riêng biệt và kết hợp các đặc trưng từ lớp pixel ở các cấp độ khác nhau, nhằm tăng cường sự tương tác giữa các nhánh thông tin RGB, hồng ngoại (IR), và độ sâu trong mạng nơ-ron. Thiết kế này cho phép tổng hợp các đặc trưng thông tin của ảnh khuôn mặt đầu vào (Feature Aggregation) và kết hợp với phương pháp học tương phản (Contrastive Learning), viết tắt là FACL. Trong các phần tiếp theo, chúng ta sẽ đi sâu vào tìm hiểu chi tiết phương pháp đề xuất này.
0	Sự khác biệt về cấu trúc là một trong những dấu hiệu chính để phân biệt giữa khuôn mặt thật và khuôn mặt giả mạo [6]. Thông tin như vậy đã được khai thác cho việc chống giả mạo khuôn mặt. Ví dụ, nhiều đặc trưng được tạo thủ công đã được nghiên cứu trong các công trình trước đó, bao gồm LBP [7] Trích xuất đặc trưng về cấu trúc bằng cách so sánh mỗi pixel với các pixel xung quanh, tuy nhiên nhược điểm của phương pháp này với sự biến đổi về ánh sáng và hướng, HOG [8] Mô tả sự phân phối của độ dốc hoặc hướng biên qua các khối chồng chéo, thích hợp cho việc phát hiện đối tượng nhưng tạo ra các vectơ đặc trưng có số chiều lớn.
0	DOG [9] Phát hiện cực trị về độ sáng ở nhiều tỷ lệ khác nhau, không thay đổi theo hướng, nhưng có thể hoạt động không tốt cho cấu trúc không giống như vết đen., SIFT [10] Trích xuất các tính năng bất biến có tính đặc biệt cao từ ảnh, mạnh mẽ trước nhiều biến thể nhưng tốn chi phí tính toán. và SURF [11] lấy cảm hứng từ SIFT nhưng tính toán nhanh hơn. Ngoài ra, các miền dữ liệu khác nhau đã được khai thác để trích xuất các đặc trưng phân biệt. Boulkenafet và cộng sự đã điều tra các không gian màu khác nhau như HSV và YCbCr [12] cung cấp thông tin bổ sung so với ảnh đa cấp. HSV tách độ sáng khỏi thông tin màu sắc trong khi YCbCr mô hình hóa quan sát thị giác của con người.
0	Các đặc trưng trong miền tần số cũng được nghiên cứu trong [8]. Chúng hiệu quả trong phân loại cấu trúc vật liệu nhưng nhạy cảm với các dịch chuyển. Vấn đề phổ biến tồn tại trong những phương pháp này là các đặc trưng được tạo thủ công không mạnh mẽ trước các biến số phiền hà khác nhau trong môi trường thực tế, như ánh sáng và che khuất. Khác với việc chỉ sử dụng hình ảnh tĩnh, các nhà nghiên cứu cố gắng tận dụng các chuyển động khuôn mặt tự nhiên trong một chuỗi khung hình cho việc chống giả mạo khuôn mặt. Ví dụ, việc chớp mắt đã được sử dụng để phát hiện sự sống động của khuôn mặt trong [13]. Chavan và cộng sự đã sử dụng các chuyển động của miệng và môi cho việc chống giả mạo khuôn mặt [14].
0	Tuy nhiên, các chuyển động khuôn mặt tự nhiên thường quá tinh tế để được ghi lại bằng các đặc trưng tạo thủ công trong thực tế. Sức mạnh biểu diễn mạnh mẽ của các mạng CNN hiện đại đã được khai thác trong nghiên cứu chống giả mạo khuôn mặt [15]. Các phương pháp trong [16] đã sử dụng một mô hình CaffeNet hoặc VGG-face được huấn luyện trước như một bộ trích xuất đặc trưng để phân biệt giữa khuôn mặt thật và giả mạo. Nhưng nó cũng đối mặt với thách thực dễ bị tấn công bằng cách áp dụng các biện pháp thêm nhiễu vào ảnh, kỹ thuật này có thể làm suy giảm khả năng phân biệt của mô hình và tạo ra kết quả giả mạo.
0	Nhưng nó cũng đối mặt với thách thực dễ bị tấn công bằng cách áp dụng các biện pháp thêm nhiễu vào ảnh, kỹ thuật này có thể làm suy giảm khả năng phân biệt của mô hình và tạo ra kết quả giả mạo. Mô hình LSTM-CNN [17] với khả năng vượt trội trong xử lý chuỗi dữ liệu và giữ lại thông tin trạng thái trước đó để nắm bắt mối quan hệ thời gian và chuỗi trong dữ liệu hình ảnh nhưng đòi hỏi lượng tài nguyên tính toán đáng kể và có thể không linh hoạt đối với dữ liệu phức tạp. Hay với kiến trúc mạng GAN [23] có thể hỗ trợ tạo ra dữ liệu giả mạo để nâng cao hiệu suất đào tạo mô hình.
0	Tuy nhiên, đối mặt với nhược điểm lớn khi đối phó với các kỹ thuật giả mạo tiên tiến như deepfake, có thể đặt thách thức đối với độ tin cậy của hệ thống phát hiện. Những nghiên cứu gần đây về Tổng quát hóa Miền (domain generalization) trong lĩnh vực chống giả mạo khuôn mặt với mục tiêu là xây dựng các mô hình có khả năng phát hiện tốt ngay cả trong những tình huống đa dạng và không được biết trước. Ví dụ, bài báo mới [21] đã đề xuất sử dụng kỹ thuật tạo dữ liệu tiêu cực (negative) thay vì sử dụng các mẫu tấn công thực tế trong quá trình đào tạo.
0	Một hướng tiếp cận khác [22] sử dụng mô hình dựa trên năng lượng (energy-based model, EBM), với mục tiêu khuyến khích các ảnh khuôn mặt thật có giá trị hàm năng lượng tự do thấp và xem xét tất cả các mẫu có năng lượng cao là các khuôn mặt giả mạo. Bên cạnh đó, các phương pháp khác [19], [20] cũng đã mở ra những hướng tiếp cận mới cho vấn đề tổng quát hóa miền. Mặc dù đã có sự tiến bộ đáng kể, việc đảm bảo tính tổng quát hóa trong bài toán chống giả mạo khuôn mặt khi chuyển đổi giữa các miền dữ liệu vẫn là một thách thức lớn. Các nghiên cứu trong lĩnh vực này đang liên tục nỗ lực để nâng cao khả năng ứng dụng thực tế của các hệ thống chống giả mạo khuôn mặt.
0	Bài toán chống giả mạo khuôn mặt là một trong những ứng dụng quan trọng của công nghệ nhận dạng khuôn mặt và bảo mật dữ liệu. Mục tiêu của bài toán này là ngăn chặn và phát hiện các hoạt động giả mạo, làm nhái hoặc sử dụng sai trái thông tin về khuôn mặt. Cụ thể, khi sử dụng các ứng dụng liên quan, thay vì sử dụng ảnh chụp khuôn mặt trực tiếp, người dùng có thể dùng ảnh của một người khác được chụp gián tiếp qua các thiết bị điện tử để giả mạo danh tính. Nhiệm vụ của chúng ta là xây dựng một mô hình học máy có khả năng phân biệt được các trường hợp ảnh thật hay ảnh giả mạo. Về mặt bản chất, đây là một bài toán phân loại nhị phân với ảnh giả mạo (nhãn là 0) và ảnh thật (nhãn là 1).
0	Tuy nhiên, các mô hình phân loại thông thường hoạt động không hiệu quả với các phương thức giả mạo mới và không có tính tổng quát hóa tốt. Do đó, nhóm tác giả đề xuất một phương pháp dựa trên khái niệm tổng quát hóa đa miền (cross-domain generalization) để tăng khả năng tổng quát hóa của mô hình học máy. Đối với phương pháp này, trong dữ liệu huấn luyện, lớp các ảnh giả mạo sẽ được chia thành các phân lớp con ứng với các miền (domain) khác nhau thể hiện các phương thức giả mạo khác nhau.
0	Trong bài báo này, nhóm tác giả đề xuất sử dụng kỹ thuật trích xuất đặc trưng chứa thông tin về phong cách (Style Information) và đặc trưng thông tin về nội dung (Content Information) để tách biệt các đặc tính toàn cục và địa phương của ảnh trong bài toán FAS. Đồng thời, phương pháp chuyển đổi phong cách (style transfer) được áp dụng giống như một cách để tăng cường và đa dạng hóa các đặc trưng giúp cải thiện khả năng tổng quát hoá của mô hình. Những kỹ thuật này được đề xuất lần đầu trong nghiên cứu của Zhuo Wang và các cộng sự [19], cho các kết quả tốt trên các tập dữ liệu chống giả mạo nổi tiếng đã được công bố.
0	Dựa trên các ý tưởng nền tảng đó, trong phần này, tác giả sẽ trình bày kiến trúc mạng tổng quan của mình với bốn module mạng cụ thể như dưới đây. 1) Module xương sống trích xuất các đặc trưng cơ bản: Đầu tiên, ảnh đầu vào ban đầu x sẽ được xử lý qua một mạng xương sống (backbone) để trích xuất các đặc trưng cơ bản. Các mô hình backbone có thể kể đến như VGG, ResNet, EfficientNet, . . . Dựa trên đánh giá về thời gian xử lý và độ chính xác của các mạng trên các tập dữ liệu nổi tiếng đã được công bố (như ImageNet, CIFAR-100, v.v), trong các thực nghiệm ở phần IV của bài báo, tác giả lựa chọn kiến trúc ResNet-18 làm mô hình xương sống. Đồng thời, các trọng số đã được huấn luyện (pre-trained weights) cũng được sử dụng để tăng khả năng của toàn bộ mô hình.
0	Module trích xuất thông tin về nội dung: Trong hệ thống nhận diện khuôn mặt, thông tin về nội dung thường được biểu diễn bằng các đặc điểm thông thường, bao gồm cả đặc điểm ngữ nghĩa và thuộc tính vật lý của hình ảnh. Với hình ảnh khuôn mặt, thường có sự chia sẻ của không gian đặc trưng ngữ nghĩa chung. Thậm chí, bất kể hình ảnh có phải là thật hay giả, các đặc điểm về hình dáng và kích thước thường khá giống nhau trong mỗi bức ảnh. Để trích xuất thông tin về nội dung, chúng ta sử dụng đặc trưng từ block cuối cùng của cấu trúc chung. Đặc trưng này có khả năng mô tả chi tiết và mang thông tin tổng quan hơn so với các khối trước đó.
0	Kiến trúc của model bao gồm 3 lớp tích chập, BatchNorm và ReLU được xếp chồng lên nhau để tạo thành một mạng sâu. Lớp tích chập để tạo ra một biểu diễn đặc trưng của hình ảnh. Các lớp tích chập sử dụng các kernel có kích thước nhỏ để quét qua hình ảnh và trích xuất thông tin về cấu trúc và đặc điểm quan trọng. Lớp Batch Normalization được sử dụng để đảm bảo rằng phân phối đầu ra của lớp tích chập ổn định, giúp tăng tốc quá trình đào tạo và cải thiện hiệu suất. Hàm kích hoạt ReLU được áp dụng sau mỗi lớp tích chập để tạo tính phi tuyến tính và kích thích việc học các đặc trưng phức tạp trong dữ liệu. Kết quả cuối cùng ta thu được đặc trưng nội dung x' (1_4) với chiều là (256, 8, 8), chứa thông tin quan trọng về nội dung hình ảnh.
0	Module trích xuất thông tin về phong cách: Thông tin về phong cách (Style Information Feature) thường liên quan đến cách mà các yếu tố như biến đổi hình dáng, màu sắc, và cấu trúc tạo nên một phong cách riêng biệt cho một hình ảnh cụ thể. Đây là một khái niệm được giới thiệu và đề xuất trong một bài báo của một nhóm tác giả [10]. Khi chúng ta sử dụng GradCAM để trực quan hóa các đặc trưng của hình ảnh, chúng ta thấy rằng các đặc trưng về nội dung thường tập trung vào phần chứa khuôn mặt, trong khi đặc trưng về phong cách tập trung chủ yếu vào phần bao quanh và nền của ảnh. Trong các mạng nơ-ron tích chập, hình ảnh được biểu diễn dưới dạng các """"đặc trưng"""" ở các lớp khác nhau của mạng."""
0	Đặc trưng thông tin về phong cách là một phần của những đặc trưng này và chứa thông tin về các mẫu, biến đổi hình dáng và các đặc điểm không gian của ảnh. Do đó, để thu thập thông tin về phong cách, chúng ta sử dụng một phương pháp dựa trên mô hình kim tự tháp, thu thập các đặc trưng đa tầng cùng với cấu trúc phân cấp. Trong phần khối ban đầu của mạng, mục tiêu là tạo ra các đặc trưng phù hợp với đặc trưng x 1 _ 2 khi đưa qua một lớp Convolution bằng cách đưa đặc trưng x 1 _ 1 qua hai lớp Convolution.
0	Các lớp Convolution này giúp biến đổi x 1 _ 1 thành một đặc trưng mới với các chiều tương tự như x 1 _ 2 . Sau đó, hai đặc trưng này được kết hợp lại với nhau thông qua một phép kết hợp đặc trưng, mục đích là cung cấp khả năng biểu diễn đa dạng hơn về thông tin trong ảnh. Tương tự, trong khối thứ hai, đặc trưng x 1 _ 3 trải qua một lớp Convolution để tạo ra một đặc trưng mới. Sau đó, đặc trưng mới này được kết hợp với đặc trưng kết quả từ x 1 _ 1 và x 1 _ 2 thông qua phép kết hợp đặc trưng, mục đích là giúp đa dạng hóa thông tin trong ảnh.
0	Khối cuối cùng của mạng đảm bảo rằng kích thước đầu ra giữ nguyên kích thước đầu vào. Điều này đảm bảo rằng thông tin từ các khối trước đó không bị mất. Các tham số của lớp Convolution trong khối cuối cùng có thể được điều chỉnh tùy theo yêu cầu cụ thể của bài toán. Sau khi đi qua khối cuối cùng, vector đặc trưng thu được được đưa vào một toán tử Adaptive Max Pooling để giảm kích thước của nó. Cuối cùng, vector này sẽ trải qua các lớp Fully Connected để trích xuất hai vector tham số affine, mỗi vector có số chiều là 256. Các lớp Fully Connected có thể được tùy chỉnh về số lượng, kích thước và hàm kích hoạt tùy thuộc vào nhiệm vụ cụ thể của bài toán.
0	Module chuyển đổi và tổng hợp phong cách, nội dung: Mục đích chính của module này là để tổng hợp và xáo trộn giữa đặc trưng nội dung và phong cách. Trong kiến trúc này, bên cạnh các lớp tích chập conv 4 và 5, tác giả bổ sung thêm các lớp AdaIN - được sử dụng để điều chỉnh phong cách giữa các hình ảnh. Điều này cho phép ta áp dụng phong cách từ một hình ảnh nguồn lên một hình ảnh đích mà không cần học các tham số phong cách. Thường thì việc áp dụng phong cách từ hình ảnh nguồn lên hình ảnh đích đòi hỏi việc huấn luyện mô hình đặc biệt để thực hiện điều này
0	AdaIN giải quyết vấn đề này bằng cách sử dụng tích hợp của hai phần: Adaptive Normalization và Instance Normalization. Trong đó: • Instance Normalization: Đây là một kỹ thuật chuẩn hóa dữ liệu trong mỗi bản đồ đặc trưng (feature map) riêng lẻ (instance). Nó giúp giảm biến động trong phân phối của các đặc trưng và giúp mô hình học được các tính năng chung của hình ảnh. • Adaptive Normalization: Đây là phần động của AdaIN. Nó cho phép thay đổi phân phối của các đặc trưng trong mỗi kênh (channel) dựa trên phân phối của các đặc trưng từ hình ảnh nguồn.
0	Hàm mất mát: Sau khi mô tả cụ thể các phương thức hoạt động của mạng ở phần trên, trong phần này, nhóm tác giả tổng hợp các hàm mất mát bao gồm hàm Cross Entropy và hàm tương phản (Contrastive Loss) để xây dựng một hàm mất mát tổng cho quá trình huấn luyện mô hình một cách ổn định và tối ưu. Hàm mất mát đối chứng (contrastive loss) được áp dụng để đối phó với vấn đề liên quan đến các đặc trưng phong cách trong quá trình tối ưu hóa mạng, đặc biệt là trong bối cảnh chuyển đổi miền. Vấn đề cơ bản là các đặc trưng phong cách dựa theo lĩnh vực cụ thể có thể che khuất hoặc làm mất đi các đặc trưng quan trọng liên quan đến tính sống còn.
0	Để khắc phục vấn đề này, một phương pháp học đối chứng được đề xuất nhằm làm nổi bật các đặc trưng phong cách liên quan đến tính sống còn và đồng thời tạo sự kìm nén đối với các đặc trưng phong cách riêng biệt cho từng miền. Sau khi kết hợp các đặc trưng nội dung và phong cách, chúng ta thu được hai tập hợp đặc trưng: một tập được gọi là F 1 (đặc trưng tự ghép cặp) và một tập được gọi là F 2 (đặc trưng tổ hợp bị xáo trộn). Tập đặc trưng F 1 được đưa vào một bộ phân loại và được giám sát bằng tín hiệu thực thể nhị phân sử dụng hàm mất mát L cls .
0	Tập đặc trưng F2 được so sánh với F1 bằng cách sử dụng độ tương tự cosin chuẩn hóa l2 nhằm đo lường sự khác biệt giữa chúng. Các đặc trưng tự ghép cặp trong F1 được coi như các điểm neo trong không gian đặc trưng đã được điều chỉnh theo phong cách. Một phép đặt dừng gradient (stop-gradient) được áp dụng trên tập F1 để giữ vị trí của chúng không thay đổi trong không gian đặc trưng. Các đặc trưng trong tập F2 sau đó được hướng dẫn để tiến gần hoặc xa khỏi các điểm neo tương ứng trong tập F1 dựa trên thông tin về tính xác thực.
0	Trong các hệ thống chống giả mạo trong thực tế, chúng ta thường gặp hai kiểu giả mạo chính bao gồm: ảnh chụp gián tiếp qua các ảnh vật lý và ảnh chụp gián tiếp qua thiết bị điện tử như máy tính, điện thoại, ti vi, v.v. Đối với kiểu giả mạo thứ hai chúng ta lại có thể chia thành hai lớp nhỏ, chi tiết hơn, gồm ảnh gián tiếp mà phần thiết bị gián tiếp được chụp rõ và trường hợp thứ hai là các ảnh mà ta không nhìn rõ thiết bị đó. Trong phần thực nghiệm của bài báo này, nhóm tác giả sử dụng bốn bộ dữ liệu giả mạo với tên gọi như sau: PFP, CRFP, URFP, và ZFF; một bộ dữ liệu ảnh khuôn mặt thật là RFP.
0	Cụ thể, để áp dụng phương pháp cross-domain, tác giả coi các dữ liệu hiện tại theo bốn domain như dưới đây: • Domain 1 (ký hiệu là P) gồm tập dữ liệu PFP, viết tắt của cụm từ Photo Face PTIT, bao gồm các ảnh chụp gián tiếp qua các ảnh vật lý. • Domain 2 (ký hiệu là C) - tương ứng với tập dữ liệu CRFP (Clear Replay Face PTIT) bao gồm các ảnh chụp gián tiếp qua thiết bị điện tử như máy tính, điện thoại, v.v. Trong đó, phần background của các ảnh này có thể nhìn rõ phần thiết bị trong khung hình. Con người có thể dễ dàng nhận diện được các trường hợp này là giả mạo.
0	Domain 3 (ký hiệu là U), gồm tập URFP (Un-clear Replay Face PTIT) trong đó các ảnh chụp gián tiếp qua thiết bị điện tử đã được đã được crop chi tiết vào phần ảnh chân dung. Các dữ liệu này có thể dễ gây nhầm lẫn, dẫn đến việc khó khăn hơn trong việc nhận diện, ngay cả với con người. • Domain 4 (ký hiệu là Z) bao gồm các dữ liệu giả mạo của Zalo, được công bố trong cuộc thi Zalo AI Challenge năm 2022, với phương thức và phong cách chụp khác so với ba bộ dữ liệu kể trên. • Bộ dữ liệu RFP (Real Face PTIT): được chia đều thành bốn phần bằng nhau và phân bổ về bốn Domain nói trên.
0	Đánh giá cụ thể khả năng phân loại của mô hình: Trước tiên, chúng ta quan sát Hình 5, để theo dõi hoạt động của hàm mất mát theo từng epoch. Về mặt tổng quan, hàm dao động lên xuống khá mạnh nhưng nhìn chung theo số epoch tăng dần cũng có xu hướng giảm. Điều này phần nào cho thấy mô hình đã học được từ dữ liệu huấn luyện. Khi triển khai mô hình chống giả mạo khuôn mặt trong thực tế, ta cần xác định một giá trị ngưỡng tối ưu để hệ thống có thể đưa ra quyết định xem ảnh khuôn mặt đầu vào có phải ảnh thật hay không. Do đó, việc sử dụng các bảng thống kê như ma trận confusion là rất cần thiết để chúng ta có thể tìm ra ngưỡng tốt nhất.
0	Hai bảng III, IV dưới đây, thể hiện ma trận nhầm lẫn của hai mô hình tốt nhất SSAN và mô hình nhóm tác giả đề xuất FACL, với các giá trị ngưỡng tối ưu của hai phương pháp lần lượt là 0.7 và 0.6. Các bảng này trình bày các giá trị của Giá trị dương tính thực (True Positive) và Âm tính thực (True Negative) là rất lớn so với giá trị của Giá trị dương tính giả (False Positive) và Âm tính giả (False Negative), đó là lý do giải thích tại sao độ chính xác cao so với kết quả đạt được sau khi đánh giá. Dựa vào đây, nhóm tác giả nhận định rằng tỉ lệ sai của mô hình của mình là chấp nhận được trong các hệ thống xác thực chống giả mạo.
0	Hai bảng cho thấy ưu điểm của FACL so với SSAN khi giá trị ngưỡng nhỏ hơn trong khi các tỷ lệ nhận diện sai FP và FN nhỏ hơn. Vẫn tương tự như các bảng kết quả trước, trong bảng V mô hình ResNet-18 cho kết quả thấp nhất và mô hình đề xuất cho kết quả cao như 91,88% với độ đo AUC và độ chính xác là 92%. Nhìn chung, với 2 mô hình SSAN và mô hình của tác giả thì có độ đo AUC và độ chính xác (ACC) với ngưỡng 0.6 là gần tương đương nhau. Tuy nhiên, ta thấy FACL cải thiện so với SSAN ngay cả với ngưỡng tốt nhất của SSAN (0.7) với độ chính xác là 0.91.
0	Đánh giá về kích thước và tốc độ xử lý của mô hình: Ở phần này, tác giả thực hiện so sánh kích thước của ba mô hình lần lượt là ResNet-18, SSAN, và FACL (do chúng có kích cỡ chênh lệch nhau không quá lớn). Quan sát Bảng VI, chúng ta có thể thấy sự tương quan giữa số lượng tham số và tốc độ xử lý. Nếu số lượng tham số tăng lên, tốc độ xử lý cũng tăng lên, và ngược lại. Ví dụ, mô hình ResNet-18 có số lượng tham số thấp nhất là 11 triệu, đi kèm với tốc độ xử lý nhanh hơn cả là 0.025 giây. Ngược lại, mô hình SSAN có số lượng tham số lớn nhất với 31 triệu tham số với thời gian xử lý là 0.091 giây.
0	Trong bài báo này, nhóm tác giả đã trình bày một phương pháp mới để phát hiện chống giả mạo khuôn mặt đã đạt được độ chính xác 91%. Nhóm tác giả đã thảo luận chi tiết về ba hướng khác nhau: dữ liệu, kiến trúc và khởi tạo, tổng hợp thành một giải pháp nhất quán, thể hiện những cải tiến đáng kể trên bộ thử nghiệm. Đầu tiên, tác giả đã chứng minh rằng việc lựa chọn cẩn thận một tập hợp con huấn luyện theo các loại mẫu giả mạo sẽ tổng quát hóa tốt hơn cho các cuộc tấn công không nhìn thấy được. Thứ hai, nhóm tác giả đã đề xuất một mô-đun tổng hợp sử dụng đầy đủ đặc trưng từ các phương thức khác nhau ở các cấp độ xử lý.
0	Bộ điều chỉnh điện áp tự động (AVR) giữ vai trò rất quan trọng trong việc duy trì điện áp ổn định cho máy phát điện đồng bộ trên tàu biển, nhằm cung cấp năng lượng điện tin cậy cho hệ thống điện tàu. Các bộ điều khiển cho AVR truyền thống, chẳng hạn như bộ điều khiển PID, thường gặp khó khăn trong việc duy trì hiệu suất tối ưu khi điều kiện tải thay đổi và có nhiễu loạn. Bài báo đề xuất một phương pháp nâng cao chất lượng điều khiển AVR bằng cách ứng dụng mạng nơ-ron nhân tạo (ANN) nhằm cải thiện đáp ứng động, tăng độ bền vững và khả năng thích nghi trong môi trường hàng hải.
0	Mạng nơ-ron nhân tạo được huấn luyện bằng dữ liệu mô phỏng từ bộ điều khiển PID truyền thống, giúp mô hình hóa quan hệ giữa sai số điện áp và tín hiệu điều khiển. Sau đó, mạng ANN được tích hợp vào mô hình MATLAB/Simulink để đánh giá hiệu suất, chất lượng điều khiển. Kết quả so sánh với bộ AVR truyền thống cho thấy phương pháp điều khiển bằng ANN giúp giảm đáng kể độ sai lệch điện áp, cải thiện đáp ứng quá độ và nâng cao độ ổn định của hệ thống. Trên tàu biển, nguồn điện năng chính được tạo ra bởi máy phát điện đồng bộ xoay chiều ba pha, truyền động bằng động cơ Diesel phụ, Diesel chính hoặc Turbin.
0	Các phụ tải điện đều được thiết kế để làm việc với điện áp định mức cho trước, chỉ khi công tác trong dải điện áp cho phép, thì hệ thống, thiết bị sẽ hoạt động ổn định, tin cậy và có tuổi thọ cao. Do vậy, để duy trì ổn định điện áp cung cấp cho các phụ tải điện, thì máy phát điện đồng bộ xoay chiều ba pha phải được trang bị hệ thống ổn định điện áp tự động cho máy phát điện tàu biển [1, 5, 6]. Phụ tải trong quá trình hoạt động thay đổi theo chế độ công tác của tàu một cách ngẫu nhiên. Do đó sẽ làm thay đổi điện áp của máy phát điện tàu biển, vì vậy, cần phải tự động điều chỉnh duy trì điện áp cung cấp trong dải cho phép.
0	Tuy nhiên, bộ tự động điều chỉnh điện áp cho máy phát điện tàu biển tổng hợp theo phương pháp truyền thống sẽ chỉ làm việc trong phạm vi thay đổi hẹp, và sẽ không làm việc tốt nếu dải thay đổi rộng. Do đó, việc xây dựng bộ điều khiển cho hệ thống điều chỉnh điện áp trên cơ sở ứng dụng mạng nơ-ron nhân tạo nhằm nâng cao chất lượng điều chỉnh điện áp cho máy phát điện tàu biển mang tính khoa học và thực tiễn. Bài báo đề cập đến việc thiết kế bộ tự động điều chỉnh điện áp trên cơ sở ứng dụng mạng nơ-ron nhân tạo cho máy phát điện tàu biển. Kết quả nghiên cứu được trình bày trong các phần sau.
0	Mạng nơ-ron truyền thẳng một lớp ẩn được thiết kế điều khiển cho bộ tự động điều chỉnh điện áp máy phát điện bao gồm lớp vào, lớp ẩn, lớp ra có hàm chuyển đổi là 'logsig', 'tansig', 'purelin'. Luật học là lan truyền ngược. Dữ liệu luyện mạng là loại mảng, được lấy từ quá trình mô phỏng hệ thống điều chỉnh điện áp tự động với bộ điều khiển PID. Xây dựng chương trình trên m.file để thiết lập cấu hình mạng nơ-ron ba lớp và luyện mạng với tệp dữ liệu đã thu thập ở trên, với đầu vào là sai lệch điện áp và đầu ra là điện áp điều khiển đưa đến bộ khuếch đại. Quá trình luyện mạng được thể hiện như Hình 5.
0	Hệ thống điều chỉnh tự động điện áp máy phát điện tàu biển sử dụng bộ điều khiển mạng nơ-ron một lớp ẩn, với luật học lan truyền ngược mang lại hiệu suất, chất lượng vượt trội so với bộ điều khiển PID truyền thống. Bộ điều khiển mạng nơ-ron một lớp ẩn, với luật học lan truyền ngược có khả năng thích nghi tốt khi có sự thay đổi tải trong phạm vi rộng, giúp duy trì điện áp của máy phát điện ổn định, bền vững trong các điều kiện vận hành khác nhau. Tuy nhiên, hệ thống điều chỉnh tự động điện áp máy phát điện tàu biển sử dụng bộ điều khiển mạng nơ-ron, cũng yêu cầu quá trình huấn luyện ban đầu tốn thời gian, tài nguyên tính toán và đòi hỏi kinh nghiệm để lựa chọn cấu trúc mạng nơ-ron phù hợp.
0	Hệ thống thuỷ lợi Bắc Hưng Hải nằm ở Trung tâm đồng bằng Bắc Bộ thuộc vùng châu thổ sông Hồng và là hệ thống lớn nhất miền Bắc. Ranh giới hệ thống được bao bọc bởi 4 sông lớn: phía Bắc là sông Đuống dài 67 km từ Xuân Canh đến Phả Lại; phía Nam là sông Luộc dài 72 km từ Hà Lão đến Quý Cao; phía Tây là sông Hồng dài 57 km từ Xuân Quan đến Hà Lão; phía Đông là sông Thái Bình dài 73 km từ Phả Lại đến Quý Cao[3]. Bắc Hưng Hải có hệ thống sông nội địa khá dày đặc, chúng nối thông với nhau tạo thành mạng lưới dẫn và tiêu nước khá thuận lợi.
0	Có 2 sông chính là Sông Kim Sơn (phía Bắc) chạy từ Xuân Quan đến thành phố Hải Dương (60 km), đây là tuyến tải nước chính của hệ thống Bắc Hưng Hải lấy từ sông Hồng qua cống Xuân Quan để cấp nước cho cả vùng. Sông Cửu An là trục chính phía Nam chạy từ Nghi Xuyên đến Cự Lộc dài 50 km. Hệ thống sông nội địa có thế dốc lòng sông và hướng chuyển nước đều theo hướng Tây Bắc xuống Đông Nam. Vì vậy nước ngọt lấy từ sông Hồng qua cống Xuân Quan vào sông Kim Sơn đã có điều kiện thuận lợi dẫn nước cho toàn khu vực.
0	Chất lượng nước trong HTTL Bắc Hưng Hải được quan trắc từ năm 2005 đến nay với tần suất quan trắc từ 2-6 lần trong năm, bố trí vào các thời gian cung cấp nước tưới cho vụ xuân (tháng 2-5) và thời gian cấp nước tưới trong vụ mùa (tháng 7-9). Trong những năm trở lại đây, có ứng dụng mô hình toán chất lượng nước để mô phỏng 3 thông số DO, BOD5 và NH4+ nhưng độ chính xác còn hạn chế do số liệu đầu vào chưa đầy đủ, nhất là số liệu về lưu lượng nguồn thải (khó điều tra).
0	Kết quả đã đánh giá được mức độ gia tăng ô nhiễm, các chỉ tiêu ô nhiễm và nguyên nhân gây ô nhiễm làm cơ sở đề xuất các giải pháp giảm thiểu ô nhiễm và giảm thiểu tác hại của ô nhiễm nước đến SXNN và NTTS [3]. Có thể thấy phương pháp giám sát chất lượng nước trong HTTL Bắc Hưng Hải là phương pháp truyền thống (thu thập các mẫu nước, sau đó được phân tích trong phòng thí nghiệm bằng các xét nghiệm hóa học và sinh học khác nhau) với các thông số lý, hóa, vi sinh...
0	Các phương pháp này thường tốn nhiều thời gian, công sức và có thể tốn kém, đặc biệt khi một số lượng lớn mẫu nước được thu thập từ các địa điểm khác nhau. Ngoài ra, phương pháp này chỉ có thể cung cấp số liệu chất lượng nước tại các thời điểm nhất thời, gây khó khăn cho việc đánh giá những thay đổi theo thời gian và không gian. Trong những năm gần đây, trên Thế giới, trí tuệ nhân tạo (AI) ngày càng được ứng dụng nhiều trong dự báo chất lượng nước nhờ khả năng xử lý lượng lớn dữ liệu và đưa ra dự đoán chính xác.
0	Các thuật toán AI, chẳng hạn như học máy và học sâu, đã được áp dụng để dự báo các thông số chất lượng nước, bao gồm pH, DO, BOD5, COD... cũng như để phát hiện các bất thường về chất lượng nước và dự đoán xu hướng chất lượng nước. Các thuật toán học sâu có thể xử lý các mối quan hệ phi tuyến tính giữa các thông số chất lượng nước, cũng như xử lý dữ liệu bị thiếu và xử lý dữ liệu nhiều chiều một cách hiệu quả. Ngoài ra, các thuật toán AI có thể học hỏi từ dữ liệu trong thời gian thực và liên tục cải thiện các dự đoán của chúng khi có dữ liệu mới.
0	Có một số loại thuật toán học sâu có thể được sử dụng để giám sát chất lượng nước, bao gồm mạng nơ-ron nhân tạo (ANN), mạng nơ-ron tích chập (CNN) và mạng nơ-ron hồi quy (RNN). Mỗi loại thuật toán học sâu đều có điểm mạnh và điểm yếu riêng và việc lựa chọn thuật toán sẽ phụ thuộc vào ứng dụng giám sát chất lượng nước cụ thể. Trong những năm gần đây, việc ứng dụng các thuật toán mạng lưới nơ-ron nhân tạo (ANN) để dự báo chất lượng nước đã có những kết quả nổi bật.
0	Các mô hình ANN có khả năng giải quyết các vấn đề mô hình hóa khác nhau ở sông, hồ, hồ chứa, nhà máy xử lý nước thải (WWTP)... [2]. Đã có một số nghiên cứu sử dụng mô hình học sâu để dự báo chất lượng nước, chủ yếu tập trung vào dự báo các thông số chất lượng nước (pH, nhiệt độ, DO, COD...) và tính toán chỉ số chất lượng nước (WQI). Về dự báo các thông số chất lượng nước như DO, nhiệt độ, pH và độ mặn, Hu và các cộng sự [5] đã dự báo chất lượng nước mới dựa trên mạng học sâu bộ nhớ ngắn - dài hạn (LSTM) được đề xuất để dự báo pH và nhiệt độ. Kết quả cho thấy, độ chính xác của mô hình rất cao lần lượt là 95,76% và 96,88%.
0	Haq [9] sử dụng các mô hình (lai) học sâu như mạng thuật toán mạng nơ-ron tích chập (CNN), LSTM và đơn vị tái phát có kiểm soát (GRU) để dự báo. Kết quả cho thấy mô hình (lai) CNN-LSTM vượt trội so với tất cả các mô hình khác về độ chính xác dự đoán và thời gian tính toán. Một kiến trúc với mô hình dự báo cho các hệ thống IoT để giám sát chất lượng nước trong nuôi trồng thủy sản được thực hiện bằng cách sử dụng thuật toán học sâu (LSTM) để dự báo các chỉ số trên cho thấy hoạt động tốt và có thể áp dụng cho các hệ thống thực tế.
0	Đối với dự báo tổng nitơ (TN), tổng phốt pho (TP) và chỉ số kali permanganat (COD-Mn), nghiên cứu của Yan [10] đã sử dụng mô hình mạng lưới nơ-ron kết hợp 1-DRCNN và BiGRU để dự báo. Kết quả thực nghiệm cho thấy mô hình đề xuất có độ chính xác dự đoán và tính tổng quát cao hơn so với các mô hình LSTM, GRU và BiGRU (hệ số xác định (R^2) có thể đạt tới 0,9431). Các thuật toán trí tuệ nhân tạo (AI) cũng được phát triển để dự báo chỉ số chất lượng nước (WQI) và phân loại chất lượng nước (WQC).
0	Để dự báo WQI, các mô hình mạng nơ-ron nhân tạo như mạng nơ-ron tự hồi quy phi tuyến tính (NARNET) và thuật toán học sâu bộ nhớ ngắn - dài hạn (LSTM) đã được ứng dụng dựa trên 7 thông số đầu vào. Kết quả dự báo cho thấy mô hình NARNET hoạt động tốt hơn so với LSTM để dự báo các giá trị WQI [1]. Trong quá trình xây dựng và vận hành các hệ thống quan trắc chất lượng nước thông minh trên nền tảng Internet vạn vật (IoT), ngày càng nhiều dữ liệu lớn được tạo ra.
0	Kết quả nghiên cứu cho thấy giá trị dự báo của mô hình và giá trị thực tế rất phù hợp và cho thấy chính xác xu hướng phát triển chất lượng nước trong tương lai, cho thấy tính khả thi và hiệu quả của việc sử dụng mạng lưới nơ-ron sâu LSTM để dự báo chất lượng của nước uống [8]. Đối với khu vực số liệu đo đạc còn hạn chế, thiếu số liệu, thuật toán học sâu đã được ứng dụng để dự báo các thông số chất lượng nước (thời gian thực). Một mạng lưới nơ-ron nhân tạo học sâu (ANN) cộng với một kỹ thuật xử lý hậu kỳ được gọi là Bộ xử lý Bayesian đa biến bất định (MBUP) được ứng dụng để dự báo. Kết quả cho thấy đã cải thiện đáng kể độ chính xác của các dự báo chất lượng nước [12].
0	Việc dự báo chất lượng nước dựa trên thuật toán Phân tích quan hệ xám cải tiến (IGRA) và mạng lưới nơ-ron Bộ nhớ ngắn - dài hạn (LSTM) [11]. Trong đó, xem xét mối tương quan đa biến của thông số chất lượng nước, IGRA, xét về mức độ tương đồng và gần gũi, được đề xuất để lựa chọn thông số đặc trưng. Bên cạnh đó cũng xem xét trình tự thời gian của thông tin chất lượng nước, mô hình dự báo chất lượng nước dựa trên LSTM, có đầu vào là các tính năng thu được từ IGRA, được thiết lập.
0	Để cảnh báo cho người dùng trước khi nguồn nước bị ô nhiễm (Hệ thống được đề xuất sẽ cảnh báo người dùng khi bất kỳ thông số nào vượt tiêu chuẩn cho phép), một giải pháp dựa trên IoT để dự báo chất lượng nước [4]. Hệ thống được đề xuất sử dụng IoT và mạng lưới nơ-ron được tối ưu hóa để dự đoán. Thuật toán mạng nơ-ron dựa trên tối ưu hóa bầy đàn Cat (CSO) được sử dụng để dự báo. Hệ thống được đề xuất sẽ cảnh báo người dùng khi bất kỳ tham số đo được nào nhỏ hơn ngưỡng cố định. Kỹ thuật này cũng có thể được thực hiện trong các nhà máy nước, sông và các ngành công nghiệp.
0	Các thuật toán học sâu đã cải thiện đáng kể độ chính xác và hiệu quả của dự báo chất lượng nước. Im và các cộng sự [7] đã sử dụng các thuật toán học sâu (LSTM, GRU, SCINet) để dự báo chất lượng nước máy theo chuỗi thời gian ở Hàn Quốc. Mô hình học sâu được tối ưu hóa đạt được độ chính xác dự đoán trung bình và tối đa lần lượt là 98,78 và 99,98%. Mô hình đề xuất có thể cung cấp thông tin chất lượng nước nhanh chóng và chính xác cho các cơ sở cấp nước quy mô lớn trên toàn quốc và cải thiện sức khỏe cộng đồng thông qua chẩn đoán sớm các bất thường về chất lượng nước.
0	Tóm lại, các nghiên cứu trước đây về sử dụng phương pháp học sâu trong dự báo chất lượng nước chủ yếu tập trung vào dự báo các thông số chất lượng nước (thông số vật lý là chủ yếu) và tính toán chỉ số chất lượng nước (WQI). Một số nghiên cứu cũng đã kết hợp thuật toán học sâu với mạng lưới quan trắc thời gian thực và cho kết quả rất khả quan. Do vậy, việc nghiên cứu ứng dụng trí tuệ nhân tạo (mô hình học sâu) để dự báo chỉ số chất lượng nước mặt dựa vào số liệu đo hiện trường của hệ thống Bắc Hưng Hải là quan trọng và cần thiết.
0	Chuẩn bị và xử lý trước dữ liệu là một bước quan trọng trong nghiên cứu này để đảm bảo dữ liệu, số liệu phù hợp nhằm loại bỏ mọi yếu tố nhiễu hoặc giá trị ngoại lai có thể ảnh hưởng đến độ chính xác của các mô hình. Bao gồm các bước sau đây: - Làm sạch dữ liệu: Dừ liệu được thu thập sẽ được làm sạch để xử lý mọi giá trị bị thiếu hoặc không đồng nhất. - Chuẩn hóa dữ liệu: số liệu sẽ được chuẩn hóa để đảm bảo rằng tất cả các biến (thông số) đều ở cùng một tỷ lệ (không thứ nguyên), điều này rất quan trọng đối với độ chính xác của các thuật toán học sâu.
0	Việc lựa chọn các thuật toán học sâu để dự báo các thông số chất lượng nước cho hệ thống thủy lợi Bắc Hưng Hải sẽ dựa trên khả năng nắm bắt các mối quan hệ phi tuyến tính phức tạp trong dữ liệu và xử lý lượng dữ liệu lớn. Một số thuật toán thường được ứng dụng để dự báo như sau: Mạng nơ-ron nhân tạo (ANN): là một loại thuật toán học sâu thường được sử dụng cho các nhiệm vụ dự báo. ANN bao gồm nhiều lớp nút được kết nối với nhau và được đào tạo bằng cách sử dụng lan truyền ngược. - Mạng nơ-ron tích chập (CNN): là một loại thuật toán học sâu thường được sử dụng để nhận dạng và xử lý hình ảnh.
0	Mạng nơ-ron hồi quy (RNNs): RNNs là một loại thuật toán học sâu rất phù hợp với dữ liệu liên tục và đa biến. RNN được thiết kế để xử lý dữ liệu tuần tự và đã được chứng minh là hoạt động tốt để dự báo chất lượng nước. - Mạng bộ nhớ ngắn - dài hạn (LSTM): Mạng LSTM là một loại RNN được thiết kế để xử lý các chuỗi dữ liệu theo thời gian. LSTM đã được chứng minh là rất hiệu quả trong việc dự báo chất lượng nước thời gian thực. Mạng RNN như là một mạng nơ-ron (Neural Network) xử lý thông tin ở dạng chuỗi (sequence/time-series) với việc tiền xử lý các data theo thứ tự.
0	Trong các mạng nơ-ron hồi quy, thông tin tuần hoàn qua một vòng lặp đến lớp ẩn ở giữa. Lớp đầu vào 'x' nhận đầu vào của mạng nơ-ron và xử lý nó rồi chuyển nó lên lớp giữa. Lớp giữa 'h' có thể bao gồm nhiều lớp ẩn, mỗi lớp có các chức năng kích hoạt và trọng số cũng như độ lệch riêng. Nếu có một mạng nơ-ron trong đó các tham số khác nhau của các lớp ẩn khác nhau không bị ảnh hưởng bởi lớp trước đó, tức là: mạng nơ-ron không có bộ nhớ, thì có thể sử dụng mạng nơ-ron hồi quy. Mạng nơ-ron hồi quy sẽ chuẩn hóa các chức năng kích hoạt khác nhau, trọng số và độ lệch để mỗi lớp ẩn có cùng tham số. Sau đó, thay vì tạo nhiều lớp ẩn, nó sẽ tạo một lớp và lặp lại nhiều lần theo yêu cầu.
0	Có 4 loại mạng nơ-ron hồi quy: 1 đầu vào cho kết quả 1 đầu ra; 1 đầu vào - nhiều đầu ra; nhiều đầu vào - 1 đầu ra; và nhiều đầu vào - nhiều đầu ra. Trong nghiên cứu này ứng dụng loại mạng RNN với nhiều đầu vào - 1 đầu ra. 2.2.3.3. Đào tạo và kiểm định mô hình Quá trình đào tạo là sử dụng tập dữ liệu đào tạo sẽ được sử dụng để đào tạo các thuật toán nhận dạng các thông số và mối quan hệ của chúng trong tập dữ liệu. Quá trình kiểm định bao gồm sử dụng bộ dữ liệu kiểm tra để đánh giá độ chính xác của thuật toán.
0	Các bước sau đây sẽ được thực hiện để đào tạo và kiểm định mô hình: - Đào tạo mô hình: thuật toán học sâu (RNN) sẽ được đào tạo bằng tập dữ liệu đào tạo, đồng thời để giảm thiểu sai số dự đoán giữa thông số chất lượng nước thực tế và thông số chất lượng nước dự đoán. - Điều chỉnh siêu tham số: các tham số của thuật toán học sâu sẽ được điều chỉnh để cải thiện hơn nữa độ chính xác. - Kiểm định mô hình: được xác thực bằng bộ dữ liệu kiểm tra. - Lựa chọn mô hình: Thuật toán hoạt động tốt nhất (theo các tham số đã hiệu chỉnh) sẽ được chọn dựa trên kết quả hiệu chỉnh.
0	Keras cung cấp các phương pháp thuận tiện để tải và chuẩn bị dữ liệu, cũng như các công cụ để trực quan hóa và giải thích kết quả đào tạo. Các chỉ số đánh giá độ chính xác của mô hình sau đây sẽ được sử dụng để đánh giá mô hình học sâu trong việc dự báo các thông số chất lượng nước ở thủy lợi Bắc Hưng Hải. Các tiêu chí đánh giá (hiệu chỉnh) các mô hình học sâu được trình bày trong các công thức (1) đến (4): - Sai số tuyệt đối trung bình (MAE): chênh lệch trung bình giữa giá trị thực và giá trị dự đoán. MAE là 1 chỉ số phổ biến để tính sai số nhằm đánh giá (kiểm định) mô hình đối với các biến liên tục, được xác định theo công thức (1).
0	Có thể thấy, mô hình học sâu được áp dụng trong nghiên cứu này đều có thể dự đoán tốt các thông số chất lượng nước cho khu vực nghiên cứu với hệ số xác định khá cao, lớn hơn 0,75. Đây là cơ sở khoa học vững chắc và cũng là kết quả quan trọng để có thể ứng dụng mô hình học sâu để dự đoán các thông số chất lượng nước cho các vùng khác có điều kiện tương tự, nhất là trong điều kiện khó khăn trong công tác quan trắc đầy đủ các thông số chất lượng nước theo phương pháp truyền thống để phục vụ công tác đánh giá và quản lý nước trong hệ thống thủy lợi.
0	Số liệu quan trắc thấm qua thân và nền đập là một trong những thông tin quan trọng góp phần đánh giá và phát hiện sớm nguy cơ mất an toàn đập. Những năm gần đây, với sự phát triển của công nghệ thông tin, nhiều mô hình ứng dụng trí tuệ nhân tạo đã và đang được ứng dụng để có thể dự báo sớm số liệu quan trắc thấm ở thân và nền đập. Trong nghiên cứu này, hai mô hình dựa trên phương pháp hồi quy tuyến tính (LR) và mạng nơ ron nhân tạo (ANN) được xây dựng để dự đoán số liệu quan trắc thấm tại 25 đầu đo thấm ở thân và nền đập Ngàn Trươi tỉnh Hà Tĩnh. Các đặc trưng thống kê R^2, NSE, MAE và RMSE được tính toán để kiểm tra độ tin cậy của hai mô hình.
0	Đập là công trình nhân tạo được xây dựng chắn ngang sông thường là tạo thành các hồ chứa nước. Các hồ chứa nước một mặt có vai trò quan trọng trong việc điều tiết nguồn nước, cung cấp nước cho các nhu cầu dùng nước (thủy điện, cấp nước cho nông nghiệp, công nghiệp, sinh hoạt, nuôi trồng thủy sản…), đồng thời là góp phần cắt lũ cho hạ lưu. Mặt khác, các đập và hồ chứa khi xảy ra sự cố, lượng nước sẽ đổ về phía hạ du, gây ra các thiệt hại lớn đến các hoạt động kinh tế xã hội, thậm chí cả tính mạng con người. Do đó, yêu cầu đảm bảo an toàn đập, phát hiện sớm nguy cơ sự cố đập là một yêu cầu quan trọng trong quản lý vận hành.
0	Theo thống kê, trong số các nguyên nhân dẫn đến sự cố đập, nguyên nhân do dòng thấm chiếm khoảng 25-30% các sự cố đập trên thế giới (Stematiu, 2009; Charles, 2011). Sự cố đập do dòng thấm có thể phát hiện sớm được trong quá trình quản lý vận hành thông qua các dấu hiệu bất thường của dòng thấm có thể nhận biết qua các biểu hiện bên ngoài của đập (hố sụt trên mặt đập, nước thấm đục chảy ra phía hạ lưu đập, sự phát triển bất thường của cây cỏ phía hạ lưu đập so với khu vực xung quanh...); hoặc thông qua số liệu quan trắc hoặc thông qua tính toán các đặc trưng của dòng thấm ở đập và nền như cột nước thấm, lưu lượng thấm, áp lực thấm.
0	Một vài ví dụ có thể kể đến như nghiên cứu của Torblaa và Kjoernsli (1968) cho thấy rằng, sự cố đập Hyttejuvet ở Áo đã được phát hiện sớm khi có sự tăng lên bất thường trong số liệu quan trắc lưu lượng thấm qua đập; hay nghiên cứu khác của Lach (2018) chỉ ra rằng 2 tuần trước thời điểm sự cố thấm ở đập Pieczyska, số liệu quan trắc cột nước thấm tại các Piezometers khu vực xuất hiện sự cố có sự tụt giảm đột ngột. Như vậy, các sự cố đập liên quan đến dòng thấm hoàn toàn có thể phát hiện sớm thông qua những thay đổi bất thường trong số liệu quan trắc thấm ở công trình. Vì vậy, việc phát triển một công cụ để phân tích, dự báo sớm số liệu quan trắc thấm dưới đập là cần thiết và có nhiều ý nghĩa thực tiễn.
0	Ở Việt Nam hiện nay, để xác định các đặc trưng của dòng thấm qua thân và nền đập, một số phương pháp tính toán dựa trên một số giả thiết để đơn giản hóa bài toán như các phương pháp gần đúng như phương pháp thủy lực, phương pháp phần tử hữu hạn (FEM). Những phương pháp này có hạn chế là thường giải quyết bài toán phẳng, ít kể đến ảnh hưởng của các yếu tố không gian, thời gian, thời tiết… đến dòng thấm qua thân, nền đập Do đó, kết quả tính toán thấm theo những phương pháp này trong các giai đoạn thiết kế thường có sự sai lệch tương đối lớn so với các giá trị quan trắc trong thực tế.
0	Những năm gần đây, với sự phát triển của công nghệ thông tin cùng với cuộc cách mạng 4.0 đưa đến nhiều ứng dụng của công nghệ thông tin, trí tuệ nhân tạo (AI), trong đó có ứng dụng để dự báo số liệu quan trắc thấm qua thân và nền đập. Tayfur và nnk (2005) đã ứng dụng mô hình Artificial Neural Network (ANN) kết hợp, so sánh với kết quả tính toán mô phỏng từ mô hình FEM để đánh giá và dự báo số liệu quan trắc thấm tại các Piezometers của đập Jeziorsko ở Ba Lan. Nourani và nnk (2012) đã phát triển mô hình ANN tích hợp sử dụng thuật toán FFBP (feed-forward back-propagation) và RBF (radial basis function) để mô hình xác định số liệu quan trắc thấm trong một nhóm các Piezometers của đập Sattarkhan, Iran.
0	Emami và nnk (2019) cũng sử dụng mô hình ANN tích hợp tương tự dùng để dự báo số liệu quan trắc thấm trong các Piezometers của đập Boukan Shahid Kazemi, Iran... Những nghiên cứu này đều cho thấy năng lực và độ chính xác khi áp dụng các mô hình AI trong dự báo số liệu quan trắc thấm ở thân và nền đập, so sánh với các phương pháp truyền thống. Trong bài báo này, các tác giả đề xuất xây dựng mô hình hồi quy tuyến tính (LR) và mô hình mạng nơ ron nhân tạo (ANN) để dự báo số liệu quan trắc thấm tại các đầu đo và ứng dụng cho đập Ngàn Trươi, tỉnh Hà Tĩnh.
1	Trí tuệ nhân tạo (Artificial Intelligence – AI) là một trong những lĩnh vực cốt lõi và có tốc độ phát triển nhanh nhất trong khoa học máy tính hiện đại. Báo cáo này tập trung trình bày tổng quan về AI, từ cơ sở lý thuyết, quá trình phát triển, cho đến các ứng dụng thực tiễn trong nhiều lĩnh vực khác nhau như xử lý dữ liệu, thị giác máy tính, y sinh và hệ thống thông minh. Ngoài ra, báo cáo cũng phân tích các thách thức kỹ thuật, vấn đề đạo đức và xu hướng nghiên cứu trong tương lai của AI. Thông qua việc tổng hợp và phân tích các công trình nghiên cứu tiêu biểu, bài viết nhằm cung cấp một cái nhìn toàn diện, có hệ thống và mang tính học thuật cao, phù hợp với tiêu chuẩn báo cáo khoa học quốc tế trong lĩnh vực khoa học máy tính.
1	Khoa học máy tính là ngành nghiên cứu nền tảng cho sự phát triển của công nghệ số, trong đó trí tuệ nhân tạo đóng vai trò then chốt. AI được định nghĩa là khả năng của hệ thống máy tính mô phỏng các quá trình tư duy, học tập và ra quyết định của con người. Trong vài thập kỷ gần đây, sự gia tăng mạnh mẽ của dữ liệu lớn và năng lực tính toán đã tạo điều kiện cho AI phát triển vượt bậc. Việc nghiên cứu AI không chỉ mang ý nghĩa học thuật mà còn có giá trị ứng dụng cao trong đời sống thực tiễn. Phần giới thiệu này nhằm làm rõ bối cảnh nghiên cứu, lý do lựa chọn đề tài và tầm quan trọng của trí tuệ nhân tạo trong sự phát triển chung của khoa học máy tính hiện đại.
1	Cơ sở lý thuyết của trí tuệ nhân tạo được xây dựng trên nhiều lĩnh vực khác nhau như toán học, xác suất thống kê, logic học và khoa học nhận thức. Các mô hình AI truyền thống thường dựa trên logic hình thức và hệ chuyên gia, trong khi các phương pháp hiện đại tập trung vào học máy và học sâu. Những thuật toán này cho phép hệ thống tự động rút trích tri thức từ dữ liệu thay vì phải lập trình thủ công. Việc hiểu rõ nền tảng lý thuyết giúp nhà nghiên cứu lựa chọn mô hình phù hợp và đánh giá chính xác hiệu quả của hệ thống. Phần này trình bày các khái niệm cốt lõi, thuật toán nền tảng và nguyên lý hoạt động của các mô hình AI trong khoa học máy tính.
1	Học máy (Machine Learning) là một nhánh quan trọng của trí tuệ nhân tạo, cho phép máy tính học từ dữ liệu và cải thiện hiệu suất theo thời gian. Trong khoa học máy tính, học máy được ứng dụng rộng rãi trong phân loại, dự đoán và tối ưu hóa hệ thống. Các phương pháp học có giám sát, không giám sát và học tăng cường đã tạo ra bước đột phá trong nhiều bài toán phức tạp. Nhờ học máy, các hệ thống có thể xử lý khối lượng dữ liệu lớn với độ chính xác cao hơn so với phương pháp truyền thống. Phần này phân tích vai trò của học máy như một công cụ trung tâm thúc đẩy sự đổi mới và mở rộng phạm vi ứng dụng của khoa học máy tính.
1	Học sâu (Deep Learning) là sự phát triển nâng cao của học máy, sử dụng mạng nơ-ron nhân tạo nhiều lớp để mô phỏng hoạt động của não người. Các mô hình học sâu đã đạt được thành tựu vượt trội trong nhận dạng hình ảnh, xử lý ngôn ngữ tự nhiên và phân tích dữ liệu phức tạp. Trong khoa học máy tính, học sâu đóng vai trò quan trọng trong việc giải quyết các bài toán có cấu trúc dữ liệu lớn và phi tuyến tính. Tuy nhiên, việc huấn luyện các mô hình này đòi hỏi tài nguyên tính toán cao và dữ liệu lớn. Phần này trình bày nguyên lý hoạt động, ưu điểm và hạn chế của học sâu trong nghiên cứu và ứng dụng khoa học máy tính.
1	Xử lý dữ liệu lớn là một trong những lĩnh vực ứng dụng quan trọng nhất của trí tuệ nhân tạo. AI giúp tự động hóa quá trình phân tích, trích xuất thông tin và phát hiện mẫu trong các tập dữ liệu khổng lồ. Trong khoa học máy tính, sự kết hợp giữa AI và dữ liệu lớn đã cải thiện đáng kể hiệu quả của hệ thống thông tin và hỗ trợ ra quyết định. Các thuật toán AI cho phép xử lý dữ liệu theo thời gian thực, giảm thiểu sai sót và tăng độ chính xác. Phần này phân tích cách AI được tích hợp vào các hệ thống xử lý dữ liệu và vai trò của nó trong việc nâng cao năng lực tính toán hiện đại.
1	Mặc dù đạt được nhiều thành tựu, trí tuệ nhân tạo vẫn đối mặt với nhiều thách thức về kỹ thuật và đạo đức. Các vấn đề như thiếu tính minh bạch, thiên lệch dữ liệu và chi phí tính toán cao đặt ra nhiều câu hỏi cho giới nghiên cứu. Trong khoa học máy tính, việc đảm bảo độ tin cậy và an toàn của hệ thống AI là một yêu cầu cấp thiết. Ngoài ra, AI còn đặt ra các vấn đề liên quan đến quyền riêng tư và trách nhiệm xã hội. Phần này trình bày những hạn chế chính của AI, đồng thời nhấn mạnh sự cần thiết của các giải pháp kỹ thuật và chính sách phù hợp.
1	Trong tương lai, trí tuệ nhân tạo được kỳ vọng sẽ tiếp tục mở rộng phạm vi ảnh hưởng trong khoa học máy tính. Các hướng nghiên cứu mới như AI giải thích được, học liên kết và AI bền vững đang nhận được sự quan tâm lớn. Những xu hướng này nhằm giải quyết các hạn chế hiện tại và nâng cao khả năng ứng dụng thực tiễn của AI. Việc kết hợp AI với các công nghệ mới như điện toán lượng tử và Internet vạn vật sẽ tạo ra nhiều cơ hội đột phá. Phần này thảo luận về các xu hướng nghiên cứu tiềm năng và vai trò của chúng trong sự phát triển dài hạn của khoa học máy tính.
1	Trí tuệ nhân tạo đã và đang trở thành trụ cột quan trọng của khoa học máy tính hiện đại. Thông qua việc phân tích cơ sở lý thuyết, ứng dụng và thách thức, báo cáo cho thấy AI không chỉ là công cụ kỹ thuật mà còn là động lực thúc đẩy đổi mới công nghệ. Mặc dù còn tồn tại nhiều hạn chế, tiềm năng phát triển của AI là rất lớn nếu được nghiên cứu và ứng dụng một cách có trách nhiệm. Kết luận này khẳng định vai trò trung tâm của trí tuệ nhân tạo và nhấn mạnh sự cần thiết của việc tiếp tục đầu tư nghiên cứu trong lĩnh vực khoa học máy tính.
1	Trí tuệ nhân tạo (AI) là lĩnh vực nghiên cứu và ứng dụng các phương pháp cho phép máy tính thực hiện các nhiệm vụ vốn đòi hỏi trí tuệ con người. Trong khuôn khổ học thuật và công nghiệp, AI kết hợp nhiều công cụ từ thống kê, tối ưu hóa đến lý thuyết thông tin để mô phỏng suy nghĩ, nhận thức và ra quyết định. Một nghiên cứu căn bản về AI cần trình bày rõ mục tiêu, giả thuyết, phương pháp thu thập dữ liệu và tiêu chí đánh giá hiệu năng. Bài báo mẫu nên nêu bật động lực của đề tài, đóng góp chính và giới hạn nghiên cứu, đồng thời xác định các ứng dụng thực tế mà kết quả có thể tác động, ví dụ tối ưu hóa quy trình, hệ khuyến nghị hay tự động hóa nhiệm vụ lặp lại.
1	Học máy (Machine Learning) là nhánh thực nghiệm của AI, tập trung vào thiết kế thuật toán học từ dữ liệu để thực hiện dự đoán hoặc phân loại. Một báo cáo chuyên sâu về học máy phải mô tả rõ dạng dữ liệu, phương pháp tiền xử lý, lựa chọn đặc trưng và chiến lược đánh giá như cross-validation. Việc so sánh nhiều mô hình dựa trên các tiêu chí thống nhất giúp nhận diện giải pháp tối ưu cho bài toán. Ngoài ra, cần thảo luận về overfitting, underfitting và kỹ thuật điều chỉnh như regularization, ensemble hay hyperparameter tuning. Kết luận phần học máy nên nêu rõ hiệu năng mô hình, phân tích sai số và hướng cải tiến trong các nghiên cứu tiếp theo.
1	Học sâu (Deep Learning) sử dụng mạng nơ-ron nhiều lớp để học biểu diễn phức tạp từ dữ liệu lớn. Trong báo cáo khoa học, phần mô tả mô hình học sâu cần trình bày kiến trúc, hàm mất mát, thuật toán tối ưu hóa và chiến lược huấn luyện như learning rate scheduling hay early stopping. Việc giải thích lựa chọn kiến trúc (ví dụ CNN cho ảnh, RNN/Transformer cho chuỗi thời gian hoặc ngôn ngữ) giúp người đọc hiểu tính phù hợp của mô hình với dữ liệu. Ngoài ra, thảo luận về khả năng diễn giải (interpretability), chi phí tính toán và yêu cầu dữ liệu huấn luyện là cần thiết để đánh giá tính khả thi triển khai mô hình trong thực tế.
1	Xử lý ngôn ngữ tự nhiên (Natural Language Processing — NLP) là một lĩnh vực ứng dụng học máy nhằm cho phép máy hiểu và sinh ngôn ngữ con người. Báo cáo về NLP cần làm rõ nguồn dữ liệu văn bản, bước tiền xử lý như tokenization, stemming/lemmatization, và biện pháp loại bỏ nhiễu. Mô tả mô hình nên giải thích cách biểu diễn từ (word embeddings, contextual embeddings) và kiến trúc mạng phù hợp cho tác vụ như phân loại, tóm tắt hoặc dịch máy. Thử nghiệm đối chiếu với các baseline, sử dụng metrics phù hợp như BLEU, ROUGE hoặc F1 giúp minh chứng hiệu quả. Cuối cùng, phần thảo luận cần nêu vấn đề đạo đức, thiên lệch dữ liệu và bảo mật thông tin cá nhân.
1	Thị giác máy tính (Computer Vision) tập trung vào việc trích xuất thông tin từ ảnh và video bằng kỹ thuật học máy. Một báo cáo hoàn chỉnh về thị giác máy tính nên mô tả rõ loại dữ liệu (ảnh tĩnh, chuỗi video), quy trình gán nhãn, augmentation và kiến trúc mô hình như CNN, ResNet hay các biến thể hiện đại. Các thước đo đánh giá như IoU, mAP hay accuracy tùy bài toán cần được chỉ ra. Ngoài ra, cần phân tích khó khăn như ánh sáng thay đổi, góc chụp đa dạng, hay dữ liệu không cân bằng, đồng thời đề xuất chiến lược khắc phục như transfer learning, fine-tuning từ mô hình tiền huấn luyện hoặc sử dụng phương pháp synthetic data.
1	Học có giám sát (supervised learning) là phương pháp phổ biến trong ML, trong đó mô hình học từ cặp dữ liệu đầu vào và nhãn. Báo cáo thảo luận về học có giám sát cần nêu rõ nguồn nhãn, độ tin cậy của nhãn và phương thức thu thập (thủ công hay tự động). Các mô hình tiêu biểu như logistic regression, decision trees, SVM hay neural networks cần được so sánh dựa trên độ chính xác, độ phức tạp và khả năng giải thích. Việc phân tích lỗi từng loại giúp chỉ ra nguyên nhân sai dự đoán. Cuối phần, đề xuất lộ trình cải thiện như thu thập thêm mẫu, cân bằng lớp hoặc áp dụng kỹ thuật ensemble để tăng tính ổn định của mô hình.
1	Học không giám sát (unsupervised learning) nhằm tìm cấu trúc ẩn trong dữ liệu mà không cần nhãn. Trong báo cáo, phần này nên giới thiệu các phương pháp phổ biến như clustering (k-means, hierarchical), dimensionality reduction (PCA, t-SNE, UMAP) và density estimation. Mục tiêu ứng dụng có thể là khám phá nhóm khách hàng, phát hiện bất thường hay trích xuất tính năng. Việc đánh giá học không giám sát khó khăn hơn do thiếu nhãn; vì vậy báo cáo cần trình bày các chỉ số nội bộ (silhouette score) hoặc minh họa bằng trực quan. Thảo luận nên đề xuất cách kết hợp với phương pháp bán giám sát hoặc sử dụng domain knowledge để cải thiện tính thực dụng.
1	Học tăng cường (Reinforcement Learning — RL) nghiên cứu việc học qua tương tác giữa tác nhân và môi trường để tối đa hóa phần thưởng tích lũy. Báo cáo về RL cần mô tả rõ mô hình MDP (states, actions, rewards) và phương pháp học như Q-learning, policy gradients hay actor-critic. Ứng dụng RL trong robotics, game AI hay điều khiển tự động đòi hỏi mô tả môi trường mô phỏng, chiến lược khám phá-exploitation và kỹ thuật ổn định hóa học như replay buffer hoặc target networks. Phần thực nghiệm phải minh họa quy trình học, đồ thị hội tụ và so sánh với baseline. Ngoài ra, cần nêu thách thức tính toán và độ an toàn khi triển khai trên hệ thống thực tế.
1	Tính diễn giải và minh bạch của mô hình (interpretability & explainability) trở nên quan trọng khi AI tham gia quyết định ảnh hưởng nhiều đến con người. Báo cáo về chủ đề này nên phân biệt các mức diễn giải: toàn cục (global) và cục bộ (local), và trình bày kỹ thuật như feature importance, LIME, SHAP hay attention visualization. Việc đánh giá tính diễn giải cần vừa có chỉ số định lượng vừa có kiểm nghiệm với chuyên gia domain. Phần thảo luận cần đề cập trade-off giữa hiệu năng mô hình và độ minh bạch, cũng như các khuyến nghị để tích hợp giải thích vào pipeline sản phẩm nhằm tăng độ tin cậy và tuân thủ quy định pháp lý.
1	Đạo đức trong AI (AI ethics) là lĩnh vực nghiên cứu các hệ quả xã hội, pháp lý và đạo đức khi áp dụng công nghệ trí tuệ nhân tạo. Báo cáo về đạo đức cần phân tích các vấn đề như thiên lệch dữ liệu dẫn tới bất công, quyền riêng tư, trách nhiệm khi AI gây hại và tác động kinh tế xã hội. Phần này nên đề xuất khung quản trị, tiêu chuẩn kiểm thử và chính sách minh bạch, đồng thời mô tả phương pháp giảm thiên lệch như debiasing dữ liệu và fairness-aware learning. Kết luận nên kêu gọi sự phối hợp liên ngành giữa nhà phát triển, nhà quản lý và cộng đồng để đảm bảo AI phát triển có trách nhiệm và bền vững.
1	Kỹ thuật tiền xử lý dữ liệu (data preprocessing) là bước then chốt trong mọi pipeline ML/DS. Báo cáo cần trình bày các bước chuẩn hóa, xử lý giá trị thiếu, mã hóa biến phân loại, và phát hiện outlier. Ngoài ra, nên thảo luận về feature engineering: tạo đặc trưng mới, lựa chọn đặc trưng bằng phương pháp thống kê hoặc thuật toán, và scaling đối với mô hình nhạy với tỉ lệ. Việc tài liệu hóa quy trình và lưu trữ pipeline tiền xử lý giúp đảm bảo tái tạo kết quả. Phần thảo luận cũng nêu bật ảnh hưởng của tiền xử lý tới hiệu suất mô hình và cách kiểm soát rủi ro rò rỉ thông tin (data leakage).
1	Kỹ thuật chọn và đánh giá đặc trưng (feature selection & evaluation) giúp giảm chiều dữ liệu và cải thiện khả năng tổng quát hóa của mô hình. Trong báo cáo, cần trình bày các phương pháp lọc (filter), wrapper và embedded, ví dụ dựa trên correlation, mutual information, recursive feature elimination hay regularization như L1. Phân tích kích thước mẫu tối thiểu cần thiết cho một số phương pháp cũng hữu ích. Thử nghiệm so sánh trước-sau khi loại bỏ đặc trưng giúp minh họa hiệu quả. Cuối phần nên đề cập đến trade-off giữa thông tin mất đi và lợi ích giảm overfitting, cũng như lưu ý khi đặc trưng mang thông tin nhạy cảm liên quan đến cá nhân.
1	Kỹ thuật đánh giá mô hình và chọn tham số (model evaluation & hyperparameter tuning) ảnh hưởng trực tiếp đến tính khách quan của kết quả. Báo cáo cần giải thích các chiến lược như hold-out, k-fold cross-validation, stratified sampling và các metrics phù hợp từng bài toán (accuracy, precision, recall, F1, AUC). Về tuning, nên mô tả grid search, random search và Bayesian optimization, kèm theo phân tích chi phí tính toán. Phần này cũng cần đề cập overfitting do tuning quá mức trên tập validation và cách phòng tránh bằng nested cross-validation. Kết luận nêu hướng tiếp cận cân bằng giữa hiệu quả mô hình và chi phí tài nguyên.
1	Hệ thống khuyến nghị (Recommendation Systems) là ứng dụng quan trọng của ML trong thương mại và nội dung số. Báo cáo về khuyến nghị nên mô tả ba nhóm phương pháp: filtering dựa trên nội dung, collaborative filtering và mô hình lai. Việc đánh giá hệ khuyến nghị cần chỉ rõ metrics như precision@k, recall@k, MAP và NDCG. Báo cáo nên thảo luận về cold-start problem, data sparsity và chiến lược giải quyết như sử dụng metadata, side information hoặc embedding người-dùng/sản phẩm. Ngoài ra, cần suy xét yếu tố công bằng và cá nhân hóa quá mức, cũng như tác động đến trải nghiệm người dùng.
1	Xây dựng pipeline ML reproducible và CI/CD cho mô hình là yêu cầu quan trọng khi triển khai sang sản phẩm. Một báo cáo về chủ đề này nên trình bày các thành phần: versioning dữ liệu, quản lý mô hình (model registry), testing cho mô hình (unit test, integration test, data drift detection), và tự động hóa deployment. Việc sử dụng containerization, orchestration (Docker, Kubernetes) và công cụ MLOps giúp đảm bảo tính nhất quán giữa môi trường phát triển và sản xuất. Phần thảo luận cần nêu các chỉ số giám sát sau triển khai, quy trình rollback và thực hành an toàn khi cập nhật mô hình trực tiếp trên hệ thống người dùng.
1	Kiến trúc hệ thống cho ứng dụng AI ở quy mô lớn đòi hỏi thiết kế tối ưu về lưu trữ, xử lý và tính sẵn sàng. Báo cáo kỹ thuật nên mô tả kiến trúc microservices, message queue, stream processing và batch processing cho các luồng dữ liệu khác nhau. Việc lựa chọn cơ sở dữ liệu phù hợp (time-series DB, graph DB, document store) cho từng loại dữ liệu là cần thiết. Đồng thời cần xem xét khả năng scale ngang, cân bằng tải và chiến lược cache để giảm độ trễ. Phần này cũng nên phân tích chi phí vận hành và đề xuất kiến trúc resilient để giảm thiểu rủi ro khi hệ thống gặp sự cố.
1	Hệ thống phân tán cho huấn luyện mô hình lớn (distributed training) giúp rút ngắn thời gian và xử lý mô hình phức tạp. Báo cáo nên giải thích hai chiến lược chính: data parallelism và model parallelism, cùng các kỹ thuật như gradient aggregation, parameter servers hoặc all-reduce. Cần thảo luận về trade-off giữa thông lượng và độ trễ, ảnh hưởng của latency mạng và đồng bộ hóa gradient. Ngoài ra, quản lý tài nguyên GPU/TPU, checkpointing và khả năng khôi phục từ lỗi là yếu tố quan trọng để đảm bảo tiến trình huấn luyện bền vững. Phần thực nghiệm nên trình bày tốc độ hội tụ khi tăng số node.
1	An toàn và bảo mật cho hệ thống AI là lĩnh vực cần chú trọng khi triển khai ứng dụng nhạy cảm. Báo cáo cần nêu các mối đe dọa như adversarial attacks, model inversion, membership inference và poisoning attacks. Mô tả biện pháp phòng ngừa như adversarial training, differential privacy, secure multi-party computation hoặc federated learning khi xử lý dữ liệu nhạy cảm. Phần phân tích nên đánh giá ảnh hưởng của các biện pháp bảo mật đến hiệu suất mô hình và chi phí tính toán. Kết luận gợi ý một lộ trình kết hợp biện pháp kỹ thuật và quy trình vận hành để giảm thiểu rủi ro bảo mật.
1	Federated Learning là phương pháp phân tán cho phép huấn luyện mô hình trên nhiều thiết bị mà không tập trung dữ liệu. Báo cáo nên mô tả quy trình tổng hợp gradient hoặc mô hình cục bộ, thách thức như non-iid data, giao tiếp hạn chế và privacy leakage. Các chiến lược tối ưu như compression, secure aggregation và personalization giúp gia tăng hiệu quả. Ứng dụng tiêu biểu bao gồm cải thiện trải nghiệm cá nhân trên thiết bị di động mà không vi phạm quyền riêng tư. Phần thảo luận cần trình bày thước đo đánh giá như communication rounds và trade-off giữa chính xác và chi phí truyền dữ liệu.
1	Kỹ thuật transfer learning và fine-tuning giúp tận dụng mô hình tiền huấn luyện để giải quyết bài toán mới với ít dữ liệu. Báo cáo cần giải thích lý do hiệu quả: mô hình tiền huấn luyện đã học được các biểu diễn chung có ích cho nhiều nhiệm vụ. Thí nghiệm so sánh giữa training từ đầu và fine-tuning nên trình bày tốc độ hội tụ, hiệu năng và yêu cầu tài nguyên. Cần thảo luận về việc chọn tầng đóng/mở khi fine-tune, learning rate khác nhau cho các phần của mạng và kỹ thuật regularization để tránh catastrophic forgetting. Kết luận chỉ ra hướng áp dụng hiệu quả trong môi trường dữ liệu hạn chế.
1	Hệ thống phát hiện bất thường (anomaly detection) có ứng dụng đa dạng trong an ninh mạng, tài chính và giám sát thiết bị. Báo cáo cần mô tả loại bất thường (point, contextual, collective), dữ liệu đặc trưng và các phương pháp như statistical tests, isolation forest, autoencoders hay probabilistic models. Việc đánh giá yêu cầu sử dụng metrics thích hợp cho imbalance data, ví dụ precision-recall curve. Phần thảo luận nên nêu chiến lược phản hồi khi phát hiện bất thường, chẳng hạn gửi alert, cách xác minh và giảm false positive, đồng thời mô tả cách sử dụng feedback để cải thiện mô hình theo thời gian.
1	Khoa học dữ liệu (Data Science) tích hợp cả kỹ thuật và domain knowledge để chuyển dữ liệu thành tri thức có giá trị. Báo cáo trong lĩnh vực này cần trình bày pipeline từ thu thập, làm sạch, phân tích khám phá đến mô hình hóa và truyền đạt kết quả. Phần EDA (exploratory data analysis) nên có trực quan hóa, thống kê mô tả và kiểm định giả thuyết. Việc lập báo cáo phân tích phải gợi ý hành động dựa trên kết quả, kèm theo đánh giá độ tin cậy. Ngoài ra, nêu rõ giới hạn dữ liệu, giả định phân tích và đề xuất thu thập dữ liệu bổ sung khi cần.
1	Trực quan hóa dữ liệu (data visualization) là công cụ thiết yếu giúp truyền đạt kết quả khoa học dữ liệu tới stakeholders. Một phần báo cáo nên thảo luận nguyên tắc chọn loại đồ thị phù hợp với mục tiêu (so sánh, phân bố, quan hệ, thành phần), tránh trực quan gây hiểu lầm và luôn ghi chú rõ nguồn dữ liệu. Việc kết hợp interactivity giúp stakeholders khám phá sâu hơn, nhưng cần cân nhắc hiệu suất khi làm việc với tập dữ liệu lớn. Kết luận phần này nên đề xuất một bộ templates trực quan cho các báo cáo định kỳ và quy trình kiểm duyệt hình ảnh để đảm bảo tính chính xác.
1	Xử lý dữ liệu thời gian thực (real-time data processing) là yếu tố then chốt cho ứng dụng streaming, giám sát và phân tích tức thời. Báo cáo nên mô tả kiến trúc stream processing, công cụ tiêu biểu như Kafka, Flink hoặc Spark Streaming, và các chỉ số SLA như latency, throughput. Thiết kế cần cân nhắc fault-tolerance, state management và scalability. Ngoài ra, cần nêu chiến lược đảm bảo tính nhất quán của dữ liệu và cách xử lý late-arriving data. Phần thực nghiệm có thể minh họa bằng bài toán phát hiện sự kiện hay phân tích hành vi người dùng theo thời gian thực.
1	Khoa học dữ liệu với dữ liệu địa lý (geospatial data) mở ra nhiều ứng dụng trong quy hoạch, logistics và môi trường. Báo cáo cần mô tả cách biểu diễn dữ liệu không gian (raster, vector), phương pháp tiền xử lý đặc thù như reprojection và spatial join, cùng thuật toán phân tích không gian như clustering theo địa lý hoặc phân tích mạng lưới. Việc trực quan hóa bản đồ thông minh giúp truyền đạt kết quả cho người dùng phi kỹ thuật. Phần thảo luận nên đề xuất lưu trữ hiệu quả và tư duy về quyền riêng tư khi dữ liệu địa lý có thể xác định vị trí cá nhân.
1	Kiểm thử và đảm bảo chất lượng phần mềm cho hệ thống AI đòi hỏi mở rộng các phương pháp SE truyền thống. Báo cáo cần trình bày test cases cho dữ liệu, mô hình và pipeline, ví dụ unit test cho preprocessing, integration test cho pipeline và regression test cho mô hình sau khi cập nhật dữ liệu. Ngoài ra, nên nêu thêm việc kiểm thử fairness, robustness và performance. Áp dụng test automation và CI cho mô hình giúp phát hiện sớm lỗi. Phần kết luận khuyến nghị thiết lập KPI chất lượng mô hình và chính sách release an toàn cho môi trường sản xuất.
1	DevOps cho các dự án Data Science, hay còn gọi là DataOps, tập trung vào việc tự động hóa và lặp nhanh chu trình phân tích dữ liệu để đảm bảo tính liên tục. Báo cáo cần miêu tả chi tiết workflow từ giai đoạn lên mã nguồn, kiểm thử đơn vị, đánh giá hiệu năng mô hình cho đến khâu triển khai và giám sát thực địa, đi kèm với các công cụ hỗ trợ mạnh mẽ như Git, CI/CD pipelines và hệ thống orchestration như Kubernetes. Việc duy trì tính reproducibility và version control cho cả dữ liệu (sử dụng DVC) và mô hình là yếu tố trung tâm của toàn bộ hệ thống.
1	Phân tích chuỗi thời gian (time series analysis) ứng dụng rộng trong dự báo tài chính, sản xuất và vận tải. Báo cáo cần giới thiệu các phương pháp truyền thống (ARIMA, Exponential Smoothing) và phương pháp ML hiện đại (RNN, LSTM, Transformer cho chuỗi). Làm rõ bước tiền xử lý như decomposition, deseasonalization và xử lý missing values là cần thiết. Việc đánh giá model nên dùng metrics phù hợp như MAPE, RMSE và phân tích residual. Phần thảo luận đề xuất chiến lược kết hợp nhiều mô hình (ensemble) để nâng cao độ chính xác và độ ổn định dự báo.
1	Kỹ thuật giảm chiều dữ liệu (dimensionality reduction) hữu ích khi dữ liệu có nhiều biến đồng thời tránh overfitting. Báo cáo nên trình bày PCA, t-SNE, UMAP và autoencoders, nêu rõ mục đích: visualization, denoising hay tiền xử lý cho mô hình học. Việc chọn số chiều sau giảm cần cân nhắc giữa mất mát thông tin và lợi ích tính toán. Thực nghiệm nên so sánh tốc độ huấn luyện và hiệu năng mô hình trước/sau giảm chiều. Kết luận gợi ý ứng dụng giảm chiều trong pipeline khi dữ liệu thưa hoặc khi cần trực quan hóa.
1	Phân tích đồ thị (graph analytics) và học trên đồ thị (Graph Neural Networks) phục vụ bài toán gắn kết quan hệ phức tạp như mạng xã hội, phân tích giao dịch hay sinh học. Báo cáo nên giới thiệu cấu trúc đồ thị, các đặc trưng node/edge và thuật toán truyền thống như PageRank hay community detection. Với GNN, cần mô tả cách lan truyền thông tin và lý do GNN phù hợp cho các nhiệm vụ như node classification, link prediction. Phần thực nghiệm nên so sánh GNN với các phương pháp baseline và phân tích độ nhạy theo cấu trúc đồ thị.
1	Khoa học dữ liệu cho y tế (healthcare analytics) đòi hỏi thận trọng về đạo đức và bảo mật. Báo cáo cần mô tả nguồn dữ liệu y tế, quy trình anonymization, và các phương pháp phân tích để phát hiện bệnh hay hỗ trợ chẩn đoán. Việc đánh giá mô hình phải minh bạch với chỉ số lâm sàng phù hợp và thí nghiệm lâm sàng bổ sung khi cần. Ngoài ra, cần thảo luận về rủi ro thiên lệch do dữ liệu không đại diện và cách phối hợp với chuyên gia y tế để đảm bảo kết quả có thể áp dụng lâm sàng an toàn.
1	Ứng dụng AI trong tài chính (FinTech) tạo nhiều cơ hội như phát hiện gian lận, định giá tín dụng và giao dịch thuật toán. Báo cáo nên mô tả nguồn dữ liệu tài chính, đặc trưng thời gian và vấn đề imbalance. Phương pháp phổ biến bao gồm các mô hình phân loại, time-series forecasting và anomaly detection. Việc triển khai cần thận trọng về explainability và compliance với quy định. Phần thảo luận có thể đề xuất khung giám sát liên tục để phát hiện drift và kiểm soát rủi ro khi mô hình tương tác trực tiếp với giao dịch tài chính.
1	Tích hợp Machine Learning với Internet vạn vật (IoT) mở rộng khả năng thu thập và phân tích dữ liệu theo thời gian thực. Báo cáo nên mô tả kiến trúc thu thập dữ liệu từ cảm biến, xử lý biên (edge computing) và gửi các bản tóm tắt lên cloud để huấn luyện mô hình. Thách thức bao gồm băng thông hạn chế, năng lượng thiết bị và heterogeneity dữ liệu. Phần thảo luận đề xuất tiêu chuẩn nén, aggregation và local inference trên thiết bị để giảm chi phí truyền dữ liệu, đồng thời đảm bảo bảo mật cho luồng dữ liệu IoT.
1	Ứng dụng AI trong giáo dục (EdTech) hỗ trợ cá nhân hóa lộ trình học và đánh giá năng lực. Báo cáo cần mô tả dữ liệu học tập, mô hình dự đoán hiệu suất và hệ khuyến nghị nội dung học phù hợp. Việc thiết kế thí nghiệm A/B để đo lường tác động trên kết quả học tập là quan trọng. Ngoài ra, cần thảo luận về rủi ro thiên lệch dẫn đến đánh giá không công bằng và đề xuất biện pháp giảm thiểu. Kết luận gợi ý tích hợp feedback của giáo viên để tinh chỉnh mô hình và đảm bảo phù hợp với mục tiêu giáo dục.
1	Tối ưu hóa (Optimization) trong Machine Learning (ML) và Software Engineering (SE) không chỉ là bước hoàn thiện mà là nền tảng cốt lõi quyết định sự thành bại của dự án, ảnh hưởng trực tiếp đến khả năng hội tụ của mô hình và trải nghiệm người dùng cuối. Báo cáo cần đi sâu phân tích cơ chế hoạt động và trường hợp sử dụng cụ thể của các thuật toán tối ưu hóa gradient-based phổ biến như SGD, Adam (cho dữ liệu lớn, thưa) hay các phương pháp bậc hai như L-BFGS (cho dữ liệu nhỏ, yêu cầu chính xác cao). Song song đó, việc thảo luận về chiến lược Regularization (L1, L2, Dropout, Early Stopping) là bắt buộc để giải quyết bài toán đánh đổi giữa Bias và Variance, qua đó ngăn chặn hiện tượng Overfitting hiệu quả.
1	MLOps (Machine Learning Operations) và quản trị vòng đời mô hình (Model Lifecycle Management) đã trở thành yêu cầu cấp thiết giúp các tổ chức chuyển đổi từ các thử nghiệm rời rạc sang vận hành AI ở quy mô công nghiệp một cách bền vững. Báo cáo cần phân tích sâu chuỗi giá trị khép kín, bắt đầu từ giai đoạn Phát triển (Development) với việc quản lý mã nguồn và dữ liệu (DVC), qua bước Đánh giá (Evaluation) nghiêm ngặt trên các tập test ẩn để kiểm tra tính tổng quát hóa. Tiếp theo, mô hình được đưa vào Đăng ký (Model Registry) – nơi lưu trữ siêu dữ liệu (metadata), phiên bản và nguồn gốc (lineage) – trước khi Triển khai (Deployment) thông qua các chiến lược như Blue-Green hoặc Canary để giảm thiểu gián đoạn.
1	Ứng dụng AI cho tự động hóa quy trình doanh nghiệp (RPA + AI) giúp tối ưu hóa tác vụ lặp lại và nâng cao hiệu suất làm việc. Báo cáo nên mô tả quy trình nhận dạng tài liệu, trích xuất thông tin và tích hợp với hệ thống ERP/CRM. Kết hợp NLP và OCR trong pipeline cho phép tự động hóa xử lý hóa đơn, hợp đồng hay email. Phần phân tích cần đánh giá ROI, thời gian hoàn vốn và rủi ro vận hành khi thay đổi quy trình cốt lõi của doanh nghiệp. Kết luận gợi ý chiến lược triển khai từng bước, bắt đầu từ các quy trình có khối lượng cao và lợi ích rõ rệt.
1	Phân tích cảm xúc là một ứng dụng NLP quan trọng giúp doanh nghiệp thực hiện Marketing Intelligence và lắng nghe mạng xã hội (Social Listening). Báo cáo cần mô tả chi tiết các nguồn dữ liệu từ bình luận, tweets cho đến đánh giá sản phẩm, cùng quy trình tiền xử lý đặc thù cho ngôn ngữ tự nhiên tiếng Việt như tách từ và chuẩn hóa teencode. Việc xử lý các sắc thái phức tạp như sự mỉa mai (sarcasm), ngôn ngữ địa phương và nội dung đa ngôn ngữ (code-switching) vẫn là những thách thức thực tế cần giải quyết. Quy trình đánh giá cần sử dụng các metrics cân bằng như Macro-F1 để tránh sai lệch do mất cân bằng lớp dữ liệu.
1	Học tự giám sát (self-supervised learning) là xu hướng mạnh mẽ giúp tận dụng dữ liệu chưa gán nhãn. Báo cáo nên giải thích nguyên tắc tạo nhiệm vụ phụ như masked prediction hoặc contrastive learning để mô hình học biểu diễn có ích. Việc áp dụng self-supervision giúp giảm nhu cầu nhãn và cải thiện hiệu quả fine-tuning cho nhiều nhiệm vụ hạ nguồn. Phần thực nghiệm so sánh performance giữa supervised và self-supervised trên dữ liệu hạn chế sẽ minh họa lợi ích. Kết luận chỉ ra cơ hội áp dụng trong môi trường thiếu nguồn lực gán nhãn.
1	Phân tích chi phí và lợi ích khi triển khai dự án AI là yếu tố thiết yếu cho quyết định đầu tư. Báo cáo cần mô tả chi phí phát triển (nhân lực, tính toán), chi phí vận hành (hosting, giám sát) và lợi ích dự kiến (tăng doanh thu, tiết kiệm chi phí, cải thiện trải nghiệm). Việc xây dựng business case nên kèm theo các kịch bản sensitivity analysis để đánh giá rủi ro. Phần kết luận đề xuất KPI theo dõi sau triển khai và lộ trình mở rộng khi mô hình đạt được các mục tiêu kinh doanh ban đầu.
1	Học máy cho hệ thống thời gian thực trên thiết bị cạnh (edge ML) yêu cầu mô hình nhẹ, latency thấp và tiêu thụ năng lượng tối thiểu. Báo cáo nên mô tả kỹ thuật pruning, quantization, knowledge distillation để giảm kích thước mô hình, cùng chiến lược inference tối ưu trên MCU hoặc SoC. Việc đo lường throughput, latency và power consumption trong thử nghiệm thực tế giúp đánh giá tính khả thi. Phần thảo luận nêu trade-off giữa độ chính xác và tiêu thụ tài nguyên, kèm theo đề xuất kiến trúc hybrid (edge + cloud) để tận dụng ưu điểm của cả hai miền.
1	Các tiêu chí đạo đức và tuân thủ (compliance) trong xử lý dữ liệu cá nhân là yêu cầu bắt buộc trong nhiều ứng dụng AI. Báo cáo nên trình bày luật và chuẩn mực liên quan, ví dụ quy định bảo vệ dữ liệu cá nhân, và kỹ thuật kỹ thuật như differential privacy để giảm rủi ro rò rỉ. Ngoài ra, cần hướng dẫn thực hành minimization principle (chỉ thu thập dữ liệu cần thiết) và auditing trails để đảm bảo khả năng kiểm toán. Phần kết luận kêu gọi tích hợp compliance từ giai đoạn thiết kế sản phẩm, không chỉ ở khâu triển khai.
1	Ứng dụng học máy trong bảo trì dự đoán (predictive maintenance) đóng vai trò then chốt giúp giảm thiểu đáng kể thời gian dừng máy ngoài kế hoạch (downtime) và tối ưu hóa chi phí vận hành cho doanh nghiệp. Báo cáo cần mô tả chi tiết nguồn dữ liệu từ các hệ thống cảm biến rung động, nhiệt độ và áp suất, kết hợp với các kỹ thuật feature extraction chuyên sâu cho tín hiệu time-series để trích xuất các đặc trưng suy giảm thiết bị. Dựa trên đó, các mô hình học máy sẽ thực hiện dự đoán thời gian còn lại trước khi xảy ra lỗi (Remaining Useful Life). Việc tích hợp hệ thống cảnh báo sớm và quy trình phân luồng công việc tự động cho đội ngũ bảo trì là vô cùng quan trọng để chuyển đổi kết quả dự báo thành hành động thực tế.
1	Việc kết hợp học máy và kỹ thuật phần mềm cho các hệ thống đa tác vụ (multi-task systems) giúp tận dụng tối đa các thông tin chia sẻ giữa những nhiệm vụ có liên quan để cải thiện độ chính xác tổng thể và tính tổng quát hóa. Báo cáo nên mô tả chi tiết kiến trúc bao gồm phần backbone chia sẻ đặc trưng (shared encoder) và các task-specific heads riêng biệt, cùng với chiến lược loss balancing thông minh nhằm tránh tình trạng một nhiệm vụ khó chi phối toàn bộ quá trình huấn luyện. Các ứng dụng điển hình bao gồm mô hình vision-language hoặc các hệ thống tích hợp đa dịch vụ trên cùng một nền tảng.
1	Tối ưu hoá chi phí năng lực tính toán bằng cách sử dụng spot instances, auto-scaling và scheduling cho huấn luyện mô hình là thực hành quan trọng. Báo cáo cần trình bày chiến lược job scheduling, checkpointing để khôi phục khi instance biến mất và tối ưu chi phí storage cho datasets. Phân tích nên so sánh chi phí giữa đào tạo tại chỗ và cloud, đồng thời đưa ra kịch bản hybrid cho dự án giai đoạn nghiên cứu và sản xuất. Kết luận nêu lợi ích tiết kiệm chi phí đáng kể với rủi ro được kiểm soát đúng cách.
1	Ứng dụng Computer Vision cho phân tích video có sự khác biệt rõ rệt so với xử lý ảnh tĩnh do tính phụ thuộc chặt chẽ vào yếu tố thời gian và khối lượng dữ liệu cực lớn cần xử lý. Báo cáo nên mô tả chi tiết pipeline thực thi bao gồm: giải mã luồng video (decoding), lấy mẫu khung hình (frame sampling), trích xuất đặc trưng không gian-thời gian, và sử dụng các mô hình xử lý chuỗi tiên tiến như 3D CNN hoặc kiến trúc Transformer. Trong quá trình triển khai, cần đặc biệt chú ý đến độ trễ xử lý (latency), dung lượng lưu trữ và các vấn đề về quyền riêng tư khi dữ liệu video chứa thông tin định danh cá nhân nhạy cảm.
1	"Khoa học dữ liệu tái tạo (reproducible data science) đảm bảo kết quả có thể lặp lại và kiểm chứng khách quan bởi các nhà nghiên cứu độc lập. Báo cáo cần trình bày chi tiết các best practices hiện nay như: thực hiện versioning chặt chẽ cho cả mã nguồn và dữ liệu bằng các công cụ như Git và DVC, quản lý môi trường chạy thông qua công nghệ container (Docker), kiểm soát số ngẫu nhiên (seed control) và thiết lập hệ thống logging tự động cho từng bước xử lý trong pipeline. Việc xây dựng các notebooks có khả năng tái chạy hoàn toàn và tài liệu hóa quy trình một cách minh bạch không chỉ giúp đội ngũ nội bộ hợp tác hiệu quả mà còn giảm thiểu rủi ro ""khủng hoảng tái lập"" trong nghiên cứu AI.
1	Phân tích chuỗi giá trị dữ liệu (data value chain) giúp tổ chức hiểu rõ cách dữ liệu tạo ra giá trị. Báo cáo nên mô tả các giai đoạn: thu thập, lưu trữ, xử lý, phân tích và tiêu thụ kết quả. Mỗi giai đoạn cần KPI đo lường hiệu quả như thời gian xử lý, độ sạch dữ liệu và tần suất sử dụng. Việc xác định choke points và cải tiến quy trình giúp tăng tốc độ ra quyết định. Kết luận khuyến nghị tổ chức hội nhập data governance và vai trò steward dữ liệu để quản lý chất lượng và tuân thủ.
1	Những thách thức pháp lý khi áp dụng AI bao gồm trách nhiệm pháp lý khi hệ thống gây hại, quyền sở hữu trí tuệ và tuân thủ quy định quốc tế. Báo cáo cần trình bày các tình huống minh họa, phân tích trách nhiệm giữa nhà phát triển, nhà triển khai và người dùng cuối. Cần nêu đề xuất khung pháp lý cơ bản như transparency requirements, auditability và cơ chế bồi thường. Phần kết luận khuyến nghị tổ chức kết hợp tư vấn pháp lý sớm trong vòng đời dự án để giảm rủi ro pháp lý khi sản phẩm được thương mại hoá.
1	Triển vọng kết hợp điện toán lượng tử với Machine Learning mở ra những hướng nghiên cứu tiềm năng nhưng cũng đầy thách thức về mặt thực nghiệm trong bối cảnh hiện nay. Báo cáo nên trình bày các khái niệm nền tảng về toán lượng tử như chồng chập và vướng víu, điểm mạnh đặc biệt trong việc xử lý các bài toán tối ưu hóa tổ hợp phức tạp và mô phỏng vật lý, cùng hiện trạng phát triển của các thuật toán hybrid như Variational Quantum Eigensolver (VQE). Phần thảo luận cần nêu rõ những hạn chế thực tế như số lượng qubit còn hạn chế, tỷ lệ lỗi vật lý cao và chi phí vận hành hệ thống cực kỳ đắt đỏ.
1	Bài báo trình bày một kiến trúc lai tích hợp EfficientNet làm backbone và một head loại YOLO được điều chỉnh nhằm cân bằng chính xác/độ trễ cho bài toán phát hiện đối tượng quy mô lớn. Chúng tôi đề xuất cải tiến trong feature fusion và augmentation nhằm tối ưu hoá mAP trong COCO trong khi giữ latency phù hợp cho ứng dụng thời gian thực. Thí nghiệm trên MS COCO (train/val 2017) cho thấy mô hình lai đạt mAP ~40.1% AP@[.5:.95] trên cấu hình trung bình, cải thiện 2.4 điểm so với baseline YOLOv5s trong cùng điều kiện huấn luyện và kích thước ảnh. Kết luận nhấn mạnh trade-off giữa kiến trúc backbone nhẹ và head phát hiện nhanh khi triển khai tại rìa (edge).
1	Phát hiện đối tượng (object detection) là nhiệm vụ then chốt trong nhiều hệ thống thị giác máy tính; các benchmark như MS COCO cung cấp tiêu chuẩn đánh giá bằng AP và mAP cho so sánh mô hình. Nhu cầu ứng dụng thực tế đòi hỏi mô hình vừa chính xác vừa có độ trễ thấp; điều này mâu thuẫn với xu hướng tăng kích thước mô hình để đạt điểm cao trên các leaderboard. Nghiên cứu này đặt mục tiêu thiết kế một pipeline lai giữa EfficientNet (backbone hiệu quả theo tỷ lệ) và head theo phong cách YOLO (thời gian thực), tập trung vào tối ưu hoá fusion feature và augmentation để cải thiện mAP trên COCO mà vẫn kiểm soát latency khi suy luận trên GPU/edge.
1	Các công trình trước tập trung vào hai hướng chính: (1) backbone hiệu quả (EfficientNet) để tối ưu accuracy/params trên ImageNet và chuyển giao sang detection; (2) head thời gian thực (YOLO series) tối ưu mAP vs throughput trên COCO. EfficientNet-B7 đạt tới ~84.4% top-1 trên ImageNet với kích thước và throughput tối ưu hóa, cho thấy khả năng chuyển biểu diễn tốt cho các nhiệm vụ detection. YOLOv5 (biến thể nhỏ đến lớn) vẫn được dùng rộng rãi nhờ cân bằng tốc độ–độ chính xác; tài liệu Ultralytics cung cấp mAP tham chiếu (ví dụ các bản nhỏ ~37 mAP theo docs khi so sánh với YOLOv8). Các nghiên cứu kết hợp backbone nhẹ với head nhanh là hướng có ý nghĩa khi triển khai thực tế.
1	Chúng tôi thiết kế backbone là EfficientNet-B3 fine-tuned trên ImageNet và thêm một feature fusion module (FPN-like) tối ưu cho nhiều độ phân giải; head dựa trên người kế thừa YOLOv5 nhưng sửa loss balancing và anchor-free head experiment để giảm false positives tại các scale nhỏ. Augmentation pipeline gồm mixup, mosaic, random resize và photometric distortions theo best-practice Ultralytics để tăng tính tổng quát. Huấn luyện dùng AdamW với LR scheduler cosine decay; sử dụng warm-up 5 epochs và weight decay 1e-4. Mục tiêu là tối ưu AP@[.5:.95] đồng thời giữ FPS trên GPU tiêu chuẩn ≥30 khi inference.
1	Thực nghiệm sử dụng bộ dữ liệu chuẩn MS COCO 2017, bao gồm tập train (118k ảnh) để huấn luyện và tập val (5k ảnh) để kiểm tra độ chính xác. Để tối ưu hóa quá trình hội tụ, chúng tôi sử dụng trọng số tiền huấn luyện (Pre-trained weights) từ ImageNet-1k cho phần backbone, được trích xuất trực tiếp từ kho lưu trữ chính thức của EfficientNet. Ảnh đầu vào được chuẩn hóa về độ phân giải 640×640 pixel. Chiến lược tăng cường dữ liệu (Data Augmentation) tuân thủ theo các best-practice của Ultralytics, bao gồm các kỹ thuật tiên tiến như: Mosaic (kết hợp 4 ảnh để cải thiện khả năng nhận diện vật thể nhỏ), Mixup, cùng các phép biến đổi hình học (xoay, cắt, lật) và thay đổi không gian màu để tăng tính bền vững (robustness) cho mô hình.
1	Kết quả thực nghiệm trên tập dữ liệu COCO val2017 cho thấy kiến trúc đề xuất EfficientNet-B3 + YOLO-head đạt được sự cải thiện đáng kể về độ chính xác so với mô hình cơ sở. Với chỉ số mAP@[.5:.95] đạt 40.1%, mô hình vượt qua YOLOv5s (37.7%) khoảng 2.4 điểm phần trăm trong cùng một điều kiện thiết lập. Việc đạt được AP50 = 62.4% và AP75 = 44.0% minh chứng cho khả năng của backbone EfficientNet trong việc trích xuất các đặc trưng phân cấp mạnh mẽ, giúp bộ giải mã (head) thực hiện phân loại và định vị vật thể chính xác hơn, đặc biệt là ở các ngưỡng IoU khắt khe.
1	Kết quả cho thấy backbone hiệu quả (EfficientNet) khi kết hợp với head thời gian thực có thể cải thiện mAP mà vẫn giữ latency chấp nhận được. Việc áp dụng augmentation và loss balancing đóng vai trò quan trọng trong cải thiện AP đối với các object nhỏ và lớp hiếm. Hạn chế gồm độ phức tạp huấn luyện và phụ thuộc vào pretraining ImageNet; với dữ liệu domain-specific, cần fine-tune lớn hơn hoặc thêm synthetic data. Hơn nữa, pruning/quantization giúp triển khai trên edge nhưng đòi hỏi cân bằng chính xác/chi phí tính toán. Phần mở rộng bao gồm thử nghiệm với backbone ViT/DeiT để so sánh biểu diễn.
1	Bài toán phát hiện đối tượng quy mô lớn đạt lợi ích rõ rệt khi kết hợp backbone tỷ lệ tốt và head thời gian thực. Thiết kế proposed cho thấy mAP tăng 2–3 điểm trên COCO so với baseline YOLOv5s đồng thời vẫn đảm bảo FPS đủ cho ứng dụng thực tế. Tương lai nghiên cứu nên mở rộng sang kiến trúc hybrid vision-transformer kết hợp với backbone conv nhằm khai thác tối đa khả năng biểu diễn không gian toàn cục, đồng thời điều chỉnh thêm cho các domain-specific datasets để tăng tính chuyên biệt. Bên cạnh đó, việc áp dụng quy trình MLOps bài bản từ khâu quản lý dữ liệu đến tự động hóa pipeline là vô cùng cần thiết để đảm bảo tính reproducibility và khả năng monitoring liên tục hiệu năng mô hình khi triển khai ngoài thực địa.
1	Bài báo mô tả quy trình fine-tuning các PLM (pre-trained language models) như BERT và DeBERTa trên bộ dữ liệu tiếng Việt đa nhiệm, xây dựng benchmark tương tự GLUE để đánh giá hiểu ngôn ngữ (classification, NLI, QA). Chúng tôi sử dụng transfer learning và data augmentation (back-translation, noising) để cải thiện hiệu năng trên dữ liệu hạn chế. Thử nghiệm cho thấy DeBERTa-like architectures thường đạt lợi thế trên SuperGLUE/GLUE benchmarks, với DeBERTa reported vượt mức human baseline trên SuperGLUE (macro ~90.3 trong báo cáo Microsoft cho ensemble). Trên bộ benchmark tiếng Việt, mô hình fine-tuned đạt ~87.2% F1 trung bình, cải thiện 4–6 điểm so với BERT-base.
1	Các pre-trained language models như BERT đã làm thay đổi hoàn toàn lĩnh vực NLP bằng khả năng transfer representations mạnh mẽ từ các kho ngữ liệu khổng lồ. Tuy nhiên, đối với một ngôn ngữ ít tài nguyên như tiếng Việt, việc áp dụng nguyên bản thường không đem lại kết quả tối ưu, do đó cần có các chiến lược fine-tuning chuyên biệt và kỹ thuật augmentation phù hợp để đạt được hiệu năng cao nhất. Mục tiêu cốt lõi của nghiên cứu này là xây dựng một pipeline có tính reproducible cao cho việc fine-tuning các PLM trên một tập hợp tác vụ đa dạng bao gồm phân loại văn bản, NLI và QA.
1	Trong bối cảnh hiện nay, GLUE và SuperGLUE đã trở thành các bộ benchmark tiêu chuẩn không thể thiếu để đánh giá năng lực hiểu ngôn ngữ tự nhiên (NLU); thực tế cho thấy nhiều mô hình tiên tiến như RoBERTa hay DeBERTa đã tiến sát, thậm chí vượt qua mốc hiệu năng của con người trên một số chỉ số quan trọng. Đáng chú ý, báo cáo của Microsoft về DeBERTa cho thấy cấu hình ensemble của họ đã vượt mức human baseline trên SuperGLUE với điểm số macro ấn tượng. Nhiều công trình nghiên cứu hiện đại đang hướng tới việc tối ưu hóa multilingual và low-resource transfer bằng cách sử dụng các mỏ neo pretraining đa ngôn ngữ như mBERT, XLM-R hoặc tiến hành fine-tune trên các bộ domain-specific corpora đặc thù.
1	Quy trình thực hiện trong pipeline của chúng tôi bao gồm các bước chặt chẽ: (1) pre-processing tập trung vào normalization và tokenization theo tiêu chuẩn SentencePiece để tối ưu hóa từ vựng; (2) áp dụng các kỹ thuật augmentation như back-translation giữa tiếng Anh và tiếng Việt kết hợp với word dropout để làm phong phú dữ liệu huấn luyện; (3) thực hiện fine-tuning với cơ chế learning rate scheduler kết hợp warmup và linear decay, thiết lập batch size 32 trong khoảng 3–5 epoch và áp dụng early stopping dựa trên chỉ số validation F1; (4) đánh giá toàn diện qua các tác vụ được mapped theo chuẩn GLUE như CoLA-style hay SST-style sentiment.
1	Dữ liệu: tập hợp dataset tiếng Việt cho classification/NLI/QA (VNESE-GLUE-like collected), tách train/val/test (80/10/10). Metrics: Accuracy, F1, MCC (với CoLA-like), và latency (ms/seq). Fine-tune trên GPU A100, batch size 32, learning rate 2e-5 (tùy backbone). Baseline: BERT-base multilingual; upper bound: DeBERTa-like fine-tuned. So sánh thực nghiệm qua 3 seed để báo error bars. Ngoài ra, phân tích lỗi theo confusion matrix và nhóm size dữ liệu để đánh giá robustness. DeBERTa-like model fine-tuned trên dataset tiếng Việt đạt F1 trung bình 87.2% (±0.9), BERT-base đạt 81.6% (±1.2). Các cải tiến augmentation (back-translation + noising) đem lại +2.0–3.1 điểm F1. Latency inference: BERT-base ~45 ms/seq trên T4, DeBERTa-large ~110 ms/seq; nếu nén (distillation → TinyBERT) latency giảm ~3–4× nhưng mất ~3–5 điểm F1. Các con số khớp xu hướng quan sát trên GLUE/SuperGLUE: mô hình lớn hơn → hiệu năng cao hơn nhưng chi phí tính toán tăng đáng kể.
1	Kết quả thực nghiệm cho thấy chiến lược augmentation đa dạng kết hợp với quy trình fine-tuning phù hợp hoàn toàn có thể bù đắp phần nào những hạn chế về mặt dữ liệu nhãn trong tiếng Việt. Việc lựa chọn mô hình tối ưu nên dựa trên yêu cầu thực tế của từng bài toán: nếu cần triển khai trên các dịch vụ thời gian thực, người dùng cần chấp nhận trade-off giữa độ chính xác và latency thông qua các kỹ thuật như distillation; ngược lại, nếu mục tiêu ưu tiên là nghiên cứu học thuật hoặc độ chính xác tối đa, các phiên bản DeBERTa-like variants sẽ là lựa chọn phù hợp nhất. Hạn chế lớn nhất hiện nay vẫn là sự thiếu vắng một bộ benchmark tiêu chuẩn toàn diện cho tiếng Việt;
1	Mục tiêu nghiên cứu là phát triển và đánh giá một mô hình dự báo sepsis sớm (6–12 giờ trước onset) trên cohort MIMIC-IV, tập trung vào tính reproducibility và interpretability để sẵn sàng thử nghiệm tiền triển khai. Chúng tôi sử dụng features thời gian thực (vitals, labs, meds) và mô hình hybrid LSTM + attention, benchmark với classical baselines (XGBoost). Trên cohort MIMIC-IV curated theo MIMIC-Sepsis framework, mô hình đạt AUROC 0.86 cho dự báo 6 giờ trước sepsis onset, so với 0.81 của XGBoost baseline; kết quả tương đồng với nhiều báo cáo gần đây và các hệ benchmark MIMIC. Công trình đề xuất pipeline preprocessing reproducible, fairness checks và external validation.
1	Sepsis hiện vẫn là nguyên nhân chính gây tử vong hàng đầu trong các đơn vị hồi sức tích cực (ICU); do đó, việc dự báo sớm tình trạng này đóng vai trò quyết định giúp cứu sống bệnh nhân thông qua các can thiệp y tế kịp thời. Mặc dù nhiều nghiên cứu trước đây đã sử dụng dữ liệu từ MIMIC-III/IV để phát triển mô hình dự đoán, nhưng các vấn đề về tính reproducibility, định kiến dữ liệu (bias) và giá trị external validity vẫn còn tồn tại phổ biến. Nghiên cứu này tập trung xây dựng một pipeline minh bạch, bắt đầu từ việc curation cohort theo đúng tiêu chuẩn MIMIC-Sepsis đề xuất, thực hiện feature engineering chuyên sâu, huấn luyện và đánh giá nghiêm ngặt qua các chỉ số AUROC/precision-recall.
1	Tổng quan các nghiên cứu gần đây cho thấy nhiều mô hình Machine Learning (ML) đã được áp dụng rộng rãi cho bài toán dự báo sepsis, tuy nhiên các báo cáo systematic reviews thường nhận xét rằng hiện đang thiếu sự so sánh trực tiếp và chuẩn hóa về mặt cohort giữa các công trình. Một số công trình mới như hệ thống MIMIC-Sepsis benchmark đang nhắm tới việc giải quyết triệt để vấn đề reproducibility bằng cách cung cấp các cohort được curate tỉ mỉ; ngoài ra, các chuyên gia reviews cũng khuyến cáo mạnh mẽ việc đánh giá đồng thời cả chỉ số AUROC cùng precision-recall, đồng thời bắt buộc phải xác minh mô hình trên các external dataset độc lập.
1	Pipeline nghiên cứu được thiết kế đồng bộ gồm các bước: (1) thực hiện cohort extraction theo đúng MIMIC-Sepsis specification để đảm bảo tính nhất quán về tuyến thời gian và xác định chính xác thời điểm onset; (2) feature engineering tập trung vào việc tính toán rolling statistics cho các chỉ số sinh tồn vitals (HR, BP, SpO2), các xét nghiệm labs (WBC, lactate), quản lý meds, và áp dụng kỹ thuật one-hot cho các interventions y tế; (3) xây dựng mô hình dựa trên kiến trúc LSTM-encoder kết hợp với attention head để nắm bắt hiệu quả các chuỗi đặc trưng temporal, đồng thời so sánh trực tiếp với XGBoost baseline; (4) ứng dụng phương pháp interpretability thông qua SHAP để giải thích rõ ràng mức độ đóng góp của từng feature cho dự đoán lâm sàng.
1	Dữ liệu nghiên cứu được trích xuất từ cơ sở dữ liệu MIMIC-IV base, bao gồm cohort khoảng 45k ICU stays sau khi đã tiến hành lọc khắt khe theo các tiêu chí lâm sàng của định nghĩa sepsis-3. Mục tiêu chính của mô hình là dự báo sớm onset sepsis trong horizon 6 giờ; sử dụng các metric đánh giá chủ đạo bao gồm AUROC và AUPRC để phản ánh chính xác khả năng phân loại. Quy trình huấn luyện áp dụng phương pháp 5-fold patient-level cross-validation kết hợp kỹ thuật early stopping để tránh hiện tượng quá khớp; các hyperparameters được tối ưu hóa tự động bằng thuật toán Bayesian optimization. Để kiểm tra tính robustness, chúng tôi thực hiện subgroup analysis chi tiết theo độ tuổi (age) và loại hình đơn vị hồi sức (ICU type);
1	Kết quả thực nghiệm cho thấy mô hình hybrid LSTM+attention đạt hiệu năng ấn tượng với AUROC 0.86 và AUPRC 0.42 cho dự báo trong horizon 6 giờ, vượt trội rõ rệt so với mô hình XGBoost baseline vốn chỉ đạt AUROC 0.81 và AUPRC 0.35. Tuy nhiên, phần subgroup analysis cho thấy có sự sụt giảm nhẹ về hiệu năng (khoảng ~0.03 AUROC) đối với nhóm bệnh nhân có độ tuổi ≥70, điều này gợi ý về khả năng tồn tại bias do sự phân bố dữ liệu không đồng nhất giữa các nhóm tuổi. Phân tích giải thích mô hình bằng SHAP analysis chỉ ra rằng các biến số như nồng độ lactate, huyết áp trung bình (MAP) và sự gia tăng số lượng bạch cầu (WBC) đóng góp trọng số lớn nhất cho các quyết định dự đoán.
1	Mặc dù chỉ số AUROC đạt được khá tốt, việc deploy mô hình sang môi trường lâm sàng thực tế cần phải cân nhắc kỹ lưỡng các yếu tố về interpretability (tính giải thích được), false positive rate và quy trình workflow cụ thể để tránh tình trạng alarm fatigue cho nhân viên y tế. Hiện nay, vấn đề bias theo nhóm tuổi và việc thiếu hụt các bước external validation trên các tập dữ liệu khác nhau vẫn là những rào cản lớn đối với sự tin cậy của mô hình. Do đó, chúng tôi khuyến nghị thực hiện các chương trình prospective pilot, tập trung đánh giá trực tiếp ảnh hưởng lâm sàng như tỉ lệ reduction mortality và thời gian can thiệp sớm, đồng thời tích hợp cơ chế human-in-the-loop để bác sĩ có thể trực tiếp kiểm chứng các tín hiệu cảnh báo.
1	Thị giác máy tính (Computer Vision) là một lĩnh vực trọng tâm của trí tuệ nhân tạo, hướng tới việc giúp máy tính hiểu và phân loại hình ảnh tương tự con người. Bộ dữ liệu ImageNet là benchmark tiêu chuẩn quốc tế với hơn 14 triệu hình ảnh thuộc 1000 lớp khác nhau, đã thúc đẩy tiến bộ mạnh mẽ trong nghiên cứu deep learning kể từ năm 2012. AlexNet, một kiến trúc CNN lớn, đã giảm sai số top-5 xuống 15.3% trong ImageNet 2012, góp phần tạo ra cuộc cách mạng học sâu trong thị giác máy tính. Các kiến trúc sau đó như VGG và ResNet xuất hiện nhằm cải thiện độ chính xác và khả năng tổng quát hóa của mô hình.
1	Trong nghiên cứu này, chúng tôi đánh giá hiệu suất của mạng nơ-ron tích chập (CNN) ResNet-50 khi huấn luyện trên bộ ImageNet chuẩn. ResNet-50 là kiến trúc 50 tầng sử dụng “shortcut connections” để giải quyết vấn đề gradient biến mất trong mô hình sâu. Dataset ImageNet được chia thành tập huấn luyện và tập kiểm tra độc lập, đảm bảo đánh giá hiệu quả tổng quát hóa. Mỗi mô hình được huấn luyện với các bước tiền xử lý chuẩn gồm resizing, augmentation và normalization. Hiệu suất phân loại được đo bằng độ chính xác Top-1 và Top-5 trên tập test. Việc đánh giá được thực hiện trên phần cứng GPU để đảm bảo thời gian huấn luyện và thử nghiệm phù hợp với tiêu chuẩn quốc tế.
1	Kết quả benchmark cho thấy ResNet-50 đạt độ chính xác Top-1 ~78.4% và Top-5 cao hơn trên bộ ImageNet, vượt trội hơn nhiều so với các kiến trúc cũ như VGG16 (~92.2% top-5 ở một số thiết lập), nhưng vẫn thấp hơn một số cải tiến gần đây trong Benchmark. Nghiên cứu khác sử dụng EfficientNet trên ImageNet cũng báo cáo độ chính xác tới 79.1%, cho thấy sự tăng trưởng nhẹ nhưng bền vững của kiến trúc CNN hiện đại. Những số liệu này minh chứng cho sự tiến bộ liên tục trong thiết kế mạng nơ-ron, cân bằng giữa độ sâu, FLOPS và hiệu năng mô hình.
1	Các kết quả trên ImageNet cho thấy kiến trúc ResNet-50 vẫn là một trong những mô hình được lựa chọn phổ biến, nhờ sự cân bằng giữa độ phức tạp và độ chính xác. Tuy nhiên, các nghiên cứu gần đây như TResNet và EfficientNet đã vượt qua ResNet một cách khiêm tốn về độ chính xác, chứng minh rằng tối ưu cấu trúc và FLOPS là yếu tố quan trọng để nâng cao hiệu năng. Điều này đặt ra yêu cầu về thiết kế mô hình phù hợp với bối cảnh ứng dụng cụ thể (ví dụ: điện thoại di động hay máy chủ GPU), và làm rõ hơn tầm quan trọng của việc lựa chọn benchmark phù hợp khi báo cáo kết quả nghiên cứu.
1	Thị giác máy tính vẫn là một lĩnh vực sôi động của AI, được thúc đẩy bởi các benchmark lớn như ImageNet. Kiến trúc ResNet-50 và các biến thể hiệu quả hơn tiếp tục là bộ công cụ trung tâm cho các nhiệm vụ phân loại hình ảnh quy mô lớn. Nghiên cứu cho thấy rằng các tiến bộ trong cấu trúc mô hình và kỹ thuật huấn luyện đem lại sự cải thiện đáng kể về độ chính xác — một bước quan trọng trong việc áp dụng thực tiễn trong y tế, giám sát và tàu tự hành. Việc tiếp tục phát triển và so sánh benchmark mới sẽ giúp cộng đồng nghiên cứu xác định các phương pháp hiệu quả hơn cho các bài toán phức tạp trong tương lai.
1	Xử lý ngôn ngữ tự nhiên (Natural Language Processing – NLP) là lĩnh vực trí tuệ nhân tạo giúp máy tính hiểu và sinh ngôn ngữ con người. GLUE (General Language Understanding Evaluation) là benchmark tiêu chuẩn quốc tế để đánh giá khả năng hiểu ngôn ngữ của các mô hình lớn. Bộ GLUE gồm nhiều tác vụ khác nhau như phân loại ngữ nghĩa, phân tích cảm xúc và trả lời câu hỏi, tổng hợp thành một điểm số tổng thể phản ánh hiệu quả mô hình trong nhiều nhiệm vụ ngôn ngữ. GLUE Score được coi là tấm gương phản chiếu khả năng hiểu ngôn ngữ tổng quát của mô hình và là thước đo chuẩn để so sánh các kiến trúc NLP khác nhau.
1	Trong nghiên cứu này, chúng tôi áp dụng kiến trúc Transformer tiền huấn luyện như BERT và các biến thể hiện đại để đánh giá trên benchmark GLUE. Mỗi mô hình được tinh chỉnh (fine-tuning) trên các task riêng lẻ trong GLUE và điểm F1 cùng accuracy được tổng hợp thành GLUE Score cuối cùng. Việc chia nhỏ mỗi nhiệm vụ giúp phân tích chi tiết hiệu suất mô hình trên các khía cạnh khác nhau như hiểu ngữ cảnh, phân loại cặp câu và xác định quan hệ giữa các câu. Tiếp đó, các kỹ thuật như học chuyển giao (transfer learning) và regularization được sử dụng để tối ưu hiệu suất mô hình trên từng task.
1	Theo leaderboard chính thức của GLUE, các mô hình NLP hiện đại đạt hiệu suất cao với điểm tổng thể GLUE Score vượt trội so với các mô hình tiền nhiệm. Điểm GLUE Score cho các model như BERT và biến thể sau đó đều thể hiện sự tiến bộ vượt bậc trong khả năng hiểu ngôn ngữ, phản ánh hiệu quả của kiến trúc Transformer trong nhiều nhiệm vụ phức tạp. Dữ liệu từ leaderboard cho thấy tổng GLUE Score ở top hiện nay đã vượt quá mức benchmark cũ, minh chứng cho sự đóng góp của học chuyển giao trong NLP.
1	Benchmark GLUE đã thu hút nhiều cuộc thi và nghiên cứu trong cộng đồng nghiên cứu NLP quốc tế, dẫn tới sự ra đời của các kiến trúc mới ngày càng mạnh mẽ hơn. Điểm số benchmark không chỉ phản ánh hiệu suất một cách chuẩn hóa mà còn thúc đẩy so sánh giữa các mô hình phát triển. Tuy nhiên, GLUE cũng bộc lộ hạn chế khi các mô hình đạt điểm cao nhưng đôi khi vẫn kém hiệu quả trên dữ liệu ngôn ngữ mở thực tế hoặc trong các ngôn ngữ ít tài nguyên. Điều này cho thấy cần mở rộng benchmark để bao quát đa dạng task và ngôn ngữ hơn.
1	NLP với benchmark GLUE là một bước ngoặt trong việc đo lường khả năng hiểu ngôn ngữ của các mô hình học máy hiện đại. Các architecture như BERT và Transformer đã đặt nền tảng cho sự phát triển mạnh mẽ của NLP trong thập kỷ qua. Tổng GLUE Score cao từ các mô hình hàng đầu cho thấy tiến bộ vượt bậc, đồng thời thách thức cộng đồng mở rộng benchmark mới để kiểm tra các khả năng phức tạp hơn của mô hình. Việc phát triển các task đa ngôn ngữ, đa ngữ cảnh sẽ mở ra hướng nghiên cứu tiếp theo cho NLP trên quy mô toàn cầu.
1	Ứng dụng AI trong y tế đang tăng trưởng nhanh, đặc biệt trong việc phân tích dữ liệu lâm sàng và NLP cho phân loại hồ sơ bệnh án. Benchmark DRAGON 2025 là một thử thách mới trong lĩnh vực NLP lâm sàng, sử dụng 28,824 báo cáo y tế được gán nhãn từ 5 trung tâm chăm sóc sức khỏe Hà Lan, nhằm đánh giá chính xác các mô hình phân loại, trích xuất thông tin và dự đoán lâm sàng. Đây là nỗ lực mở rộng đánh giá NLP từ tác vụ đơn thuần sang các ứng dụng y tế đặc thù, với mục tiêu nâng cao tính tự động và hiệu quả trong xử lý báo cáo y tế.
1	DRAGON benchmark thiết kế 28 task đa dạng bao gồm phân loại, hồi quy và nhận dạng thực thể y tế trong báo cáo MRI, CT, X-ray và các hồ sơ bệnh khác, với dữ liệu 24,021 báo cáo y tế được gán nhãn và 4,990 báo cáo được dùng để phát triển. Mỗi mô hình được huấn luyện riêng trên các task tương ứng và đánh giá bằng các chỉ số chuẩn như F1, accuracy và recall. Ngoài ra, chúng tôi nghiên cứu hiệu quả của pretraining domain-specific so với pretraining general domain, nhằm xác định phương pháp nào phù hợp hơn cho dữ liệu y tế đặc thù.
1	Kết quả đánh giá benchmark DRAGON cho thấy mô hình được pretrained trực tiếp trên dữ liệu lâm sàng domain-specific đạt test score xấp xỉ 0.770, vượt trội rõ rệt so với mô hình pretrained trên dữ liệu ngôn ngữ chung với test score khoảng 0.734, trong đó mức chênh lệch này có ý nghĩa thống kê (p < 0.005). Điều này chứng tỏ rằng quá trình pretraining trên dữ liệu domain-specific giúp mô hình học được các biểu diễn ngữ nghĩa phù hợp hơn với bối cảnh y tế, từ đó tăng cường khả năng xử lý các ngữ cảnh lâm sàng phức tạp. Ngoài ra, mô hình còn cho thấy hiệu quả cao hơn trong các nhiệm vụ dự đoán lâm sàng mang tính đặc thù, góp phần nâng cao độ tin cậy khi ứng dụng trong thực tế y học.
1	DRAGON challenge minh chứng rằng dữ liệu domain-specific là yếu tố quyết định đến hiệu quả của mô hình AI NLP trong ứng dụng y tế lâm sàng. Mặc dù các mô hình pretrained trên dữ liệu chung có hiệu năng khá tốt, sự khác biệt rõ rệt về test score cho thấy pretraining domain-specific giúp mô hình khớp ngữ cảnh y tế năng hơn. Điều này đặt ra yêu cầu phát triển các kho dữ liệu y tế lớn hơn và đa dạng hơn để huấn luyện các mô hình mạnh mẽ cho hệ thống chăm sóc sức khỏe toàn cầu.
1	Ứng dụng AI trong y tế qua benchmark DRAGON 2025 minh hoạ sự cần thiết của dữ liệu và benchmark domain-specific để thúc đẩy nghiên cứu có tính thực tiễn cao. Mô hình pretrained trực tiếp trên dữ liệu y tế đạt hiệu suất cao hơn mô hình general domain, nhấn mạnh tầm quan trọng của dữ liệu khi triển khai ứng dụng thực tế. Hướng nghiên cứu tiếp theo nên tập trung vào mở rộng số lượng task và dữ liệu từ nhiều trung tâm y tế khác nhau để nâng cao độ tin cậy và khả năng tổng quát hóa cho mô hình AI trong y tế.
1	Deep learning đang được sử dụng rộng rãi trong y học để phân loại hình ảnh y khoa, đặc biệt các mô hình CNN như ResNet-50. Các nghiên cứu ứng dụng ResNet-50 trong phân tích hình ảnh ung thư đại trực tràng và các loại ảnh X-ray đã báo cáo độ chính xác phân loại trên 80–99% tùy bài toán và test set khác nhau, chứng tỏ tiềm năng ứng dụng trong chẩn đoán tự động. Điều này mang lại lợi ích lớn trong việc hỗ trợ quyết định lâm sàng và giảm tải công việc cho nhân viên y tế.
1	Nghiên cứu sử dụng kiến trúc ResNet-50 đã được huấn luyện trước trên tập dữ liệu ImageNet và tiến hành fine-tuning trên các tập dữ liệu hình ảnh y tế chuyên biệt nhằm phân biệt các bệnh lý khác nhau, chẳng hạn như phân loại khối u ác tính (ung thư) và lành tính. Dữ liệu được thu thập từ nhiều nguồn chẩn đoán hình ảnh, sau đó chia theo các tỉ lệ train/validation/test khác nhau (ví dụ 60/20/20) để đảm bảo tính khách quan và khả năng tổng quát hóa của mô hình. Trước khi huấn luyện, ảnh được tiền xử lý bằng các kỹ thuật data augmentation như xoay, lật, thay đổi độ sáng và thu phóng nhằm tăng tính đa dạng dữ liệu và giảm hiện tượng overfitting.
1	Kết quả trên ba tập test khác nhau cho thấy ResNet-50 đạt độ chính xác trên 80% và độ nhạy/specificity lần lượt dao động trong khoảng 87–83% trong bài toán phân loại ung thư đại trực tràng. Một nghiên cứu khác sử dụng hình ảnh X-ray và ảnh nhuộm mô học cũng báo cáo rằng ResNet-50 đạt gần 99% accuracy trên một số tập dữ liệu nhất định. Những kết quả này chỉ ra rằng các mô hình deep learning, đặc biệt là mạng nơ-ron tích chập sâu, có khả năng học và phân biệt chính xác các đặc trưng y khoa phức tạp mà phương pháp truyền thống khó khai thác. Do đó, ResNet-50 nói riêng và deep learning nói chung được xem là công cụ hỗ trợ chẩn đoán hiệu quả, góp phần nâng cao độ chính xác và giảm tải cho bác sĩ lâm sàng.
1	Các số liệu benchmark cho thấy ResNet-50 pretrained trên ImageNet rồi tinh chỉnh cho domain y tế có thể đạt hiệu suất rất cao. Tuy vậy, thành công của mô hình còn phụ thuộc vào chất lượng và sự đa dạng của dữ liệu y tế, cũng như phương pháp tiền xử lý ảnh. Việc sử dụng các bộ dữ liệu nhỏ hoặc imbalance có thể khiến mô hình bị overfit và giảm khả năng tổng quát hóa. Vì vậy, mở rộng dataset y tế có nhãn rõ ràng và áp dụng kỹ thuật tăng cường dữ liệu là cần thiết cho việc triển khai thực tế.
1	Deep learning với kiến trúc ResNet-50 được xem là một công cụ mạnh mẽ trong lĩnh vực nhận diện và phân tích hình ảnh y khoa, khi liên tục cho thấy hiệu suất vượt trội trong nhiều bài toán phân loại bệnh lý phức tạp. Nhờ khả năng học đặc trưng sâu và tận dụng trọng số pretrained, ResNet-50 giúp mô hình hội tụ nhanh hơn và đạt độ chính xác cao ngay cả với các tập dữ liệu y tế có quy mô hạn chế. Kết quả thử nghiệm trên các tập test benchmark cho thấy mô hình không chỉ đạt độ chính xác cao mà còn duy trì tốt độ nhạy và độ đặc hiệu, chứng tỏ tiềm năng ứng dụng thực tế trong các hệ thống hỗ trợ quyết định lâm sàng (CDSS).
0	Biến đổi khí hậu ảnh hưởng đến mọi mặt đời sống kinh tế - xã hội của con người. Dự báo các yếu tố khí hậu ngày càng quan trọng và cần thiết, trở thành mối quan tâm lớn của tất cả các quốc gia trên thế giới, trong đó có Việt Nam. Nhiệt độ và lượng mưa là hai yếu tố chịu tác động lớn trong hệ thống khí hậu, chúng ảnh hưởng trực tiếp đến hệ sinh thái, nước, tài nguyên, các hoạt động nông nghiệp và nuôi trồng thủy sản ở đồng bằng sông Cửu Long nói riêng và cả nước nói chung. Theo Chính (2020), miền Trung nước ta bị thiệt hại khoảng 30.000 tỷ đồng do thiên tai dị thường.
0	Các trận mưa bão lũ năm 2020-2021 gây ra nhiều hậu quả nặng nề cho người dân miền Trung với lượng mưa 3.000 mm, có nơi lên đến 4.526 mm, nhiều ngôi nhà bị sập, hư hỏng nặng; về nông nghiệp, nhiều héc-ta lúa, hoa màu bị thiệt hại nặng nề; về hạ tầng đê điều, thủy lợi, giao thông nhiều ki-lô-mét đường bộ, sông, bị hư hỏng nặng; về giáo dục, y tế nhiều trường học bị thiệt hại nặng nề. Tổng thiệt hại kinh tế khoảng 30.000 tỷ đồng. Từ những nguyên nhân trên, việc dự báo nhiệt độ và lượng mưa là vấn đề cần thiết nhằm hỗ trợ người dân ứng phó biến đổi khí hậu kịp thời.
0	Trong bài viết này, kỹ thuật học sâu với mô hình đa biến bộ nhớ dài-ngắn hạn (Multivariate long short-term memory- MLSTM) được đề xuất để dự báo các chỉ số về nhiệt độ và lượng mưa trung bình hàng tháng ở Việt Nam. Mô hình dự báo dựa trên mạng bộ nhớ dài-ngắn hạn (long - short term memory- LSTM). Mô hình này cải tiến từ mạng nơ-ron thông thường với nhiều thuộc tính làm đầu vào mạng khi huấn luyện. Đầu tiên dữ liệu được sắp xếp thứ tự theo thời gian, chúng có thể được xem là chuỗi thời gian hoặc dữ liệu tuần tự. Kế đến là bước biến đổi dữ liệu từ chuỗi tuần tự thành đa biến đầu vào cho mô hình. Sau cùng, mô hình sẽ được huấn luyện và kiểm thử nhằm đánh giá độ tin cậy.
0	Những năm gần đây thì bài toán dự báo khí hậu là bài toán nhận được nhiều sự chú ý trong cộng đồng nghiên cứu khoa học trong và ngoài nước. Nghiên cứu này thì các tài liệu liên quan đến các phương pháp dự báo được tập trung xem xét. Các công trình trước đây cho thấy học sâu với LSTM có thể là một phương pháp thích hợp cho dữ liệu chuỗi thời gian, Lim and Zohren (2020) đã nghiên cứu mô hình học sâu kết hợp mô hình thống kê và các thành phần mạng nơ-ron để cải thiện các phương pháp dự báo chuỗi thời gian. Ikram et al. (2019) đã trình bày một nghiên cứu sử dụng mạng nơ-ron tuần hoàn (RNN) với kiến trúc LSTM để dự đoán nhiệt độ môi trường xung quanh (TA).
0	Nghiên cứu của Poornima et al. (2019) sử dụng mạng lưới thần kinh nhân tạo dựa trên bộ nhớ ngắn hạn tăng cường (LSTM tăng cường) để dự đoán lượng mưa cho kết quả đánh giá theo phương pháp so sánh có độ chính xác 87,99%. Một nghiên cứu khác của Xingjian et al. (2015) đã đề xuất LSTM tích hợp (ConvLSTM) và sử dụng nó để xây dựng mô hình có thể huấn luyện cho bài toán dự báo lượng mưa bằng tiếp cận máy học. Ngoài ra, còn có các nghiên cứu khác như Kratzert et al. (2018), tác giả đã dùng nhiều lưu vực của bộ dữ liệu CAMELS đề xuất mô hình dự báo lượng mưa theo phương pháp LSTM đạt độ chính xác tương tự như mô hình SAC-SMA.
0	Trong nghiên cứu này, phương pháp tiếp cận dựa trên các mô hình máy học như: mạng nơ-ron đa tầng (MLP), hồi quy vector hỗ trợ (SVR), bộ nhớ dài-ngắn hạn. Ngoài ra, mô hình đa biến bộ nhớ dài - ngắn hạn cũng được đề xuất với chuỗi thời gian có nhiều hơn một biến làm dữ liệu đầu vào cho mạng LSTM nhằm giải quyết được vấn đề của các phương pháp dự báo cũ (phụ thuộc xa của dữ liệu chuỗi thời gian trong mạng nơ-ron thông thường, nâng cao độ chính xác dự báo). Với nghiên cứu này, bộ nhớ dài - ngắn hạn được sử dụng làm mạng để dự báo nhiệt độ và lượng mưa trong tháng với các phương pháp đề xuất được đánh giá trên hai tập dữ liệu,
0	Trong kỹ thuật học sâu (deep learning - DL). Mạng nơ-ron hồi quy (Recurrent Neural Network - RNN), mở rộng hơn LSTM, là một thuật toán được chú ý rất nhiều trong thời gian gần đây bởi chúng cho các kết quả khá tốt. Mạng bộ nhớ dài - ngắn hạn là một dạng đặc biệt của RNN, nó có khả năng học được các phụ thuộc xa. LSTM được giới thiệu bởi Hochreiter and Schmidhuber (1997) và sau đó đã được cải tiến và phổ biến bởi rất nhiều người trong ngành công nghệ thông tin. LSTM hoạt động cực kì hiệu quả trên nhiều bài toán khác nhau nên nó dần đã trở nên phổ biến như hiện nay. LSTM được thiết kế để tránh được vấn đề phụ thuộc xa (long-term dependency).
0	Việc nhớ thông tin trong suốt thời gian dài là đặc tính mặc định của chúng, nên không cần phải huấn luyện mà nó có thể nhớ được; tức là ngay nội tại của nó đã có thể ghi nhớ được mà không cần bất kì can thiệp nào. Điểm khác biệt khi xây dựng mô hình LSTM với MLSTM là ở dữ liệu đầu vào của mạng. Ở đây, giá trị ở thời gian (t-1) trước đó được sử dụng để dự báo cho giá trị ở thời gian t. Dữ liệu đơn biến đầu vào mạng LSTM được mô tả trong ví dụ ở Hình 2. Trong đó, var1(t) là thuộc tính cần dự báo và var1(t-1) là giá trị của thuộc tính var1 ở thời điểm t-1 trước đó dùng để làm thuộc tính đầu vào cho mô hình.
0	Kiến trúc mạng LSTM đơn biến sử dụng dữ liệu đầu vào là luồng dữ liệu bước thời gian có trình tự, tầng LSTM có 50 nút và một lớp ẩn có 1 nút. Kết quả của dự báo sử dụng kỹ thuật Early Stopping cho 5 epoch liên tục và huấn luyện tối đa 100 epochs. Việc xây dựng mô hình sẽ thực hiện sau khi chuẩn hóa dữ liệu đầu vào. Chuẩn hóa dữ liệu đầu vào được thực hiện qua các bước như mô tả dưới đây. Mô hình đa biến đầu vào LSTM sử dụng chuỗi thời gian có nhiều hơn một biến làm dữ liệu đầu vào cho mạng LSTM.
0	Phương pháp trong dự báo chuỗi thời gian là sử dụng các quan sát có độ trễ (t-1) làm biến đầu vào để dự báo bước thời gian hiện tại (t). Với các quan sát có nhiều hơn một thuộc tính, ta sử dụng tất cả các giá trị thuộc tính có độ trễ (t-1) để làm đầu vào. Ví dụ dưới đây sử dụng giá trị var1(t-1) và var2(t-1) để dự báo cho giá trị var1(t), với t là bước thời gian. Hình 3 mô tả ví dụ với 2 biến đầu vào cho mạng LSTM (cột cần dự báo được đóng khung). Trong bài toán dự báo lượng mưa và nhiệt độ, dữ liệu trước khi đưa vào mô hình sẽ được chuẩn hóa từ dữ liệu đầu vào. Việc chuẩn hóa dữ liệu đầu vào được minh hoạ trong các Hình 4 và 5.
0	Dữ liệu sau khi chuẩn hóa sẽ được chuyển thành dữ liệu đa biến đầu vào, sử dụng 3 bước thời gian (việc lựa chọn dựa trên phương pháp tìm kiếm siêu tham số cho từng tập dữ liệu). Nghĩa là các giá trị ở thời gian (t-1, t-2, t-3) kết hợp với các thuộc tính khác ở bước thời gian t sẽ được sử dụng để dự báo thuộc tính đích (target/class attribute) ở thời gian t. Trong ví dụ này, cột dữ liệu cần dự báo là var9(t) như Hình 5. Sau các bước chuẩn hóa dữ liệu, mô hình MLSTM được xây dựng với sự hỗ trợ của thư viện Keras.
0	Để đảm bảo mô hình không học vẹt (overfiting), kỹ thuật early stopping được sử dụng khi huấn luyện. Kỹ thuật này sử dụng một hàm gọi lại (callback) sau khi xem xét 5 kỳ huấn luyện (epochs). Trong 5 epochs liên tiếp, nếu độ lỗi loss và val_loss không biến động (theo chiều hướng giảm dần) quá 0.01 thì mô hình sẽ dừng việc huấn luyện. Nếu overfitting không xảy ra thì quá trình huấn luyện sẽ chạy tối đa 100 epoch. Ngoài ra, kỹ thuật này cũng giúp giảm đáng kể thời gian huấn luyện của mô hình. MLP là một phiên bản cải tiến của Perceptron để làm việc hiệu quả hơn với dữ liệu phi tuyến tính trong thế giới thực, nhiều nơ-ron và nhiều tầng ẩn được thêm vào.
0	Thư viện Keras được sử dụng để xây dựng mô hình MLP kiến trúc chi tiết gồm dữ liệu đầu vào và 5 tầng ẩn. Tầng ẩn đầu tiên có 18 node sử dụng hàm kích hoạt là ReLU (rectified linear unit). Tầng ẩn thứ 2 và 3 có 54 node, sử dụng hàm kích hoạt là Sigmoid. Tầng ẩn thứ 4 có 18 node, sử dụng hàm kích hoạt là ReLU. Tầng thứ 5 là tầng output có 1 node cho giá trị đầu ra. Do không có quy định chuẩn nào về việc chọn số tầng ẩn, mô hình được thiết kế bắt đầu với 2 tầng ẩn và chạy thử nghiệm để tìm ra số tầng ẩn phù hợp. Kết quả thích hợp nhất ở 5 tầng ẩn.
0	SVR là một dạng máy học véc tơ hỗ trợ dành cho việc dự đoán các giá trị liên tục. Với sự hỗ trợ của thư viện học máy Scikit-learn, xây dựng model SVR sử dụng hàm nhân Radial Basis Function (RBF), do không truyền các tham số trực tiếp vào mô hình nên các tham số sẽ lấy mặc định từ thư viện, thông số (kernel) chỉ định kiểu hạt nhân sẽ được sử dụng trong thuật toán, tiếp theo là hệ số (gamma), tham số (C) điều chỉnh độ mạnh yếu của mô hình, một tham số (epsilon) kiểm soát chức năng mất tập luyện để so sánh với giá trị thực, tham số (cache_size) chỉ định kích thước của bộ nhớ đệm hạt nhân cùng một số tham số khác,…. Hàm này phù hợp với dữ liệu đa biến và phi tuyến tính như dữ liệu đang thực nghiệm.
0	Để đánh giá kết quả của các phương pháp, hai độ đo lỗi là RMSE và MAE được sử dụng trong nghiên cứu này. Đây là hai trong số các số liệu phổ biến nhất được sử dụng để đo độ chính xác cho các biến liên tục. Để tiến hành đánh giá các mô hình dự báo nhiệt độ và lượng mưa, ngôn ngữ lập trình Python chạy trên Google Colab và các gói thư viện mã nguồn mở Keras và Sklearn được sử dụng. Chương trình bao gồm các mô hình: đa biến bộ nhớ dài - ngắn hạn, bộ nhớ dài - ngắn hạn, mạng nơ-ron đa tầng. Hồi quy vector hỗ trợ dự báo nhiệt độ và lượng mưa. Để đánh giá mô hình, phương pháp tính độ lỗi phổ biến là RMSE và MAE được sử dụng, minh hoạ như hai công thức trên.
0	Hai tập dữ liệu đã được sử dụng trong thực nghiệm này. Tập dữ liệu thứ nhất của ICRISA là tập dữ liệu nhiệt độ và lượng mưa tại Ấn Độ (gồm 41 năm từ năm 1978 đến 2018, tập dữ liệu gốc theo ngày với 14.852 dòng. Mỗi dòng có 9 thuộc tính là nhiệt độ (MaxT, MinT), lượng mưa (rainfall), độ ẩm cao, thấp (RH1, RH2), hướng gió (wind), bốc hơi (evaporation), chỉ số tính lượng mưa (SSH), bức xạ mặt trời (radiation), lượng hơi nước bốc lên (FAO56_ET). Tập dữ liệu thứ hai Temper_Rainfall là tập dữ liệu nhiệt độ và lượng mưa trung bình hàng tháng tại Việt Nam, gồm 115 năm từ năm 1901 đến 2015, được trích từ tập dữ liệu chuẩn công khai trên trang OpenDevelopment Mê Kông bao gồm 1.380 dòng có 2 thuộc tính là nhiệt độ (temperature) và lượng mưa (rainfall).
0	Tập dữ liệu ICRISAT_Weather dạng chuỗi thời gian theo ngày, có nhiều thuộc tính ảnh hưởng đến dự báo và phù hợp cho mô hình MLSTM đa biến. Dữ liệu được tiền xử lý bằng cách tính như sao: 1 tuần có 7 ngày, lấy trung bình mỗi 7 ngày để chuyển về dạng chuỗi thời gian theo tuần, tương tự như vậy lấy tất cả dữ liệu của một tháng nào đó trong năm chia trung bình ra dữ liệu theo tháng (ví dụ lấy tất cả dữ liệu của tháng 1 năm 2018 chia trung bình ra dữ liệu của 1 tháng), dữ liệu sau khi xử lý được mô tả trong Bảng 2.
0	Riêng tập dữ liệu Temperature_Rain_Month nhiệt độ và lượng mưa trung bình hàng tháng từ tháng 1 năm 1901 đến tháng 12 năm 2015. Ví dụ: tổng dữ liệu nhiệt độ và lượng mưa trung bình hàng tháng cho năm 1901 lần lượt là 2883872oC và 17237291 mm, đây là biểu đồ thể hiện nhiệt độ và lượng mưa trung bình hàng tháng năm 1901 tại Việt Nam được thể hiện như Hình 8. Bảng 3 so sánh kết quả dự báo trên các tập dữ liệu từ các mô hình MLP, SVR, LSTM và MLSTM, kết quả tốt nhất cho từng tập dữ liệu được in đậm. Dữ liệu được lấy ngẫu nhiên 80% các mẫu tin đầu cho huấn luyện và 20% mẫu tin cuối cho kiểm tra (theo trình tự thời gian).
0	Mục đích của việc chia dữ liệu trên nhằm bám sát theo thực tế, dựa trên các thuộc tính đã học để dự báo cho các thuộc tính tiếp theo. Vì tính ngẫu nhiên của giải thuật nên phương pháp MLP không cần trình tự thời gian, được huấn luyện một lần trên mỗi tập dữ liệu. Từ tập dữ liệu ICRISAT_weather_week gồm chín thuộc tính liên quan đến nhiệt độ và lượng mưa cho thấy mô hình MLSTM đạt hiệu quả với độ lỗi RMSE là 1.335 và MAE là 1.012, tương ứng trên tập dữ liệu lượng mưa là 4.190 và 2.493.
0	Trên tập dữ liệu ICRISAT_weather_month cho thấy mô hình MLSTM đạt hiệu quả khá tốt với độ lỗi RMSE trên tập nhiệt độ là 1.311 và MAE là 1.051, tương ứng trên tập dữ liệu lượng mưa là 2.299 và 1.450. Với kết quả thu được từ tập dữ liệu dự báo bước thời gian theo tuần, theo tháng từ năm (1978-2018) thì mô hình dự báo MLSTM luôn cho kết quả tốt hơn. Tuy nhiên, độ lỗi MAE của lượng mưa lại chưa tốt hơn mô hình MLP. Từ đó cho thấy lượng mưa phân bố không đồng đều, biến động liên tục trong năm cũng ảnh hưởng đến kết quả dự báo. Bên cạnh, khi sử dụng mô hình đa biến với các thuộc tính ngữ cảnh cũng có sự ảnh hưởng đến kết quả dự báo.
0	Vì vậy, việc xác định các thuộc tính, ngữ cảnh, các dữ liệu liên quan đưa vào huấn luyện rất quan trọng trong bài toán dự báo thời gian với mô hình MLSTM. Tập dữ liệu TEMPER_Rain_Month gồm hai thuộc tính. Nhiệt độ trung bình của tháng và lượng mưa, năm 1901 lần lượt là 2883872oC và 17237291 mm được trình bày trong Hình 8 cho thấy nhiệt độ cao hơn và lượng mưa phổ biến trong thời gian giữa năm. Vào tháng Giêng và tháng Hai, lượng mưa được quan sát rất thấp. Mặt khác, nhiệt độ trung bình tháng dao động quanh năm ít hơn 38oC.
0	Dữ liệu thời tiết trong 115 năm được sử dụng và lấy ngẫu nhiên 80% các mẫu tin đầu cho huấn luyện và 20% mẫu tin cuối cho kiểm tra (theo trình tự thời gian). Do độ chênh lệch giữa nhiệt độ và lượng mưa quá lớn nên dẫn đến độ lỗi dự báo của lượng mưa qua mô hình MLSTM khá cao RMSE là 54.2909 và MAE là 40.4946 so với nhiệt độ chỉ có RMSE là 1.133 và MAE là 0.863. Với kết quả thu được từ tập dữ liệu dự báo bước thời gian theo tháng từ năm (1901- 2015), mô hình dự báo MLSTM vẫn chiếm độ lỗi thấp nhất so với các mô hình LSTM, SVR, MLP.
0	Từ đó cho thấy mô hình đa biến đầu vào mạng LSTM sử dụng chuỗi thời gian có nhiều hơn một biến làm dữ liệu đầu vào cho kết quả dự báo tốt hơn. Tuy nhiên, thử nghiệm cho thấy rằng độ lệch chuẩn của dữ liệu quá lớn và thuộc tính dự báo ít cũng ảnh hưởng đến độ lỗi dự báo. Kết quả so sánh độ lỗi tập dữ liệu ICRISAT_Weather_Month cho thấy mô hình LSTM và MLSTM đạt kết quả tốt hơn các mô hình khác trên cùng từng tập dữ liệu. Có thể thấy rằng mạng LSTM hoạt động khá tốt trên luồng dữ liệu bước thời gian có trình tự.
0	Tổn thất đối với dự báo nhiệt độ hội tụ sau 10 kỷ nguyên, tương ứng với lượng mưa hội tụ sau 25 kỷ nguyên. Đồ thị về kết quả dự báo nhiệt độ và lượng mưa theo tháng với tập dữ liệu ICRISAT_Weather_Month của mô hình MLSTM được trình bày trong Hình 11. Quan sát biểu đồ này, ta có thể thấy rằng mô hình MLSTM dự báo khá tốt theo tiêu chí RMSE so với các mô hình khác. Phương pháp dự báo nhiệt độ và lượng mưa bằng kỹ thuật học sâu được đề xuất nhằm hỗ trợ người dân có kế hoạch gieo trồng phù hợp, góp phần thúc đẩy ngành nông nghiệp tại Việt Nam nói chung cũng như khu vực đồng bằng sông Cửu Long nói riêng.
0	Dữ liệu sau khi thu thập và xử lý được tiến hành huấn luyện và kiểm tra với mô hình đa biến bộ nhớ dài - ngắn hạn. Kết quả thực nghiệm được so sánh với các mô hình dự báo khác như LSTM, MLP, SVR và cho thấy phương pháp tiếp cận được đề xuất có thể tạo ra các dự báo khá chính xác, có thể áp dụng vào hệ thống thực tế. Nghiên cứu này sẽ tiếp tục được cải tiến nhằm nâng cao độ chính xác của mô hình dự báo cũng như phát triển mô hình để có thể dự báo cho nhiều hơn một bước thời gian đầu vào, đầu ra và hình thành công cụ để thuận tiện cho người dùng cuối sử dụng. Bên cạnh, so sánh với các nghiên cứu liên quan cũng sẽ được thực hiện trong tương lai.
0	Điều khiển xe máy không đội mũ bảo hiểm là một hành vi vi phạm Luật Trật tự, an toàn giao thông đường bộ năm 2024. Khi tai nạn xảy ra, hậu quả đối với những người đi xe máy không đội mũ bảo hiểm thường nguy hiểm và có nguy cơ ảnh hưởng tới tính mạng. Mặc dù Luật đã quy định nhưng vẫn có những người lái xe máy không đội mũ khi tham gia giao thông. Trong bài báo này, chúng tôi đề xuất ứng dụng trí tuệ nhân tạo sử dụng mô hình mạng YOLO11 đối với các hình ảnh thu nhận được từ các camera giám sát tại các điểm nút giao thông để giải quyết bài toán phát hiện người điều khiển xe máy không đội mũ bảo hiểm.
0	Kết quả sau khi huấn luyện trên tập dữ liệu ảnh có 4267 ảnh chứng tỏ cách tiếp cận này có độ chính xác khá tốt (độ chính xác trung bình - mean Average Precision mAP@0,5 là 89,8% đối với phiên bản YOLO11n với quá trình huấn luyện 200 epoch). Kết quả thử nghiệm với các dữ liệu test tự thu thập sử dụng mô hình sau huấn luyện cũng cho thấy hệ thống ứng dụng mô hình YOLO11 có độ chính xác khá tốt trong phát hiện người điều khiển xe máy không đội mũ bảo hiểm và có tiềm năng ứng dụng trên thực tế.
0	Trong bối cảnh an toàn giao thông đường bộ ngày càng trở thành vấn đề cấp thiết, việc phát hiện, xử lý người tham gia giao thông vi phạm luật, như không đội mũ bảo hiểm khi đi xe máy, đóng vai trò thiết yếu trong việc giảm thiểu tai nạn giao thông và bảo vệ tính mạng người tham gia giao thông. Theo cách tiếp cận truyền thống, các phương pháp giám sát và phát hiện tình huống không đội mũ bảo hiểm của người đi xe máy thường dựa vào lực lượng cảnh sát giao thông có nhiệm vụ tuần tra, lập chốt kiểm tra và xử phạt trực tiếp. Tuy nhiên, cách tiếp cận này có nhiều hạn chế, thứ nhất là đòi hỏi một lực lượng nhân lực tham gia lớn nhưng vẫn không bao quát được hết các vị trí cần kiểm tra, phát hiện và xử lý vi phạm.
0	Thứ hai là người vi phạm sẽ tìm cách tránh, né các điểm kiểm tra, giám sát của lực lượng chức năng nên dẫn tới không hiệu quả. Thứ ba là nguy cơ xảy ra sai sót do yếu tố con người. Với sự phát triển của công nghệ hiện đại, đặc biệt trong lĩnh vực trí tuệ nhân tạo, các giải pháp tự động hóa đã ra đời nhằm cải thiện hiệu quả phát hiện và xử lý vi phạm giao thông từ việc phân tích các hình ảnh thu nhận được từ các camera giám sát trên đường và tại các điểm nút giao thông. Trong bài báo này, chúng tôi nghiên cứu ứng dụng kiến trúc mạng YOLO11 nhằm phát hiện hành vi không đội mũ bảo hiểm qua hình ảnh từ các camera giám sát tại các giao lộ.
0	So với các cách tiếp cận truyền thống, việc áp dụng công nghệ hiện đại như YOLO11 đánh dấu bước tiến đáng kể trong lĩnh vực quản lý giao thông, mang đến những lợi ích thiết thực và tiềm năng ứng dụng rộng rãi trong thực tế. Để xây dựng một hệ thống thị giác máy tính nhằm phát hiện người đi xe máy không đội mũ bảo hiểm, Dahiya và các cộng sự [1] đã sử dụng các phương pháp trích chọn đặc trưng cục bộ như LBP (Local Binary Patterns) [2], SIFT (Scale Invariant Feature Transformation) [3] và HOG [4] trên một tập dữ liệu video quay trong 2 tiếng và đạt được kết quả tốt nhất với HOG (độ chính xác 93,8%) và thời gian xử lý khoảng 11,58ms cho 1 frame ảnh.
0	Các cách tiếp cận được đề cập trên đây có hai hạn chế: Một là đa số thực hiện trên các tập dữ liệu khá nhỏ với số lượng ảnh ít nên hạn chế về độ tin cậy và khả năng áp dụng thực tế, thứ hai là tốc độ xử lý bị hạn chế do sử dụng các mô hình mạng học sâu có kích thước khá lớn không đảm bảo khi đưa vào áp dụng thực tế đòi hỏi tốc độ cao theo thời gian thực. Gần đây, một kiến trúc mới trong họ các mô hình mạng học sâu YOLO là YOLO11 [12] đã được đề xuất với khả năng phát hiện chính xác và tốc độ tốt hơn các phiên bản trước đó.
0	Trong bài viết này, nhóm tác giả nghiên cứu ứng dụng mạng YOLO11 để xây dựng hệ thống phát hiện người điều khiển xe máy không đội mũ bảo hiểm. Nội dung bài báo được chia thành các phần như sau: Phần 2 bao gồm các chi tiết về mô hình mạng học sâu YOLO11, phần 3 trình bày về ứng dụng của YOLO11 cho bài toán phát hiện người điều khiển xe máy không đội mũ bảo hiểm và các kết quả huấn luyện, thực nghiệm, cuối cùng là phần Kết luận.YOLO (thuật ngữ thành lập từ các chữ cái đầu của các từ You Only Look Once) [13] là một họ các mô hình mạng học sâu được thiết kế để thực hiện các thao tác cơ bản trong các bài toán thị giác máy tính là phát hiện đối tượng (Object Detection), Phân lớp (Classification), Phân đoạn ảnh (Segmentation).
0	Một trong các lý do các mô hình YOLO thường được sử dụng giải quyết các bài toán phân tích nội dung ảnh sử dụng hướng tiếp cận phát hiện đối tượng là do tính chính xác và tốc độ thực hiện của chúng khá tốt và khả thi trên thực tế, kể cả với các hệ thống máy tính chỉ có CPU. Các phiên bản sau của YOLO như YOLOv8, YOLOv10, YOLO11 đều được xây dựng dựa trên việc kế thừa kiến trúc và các ưu điểm của phiên bản trước đó và bổ sung thêm các kỹ thuật mới và cải tiến trong phần kiến trúc của mạng.
0	Xét về mặt kiến trúc mạng, phiên bản YOLO11 (xem chi tiết tại địa chỉ: [14]) dựa trên các phiên bản trước đó là YOLOv8, YOLOv9. YOLO11 (You Only Look Once version 11) là một cải tiến mới trong các mô hình phát hiện đối tượng thời gian thực dựa trên deep learning. YOLO11 có nhiều điểm ưu điểm so với các phiên bản trước nhờ sử dụng kiến trúc backbone hiệu quả hơn, được tối ưu hóa để giảm chi phí tính toán mà vẫn đạt được độ chính xác cao. Một trong những điểm nổi bật của YOLO11 là việc áp dụng cơ chế chú ý (attention mechanism) để tăng khả năng phát hiện các đối tượng kích thước nhỏ hoặc các đối tượng trong điều kiện quan sát phức tạp.
0	Kiến trúc của YOLO11 được hình thành dựa trên các lớp convolutional cải tiến, kết hợp với các khối residual để tăng cường khả năng huấn luyện mạng sâu. Mô hình này sử dụng head prediction đa cấp độ, cho phép phát hiện đối tượng ở nhiều kích thước khác nhau. YOLO11 cũng được trang bị thuật toán non-maximum suppression (NMS) cải tiến để giảm các lỗi chồng lặp vùng dự đoán. Mô hình hỗ trợ tích hợp các hàm mất mát (loss functions) tiên tiến, như CIOU và Focal Loss, để cải thiện chất lượng dự đoán các bounding box. Với khả năng xử lý thời gian thực, YOLO11 phù hợp với các tình huống yêu cầu tốc độ và độ chính xác cao.
0	Về cơ bản sự khác biệt và cải tiến của YOLO11 so với các mô hình YOLOv8, YOLOv9 và YOLOv10 thể hiện ở 3 yếu tố: Các khối kiến trúc C3K2 ở phần mạng xương sống (backbone), kỹ thuật SPPF (Spatial Pyramid Pooling Fast) ở phần cổ của mạng (neck) và cơ chế chú ý C2PSA (Cross Stage Partial with Spatial Attention). Kiến trúc khối nhân chập mới C3K2: YOLOv11 sử dụng các khối C3K2 (xem chi tiết trong Hình 1) để xử lý trích xuất đặc trưng ở các giai đoạn khác nhau của phần backbone. Các nhân kích thước 3x3 nhỏ hơn cho phép tính toán hiệu quả hơn trong khi vẫn giữ nguyên khả năng của mô hình trong việc nắm bắt các tính năng cần thiết trong hình ảnh.
0	Trọng tâm của phần backbone YOLO11 là khối C3K2, đây là sự phát triển của khối CSP (Cross Stage Partial) được giới thiệu trong các phiên bản trước đó. Khối C3K2 tối ưu hóa luồng thông tin qua mạng bằng cách chia tách bản đồ đặc trưng và áp dụng một loạt các phép tích chập nhân nhỏ hơn (3x3), nhanh hơn và rẻ hơn về mặt tính toán so với các phép tích chập nhân kích thước lớn. Bằng cách xử lý các bản đồ đặc trưng nhỏ hơn, riêng biệt và hợp nhất chúng sau một số phép tích chập, khối C3K2 cải thiện biểu diễn tính năng với ít tham số hơn so với các khối C2f của YOLOv8.
0	Khối C3K có cấu trúc tương tự như khối C2f nhưng không có sự phân tách nào được thực hiện ở đây, đầu vào được truyền qua một khối Conv theo sau là một loạt các lớp Bottleneck có nối và kết thúc bằng khối Conv cuối cùng. Kỹ thuật SPPF: YOLO11 vẫn giữ lại mô-đun SPPF (Spatial Pyramid Pooling Fast), được thiết kế để gộp các đặc điểm từ các vùng khác nhau của một hình ảnh ở nhiều tỷ lệ khác nhau. Điều này tăng sức mạnh của mạng trong việc phát hiện các đối tượng có kích thước thay đổi, đặc biệt là các đối tượng nhỏ, vốn là một thách thức đối với các phiên bản YOLO trước đó.
0	SPPF gộp các đặc điểm bằng nhiều hoạt động gộp tối đa (với các kích thước bộ lọc khác nhau) để tổng hợp thông tin theo ngữ cảnh đa tỷ lệ. Mô-đun này đảm bảo rằng ngay cả các đối tượng nhỏ cũng được mô hình nhận dạng, vì nó kết hợp hiệu quả thông tin trên các độ phân giải khác nhau. Việc đưa SPPF vào đảm bảo rằng YOLO11 có thể đạt được tốc độ thời gian thực đồng thời tăng cường khả năng phát hiện các đối tượng trên nhiều tỷ lệ. Tuy nhiên điểm khác biệt ở đây là khối C2PSA được sử dụng ngay sau khối SPPF.
0	Khối C2PSA: Khối C2PSA sử dụng hai mô-đun PSA (Partial Spatial Attention) hoạt động trên các nhánh riêng biệt của bản đồ đặc trưng và sau đó được nối lại, tương tự như cấu trúc khối C2f. Thiết lập này đảm bảo mô hình tập trung vào thông tin không gian trong khi vẫn duy trì sự cân bằng giữa chi phí tính toán và độ chính xác phát hiện. Khối C2PSA tinh chỉnh khả năng tập trung có chọn lọc vào các vùng quan tâm của mô hình bằng cách áp dụng sự chú ýkkhông gian vào các đặc điểm được trích xuất. Điều này cho phép YOLO11 hoạt động tốt hơn các phiên bản trước như YOLOv8 trong các tình huống cần có chi tiết đối tượng tốt để phát hiện chính xác.
0	Khả năng phát hiện đối tượng của YOLO11: Theo [12] và các số liệu được công bố tại website của Ultralytics [15] thì YOLO11 có khả năng phát hiện chính xác hơn và tốc độ nhanh hơn khi đối sánh với các phiên bản YOLO trước. Với mục tiêu ứng dụng mô hình YOLO11 xây dựng hệ thống phát hiện người điều khiển xe máy không đội mũ bảo hiểm, chúng tôi đã lựa chọn kiến trúc mạng học sâu YOLO11 do độ chính xác cao và tốc độ phát hiện nhanh chóng mà nó mang lại. Để xây dựng hệ thống phát hiện người điều khiển xe máy không đội mũ bảo hiểm, nhóm tác giả đã tìm kiếm, đánh giá và sử dụng 1 tập dữ liệu công cộng gồm 4267 ảnh với tập huấn luyện có 3912 ảnh và tập kiểm thử trong quá trình huấn luyện có 355 ảnh.
0	Hai tập ảnh này gồm các ảnh chụp có kích thước tối đa 640x640 được chọn lọc từ các camera giám sát và ảnh chụp đường phố của các nước trong khu vực Châu Á như Thái Lan, Ấn Độ, Myanmar, Việt Nam nên khá tương đồng với Việt Nam. Quá trình huấn luyện đã sử dụng phương pháp học chuyển tiếp các mô hình YOLO11 huấn luyện trên tập ảnh COCO và các kỹ thuật tăng cường dữ liệu bằng các thuật toán xử lý ảnh như biến đổi HSV, xoay ảnh, lật ảnh theo chiều ngang, dịch ảnh, thay đổi tỉ lệ, tạo ảnh khảm (mosaic) nên trên thực tế số ảnh sử dụng để huấn luyện là 23472 ảnh (6x3912).
0	Cũng như các phiên bản trước, YOLO11 có 5 cấu hình là siêu nhỏ - Nano (n), nhỏ - Small (s), trung bình - Medium (m), lớn - Large (l) và siêu lớn - Extra Large (x). Các cấu hình này khác nhau về số lượng tham số, do đó sẽ khác nhau về độ chính xác và tốc độ huấn luyện, thực hiện. Cấu hình Nano là nhỏ nhất với số tham số ít nhất và cũng phù hợp nhất với các đối tượng kích thước nhỏ và ảnh huấn luyện có độ phân giải hạn chế trong khi cấu hình Extra Large có số tham số nhiều nhất, độ chính xác theo lý thuyết là cao nhất và tốc độ huấn luyện và phát hiện đối tượng chậm nhất.
0	Trong bài báo này, nhóm tác giả đã huấn luyện và thử nghiệm cả 5 cấu hình của YOLO11 với thư viện PyTorch 2.6 trên nền tảng CUDA 12.4 với 1 GPU RTX 3060 12 GB RAM. Toàn bộ các mô hình được huấn luyện với độ phân giải ảnh là 640x640 trong 200 epoch. Kết quả trong Bảng 1 chứng tỏ cách tiếp cận ứng dụng mô hình mạng YOLO11 cho bài toán phát hiện người đi xe máy không đội mũ bảo hiểm là đúng và có hiệu quả với độ chính xác mAP@0,5 của toàn bộ 5 cấu hình YOLO11 đều khá cao: Cấu hình YOLO11l và YOLO11x đạt kết quả thấp nhất là khoảng 87% và cấu hình đạt độ chính xác cao nhất là 89,8% với cấu hình YOLO11n.
0	Các kết quả đối mAP@0,5-0,95 trên dữ liệu đào tạo cũng khá tốt khi có giá trị tối thiểu 60,3% với cấu hình Nano và tăng đôi chút với các cấu hình còn lại. Các kết quả mAP@0,5 và mAP@0,5-0,95 trên cơ sở dữ liệu ảnh kiểm tra cũng tương đương (khác biệt không quá lớn) với các số liệu trên tập ảnh huấn luyện. Để kiểm tra các kết quả của cách tiếp cận sử dụng YOLO11 là đúng đắn, chúng tôi đã huấn luyện thêm các mô hình của YOLOv8 trên cùng tập dữ liệu và chọn kết quả tốt nhất đạt được (với mô hình YOLOv8m) và so sánh trong Bảng 1. Có thể thấy kết quả tốt nhất của YOLOv8 thấp hơn YOLO11 (phiên bản YOLO11m) khoảng 1% độ chính xác @mAP0,5-0,95 trên cả tập huấn luyện và tập kiểm tra.
0	Để kiểm tra tính chính xác các mô hình đã đào tạo, nhóm tác giả đã thu thập một số video thực tế (5 video) từ mạng Internet và kết quả cho thấy các mô hình sau huấn luyện có khả năng phát hiện được người điều khiển xe máy không đeo mũ bảo hiểm khá tốt (xem minh họa trong Hình 3). Để đánh giá tốc độ hướng tới việc triển khai thực tế, chúng tôi đã chuyển đổi format các mô hình từ PyTorch (.pt) sang TensorRT (.engine) và thử nghiệm. Kết quả thực hiện với video thì tốc độ xử lý đạt được là 51,8 khung hình 1 giây. Tốc độ này cho thấy cách tiếp cận của bài báo có khả năng ứng dụng vào thực tế với yêu cầu xử lý dữ liệu theo tốc độ thời gian thực.
0	Với mục đích ứng dụng các mô hình AI để nhằm xây dựng một hệ thống phát hiện người điều khiển xe máy không đội mũ bảo hiểm, nhóm tác giả đã đề xuất sử dụng kiến trúc mạng YOLO11. Chúng tôi đã huấn luyện các cấu hình khác nhau của YOLO11 trên một cơ sở dữ liệu ảnh và kiểm tra trên tập dữ liệu ảnh test cũng như dữ liệu thực tế. Các kết quả nhận được (tỉ lệ phát hiện chính xác tính theo mAP@0,5 và mAP@0,5-0,95, tốc độ thực hiện) cho thấy cách tiếp cận của chúng tôi là đúng và kết quả có thể triển khai thực tế. Sắp tới, nhóm tác giả sẽ bổ sung thêm dữ liệu thực tế, đánh nhãn, huấn luyện, đánh giá nhằm nâng cao độ chính xác cũng như kết hợp với các mô hình khác để có thể ứng dụng vào thực tế.
0	Kết quả phân tích ANOVA cho thấy cả ba thông số cắt đều có tác động đáng kể đến Ra, trong đó tham số f có ảnh hưởng mạnh nhất, nhấn mạnh vai trò của nó trong việc kiểm soát độ nhám bề mặt. Mô hình dự đoán ANFIS được phát triển thông qua hai phương pháp đào tạo Hybrid và Backpropagation tương ứng với tám hàm thuộc khác nhau. Kết quả cho thấy, mô hình đào tạo Hybrid sử dụng hàm thuộc Gaussmf đạt hệ số xác định R^2 cao nhất là 0,986081 và căn bậc hai của sai số bình phương trung bình (RMSE) thấp nhất là 0,013055. Những kết quả này chứng minh rằng, mô hình ANFIS có khả năng dự đoán Ra một cách tương đối chính xác dựa trên các thông số gia công.
0	Trong lĩnh vực gia công kim loại, độ nhám bề mặt là một trong những yếu tố quan trọng ảnh hưởng đến chất lượng sản phẩm [1]. Đặc biệt, trong quá trình tiện, độ nhám bề mặt không chỉ quyết định tính năng kỹ thuật của chi tiết mà còn ảnh hưởng đến tuổi thọ và hiệu suất của các linh kiện máy móc. Do đó, việc dự đoán chính xác độ nhám bề mặt cho phép các nhà sản xuất tối ưu hóa các thông số gia công như tốc độ cắt, tốc độ chạy dao, chiều sâu cắt, v.v. để đạt được chất lượng bề mặt mong muốn.
0	Misak và cộng sự đã tích hợp một mô hình để phân tích về độ nhám bề mặt và đo lường dữ liệu khi gia công trên máy tiện CNC nhằm phát triển một phương pháp mô hình hóa không phụ thuộc quá nhiều vào dữ liệu dựa trên phương pháp Co-Kriging [2]. Zain và cộng sự đã nghiên cứu khả năng của mạng nơ ron (ANN) trong việc dự đoán cho độ nhám bề mặt dựa trên thí nghiệm gia công phay thực tế. Họ kết luận rằng mô hình cho độ nhám bề mặt có thể được cải thiện bằng cách sửa đổi số lớp và nút trong các lớp ẩn của cấu trúc mạng ANN, đặc biệt là để dự đoán giá trị của phép đo hiệu suất độ nhám bề mặt [3].
0	Wang và cộng sự đã giới thiệu kết quả nghiên cứu khi xây dựng mô hình dự báo độ nhám bề mặt sử dụng mạng nơ ron dựa trên kiến thức (knowledge-based neural networks - KBaNN) và mạng hàm cơ sở bán kính (Radial Basis Functions networks - RBF). Kết quả thực nghiệm cho thấy, KBaNN-RBF có hiệu quả cao trong việc cải thiện độ chính xác dự đoán và so sánh cho thấy hiệu quả của KBaNN_RBF vượt trội hơn nhiều so với các phương pháp truyền thống như mạng nơ ron lan truyền ngược (BPNN) và máy vectơ hỗ trợ (SVM) [4]. Nhiều nghiên cứu khác ứng dụng ANN cũng như kết hợp các thuật toán khác cũng đã được công bố [5-11].
0	Các nghiên cứu này đã chứng minh được hiệu quả vượt trội trong dự báo độ nhám bề mặt. Tuy nhiên, ANN thường yêu cầu một lượng dữ liệu lớn để đạt được hiệu suất tối ưu và thời gian đào tạo có thể kéo dài. Hơn nữa, ANN vẫn gặp khó khăn trong việc xử lý các tình huống bất định và phi tuyến mạnh, đặc biệt khi dữ liệu không đầy đủ hoặc có tính chất phức tạp. Một bước tiến quan trọng trong lĩnh vực này là sự kết hợp giữa mạng nơ ron và hệ mờ, cụ thể là mạng nơ ron thích nghi mờ (Adaptive Neuro-Fuzzy Inference System - ANFIS). ANFIS có khả năng xử lý tính phi tuyến phức tạp và tính bất định trong mối quan hệ giữa các thông số gia công và độ nhám bề mặt.
0	Kannadasan và cộng sự đã phát triển một mô hình dự đoán thông minh dựa trên phương pháp ANFIS, mô hình này có khả năng dự đoán các thông số hiệu suất như độ nhám bề mặt và dung sai hình học trong gia công CNC [12]. Stephen và cộng sự đã ứng dụng ANFIS để mô hình hóa các thông số chất lượng gia công chính là độ nhám bề mặt, tốc độ bóc tách vật liệu và lực cắt khi gia công hợp kim titan bằng cách sử dụng đá mài ống nano carbon tiên tiến [13]. ANFIS cũng đã được kết hợp với nhiều thuật toán khác như ANN, giải thuật bầy đàn (PSO), giải thuật di truyền (GA),... nhằm nâng cao hiệu quả xây dựng mô hình [14-16].
0	Tuy nhiên, việc ứng dụng ANFIS trong việc xây dựng mô hình dự báo độ nhám bề mặt tại Việt Nam còn hạn chế, đặc biệt là các nghiên cứu trong việc xác định chính xác các hàm thuộc và phương pháp đào tạo khi xây dựng mô hình. AISI 304 là một loại thép không gỉ phổ biến, thường được sử dụng trong các ứng dụng yêu cầu độ bền cao và khả năng chống ăn mòn tốt [17, 18]. Tuy nhiên, việc gia công loại thép này lại gặp nhiều thách thức do tính chất cơ học của nó [16, 19]. Do đó, nghiên cứu này tập trung vào việc ứng dụng ANFIS để xây dựng mô hình dự báo độ nhám bề mặt khi tiện thép AISI 304.
0	Các mục tiêu chính bao gồm: (1) thu thập và phân tích dữ liệu thực nghiệm từ quá trình tiện thép AISI 304; (2) phát triển mô hình ANFIS với các thông số đầu vào là chiều sâu cắt (t), lượng chạy dao (f), và tốc độ cắt (n); và (3) đánh giá hiệu suất của các mô hình ANFIS thông qua việc phân tích 8 hàm thuộc và 2 phương pháp đào tạo khác nhau, từ đó lựa chọn được mô hình tối ưu nhất cho việc dự báo chính xác độ nhám bề mặt. Vật liệu sử dụng trong nghiên cứu này là thép AISI 304. Thành phần hóa học và tính chất cơ học theo nhà sản xuất giới thiệu trong Bảng 1.
0	Trong nghiên cứu này bộ thông số chế độ gia công được sử dụng lại với 3 thông số chế độ cắt (t, f, n) dựa trên mảng trực giao L_25 (5^3). Các mức cụ thể được chỉ định cho từng thông số và các giá trị tương ứng của chúng được lựa chọn dựa trên dữ liệu gia công sản phẩm trước đó được giới thiệu trong Bảng 2. Máy tiện được sử dụng trong nghiên cứu này là máy tiện CNC EL 550TM (Hình 2) với dụng cụ cắt là dao tiện ngoài mảnh hợp kim cứng CNMG 120408 – MG với chiều dài cạnh ngoài là 12,9 mm, độ dày là 4,76 mm và bán kính mũi là 0,8 mm. Tất cả các thí nghiệm đều sử dụng mảnh cắt mới hoặc độ mòn của mặt bên nhỏ hơn 0,10 mm.
0	Ngoài ra, dung dịch trơn nguội PV được sử dụng trong quá trình tiện. Giá trị độ nhám của bề mặt sau khi gia công (R_a) được đo bằng máy đo độ nhám tiếp xúc loại đầu dò SV3100 của Mitutoyo, Nhật Bản. Chi tiết được đặt trên khối V và được đo dọc theo đường tâm của phôi [20]. Mạng nơ ron thích nghi mờ (ANFIS) là một mô hình học máy mạnh mẽ kết hợp giữa mạng nơ ron và lý thuyết tập mờ. ANFIS được thiết kế nhằm tận dụng những ưu điểm của cả hai phương pháp, cho phép nó xử lý các vấn đề phức tạp và không chắc chắn trong nhiều lĩnh vực, bao gồm dự đoán, phân loại và điều khiển. Cụ thể, các hệ thống suy diễn mờ rất hiệu quả trong việc biểu diễn kiến thức chuyên gia, nhưng lại thiếu khả năng tự động học.
0	Ngược lại, mạng nơ ron có khả năng học hỏi tốt từ dữ liệu mẫu, đặc biệt khi kiến thức chuyên môn hạn chế, nhưng không có khả năng biểu diễn kiến thức. Trong nghiên cứu này, mô hình ANFIS bao gồm ba đầu vào là chiều sâu cắt (t), lượng chạy dao (f) và tốc độ cắt (n) cùng với một đầu ra là độ nhám bề mặt (R_a) được giới thiệu trên Hình 2. Mô hình ANFIS được cấu thành từ năm lớp khác nhau, mỗi lớp thực hiện một nhiệm vụ cụ thể trong quá trình xử lý dữ liệu. Cụ thể, lớp đầu tiên là các nút đầu vào còn được gọi là lớp mờ hóa. Lớp thứ hai trong mô hình ANFIS là lớp nút quy tắc.
0	Trong đó, y_i là tập đầu ra nằm trong vùng mờ được xác định bởi quy tắc mờ. Các tham số p_i, q_i, r_i và s_i (i là các biến từ 1 đến 3) được xác định trong quá trình đào tạo mô hình. Để lựa chọn mô hình dự báo, nghiên cứu này lựa chọn quá trình đào tạo hệ thống suy luận mờ bằng hai phương pháp học lai (hybrid learning) và lan truyền ngược (backpropagation) tương ứng với 8 hàm thuộc là hình tam giác (trimf), hình thang (trapmf), hình chuông (gbellmf), hàm Gaussian có hai tham số (gaussmf), hàm Gaussian hai chiều (gauss2mf), dạng pi (pimf), hiệu hàm sigmoid (dsigmf), tích hàm sigmoid (psigmf).
0	Để đánh giá mô hình ANFIS, hai chỉ số quan trọng được lựa chọn là hệ số xác định R^2 (Coefficient of Determination) và căn bậc hai của trung bình bình phương sai số RMSE (Root Mean Square Error) để đo lường mức độ phù hợp của mô hình và ý nghĩa của sự biến đổi tổng thể trong biến phụ thuộc [22, 23]. Nếu chỉ sử dụng R^2 thì không thể hiện hoặc xác định quy mô thực tế của sai số dự đoán. Do đó, chỉ số RMSE được áp dụng để kiểm tra tính chính xác của kết quả khi cho phép đo lường trực tiếp sai số dự đoán trung bình và có cùng đơn vị với biến mục tiêu. Một giá trị RMSE nhỏ cho thấy, các dự đoán của mô hình gần giống với giá trị thực tế hơn.
0	Việc kết hợp R^2 và RMSE mang lại sự cân bằng giữa quy mô tổng thể của mô hình và độ chính xác trong các dự đoán, giúp đánh giá một cách toàn diện chất lượng và hiệu quả của mô hình ANFIS. Công thức tính R^2 và RMSE được trình bày trong (6) và (7). Phân tích ANOVA (Phân tích Phương sai) được thực hiện để đánh giá tác động của các yếu tố t, f và n đến R_a trong quá trình tiện thép AISI 304. Kết quả phân tích ANOVA với mức ý nghĩa alpha = 0,05 được trình bày trong Bảng 3. Trong nghiên cứu này, Fuzzy Logic Toolbox của phần mềm Matlab R2018b (MathWorks, Natick, Massachusetts) đã được sử dụng để phát triển một mô hình dự báo.
0	Tuy nhiên, với 25 bộ số liệu thí nghiệm có thể không đảm bảo độ tin cậy cho quá trình đào tạo. Để tăng độ tin cậy cho phương pháp đề xuất, mỗi mẫu thí nghiệm sẽ được đo 3 lần trên 3 vị trí cách nhau 120°. Do đó, tổng số bộ số liệu thí nghiệm là 75 và được chia ngẫu nhiên thành bộ dữ liệu đào tạo (75%) và bộ dữ liệu kiểm tra (25%). Đây là một trong những tỷ lệ đã được áp dụng rộng rãi trong các nghiên cứu trước đó, cho thấy tính hiệu quả và có thể được coi là tiêu chuẩn trong lĩnh vực học máy [24].
0	Kết quả phân tích từ Bảng 4 và Bảng 5 cho thấy, phương pháp đào tạo Hybrid với hàm thuộc Gaussmf là lựa chọn tối ưu nhất trong dự báo độ nhám bề mặt. Cụ thể, R^2 đạt giá trị cao nhất (0,986081) phản ánh khả năng giải thích sự biến thiên dữ liệu thực nghiệm một cách chính xác. Đồng thời, RMSE thấp nhất (0,013055) chứng tỏ chênh lệch giữa giá trị dự đoán và giá trị thực tế được giảm thiểu đáng kể. Phương pháp đào tạo Backpropagation thuần túy với hàm thuộc Pimf cũng cho kết quả tốt (với R^2 = 0,966112 và RMSE = 0,020371) nhưng vẫn kém hơn so với Hybrid.
0	Trong khi đó nếu phương pháp đào tạo Backpropagation thuần túy mô hình có thể dễ dàng bị quá khớp (overfitting) do học quá nhiều từ dữ liệu huấn luyện. Hàm thuộc Gaussmf có khả năng mô tả tốt mối quan hệ phi tuyến, được tối ưu hóa hiệu quả hơn trong phương pháp Hybrid giúp tăng cường độ chính xác và độ ổn định của mô hình. Do đó, trong nghiên cứu này phương pháp đào tạo Hybrid với hàm thuộc Gaussmf là lựa chọn tối ưu cho bài toán dự báo độ nhám bề mặt, vừa đảm bảo hiệu suất cao vừa phù hợp với tính chất phức tạp của dữ liệu (Hình 3). Hình 4-a giới thiệu kết quả khi so sánh các giá trị dự báo từ mô hình ANFIS với các giá trị thực tế của R_a trong tập dữ liệu đào tạo.
0	Các giá trị dự báo gần với các giá trị thực tế, điều này cho thấy các mô hình ANFIS được đào tạo có độ tin cậy cao. Hình 4-b cũng cho thấy kết quả so sánh của tập dữ liệu kiểm tra. Để đánh giá mức độ phù hợp của mô hình ANFIS, kiểm định thống kê t-Test đã được thực hiện trên bộ dữ liệu này. Kết quả cho thấy, không có sự khác biệt đáng kể giữa kết quả thực nghiệm và kết quả dự đoán từ mô hình (F = 0,954; p = 0,465). Do đó, mô hình ANFIS trong nghiên cứu này được xem là phù hợp về mặt thống kê từ quan điểm mô hình hóa. Nghiên cứu đã chứng minh hiệu quả của hệ thống suy luận thần kinh mờ thích ứng (ANFIS) trong việc dự đoán độ nhám bề mặt (R_a) khi tiện thép AISI 304.
0	Kết quả phân tích ANOVA cho thấy, cả ba thông số gia công t, f và n đều ảnh hưởng đáng kể đến R_a, trong đó f là yếu tố quan trọng nhất. Mô hình ANFIS được xây dựng thông qua hai phương pháp đào tạo, Hybrid và Backpropagation với tám hàm thuộc khác nhau cho thấy, phương pháp đào tạo Hybrid kết hợp hàm thuộc Gaussmf thu được kết quả tốt nhất. Kết quả này không chỉ khẳng định tính chính xác của mô hình ANFIS trong việc dự đoán R_a mà còn mở ra khả năng ứng dụng rộng rãi trong các quy trình sản xuất cơ khí.
0	Lũ quét là một hiện tượng tự nhiên nguy hiểm xảy ra hầu khắp các lưu vực sông suối khu vực miền núi trên thế giới cũng như ở Việt Nam, trong đó khu vực Tây Bắc Việt Nam là một điểm nóng về tiềm ẩn nhiều nguy cơ xảy ra lũ quét. Bằng việc ứng dụng mạng nơ-ron nhân tạo đa lớp trong thành lập mô hình phân vùng lũ quét, thực nghiệm tại tỉnh Yên Bái đã cho phép thành lập được mô hình phân vùng lũ quét độ chính xác cao với các chỉ số thống kê ROC = 0,960 và giá trị AUC = 0,951.
0	Trong những năm gần đây, do ảnh hưởng của biến đổi khí hậu, mưa lớn và lũ quét đã xảy ra ngày càng nghiêm trọng, gây thiệt hại rất lớn đến kinh tế - xã hội, tính mạng người dân. Do vậy cần thiết phải xây dựng bản đồ phân vùng và cảnh báo lũ quét độ chính xác cao nhằm tăng cường khả năng chống chịu, ứng phó của người dân với biến đổi khí hậu, đảm bảo ổn định cuộc sống lâu dài của cộng đồng dân cư trong vùng lũ quét nói chung và các mục tiêu khác. Mặc dù các đề tài nghiên cứu về lũ quét trước đây thực hiện tại Việt Nam đã có nhiều nghiên cứu, tuy nhiên mô hình sử dụng là mô hình chuyên gia cho phân vùng dự báo lũ quét.
0	Hiện nay trên thế giới, không nơi nào còn sử dụng mô hình này, do độ chính xác thấp và mang tính chủ quan. Quan trọng hơn, không có đề tài nào thực hiện khâu đánh giá độ chính xác. Trong những năm gần đây, với sự tiến bộ nhanh chóng của hệ thống thông tin địa lý (GIS), viễn thám (RS) và máy học (ML) đã mang đến cho những nhà khoa học bộ công cụ hữu ích để giải quyết sự phức tạp của các mô hình lũ không gian [3,15,19]. Dữ liệu không gian trích xuất từ GIS đã cải thiện sự hiểu biết và đánh giá nguy cơ lũ toàn bộ khu vực phân tích.
0	Hơn nữa, những bộ dữ liệu hệ thống thông tin địa lý này được kết hợp với những tiếp cận máy học hiện đại để tạo ra những công cụ mạnh mẽ cho việc dự báo không gian lũ. Những cảm biến viễn thám mới như Sentinel-1A và B đã cung cấp một công cụ mới để phát hiện lũ và lập bản đồ với độ chính xác cao [4, 9]. Dữ liệu này kết hợp với các dữ liệu không gian trong GIS tạo ra bộ dữ liệu tổng hợp, được đưa vào mô hình máy học hiện đại đã cải thiện lớn sự hiểu biết và đánh giá nguy cơ lũ toàn bộ khu vực phân tích, cũng như dự báo về không gian lũ sát với thực tế [2].
0	Trong số các phương pháp học máy, mạng nơ-ron nhân tạo (ANN) có lẽ được sử dụng rộng nhất trong mô hình lũ lụt [12, 14, 22] cũng như dự báo không gian về các mối nguy tự nhiên khác [1, 6, 7, 13]. Phương pháp này có một khả năng lớn là phân tích dữ liệu phi tuyến và đa biến cũng như là khả năng mô hình hóa. Mặc dù có những ưu điểm như vậy nhưng việc ứng dụng ANN vào việc lập mô hình nguy cơ lũ quét dựa trên GIS còn nhiều hạn chế. Ngoài ra, những công trình nghiên cứu trước đây áp dụng ANN vào mô hình không gian nguy cơ tự nhiên thường dùng tới thuật toán Gradient với sự truyền ngược như là cách thông thường cho việc huấn luyện các mô hình.
0	Cách tiếp cận thông thường này cập nhật trọng số của một mô hình ANN để tối giản những lỗi dự báo trong giai đoạn huấn luyện. Mặc dù thuật toán Gradient với sự truyền ngược rất nhanh chóng, phương pháp huấn luyện này có nguy cơ bị mắc bẫy tối thiểu cục bộ, đặc biệt là ở không gian lỗi đa phương thức [21]. Sự bất lợi này làm giảm đáng kể khả năng dự báo của các mô hình dự báo lũ quét dựa trên ANN. Mạng nơ-ron nhân tạo đa lớp (MLP - Multi Layer Perceptron Neural Networks) là mô hình mạng nhiều tầng truyền thẳng. Một mạng MLP tổng quát là mạng có n (n >= 2) tầng. Thông thường tầng đầu vào không được tính đến, trong đó gồm một tầng đầu ra (tầng thứ n) và (n-1) tầng ẩn.
0	Kiến trúc của một mạng MLP tổng quát có thể mô tả như sau: Đầu vào là các vector (x_1, x_2, ..., x_n) trong không gian $n$ chiều, đầu ra là các vector (y_1, y_2, ..., y_m) trong không gian m chiều. Đối với các bài toán phân loại, n chính là kích thước của mẫu đầu vào, m chính là số lớp cần phân loại. Mỗi nơ-ron thuộc tầng trước liên kết với tất cả các nơ-ron thuộc tầng liền sau nó. Đầu ra của các nơ-ron tầng trước là đầu vào của các nơ-ron thuộc tầng liền sau nó.
0	Hoạt động của mạng MLP: tại tầng đầu vào các nơ-ron nhận tín hiệu vào xử lý (tính tổng trọng số, gửi tới hàm truyền); kết quả này sẽ được truyền tới các nơ-ron thuộc tầng ẩn thứ hai..., quá trình tiếp tục cho đến khi các nơ-ron thuộc tầng ra cho kết quả. Mạng trí tuệ nhân tạo đa lớp ANN là một thuật toán máy học giám sát mô phỏng những đặc điểm của các mạng trí tuệ sinh học thực tế khác. Một ANN có thể được huấn luyện với dữ liệu đầu vào (những bản đồ thành phần lũ quét) với nhãn sự thật (lũ quét và không phải lũ quét); mô hình ANN được huấn luyện sau đó được sử dụng để dự báo các nhãn các lớp đầu ra của sự việc xảy ra lũ quét.
0	Nhìn chung, cấu trúc của một ANN được phân thành ba lớp kết nối với nhau: lớp vào, lớp ẩn, và lớp ra. Lớp vào có những nơ-ron mô tả đặc điểm của một pixel trong bản đồ. Những lớp ẩn, bao gồm những nơ-ron riêng lẻ, thực hiện nhiệm vụ xử lý thông tin để tạo ra các nhãn loại của nguy cơ lũ trong lớp ra. Hiệu suất của các mô hình thu được được đánh giá bằng cách sử dụng sai số bình phương trung bình quân phương (RMSE), sai số tuyệt đối trung bình (MAE), và hệ số tương quan (r)[11].
0	Đường cong ROC [20] cũng được sử dụng để đánh giá hiệu suất của khả năng dự báo của mô hình. Đường cong ROC được tạo ra bằng cách vẽ tỷ lệ thực dương (TP) so với tỷ lệ dương tính giả (FP). Ngoài ra, khu vực dưới đường cong ROC (AUC) là thước đo thống kê tiêu chuẩn được tạo ra để xác nhận và so sánh các thuật toán học máy đã chọn được sử dụng trong nghiên cứu này [8]. Giá trị AUC cao hơn mô tả tốt hơn sự phù hợp của mô hình; và mô hình dự báo với giá trị AUC dao động từ 0,8 đến 0,9 cho thấy hiệu suất rất tốt [16].
0	Phương pháp này ngày càng được sử dụng rộng rãi và trở thành một phương pháp quan trọng trong nghiên cứu và đánh giá tai biến thiên nhiên trong đó có lũ quét. Các loại ảnh vệ tinh (LANDSAT, ảnh máy bay...), các loại bản đồ địa hình và các phần mềm GIS (MapInfo, ArcView, ArcGIS...) để xây dựng DEM, phân tích tổ hợp, quản lý số liệu... Trong nghiên cứu lũ quét, ảnh viễn thám có vai trò như một dữ liệu đầu vào quan trọng cung cấp các thông tin về cấu trúc và các đơn vị địa hình, lớp phủ thực vật, mạng lưới sông suối. Công nghệ GIS góp phần quan trọng nâng cao độ tin cậy của ảnh viễn thám, xây dựng mô hình số độ cao DEM giúp chính xác hóa các dạng địa hình, yếu tố địa mạo.
0	Đối với GIS trong nghiên cứu lũ lụt và lũ quét nói riêng có vai trò là phân tích địa hình lưu vực, mạng lưới thủy văn, tích hợp các lớp thông tin đơn tính có liên quan đến sự hình thành và phát sinh tai biến lũ quét. Đây là công cụ quan trọng trong nghiên cứu tai biến lũ quét, lập bản đồ và đưa ra những quyết định trong công tác cảnh báo. Để xây dựng một mô hình dự đoán lũ, bên cạnh bản đồ thống kê lũ quét, điều quan trọng là cần phải xác định được những yếu tố ảnh hưởng lũ quét. Một chú ý quan trọng nữa là việc lựa chọn những yếu tố ảnh hưởng lũ thay đổi tùy theo các đặc điểm khác nhau của các khu vực nghiên cứu và dữ liệu sẵn có.
0	Trong phương pháp mô hình hóa được đề xuất trên cơ sở dữ liệu địa không gian, kết quả thử nghiệm cho thấy mô hình MLP hoạt động tốt với cả bộ dữ liệu huấn luyện và kiểm tra. Mô hình được đánh giá là sát với thực tế xảy ra lũ tại khu vực Tây Bắc, Việt Nam. Nghiên cứu có thể đưa ra một số kết luận: (i) hiệu suất của mô hình tốt khi sử dụng MLP được đánh giá cao. (ii) bản đồ phân vùng lũ quét thành lập dựa vào phương pháp ứng dụng trí tuệ nhân tạo đa lớp cho độ chính xác thống kê tiêu chuẩn ROC = 0,960 và giá trị AUC = 0,951.
0	SVM (Support Vector Machine) là một phương pháp học có giám sát dựa trên lý thuyết học thống kê. Mục đích của SVM là sử dụng thuật toán học nhằm xây dựng một siêu phẳng làm cực tiểu hóa độ phân lớp sai của một đối tượng dữ liệu mới. SVM cũng là một trong những kỹ thuật được nhiều nhà nghiên cứu quan tâm để sử dụng trong việc phân loại ý kiến về sản phẩm hay dịch vụ dựa trên khai phá dữ liệu. Bài báo này nhằm mục đích tìm hiểu và ứng dụng triển khai thực nghiệm kỹ thuật SVM trong bài toán phân loại ý kiến đánh giá Vịnh Hạ Long với 1.213 ý kiến bằng tiếng Việt thu thập từ mạng xã hội Facebook. Qua 7 lần thực nghiệm kết quả tốt nhất thu được với độ chính xác gần 77%.
0	Ngày nay, du khách thường có xu hướng tham khảo ý kiến của những người có kinh nghiệm trước khi quyết định đến một địa điểm nào đó. Các mạng xã hội là nguồn tham khảo tuyệt vời khi rất nhiều người dùng có thể để lại bình luận bày tỏ quan điểm cá nhân về một địa điểm du lịch nào đó. Các ý kiến này được thu thập, phân chia thành các loại tích cực hoặc tiêu cực giúp các du khách khác có thể tham khảo hoặc các nhà quản lý lắng nghe ý kiến khách hàng và cải thiện chất lượng dịch vụ. Phân loại ý kiến khách hàng về sản phẩm hay dịch vụ dựa trên khai phá dữ liệu đã nhận được sự quan tâm của nhiều nhà nghiên cứu như Bing Liu (2011, 2012), nhóm nghiên cứu của Erik Cambria (2013), Kushal Dave và cộng sự (2003).
0	Nhiều kỹ thuật đã được đề xuất, trong đó có SVM, một phương pháp học có giám sát dựa trên lý thuyết học thống kê. Kỹ thuật này đã được áp dụng trong nhiều công trình nghiên cứu, ứng dụng và thu được những kết quả khả quan. Các công trình này chủ yếu áp dụng cho tiếng Anh. Bài toán phân loại ý kiến đối với văn bản tiếng Việt cũng đã được nhiều nhà nghiên cứu quan tâm. Có một số công trình đã sử dụng SVM thực hiện phân loại tài liệu tiếng Việt về âm nhạc, ẩm thực, thời trang, vi tính, kinh doanh... cho kết quả khả quan như Trần Cao Đệ và Phạm Nguyên Khang (2012), Nguyễn Thị Lan Anh (2013).
0	Việc thực hiện phân loại ý kiến đánh giá về một địa điểm du lịch vẫn còn khá mới mẻ nên tôi đã lựa chọn đề tài này cho nghiên cứu của mình, địa điểm du lịch cụ thể là Vịnh Hạ Long. Các ý kiến này được thu thập từ mạng xã hội Facebook. Khai phá dữ liệu văn bản hay phát hiện tri thức từ các cơ sở dữ liệu văn bản (textual databases) là quá trình phát hiện tri thức từ các cơ sở dữ liệu có cấu trúc. Phân loại ý kiến là một trường hợp cụ thể của bài toán phân loại văn bản. Các """"văn bản"""" được xét phân loại là ý kiến về một vấn đề nhất định nên tương đối ngắn gọn, không quá phức tạp, thể hiện trực tiếp quan điểm người đưa ra ý kiến."""
0	Theo Bing Liu (2011), bài toán phân loại ý kiến được hiểu như sau: “Cho một tập hợp các văn bản cần đánh giá D, một bộ phân lớp quan diêm sẽ phân lớp mỗi tài liệu deD vào một trong hai lớp: Tích cực (positive) hoặc Tiêu cực (negative). Tích cực có nghĩa là tài liệu d thể hiện quan điểm tích cực và ngược lại nếu d thể hiện quan điểm tiêu cực thì d thuộc lớp Tiêu cực ”.  Có 3 cẫch tiếp cận phổ biến cho bài toán phân loại ý kiến, đó là: Phân loại dựa trên các cụm từ chì cảm xúc (Classification Based on Sentiment Phrases).Phân loại bằng cách sử dụng các phương pháp phân loại văn bản (Classification Using Text Classification Methods) .  Phân loại bằng cách sử  dụng hàm tính điểm (Classification Using a Score Function)
0	Trong bài báo này, tôi đã lựa chọn hướng tiếp cận là các phương pháp phân loại văn bản. Đây là hướng tiếp cận đơn giản nhất để phân loại cảm xúc nhằm giải quyết các bài toán tương tự như một bài toán phân loại văn bản theo chủ đề. Ý tưởng của hướng tiếp cận này chính là: coi mỗi ý kiến là một văn bản có chủ đề xác định. Sau đó sử dụng một thuật toán học máy phù hợp để tiến hành gán nhãn, phân loại cho những văn bản này theo chủ đề. Cách tiếp cận này được nhiều nhà nghiên cứu áp dụng và cho kết quả tốt. Sau khi đưa bài toán phân loại ý kiến về bài toán phân loại văn bản, ta sẽ sử dụng một kỹ thuật học máy để xác định nhãn cho mỗi ý kiến (Tích cực, Tiêu cực hay Không xác định).
0	SVM là phương pháp học có giám sát do Vapnik và Chervonenkis xây dựng. Mục đích của SVM là sử dụng thuật toán học nhằm xây dựng một siêu phang làm cực tiểu hóa độ phân lớp sai của một đối tượng dữ liệu mới. Độ phân lóp sai của một siêu phẳng được đặc trưng bởi khoảng cách bé nhất tới siêu phang đó. So với các phương pháp phân loại văn bản khác, SVM có một số ưu điểm như sau: SVM xử lý được không gian đặc trưng với số chiều lớn.  SVM giải quyết tốt vấn đề overfitting (dữ liệu có nhiễu hoặc quá ít dữ liệu huấn luyện).  SVM là phương pháp tiếp cận phân loại vãn bản hiệu quả.
0	Bài toán phân loại với SVM thường sử dụng hàm nhân (Kernel function) để tính toán tích vô hướng của hai vectơ như hàm nhân tuyến tính (linear), hàm nhân đa thức (polynomial function), hàm RBF (radial basis function), hàm sigmoid. Sau khi xem xét, so sánh giữa các phương pháp dùng để phân loại văn bản như: SVM, K láng giềng gần nhất, Naive Bayes, mạng nơ-ron, tôi nhận thấy RBF là hàm nhân có nhiều ưu điểm: RBF có thể giải quyết những trường hợp mà các nhãn lớp và các thuộc tính có quan hệ phi tuyến với nhau. Hơn nữa, nó khá tổng quát vì hàm tuyến tính là trường hợp đặc biệt của RBF và với những thông số nhất định thì hàm nhân sigmoid cũng xử lý tương tự RBF.
0	Bước 1: Hỏi ý kiến du khách bằng nhiều cách như: lập một trang (fanpage) về Vịnh Hạ Long và đăng các câu hỏi khảo sát trên các diễn đàn, các nhóm, hội... Đối tượng hướng đến là những người đã từng đi du lịch tại Hạ Long hoặc có quan tâm đến địa điểm này. Bước 2: Lấy dữ liệu về máy. Sử dụng Graph API – phương thức chính để ứng dụng có thể đọc và ghi dữ liệu từ Facebook. Các ý kiến này được đưa vào cùng một file .txt, mỗi ý kiến trên một dòng (tức là các ý kiến phân cách nhau bởi dấu xuống dòng).
0	Bước 3: Tách từ (Trong văn bản gốc bạn ghi là Bước 2 nhưng theo trình tự logic sau khi lấy dữ liệu, đây thường là bước xử lý tiếp theo). Module tách từ sử dụng thuật toán So khớp cực đại với một số thay đổi. Thuật toán So khớp cực đại nguyên bản có ý tưởng là ghép các tiếng lại sao cho được một từ có nhiều tiếng nhất mà từ đó có mặt trong từ điển. Tôi đề xuất một cách làm ngược lại, đó là cắt dần một cụm từ cho đến khi nhận được từ trong từ điển. Xét thấy từ dài nhất trong từ điển có 6 tiếng, nên với mỗi câu ta lấy cụm tối đa 6 tiếng để xét. Nếu cụm này không có trong từ điển thì cắt bớt tiếng cuối cùng rồi lại kiểm tra cho đến khi tìm thấy từ phù hợp.
0	Nếu cắt đến khi chỉ còn một tiếng mà vẫn không tìm thấy trong từ điển thì tiếng này được coi là từ mới. Tiến hành cập nhật từ này vào từ điển và coi từ này là từ dừng. Tiến hành tương tự cho đến hết câu. Bước 4: Xử lý một số trường hợp gây ngược nghĩa Việc xác định một ý kiến là tích cực hay tiêu cực không đơn thuần chỉ là xác định các từ trong ý kiến đó thuộc về nhãn lớp nào. Những ý kiến xuất hiện các từ “không”, “ít”, “thiếu”, “chưa”... làm cho ý nghĩa của từ theo sau nó thay đổi từ tích cực thành tiêu cực và ngược lại.
0	Từ “đẹp” là tích cực nhưng “không đẹp” lại là tiêu cực. Vì tôi tiến hành phân lớp nhị phân nên để giải quyết vấn đề này, chương trình xử lý bằng cách xác định các từ đi sau những từ trên là tích cực hay tiêu cực và thay thế bằng từ với ý nghĩa ngược lại. Những từ gây ngược nghĩa được coi là từ dừng nên sẽ bị loại bỏ ở bước sau. Bước 5: Loại bỏ từ dừng, từ không có ý nghĩa trong phân loại và các ký tự đặc biệt. Tại bước này, các phần tử của danh sách từ wordArray sẽ được kiểm tra, nếu là từ dừng hoặc ký tự đặc biệt (như dấu chấm, phẩy, icon không mang nghĩa), phần tử này sẽ bị loại bỏ khỏi dữ liệu huấn luyện.
0	Trọng số TF-IDF là sự kết hợp trọng số TF và giá trị tần suất tài liệu nghịch đảo IDF. Khi một từ xuất hiện trong càng ít tài liệu (tương ứng với giá trị DF nhỏ) thì khả năng phân biệt các tài liệu dựa trên từ đó càng cao. Các từ được dùng để biểu diễn các tài liệu thường được gọi là các đặc trưng. Để nâng cao tốc độ và độ chính xác phân loại, tại bước tiền xử lý văn bản, ngoài việc loại bỏ các từ dừng là từ không có ý nghĩa cho phân loại văn bản, thông thường người ta cũng loại bỏ những từ có số lần xuất hiện quá ít hoặc quá nhiều để làm giảm số chiều cho phân loại.
0	Sau khi tách từ, lọc tách các từ dừng, từ vô nghĩa, các ký tự đặc biệt, chương trình sẽ so sánh nội dung còn lại của ý kiến với tập từ huấn luyện để phân lớp cho ý kiến đó. Để phân lớp được dữ liệu, cần cập nhật tập huấn luyện Tích cực và Tiêu cực để chương trình xác định được ý kiến nào là Tích cực, ý kiến nào là Tiêu cực. Tiến hành huấn luyện cho từng phân lớp, đối với từng lớp ý kiến ta truyền vào các từ, cụm từ đặc trưng cho lớp đó. Chương trình càng học được lượng từ lớn thì khả năng phân lớp được dữ liệu càng cao.
0	Để đánh giá hiệu quả phân loại, tôi sử dụng các độ đo Precision, Recall và F-score được đề xuất bởi Van Rijsbergen, C.J. (1979). Theo đó, độ chính xác (Precision) là số ý kiến Tích cực được phân loại đúng chia cho tổng số ý kiến được phân loại là Tích cực. Nếu độ chính xác cao thì hệ thống có khả năng phân loại ý kiến vào lớp tương ứng tốt. Độ phủ (Recall) là số ý kiến Tích cực được phân loại đúng chia cho tổng số các ý kiến thực tế là Tích cực trong tập kiểm tra. Độ phủ thể hiện khả năng không phân loại sót của hệ thống. Độ phủ càng cao, hệ thống càng có khả năng phân loại tốt. Giá trị hàm tính điểm F-score được tính dựa trên độ chính xác và độ phủ.
0	Có thể thấy, tại lần thử nghiệm này, độ chính xác Precision, độ phủ Recall và hiệu suất F-score khá đồng đều, khoảng 77%. Trường hợp thấp nhất là lần thử nghiệm thứ 7, F-score đạt 62,60%, trong đó tập huấn luyện chỉ chiếm 10% tổng dữ liệu. Độ phủ trung bình tốt nhất đạt được là 79,14% ở lần thử nghiệm thứ 4. Độ chính xác trung bình cao nhất là 77,41%, nhận được ở lần thử nghiệm thứ 2. Ngoài ra, khi so sánh giữa ba nhãn lớp, tôi nhận thấy tỷ lệ phân loại đúng cho lớp Tích cực khá cao: trên 84%.
0	Tỷ lệ phân loại đúng cho lớp Tiêu cực chấp nhận được: >73%. Tỷ lệ phân loại ý kiến Không xác định thấp: <64%. Nguyên nhân do lượng dữ liệu thuộc lớp Không xác định ít hơn hai lớp còn lại khá nhiều. Lớp Tích cực có độ chính xác và độ phủ khá cao: >87% và >85%. Lớp Tiêu cực có các độ đo khá đồng đều ở cả 7 lần thực nghiệm, độ chính xác vẫn cao hơn độ phủ khoảng 10%. Khác với hai lớp trên, lớp Không xác định có độ phủ cao hơn hẳn độ chính xác: >15%, tuy nhiên lớp này đạt độ chính xác thấp <60%. Chính điều này khiến kết quả trung bình của 3 nhãn lớp có độ phủ cao hơn độ chính xác.
0	Nhìn chung, tập dữ liệu càng lớn thì hiệu quả đạt được càng cao. Khi lượng dữ liệu trong tập huấn luyện giảm, độ chính xác và hiệu suất F-score cũng giảm rõ rệt. Điều này có thể thấy rõ khi so sánh kết quả phân loại của 4 lần thực nghiệm đầu (F-score trung bình đạt >75%) và 3 lần thử nghiệm cuối (F-score trung bình đạt <70%). Với đề tài ứng dụng kỹ thuật SVM (Support Vector Machine) vào việc phân loại ý kiến đánh giá địa điểm du lịch từ mạng xã hội, tôi đã tiến hành thu thập dữ liệu từ mạng xã hội, đưa bài toán phân loại ý kiến về bài toán phân loại văn bản, lựa chọn kỹ thuật SVM sau khi khảo sát và phân tích các kỹ thuật thường dùng cho phân loại ý kiến.
0	Tôi đã xây dựng, cài đặt và thử nghiệm thành công bộ phân loại ý kiến đánh giá địa điểm du lịch Vịnh Hạ Long sử dụng kỹ thuật SVM. Cụ thể, tôi đã chia tập dữ liệu gồm 1.213 ý kiến thành hai tập huấn luyện và kiểm thử, thực hiện 7 lần thử nghiệm và kết quả tốt nhất thu được trong lần thử nghiệm thứ 2 với 80% dữ liệu dành cho tập huấn luyện và 20% dữ liệu dành cho tập kiểm thử, đạt hiệu suất trung bình của 3 nhãn lớp là 76,88%. Kết quả nghiên cứu chưa cao, do tôi chưa xử lý được nhiều trường hợp phức tạp của ngôn ngữ tiếng Việt. Đây cũng là hướng phát triển mà tôi muốn hướng tới trong những nghiên cứu tiếp theo nhằm cải thiện hiệu quả của nghiên cứu.
0	Mô hình định giá tài sản vốn (CAPM) lượng hóa mối quan hệ tuyến tính giữa lợi nhuận và rủi ro hệ thống của các tài sản rủi ro. CAPM là một trong những nền tảng lý thuyết của ngành tài chính hiện đại. Tuy nhiên, tính thực nghiệm của CAPM là một chủ đề gây tranh luận đối với các nhà nghiên cứu bởi vì CAPM sử dụng rất nhiều giả định mà khó có thể được đáp ứng trong thực tế. Xu hướng kết hợp trí tuệ nhân tạo và lý thuyết nền tảng tài chính đã tạo ra nhiều mô hình dự báo hiệu quả và phù hợp hơn trong thực nghiệm.
0	Nghiên cứu này thực hiện nhằm 02 mục tiêu chính: Sử dụng thuật toán Support Vector Regression (SVR) trên nền tảng CAPM để dự báo tỷ suất sinh lời của các cổ phiếu riêng lẻ và xác định các yếu tố tác động đến sai số trong dự báo của mô hình kết hợp này. Nghiên cứu sử dụng dữ liệu của các công ty niêm yết trên thị trường chứng khoán Thành phố Hồ Chí Minh giai đoạn từ tháng 12/2012 đến tháng 09/2020, chu kỳ theo tháng. Nghiên cứu chia dữ liệu thành 02 giai đoạn: giai đoạn 01 sử dụng để tối ưu hóa các tham số và giai đoạn còn lại được sử dụng để đánh giá sai số của mô hình dựa trên Spark MLlib.
0	Dữ liệu chuỗi thời gian về giá chứng khoán thông thường là chuỗi không dừng và rất khó xác định (Tay & Cao, 2001; Zhang, Lin, & Shang, 2017) bởi vì chúng là những chuỗi ngẫu nhiên có xu hướng phi tuyến tính do bị ảnh hưởng bởi nền kinh tế chung, đặc điểm của các ngành, chính trị và thậm chí là tâm lý của các nhà đầu tư (Chen & ctg., 2017; Zhong & Enke, 2017). Giả thuyết thị trường hiệu quả (Efficient Market Hypothesis) cho rằng giá của chứng khoán là một bước đi ngẫu nhiên (Random Walk), do đó khó có thể đoán trước được (Fama, 1970, 1991); mặc dù việc nghiên cứu các mô hình dự báo tỷ suất sinh lợi vẫn đang thu hút rất nhiều sự quan tâm từ giới học thuật và thực nghiệm (Weng, Ahmed, & Megahed, 2017).
0	Nghiên cứu của Atsalakis và Valavanis (2009); Kumar và Thenmozhi (2014); Malkiel (2003) đã nêu ra bằng chứng trái ngược nhau về tính hiệu quả của thị trường tài chính. Các nghiên cứu gần đây đã đề xuất các mô hình nhằm tăng hiệu quả dự báo dựa trên dữ liệu lịch sử. Những phương pháp phổ biến được sử dụng để dự báo kết quả như chỉ báo trung bình động, mô hình tự hồi quy, phân tích khác biệt và mối tương quan (Kumar & Thenmozhi, 2014; Wang, Wang, Zhang, & Guo, 2012). Gần đây hơn, một xu thế mới được tập trung nghiên cứu trong việc dự đoán chuỗi thời gian là học máy, nhằm xử lý dữ liệu ngẫu nhiên và phi tuyến tính (Chen & ctg., 2017).
0	Nền tảng mô hình định giá tài sản vốn (CAPM) được đề xuất từ những năm 1960 dựa trên lý thuyết về đa dạng hóa và lý thuyết quản lý danh mục đầu tư của Markowitz (Bui & Thai, 2021; Treynor, 1961). Mô hình CAPM lượng hóa mối quan hệ tuyến tính giữa rủi ro hệ thống và lợi nhuận kỳ vọng của các tài sản rủi ro. Mô hình CAPM theo phiên bản của Sharpe–Lintner–Black đã là một công cụ quản lý tài sản quan trọng trong những năm gần đây nhờ lợi thế là đơn giản và dễ sử dụng. Mặc dù vậy, việc sử dụng CAPM trong thực tiễn còn gây nhiều tranh cãi. Những nghiên cứu đầu tiên về CAPM đã minh chứng tồn tại mối quan hệ tuyến tính giữa tỷ suất sinh lời và rủi ro hệ thống beta (Black, 1972; Bui & Tran, 2021).
0	Một số nghiên cứu phát hiện đường thị trường chứng khoán khá phẳng, cũng là một thách thức đối với khung lý thuyết CAPM (Amihud, Christensen, & Mendelson, 1992; Breen & Korajczyk, 1993; Fama & French, 2021; Jagannathan & McGrattan, 1995). Bên cạnh các nghiên cứu ủng hộ lý thuyết CAPM, lại có nhiều nghiên cứu phủ nhận tính thực tiễn của mô hình này (Banz, 1981; Basu, 1983; Chaudhary, 2017; Fama & James, 1973; Lohano & Kashif, 2018). Mặc dù có nhiều ý kiến trái chiều, CAPM cũng đã trở thành một khung lý thuyết nền tảng trong lĩnh vực tài chính hiện đại; hơn nữa, nó cũng được sử dụng phổ biến trong thực nghiệm.
0	Học máy (Machine Learning) là một phần của ngành khoa học dữ liệu. Thuật ngữ “học máy” đề cập đến lĩnh vực nghiên cứu tập trung vào việc sử dụng các mô hình để đưa ra dự báo. Để xử lý khối lượng lớn dữ liệu, có sẵn các công cụ cho phép phân phối các tác vụ tính toán giữa các nút khác nhau trong một cụm máy tính, nhằm đảm bảo khối lượng công việc được cân bằng và thời gian xử lý được giảm xuống. Về vấn đề này, các công cụ như Apache Hadoop hoặc Apache Spark cho phép các thuật toán được chạy theo mô hình phân tán, giúp nhà phát triển tránh được tất cả những bất tiện mà điều này gây ra, chẳng hạn như đồng bộ hóa, truyền dữ liệu và khả năng chịu lỗi, ...
0	Đặc biệt, Apache Spark có thư viện Spark ML, chứa việc triển khai một số thuật toán học máy như mạng nơ-ron, cây quyết định, Random Forest, hồi quy, máy véc-tơ hỗ trợ (SVM) và các thuật toán khác. Kỹ thuật hồi quy véc-tơ hỗ trợ (SVR) đã dự báo được lượng mây và sản lượng điện trong hệ thống năng lượng mặt trời tại Nhật Bản. Kết quả dự báo rất khả quan, sai số trung bình bình phương (Root Mean Squared Error – RMSE) chỉ khoảng 10% và sai số tuyệt đối (Mean Absolute Error – MAE) xấp xỉ 6% (da Silva Fonseca & ctg., 2012). Một nghiên cứu liên quan đến ngành năng lượng đã sử dụng thuật toán SVR với hàm kernel mũ để dự báo lượng điện của máy phát và so sánh với giá trị thực tế; kết quả dự báo rất tốt (Ramedani, Omid, Keyhani, Shamshirband, & Khoshnevisan, 2014).
0	Phân tích thực nghiệm chỉ ra rằng mô hình SVR với hàm kernel mũ có khả năng dự báo tốt hơn. Cách tiếp cận SVR trong máy học để ước tính chi tiêu mua máy bay quân sự, sử dụng hàm kernel dạng mũ, đã cho kết quả đáng kinh ngạc: sai số trung bình tối thiểu (MSE) là 5.37%, và R² là 99%, một kết quả tốt ngoài kỳ vọng (Tong, 2015). Ứng dụng máy học trong lĩnh vực tài chính khá đa dạng, ví dụ như nghiên cứu việc sử dụng mô hình Fama ba và năm nhân tố (Gogas, Papadimitriou, & Karagkiozis, 2018).
0	Các tác giả đã so sánh SVR với phương pháp OLS trong mô hình CAPM, mô hình Fama ba và năm nhân tố, cũng như trong mô hình lý thuyết kinh doanh chênh lệch giá (APT), sử dụng dữ liệu từ thị trường chứng khoán Mỹ cho mô hình Fama ba nhân tố với 1.062 quan sát (07/1926–12/2014), mô hình Fama năm nhân tố với 618 quan sát (07/1963–12/2014), và mô hình APT với 346 quan sát (02/1986–12/2014). Hệ số R² hiệu chỉnh và MAPE được sử dụng để đo lường chất lượng dự báo của mô hình. Theo kết quả nghiên cứu, phương pháp sử dụng SVR với hàm kernel dạng mũ và dạng đa thức đã tỏ ra vượt trội so với phương pháp hồi quy OLS truyền thống khi xét tới MAPE và hệ số R² hiệu chỉnh.
0	Henrique, Sobreiro, và Kimura (2018) đã sử dụng SVR để ước tính giá cổ phiếu theo ngày với các mô hình được hiệu chỉnh theo thời gian. Bộ dữ liệu NASDAQ-100 được Abraham, Nath, và Mahanti (2001) sử dụng; các tác giả này đã tiên phong sử dụng máy học trong các nghiên cứu thực nghiệm về thị trường chứng khoán. Các thuật toán được đưa ra so sánh bao gồm Phân tích thành phần chính (PCA), Mạng thần kinh nhân tạo (ANN) và Mạng thần kinh bóng mờ tiến hóa (NFUZZ) (Abraham & ctg., 2001). Tương tự, ANN và thuật toán máy véc-tơ hỗ trợ (SVM) được sử dụng tại thị trường Chicago dành cho năm hợp đồng tương lai, và tác giả sử dụng các thước đo Sai số bình phương trung bình chuẩn hóa (NMSE), Độ cân xứng có hướng (DS) và Sai số tuyệt đối trung bình (MAE) (Cao & Tay, 2003).
0	Gần đây, kỹ thuật hồi quy véc-tơ hỗ trợ (SVR) được sử dụng để dự báo giá vàng (Yuan, Lee, & Chiu, 2020), và thuật toán Di truyền – hồi quy véc-tơ hỗ trợ bình phương nhỏ nhất (GA-LSSVR) được sử dụng để kiểm định độ nhạy và đánh giá chất lượng mô hình thông qua chỉ số MAPE. Kỹ thuật SVR có thể phát hiện mối quan hệ phi tuyến tính mà phương pháp OLS không thực hiện được. Tại Việt Nam, K. T. Tran, Banh, và Nguyen (2012) đã kết hợp giải thuật di truyền và SVR để dự đoán giá cổ phiếu trên thị trường chứng khoán Việt Nam; Trinh (2013) đã ứng dụng kỹ thuật học máy SVR để xây dựng chương trình dự đoán xu hướng tăng giảm của cổ phiếu dựa trên dữ liệu từ tập dữ liệu Twitter.
0	Do đó, nghiên cứu sử dụng SVR dựa trên mô hình CAPM để dự báo tỷ suất sinh lời của cổ phiếu đơn lẻ, đồng thời xác định các yếu tố ảnh hưởng đến sự khác biệt sai số dự báo tỷ suất sinh lời đối với từng cổ phiếu đơn lẻ tại Việt Nam vẫn còn rất hạn chế. Vì vậy, thông qua việc sử dụng Spark MLlib, nghiên cứu này tận dụng các lợi thế của mô hình CAPM cùng với tính hiệu quả của thuật toán SVR bằng cách kết hợp CAPM và SVR, qua đó tạo ra kết quả dự báo chính xác hơn so với các nghiên cứu trước đó nhờ vào tính ưu việt cũng như mức độ phổ biến của SVR. Mô hình kết hợp này được xem như một phương pháp thay thế cho mô hình CAPM truyền thống.
0	Lợi thế của mô hình này là khả năng “học” để cải thiện độ chính xác thông qua việc sử dụng thuật toán máy học, kiểm soát nhiễu, khám phá các thành phần ẩn của dữ liệu và ước tính các hàm phi tuyến. Mô hình có sử dụng SVR đã tỏ ra vượt trội so với cách tiếp cận CAPM truyền thống nhờ vào các điểm này. Ngoài phần giới thiệu, bố cục của bài báo như sau: tổng quan lý thuyết về mô hình CAPM và thuật toán SVR được trình bày trong Phần 2; phương pháp nghiên cứu được giải thích trong Phần 3; và kết quả thực nghiệm được đưa ra trong Phần 4. Cuối cùng, phần kết luận của bài báo được trình bày trong Phần 5.
0	CAPM là một tập hợp các ước tính tỷ suất sinh lời kỳ vọng của các tài sản rủi ro ở trạng thái cân bằng. Mô hình này được hình thành trên nền tảng lý thuyết lựa chọn danh mục đầu tư (Markowitz, 1952; H. T. Tran, 2020). Các giả định của mô hình bao gồm:Các nhà đầu tư là e ngại rủi ro và luôn lựa chọn danh mục trung bình – phương sai hiệu quả.Thời gian nắm giữ danh mục chỉ trong một kỳ đơn lẻ.Kỳ vọng của các nhà đầu tư là thuần nhất.Tất cả các tài sản đều công khai, được giao dịch đại chúng, có thể chia nhỏ tùy ý và cho phép bán khống.Các nhà đầu tư có thể vay và cho vay một lượng tùy ý ở mức lãi suất phi rủi ro.Thông tin là có sẵn và được công khai.Không tồn tại thuế và chi phí giao dịch.
0	Spark là một công cụ hàng đầu trong hệ sinh thái Hadoop. MapReduce với Hadoop chỉ có thể được sử dụng để xử lý hàng loạt và không thể hoạt động trên dữ liệu thời gian thực. Spark có thể hoạt động độc lập hoặc trên khuôn khổ Hadoop để tận dụng dữ liệu lớn và thực hiện phân tích dữ liệu thời gian thực trong môi trường máy tính phân tán. Học máy là một trong những ứng dụng chính của Spark. Spark MLlib bao gồm các thuật toán học máy phổ biến để hồi quy, phân loại, phân cụm, lọc cộng tác và khai thác mẫu thường xuyên. Nó cũng cung cấp một loạt các tính năng để xây dựng đường ống (pipelines), lựa chọn và điều chỉnh mô hình, cũng như lựa chọn, khai thác và chuyển đổi dữ liệu.
0	Các phiên bản đầu tiên của Spark MLlib chỉ bao gồm một giao diện lập trình ứng dụng (Application Programming Interface – API) dựa trên bộ dữ liệu phân tán linh hoạt (Resilient Distributed Dataset – RDD). API dựa trên DataFrame hiện là API chính cho Spark. API dựa trên DataFrame giúp dễ dàng chuyển đổi các tính năng bằng cách cung cấp tính trừu tượng cấp cao hơn để biểu diễn dữ liệu dạng bảng, tương tự như bảng cơ sở dữ liệu quan hệ, làm cho nó trở thành một lựa chọn tự nhiên để triển khai các đường ống. Nghiên cứu này sử dụng phiên bản Hadoop và Spark ML phiên bản 3.1.1.
0	Nghiên cứu xây dựng một hệ thống phân tích cổ phiếu để dự đoán mức tăng hàng ngày trên thị trường chứng khoán dựa trên dữ liệu từ cafef.vn, vn.investing.com hoặc các tài nguyên trực tuyến khác. Nghiên cứu này thu thập dữ liệu từ Sở Giao dịch Chứng khoán Thành phố Hồ Chí Minh (Ho Chi Minh Stock Exchange, HOSE), đồng thời loại bỏ những cổ phiếu niêm yết sau tháng 12/2012 và hủy niêm yết trước tháng 09/2020. Do đó, dữ liệu nghiên cứu bao gồm giá cổ phiếu đóng cửa điều chỉnh của 212 cổ phiếu và lãi suất trái phiếu chính phủ kỳ hạn 01 năm trong giai đoạn từ tháng 12/2012 đến tháng 09/2020 (gồm 94 tháng).
0	Theo đó, dữ liệu được xử lý bằng cách xóa bỏ các dữ liệu bị thiếu và các dữ liệu ngoại lai. Bảng 1 mô tả các biến. Nghiên cứu sử dụng mô hình CAPM kết hợp với thuật toán SVR. Qua đó, quy trình nghiên cứu của dự án được xây dựng như Hình 2 và bao gồm 02 bước. Bước 1: Đối với nhóm huấn luyện, nghiên cứu sử dụng 50 tháng đầu tiên (tương ứng với 50 × 212 quan sát) để làm tập huấn luyện. Tổng cộng có 60 mô hình được kiểm định với các quan sát này, tương ứng với các tham số: cost = 1, 0.5, 0.1, 0.05, 0.01, 0.001; epsilon = 1, 0.8, 0.6, 0.2, 0.1; và kernel = linear, radial, polynomial.
0	Các quan sát từ tháng 51 đến tháng 60 được sử dụng làm tập kiểm tra thứ nhất nhằm lựa chọn mô hình có sai số MAE thấp nhất trong số 60 mô hình trên. Bước 2: Sử dụng các quan sát từ tháng 61 đến tháng 94 để làm tập kiểm tra thứ hai nhằm đánh giá hiệu quả của việc kết hợp giữa SVR và CAPM so với mô hình CAPM gốc, qua đó chỉ ra tính hiệu quả của thuật toán. Công thức xác định kết quả đầu ra của mô hình SVR là:rᵢt = r_f + f(premiumᵢt),trong đó f là hàm được xác định bởi thuật toán SVR thông qua các tham số đã xác định trong Bước 1.Sau khi xác định kết quả dự báo của cả hai mô hình, nghiên cứu tiến hành tính toán độ lệch giữa kết quả ước lượng và giá trị thực tế.
0	Phân phối của beta và MEAN được thể hiện bằng biểu đồ histogram (Hình 3). Phân phối của beta có dạng đối xứng với trung bình và trung vị bằng xấp xỉ 0; hai giá trị ngoại biên nằm xa tương đối so với các quan sát còn lại. Cổ phiếu KSB có hệ số beta thấp nhất -0.489 và tỷ suất trung bình tương ứng là -0.387 (-38.7 %/năm). Ngoài ra, cổ phiếu KSB cũng là cổ phiếu có MEAN thấp nhất trong số 212 công ty. Ở chiều ngược lại, beta của SVC là 0.394, cao nhất trong 212 mã cổ phiếu. Nhìn chung, biến động về tỷ suất sinh lời của tất cả các cổ phiếu đều thấp hơn thị trường (tất cả beta đều nhỏ hơn 1).
0	Hình 4 chỉ ra biểu đồ phân phối của R (R = r − r_f), và phần bù rủi ro phân tán đều xung quanh 0. Tương quan giữa R và phần bù rủi ro có ý nghĩa thống kê ở mức 5% nhưng độ lớn khá yếu (chỉ 0.16). Kết quả này hàm ý rằng biến động của tỷ suất sinh lời vượt trội chỉ được giải thích một phần bởi phần bù rủi ro theo mô hình CAPM. Biểu đồ phân tán trong Hình 3 cho thấy có 02 dạng kernel phù hợp, bao gồm kernel tuyến tính và kernel hàm mũ. Do đó, các tham số cần kiểm định bao gồm cả hai dạng kernel này, với tham số cost nhận các giá trị 1, 0.5, 0.1, 0.05, 0.01, 0.001 và tham số epsilon nhận các giá trị 1, 0.8, 0.6, 0.2, 0.1.
0	Tổng cộng có 60 mô hình kiểm định được thực hiện. Kết quả cho thấy hàm kernel dạng mũ với tham số cost = 1 và epsilon = 0.1 đạt giá trị MAE thấp nhất là 0.8833, trong khi hàm kernel dạng tuyến tính với cost = 0.001 và epsilon = 0.1 cho giá trị MAE nhỏ nhất là 0.8834. Mặc dù kernel dạng mũ cho kết quả MAE tốt hơn so với kernel tuyến tính, mức chênh lệch là không đáng kể; hơn nữa, kernel tuyến tính có tốc độ xử lý nhanh hơn và do đó được đánh giá là vượt trội hơn so với kernel dạng hàm mũ.
0	Cụ thể, sai số trung bình của dự báo theo mô hình SVR là 0.9087, thấp hơn so với giá trị tương ứng của mô hình CAPM là 0.9251. Các thống kê mô tả khác của SVR cũng nhỏ hơn so với CAPM, ngoại trừ giá trị cực tiểu của RMSE. Nghiên cứu sử dụng kiểm định Wilcoxon để so sánh hiệu quả dự báo giữa hai mô hình SVR và CAPM. Giả thuyết không của kiểm định là không tồn tại sự khác biệt về sai số dự báo giữa hai mô hình (H₀), trong khi giả thuyết đối cho rằng mô hình SVR có sai số dự báo nhỏ hơn so với mô hình CAPM (H₁). Kết quả kiểm định Wilcoxon cho thấy p-value = 0.04848, nhỏ hơn mức ý nghĩa 0.05, do đó giả thuyết H₀ bị bác bỏ.
0	Tuy nhiên, Gogas và cộng sự (2018) sử dụng danh mục cổ phiếu thay vì cổ phiếu đơn lẻ, do đó hệ số R² đạt kết quả tốt hơn, trong khoảng từ 0.59 đến 0.75. Mô hình SVR được xây dựng dựa trên nền tảng của mô hình CAPM, nên khả năng dự báo của SVR phụ thuộc vào độ chính xác của CAPM. Vì vậy, chỉ số RMSE_CAPM tác động trực tiếp đến sai số dự báo của mô hình SVR. Bên cạnh đó, các yếu tố ảnh hưởng đến khả năng dự báo của CAPM được trình bày trong Bảng 3. Hầu hết các biến đều có ý nghĩa thống kê ở mức ý nghĩa 1%, ngoại trừ biến BETA. Kết quả này hàm ý rằng biến động của hệ số beta không tác động đến mức độ chính xác trong dự báo của mô hình.
0	Hệ số ước lượng của biến SD có giá trị là 0.886, hàm ý rằng nếu các nhân tố tác động khác không đổi, mỗi đơn vị rủi ro tổng thể tăng thêm sẽ làm sai số RMSE_SVR tăng thêm 0.886 đơn vị. Hệ số ước lượng của biến RMSE_CAPM có giá trị lớn thứ hai (0.1166) và có p-value = 0.000, cho thấy sai số của mô hình CAPM tác động có ý nghĩa thống kê đến sai số của mô hình SVR. Cụ thể, khi RMSE_CAPM tăng thêm một đơn vị và giữ nguyên các yếu tố còn lại, kỳ vọng RMSE_SVR sẽ tăng thêm 0.1166 đơn vị. Các biến VAR và MEAN đều có hệ số ước lượng dương và có ý nghĩa thống kê ở mức 5%, cho thấy chúng tác động cùng chiều đến biến phụ thuộc.
0	Trong nghiên cứu này, thuật toán SVR được sử dụng với các bộ tham số khác nhau nhằm xác định tập tham số phù hợp nhất. Cụ thể, hàm kernel dạng tuyến tính với tham số cost = 0.001 và epsilon = 0.1 cho giá trị MAE nhỏ nhất là 0.8834. Nghiên cứu đã kết hợp thuật toán SVR với mô hình CAPM thay vì chỉ sử dụng riêng lẻ mô hình CAPM để dự báo tỷ suất sinh lợi của các cổ phiếu đơn lẻ. Tổng cộng có 60 mô hình dự báo được xây dựng từ tập dữ liệu huấn luyện, cho phép quá trình xử lý đạt hiệu quả cao về mặt thời gian tính toán.
0	Chúng tôi đã đưa ra dự đoán sau khi lựa chọn mô hình tốt nhất và áp dụng mô hình này vào dữ liệu thử nghiệm. Nghiên cứu của chúng tôi cho thấy mô hình SVR tuyến tính có sai số nhỏ hơn so với mô hình KNN, đồng thời SVR tuyến tính cũng đạt giá trị R² cao hơn và hệ số R² hiệu chỉnh tốt hơn trên cả tập huấn luyện và tập kiểm tra. Do đó, SVR tuyến tính hoạt động hiệu quả hơn và có thể được sử dụng để dự đoán trước 01 ngày giá đóng cửa của thị trường chứng khoán, dựa trên dữ liệu lịch sử sẵn có. Tóm lại, do thị trường chứng khoán là một lĩnh vực tài chính quan trọng, việc so sánh giữa các mô hình chuỗi thời gian có thể hỗ trợ việc ra quyết định mua hoặc bán cổ phiếu.
0	Trong các nghiên cứu trước đây, chúng tôi nhận thấy rằng mô hình SVR cho kết quả tốt hơn, tuy nhiên chưa xác định rõ mô hình này là tuyến tính hay phi tuyến, đồng thời cũng chưa có sự so sánh với mô hình KNN. Trong nghiên cứu này, chúng tôi nhận thấy rằng SVR tuyến tính hoạt động tốt hơn so với KNN. Tuy nhiên, nghiên cứu chỉ được thực hiện trên ba công ty được lựa chọn, do đó hiệu suất của các mô hình SVR và KNN cần được kiểm định thêm trên các bộ dữ liệu chuỗi thời gian khác.
0	Tham số cost đặc trưng cho chi phí sai lệch; cost cao hàm ý cho phép sai lệch lớn (thường dẫn đến hiện tượng underfitting), trong khi cost thấp sẽ hạn chế sai lệch hơn trong tập huấn luyện và có thể gây ra hiện tượng overfitting. Epsilon là tham số điều chỉnh khoảng cách giữa giá trị thực và giá trị dự báo (khoảng cách này được xem là bằng 0 nếu nhỏ hơn epsilon). Tham số gamma cho phép thay đổi hình dạng của hàm mật độ Gaussian trong kernel radial. Sai số dự báo, được đo lường bằng chỉ số RMSE ở các mã chứng khoán, phụ thuộc vào các nhân tố bao gồm: sai số của mô hình CAPM, rủi ro đặc thù, tỷ suất sinh lợi trung bình và rủi ro tổng thể.
0	Khi sử dụng các nhân tố trên để giải thích sự biến động của RMSE_SVR, hệ số xác định R² đạt giá trị 0.99, rất cao, cho thấy các nhân tố này gần như đã giải thích toàn bộ biến động của RMSE_SVR. Kết quả phân tích hồi quy cho thấy các hệ số ước lượng đều mang dấu dương; nói cách khác, các biến giải thích có tác động cùng chiều với biến phụ thuộc. Kết quả này hàm ý rằng việc kiểm soát các biến độc lập theo hướng giảm kỳ vọng sẽ góp phần làm giảm sai số trong mô hình SVR. Mặc dù CAPM là một lý thuyết nền tảng quan trọng, khả năng ứng dụng trong thực nghiệm vẫn còn gây nhiều tranh cãi do mô hình này dựa trên nhiều giả định khó đảm bảo trong thực tế.
0	Kết quả kiểm định Wilcoxon cho thấy mô hình SVR dự báo tốt hơn so với mô hình CAPM truyền thống, với giá trị p-value nhỏ hơn 0.05. Một số nhân tố giải thích cho sự biến động của RMSE_SVR bao gồm RMSE_CAPM, VAR, SD và MEAN; trong đó RMSE_CAPM là nhân tố có ảnh hưởng lớn nhất, hàm ý rằng sai số trong dự báo của mô hình SVR phụ thuộc phần lớn vào mô hình CAPM. Dựa trên các kết quả thu được, nghiên cứu đưa ra một số khuyến nghị như sau. Đối với các nhà đầu tư: Nên xem xét sử dụng mô hình kết hợp SVR như một phương pháp thay thế cho mô hình CAPM truyền thống, do mô hình kết hợp cho độ chính xác dự báo cao hơn.
0	Trong nghiên cứu này, mặc dù mô hình kết hợp giữa SVR và CAPM cho hiệu quả dự báo tốt hơn so với mô hình CAPM đơn lẻ, sai số dự báo vẫn còn ở mức tương đối cao. Do đó, các nghiên cứu tiếp theo có thể tiếp cận thông qua một số thuật toán học máy khác như mạng nơ-ron hồi quy (Recurrent Neural Network – RNN), mạng nơ-ron nhân tạo (Artificial Neural Network – ANN), và mạng nơ-ron tích chập (Convolutional Neural Network – CNN) nhằm cải thiện độ chính xác của dự báo. Ngoài ra, nghiên cứu này chỉ được xem xét trong bối cảnh thị trường HOSE, do đó cần mở rộng sang nhiều thị trường tài chính khác để tăng độ tin cậy và khả năng khái quát của kết quả nghiên cứu.
1	Nghiên cứu này đề xuất một phương pháp tối ưu hóa mô hình deep learning để phân loại tổn thương da dựa trên kỹ thuật transfer learning, kết hợp với các phương pháp data augmentation tiên tiến. Chúng tôi sử dụng kiến trúc EfficientNet-B4 được pre-trained trên ImageNet và fine-tune trên tập dữ liệu HAM10000 gồm 10.015 hình ảnh da liễu được gán nhãn bởi chuyên gia. Để giải quyết vấn đề mất cân bằng dữ liệu, nghiên cứu áp dụng kỹ thuật SMOTE kết hợp với mixup augmentation và sử dụng hàm mất mát focal loss. Kết quả thực nghiệm cho thấy mô hình đạt độ chính xác 94.3%, độ nhạy 92.8% và độ đặc hiệu 95.6% trên tập kiểm tra, vượt trội hơn 5.2% so với mô hình baseline ResNet-50.
1	Ung thư da là một trong những loại ung thư phổ biến nhất trên toàn thế giới, với hơn 5 triệu ca mới được chẩn đoán mỗi năm tại Hoa Kỳ. Chẩn đoán sớm là yếu tố then chốt quyết định tỷ lệ sống sót của bệnh nhân, đặc biệt đối với melanoma – loại ung thư da ác tính nhất. Tuy nhiên, việc phân loại chính xác các tổn thương da đòi hỏi trình độ chuyên môn cao và kinh nghiệm lâm sàng phong phú, trong khi nguồn lực bác sĩ da liễu còn hạn chế, đặc biệt tại các khu vực nông thôn và các quốc gia đang phát triển. Trong những năm gần đây, deep learning đã chứng minh khả năng vượt trội trong phân tích hình ảnh y tế, đạt hiệu suất tương đương hoặc thậm chí vượt qua bác sĩ chuyên khoa.
1	Tuy nhiên, các thách thức vẫn còn tồn tại, bao gồm: (1) sự khan hiếm dữ liệu y tế được gán nhãn bởi chuyên gia, (2) sự mất cân bằng nghiêm trọng giữa các lớp bệnh, (3) tính biến đổi cao của hình ảnh da (điều kiện chiếu sáng, góc chụp, màu da), và (4) yêu cầu về khả năng giải thích của mô hình trong các ứng dụng y tế. Transfer learning đã trở thành phương pháp tiêu chuẩn trong phân loại hình ảnh y tế khi dữ liệu huấn luyện còn hạn chế. Nghiên cứu của Tajbakhsh et al. (2016) cho thấy các mô hình được pre-trained trên ImageNet có thể được fine-tune hiệu quả cho các nhiệm vụ y tế, mặc dù tồn tại sự khác biệt về miền dữ liệu (domain).
1	Về data augmentation, các kỹ thuật truyền thống như rotation, flipping và color jittering đã được áp dụng rộng rãi. Gần đây, các phương pháp tiên tiến hơn như mixup (Zhang et al., 2018) và cutout (DeVries và Taylor, 2017) đã cho thấy khả năng cải thiện tính tổng quát hóa của mô hình. Đối với vấn đề mất cân bằng dữ liệu, focal loss (Lin et al., 2017) đã chứng minh hiệu quả trong việc tập trung quá trình học vào các mẫu khó phân loại.Nghiên cứu này đóng góp các điểm mới sau: (1) Đề xuất một pipeline tối ưu hóa toàn diện kết hợp transfer learning (EfficientNet-B4), các kỹ thuật data augmentation nâng cao (mixup, cutmix, augmix) và hàm mất mát chuyên biệt (focal loss) được thiết kế riêng cho bài toán phân loại tổn thương da.
1	Áp dụng kỹ thuật Grad-CAM để trực quan hóa các vùng mà mô hình tập trung, nhằm đảm bảo tính giải thích được và xây dựng lòng tin cho các ứng dụng y tế. Nghiên cứu tiến hành đánh giá toàn diện mô hình trên nhiều chỉ số khác nhau, bao gồm accuracy, sensitivity, specificity và AUC-ROC, đồng thời so sánh với các mô hình baseline nhằm chứng minh hiệu quả của phương pháp đề xuất. Chúng tôi sử dụng tập dữ liệu HAM10000 (Human Against Machine with 10000 training images) được công bố bởi Tschandl et al. (2018). Tập dữ liệu bao gồm 10,015 hình ảnh da liễu được chụp bằng thiết bị dermatoscope và được phân loại thành bảy loại tổn thương da khác nhau, bao gồm: Melanocytic nevi (nv) với 6,705 hình ảnh (67.0%); Melanoma (mel) với 1,113 hình ảnh (11.1%);
1	Actinic keratoses (akiec) gồm 327 hình ảnh (3.3%). Vascular lesions (vasc) gồm 142 hình ảnh (1.4%). Dermatofibroma (df) gồm 115 hình ảnh (1.1%). Tập dữ liệu được chia theo tỷ lệ 70% cho tập huấn luyện, 15% cho tập xác thực và 15% cho tập kiểm tra, đồng thời đảm bảo phân bố đồng đều các lớp thông qua phương pháp stratified splitting. Việc phân chia này đảm bảo không có sự trùng lặp bệnh nhân giữa các tập dữ liệu nhằm tránh hiện tượng rò rỉ dữ liệu (data leakage). Chúng tôi sử dụng EfficientNet-B4 làm kiến trúc backbone dựa trên các lý do sau.
1	Transfer learning capability: Các trọng số được huấn luyện trước (pre-trained weights) trên ImageNet cung cấp khả năng trích xuất đặc trưng mạnh mẽ, có thể được fine-tune hiệu quả cho miền dữ liệu y tế. Compound scaling: Phương pháp scaling đồng đều cho phép mô hình mở rộng (scale) hiệu quả khi cần tăng năng lực biểu diễn (model capacity). Loss function: Chúng tôi sử dụng hàm mất mát Focal Loss nhằm giải quyết vấn đề mất cân bằng dữ liệu giữa các lớp. Hàm Focal Loss được xác định như sau: FL(pt) = −αt(1 − pt)^γ log(pt) trong đó αt là trọng số của từng lớp và γ = 2 là tham số tập trung (focusing parameter).
1	Phase 1 (Frozen backbone): Đóng băng tất cả các layer của EfficientNet backbone, chỉ huấn luyện classification head trong 10 epochs với learning rate lr = 1e-3. Phase 2 (Fine-tuning): Mở băng toàn bộ mô hình và fine-tune với learning rate lr = 3e-4, sử dụng discriminative learning rate, trong đó các layer sâu hơn có learning rate thấp hơn. Để đảm bảo tính giải thích được của mô hình, chúng tôi sử dụng phương pháp Gradient-weighted Class Activation Mapping (Grad-CAM) nhằm trực quan hóa các vùng mà mô hình tập trung khi đưa ra dự đoán. Grad-CAM tính toán gradient của điểm số lớp đầu ra (output class score) đối với các feature maps của layer tích chập cuối cùng để tạo ra heatmap, làm nổi bật các vùng quan trọng trong hình ảnh.
1	Chúng tôi áp dụng Grad-CAM để trực quan hóa các activation maps cho các dự đoán của mô hình. Hình 1 cho thấy mô hình tập trung chính xác vào vùng tổn thương, không bị ảnh hưởng bởi background noise hoặc các artifacts như tóc (hair), thước đo (rulers) hay các dấu vết trên da (skin marks). Phân tích định tính: Trong 95% các trường hợp melanoma được phân loại đúng, heatmap tập trung chủ yếu vào các đặc điểm như đường viền bất đối xứng (asymmetric borders) và sự phân bố sắc tố không đồng đều (irregular pigmentation). Đối với melanocytic nevi, mô hình chú ý nhiều hơn đến tính đối xứng (symmetry) và sự đồng nhất về màu sắc (uniform color distribution).
1	Kết quả nghiên cứu chứng minh rằng việc kết hợp transfer learning với các kỹ thuật data augmentation nâng cao và specialized loss function có thể đạt được hiệu suất vượt trội trong bài toán phân loại tổn thương da. Việc mô hình đạt độ nhạy (sensitivity) 92.8% đặc biệt quan trọng trong chẩn đoán y khoa, nơi mà việc bỏ sót một ca ung thư (false negative) có hậu quả nghiêm trọng hơn so với việc chẩn đoán nhầm (false positive). Nghiên cứu ablation study cung cấp những insight quan trọng về vai trò của từng thành phần trong mô hình. Focal loss đóng góp đáng kể vào việc cải thiện hiệu suất trên các lớp thiểu số (minority classes), trong khi kỹ thuật mixup augmentation giúp mô hình có khả năng khái quát hóa (generalize) tốt hơn thông qua việc tạo ra các mẫu huấn luyện ảo (virtual training examples).
1	So với nghiên cứu mang tính nền tảng (landmark) của Esteva et al. (2017) sử dụng kiến trúc Inception-v3, phương pháp được đề xuất trong nghiên cứu này đạt độ chính xác cao hơn (94.3% so với 72.1% trên tập dữ liệu Stanford). Tuy nhiên, cần lưu ý rằng hai nghiên cứu sử dụng các tập dữ liệu khác nhau, đồng thời trong những năm gần đây đã có nhiều tiến bộ đáng kể trong kiến trúc mạng và các kỹ thuật huấn luyện. Haenssle et al. (2018) đã thực hiện so sánh giữa mạng nơ-ron tích chập (CNN) và 58 bác sĩ da liễu, trong đó CNN đạt giá trị AUC là 0.86, trong khi các chuyên gia con người đạt AUC là 0.79.
1	Nghiên cứu này có một số limitations cần được address trong tương lai: Dataset bias: HAM10000 chủ yếu chứa hình ảnh từ bệnh nhân da trắng. Performance trên các skin phototypes khác có thể thấp hơn. Future work cần validate trên diverse populations. Clinical deployment: Mặc dù đạt high accuracy in vitro, cần clinical trials để đánh giá hiệu quả thực tế trong workflow của bác sĩ. Rare lesion types: Performance trên các loại tổn thương cực kỳ hiếm gặp (< 0.1% incidence) chưa được đánh giá đầy đủ. Temporal validation: Cần kiểm tra xem model performance có degraded theo thời gian không khi distribution of lesions thay đổi.
1	Hướng phát triển tiếp theo: Multi-modal learning: Kết hợp dermatoscopic images với metadata (tuổi, giới tính, vị trí tổn thương) để cải thiện accuracy. Uncertainty quantification: Implement Bayesian deep learning để mô hình có thể express uncertainty, giúp bác sĩ identify các cases cần review kỹ. Active learning: Develop pipeline để continuously improve model bằng cách efficiently select informative samples cho annotation. Federated learning: Explore khả năng train model trên distributed hospital data mà không cần centralize sensitive patient information. Nghiên cứu này đã đề xuất một pipeline comprehensive cho phân loại tổn thương da sử dụng deep learning, đạt được kết quả state-of-the-art với accuracy 94.3% và AUC-ROC 0.971 trên HAM10000 dataset.
1	Nghiên cứu này đề xuất một mô hình kết hợp giữa Hồi quy véc-tơ hỗ trợ (SVR) và Random Forest nhằm dự báo tỷ suất sinh lợi cổ phiếu trong bối cảnh dữ liệu tài chính có tính phi tuyến và nhiễu cao. Dữ liệu thực nghiệm được thu thập từ thị trường chứng khoán Việt Nam trong giai đoạn 2013–2022. Mô hình được đánh giá thông qua các chỉ số MAE, RMSE và kiểm định Wilcoxon để so sánh với các mô hình truyền thống. Kết quả cho thấy mô hình kết hợp cho độ chính xác vượt trội so với SVR và CAPM riêng lẻ, đặc biệt trong các giai đoạn thị trường biến động mạnh. Nghiên cứu đóng góp một phương pháp tiếp cận hiệu quả trong việc kết hợp học máy và lý thuyết tài chính.
1	Dự báo tỷ suất sinh lợi cổ phiếu là một bài toán trung tâm trong tài chính định lượng, song vẫn gặp nhiều hạn chế do giả định tuyến tính của các mô hình truyền thống. Trong khi CAPM cung cấp một khuôn khổ lý thuyết đơn giản, khả năng ứng dụng thực nghiệm còn gây tranh cãi. Các thuật toán học máy như SVR và Random Forest cho phép khai thác mối quan hệ phi tuyến và tương tác phức tạp giữa các biến. Tuy nhiên, mỗi thuật toán đều có ưu và nhược điểm riêng. Do đó, nghiên cứu này đề xuất kết hợp SVR và Random Forest nhằm tận dụng ưu thế của cả hai phương pháp, qua đó cải thiện độ chính xác dự báo và giảm sai số mô hình.
1	Nghiên cứu sử dụng dữ liệu giá cổ phiếu đã được điều chỉnh cùng với lãi suất phi rủi ro để xây dựng tập dữ liệu huấn luyện và kiểm tra, nhằm đảm bảo tính nhất quán và độ tin cậy của kết quả phân tích. Mô hình SVR được áp dụng để nắm bắt các mối quan hệ phi tuyến cơ bản giữa các biến đầu vào và biến mục tiêu, trong khi Random Forest được sử dụng nhằm khai thác các tương tác phi tuyến bậc cao và giảm ảnh hưởng của nhiễu trong dữ liệu. Các siêu tham số của hai mô hình được tối ưu hóa thông qua kỹ thuật xác nhận chéo để nâng cao hiệu suất dự báo và hạn chế hiện tượng quá khớp. Sai số dự báo được đánh giá bằng các chỉ số MAE và RMSE.
1	Kết quả thực nghiệm cho thấy mô hình kết hợp SVR–Random Forest đạt giá trị MAE và RMSE thấp hơn đáng kể so với SVR và CAPM. Đặc biệt, trong các giai đoạn thị trường biến động mạnh, mô hình kết hợp duy trì độ ổn định cao hơn. Kiểm định Wilcoxon cho thấy sự khác biệt về sai số dự báo là có ý nghĩa thống kê ở mức 5%. Phân tích hồi quy chỉ ra rằng rủi ro tổng thể và sai số của mô hình cơ sở là các yếu tố ảnh hưởng chính đến hiệu quả dự báo. Điều này khẳng định vai trò của học máy trong việc cải thiện các mô hình tài chính truyền thống.
1	Nghiên cứu đã chứng minh rằng việc kết hợp SVR và Random Forest mang lại hiệu quả dự báo vượt trội so với các mô hình riêng lẻ. Kết quả thực nghiệm cho thấy học máy có khả năng bổ trợ mạnh mẽ cho các mô hình lý thuyết tài chính truyền thống như CAPM, đặc biệt trong bối cảnh dữ liệu thị trường có tính chất phi tuyến và biến động cao. Việc sử dụng cơ chế tối ưu hóa tham số giúp mô hình đạt được sự cân bằng giữa độ chính xác và khả năng tổng quát hóa dữ liệu. Trong tương lai, nghiên cứu có thể được mở rộng bằng cách tích hợp thêm các kiến trúc mạng nơ-ron học sâu như LSTM hoặc GRU để xử lý chuỗi thời gian.
1	Nghiên cứu này ứng dụng mạng nơ-ron hồi quy dài ngắn hạn (Long Short-Term Memory – LSTM) để dự báo giá đóng cửa cổ phiếu trong bối cảnh dữ liệu chuỗi thời gian có tính phụ thuộc mạnh và độ nhiễu cao. Dữ liệu thực nghiệm được thu thập từ Sở Giao dịch Chứng khoán TP. Hồ Chí Minh (HOSE) trong giai đoạn từ năm 2015 đến năm 2023, bao gồm nhiều chu kỳ biến động khác nhau của thị trường. Mô hình LSTM được so sánh trực tiếp với thuật toán SVR tuyến tính và các mô hình hồi quy truyền thống dựa trên các chỉ số đánh giá tiêu chuẩn như RMSE, MAE và hệ số xác định R^2.
1	Dự báo giá cổ phiếu là một bài toán thách thức bậc nhất trong khoa học dữ liệu do đặc trưng phi tuyến, độ nhiễu cực cao và tính phụ thuộc thời gian phức tạp của dữ liệu tài chính. Các mô hình thống kê truyền thống thường dựa trên giả định về tính độc lập hoặc quan hệ tuyến tính giữa các biến, dẫn đến hiệu quả dự báo còn nhiều hạn chế khi áp dụng vào thực tế thị trường. Trong những năm gần đây, mạng nơ-ron LSTM đã chứng minh hiệu quả vượt trội trong việc xử lý dữ liệu chuỗi thời gian nhờ vào khả năng ghi nhớ thông tin dài hạn và cơ chế cổng kiểm soát dòng thông tin.
1	Nghiên cứu sử dụng dữ liệu giá cổ phiếu đã được chuẩn hóa và chia thành tập huấn luyện, tập xác nhận và tập kiểm tra theo trình tự thời gian nghiêm ngặt để tránh hiện tượng rò rỉ dữ liệu. Mô hình LSTM được xây dựng với cấu trúc nhiều lớp ẩn nhằm nắm bắt các mẫu động học phức tạp và các phụ thuộc dài hạn trong chuỗi thời gian tài chính. Các siêu tham số quan trọng như số lượng nút ẩn, tỷ lệ học (learning rate) và kích thước lô (batch size) được tối ưu hóa thông qua quá trình thử nghiệm lặp và kiểm chứng chéo. Hiệu quả dự báo của hệ thống được đánh giá khách quan bằng các chỉ số đo lường sai số phổ biến như MAE, RMSE và hệ số xác định R^2.
1	Kết quả thực nghiệm cho thấy mô hình LSTM đạt giá trị RMSE và MAE thấp hơn đáng kể so với các mô hình đối chứng là SVR và hồi quy tuyến tính. Hệ số xác định $R^2$ của LSTM cũng đạt mức cao hơn, phản ánh khả năng giải thích các biến động giá cổ phiếu tốt hơn trong điều kiện thực tế. Đặc biệt, tại các giai đoạn thị trường xảy ra biến động mạnh hoặc có các cú sốc kinh tế, mạng học sâu LSTM đã thể hiện tính ổn định và khả năng thích nghi vượt trội so với các thuật toán học máy cơ bản. Phân tích chi tiết sai số cho thấy mô hình học sâu này giảm đáng kể sai lệch trong các dự báo ngắn hạn, giúp tăng độ tin cậy cho các quyết định đầu tư.
1	Nghiên cứu này phân tích khả năng ứng dụng mạng nơ-ron hồi quy dài ngắn hạn (Long Short-Term Memory – LSTM) trong dự báo dữ liệu chuỗi thời gian tài chính. Dữ liệu nghiên cứu bao gồm giá đóng cửa hàng ngày của các cổ phiếu niêm yết trong giai đoạn 2015–2023. Mô hình LSTM được so sánh với các phương pháp truyền thống như hồi quy tuyến tính và Support Vector Regression (SVR) dựa trên các chỉ tiêu RMSE, MAE và R². Kết quả thực nghiệm cho thấy LSTM có khả năng khai thác cấu trúc phụ thuộc theo thời gian tốt hơn, từ đó cải thiện độ chính xác dự báo. Nghiên cứu khẳng định vai trò của học sâu trong việc xử lý dữ liệu chuỗi thời gian phức tạp.
1	Dự báo chuỗi thời gian là một trong những bài toán quan trọng trong khoa học dữ liệu và tài chính, do dữ liệu thường có tính phụ thuộc mạnh theo thời gian và phi tuyến. Các mô hình truyền thống như hồi quy tuyến tính hoặc ARIMA thường gặp hạn chế khi dữ liệu có độ biến động cao. Trong bối cảnh đó, mạng nơ-ron LSTM được phát triển nhằm khắc phục vấn đề “quên thông tin” của các mạng hồi quy thông thường. LSTM cho phép lưu giữ thông tin dài hạn, phù hợp với dữ liệu tài chính, nơi các cú sốc trong quá khứ có thể ảnh hưởng kéo dài đến tương lai. Do đó, nghiên cứu này tập trung đánh giá hiệu quả của LSTM trong dự báo tài chính.
1	Nghiên cứu sử dụng dữ liệu chuỗi thời gian đã được chuẩn hóa và chia thành tập huấn luyện, tập xác nhận và tập kiểm tra theo trình tự thời gian. Mô hình LSTM được xây dựng với một lớp đầu vào, một hoặc nhiều lớp ẩn và một lớp đầu ra. Các siêu tham số như số lượng neuron, learning rate và số epoch được tối ưu thông qua phương pháp thử nghiệm lặp và xác nhận chéo. Hiệu quả dự báo được đo lường bằng các chỉ số RMSE, MAE và R². Để đánh giá mức độ cải thiện, kết quả của LSTM được so sánh trực tiếp với SVR tuyến tính và hồi quy tuyến tính trên cùng tập dữ liệu.
1	Kết quả thực nghiệm cho thấy mô hình LSTM đạt sai số RMSE và MAE thấp hơn so với SVR và hồi quy tuyến tính trong hầu hết các cổ phiếu nghiên cứu. Hệ số R² của LSTM cũng cao hơn, cho thấy khả năng giải thích biến động dữ liệu tốt hơn. Đặc biệt, trong các giai đoạn thị trường biến động mạnh, LSTM duy trì được độ ổn định trong dự báo. Kiểm định thống kê cho thấy sự khác biệt về sai số giữa LSTM và các mô hình so sánh là có ý nghĩa ở mức 5%. Điều này khẳng định ưu thế của học sâu trong việc xử lý dữ liệu chuỗi thời gian tài chính.
1	Nghiên cứu cho thấy mạng nơ-ron LSTM là công cụ hiệu quả trong dự báo dữ liệu chuỗi thời gian tài chính. So với các mô hình truyền thống, LSTM khai thác tốt hơn cấu trúc phụ thuộc theo thời gian, từ đó nâng cao độ chính xác dự báo. Kết quả nghiên cứu gợi ý rằng học sâu có thể được xem là hướng tiếp cận thay thế hoặc bổ trợ cho các phương pháp thống kê cổ điển. Trong tương lai, nghiên cứu có thể mở rộng bằng cách kết hợp LSTM với các biến kinh tế vĩ mô hoặc các cơ chế attention nhằm cải thiện hơn nữa hiệu quả dự báo.
1	Nghiên cứu này so sánh hiệu quả dự báo của hai thuật toán học máy phổ biến là Support Vector Regression (SVR) và K-Nearest Neighbors (KNN) trong bối cảnh dữ liệu chuỗi thời gian tài chính. Dữ liệu được thu thập từ giá cổ phiếu hàng ngày của các doanh nghiệp niêm yết. Các mô hình được đánh giá dựa trên các chỉ tiêu MAE, RMSE và kiểm định Wilcoxon. Kết quả thực nghiệm cho thấy SVR tuyến tính cho sai số thấp hơn và ổn định hơn so với KNN, đặc biệt khi dữ liệu có nhiễu cao. Nghiên cứu cung cấp bằng chứng thực nghiệm cho việc lựa chọn thuật toán phù hợp trong dự báo tài chính.
1	Trong lĩnh vực dự báo tài chính, việc lựa chọn thuật toán học máy phù hợp đóng vai trò quan trọng đối với độ chính xác của kết quả. KNN là một phương pháp phi tham số đơn giản, dựa trên khoảng cách giữa các điểm dữ liệu, trong khi SVR có khả năng kiểm soát sai số thông qua các siêu tham số. Tuy nhiên, dữ liệu tài chính thường có tính nhiễu và phi tuyến, khiến hiệu quả của các thuật toán khác nhau đáng kể. Do đó, nghiên cứu này được thực hiện nhằm đánh giá và so sánh hiệu quả của SVR và KNN trong dự báo chuỗi thời gian tài chính, từ đó đưa ra khuyến nghị cho các nhà nghiên cứu và nhà đầu tư.
1	Dữ liệu được chuẩn hóa và chia thành tập huấn luyện và tập kiểm tra theo thứ tự thời gian nhằm tránh hiện tượng rò rỉ thông tin. Mô hình KNN được xây dựng với nhiều giá trị k khác nhau để đánh giá độ nhạy của thuật toán. Mô hình SVR sử dụng kernel tuyến tính với các tham số cost và epsilon được tối ưu thông qua xác nhận chéo. Hiệu quả dự báo của hai mô hình được đo lường bằng MAE và RMSE. Ngoài ra, kiểm định Wilcoxon được sử dụng để đánh giá sự khác biệt có ý nghĩa thống kê giữa các sai số dự báo của hai thuật toán.
1	Kết quả thực nghiệm cho thấy mô hình SVR tuyến tính đạt giá trị MAE và RMSE thấp hơn so với KNN trong phần lớn các trường hợp nghiên cứu, cho thấy độ chính xác dự báo vượt trội. Ngược lại, KNN tỏ ra nhạy cảm với nhiễu và phụ thuộc mạnh vào việc lựa chọn tham số k, dẫn đến sai số dự báo cao và thiếu ổn định khi dữ liệu có biến động lớn. Kết quả kiểm định Wilcoxon xác nhận rằng sự khác biệt về hiệu quả dự báo giữa SVR và KNN là có ý nghĩa thống kê ở mức 5%. Điều này chứng tỏ SVR tuyến tính phù hợp hơn trong bối cảnh dữ liệu tài chính có tính biến động mạnh và cấu trúc phức tạp.
1	Nghiên cứu kết luận rằng mô hình SVR tuyến tính là một thuật toán hiệu quả hơn so với KNN trong bài toán dự báo dữ liệu chuỗi thời gian tài chính. Kết quả thực nghiệm cho thấy SVR tuyến tính đạt độ chính xác cao hơn và ổn định hơn, đặc biệt khi xử lý dữ liệu có nhiễu và biến động mạnh, vốn là đặc trưng phổ biến của thị trường tài chính. Những phát hiện này góp phần cung cấp cơ sở thực nghiệm quan trọng cho việc lựa chọn mô hình học máy phù hợp trong lĩnh vực phân tích và dự báo tài chính. Bên cạnh đó, nghiên cứu cũng làm rõ những hạn chế của KNN khi áp dụng cho dữ liệu chuỗi thời gian có số chiều lớn.
1	Nghiên cứu này đánh giá hiệu quả của thuật toán Random Forest trong bài toán phân loại dữ liệu lớn thuộc lĩnh vực khoa học máy tính. Dữ liệu nghiên cứu được thu thập từ nhiều nguồn khác nhau và có đặc điểm chiều cao, nhiễu lớn và phân bố không đồng đều. Mô hình Random Forest được triển khai trên môi trường Spark ML nhằm tận dụng khả năng xử lý song song. Hiệu quả mô hình được đánh giá thông qua các chỉ số Accuracy, Precision, Recall và F1-score. Kết quả thực nghiệm cho thấy Random Forest vượt trội so với Decision Tree đơn lẻ và Logistic Regression. Nghiên cứu khẳng định Random Forest là lựa chọn phù hợp cho các bài toán phân loại dữ liệu lớn trong môi trường phân tán.
1	Sự phát triển nhanh chóng của dữ liệu lớn đặt ra thách thức đáng kể cho các thuật toán phân loại truyền thống. Các mô hình đơn giản thường gặp hiện tượng overfitting hoặc suy giảm hiệu suất khi dữ liệu có nhiều chiều và nhiễu. Random Forest, một phương pháp học máy dựa trên tập hợp nhiều cây quyết định, được đề xuất nhằm khắc phục các hạn chế này. Thuật toán này cải thiện khả năng tổng quát hóa bằng cách kết hợp nhiều mô hình con độc lập. Trong bối cảnh dữ liệu lớn, Random Forest còn thể hiện ưu thế khi có thể triển khai song song. Do đó, nghiên cứu này tập trung phân tích hiệu quả của Random Forest trong bài toán phân loại dữ liệu lớn trên nền tảng Spark.
1	Dữ liệu nghiên cứu được tiền xử lý bằng cách loại bỏ giá trị thiếu, chuẩn hóa và mã hóa các biến phân loại. Mô hình Random Forest được xây dựng với các siêu tham số như số lượng cây, độ sâu tối đa và số biến được chọn ngẫu nhiên tại mỗi nút. Các tham số này được tối ưu thông qua phương pháp xác nhận chéo. Toàn bộ quá trình huấn luyện và kiểm tra được triển khai trên Spark ML nhằm tăng tốc độ xử lý. Hiệu quả mô hình được so sánh với Decision Tree và Logistic Regression dựa trên các chỉ số đánh giá phổ biến. Phân tích thống kê được thực hiện để kiểm định sự khác biệt về hiệu suất giữa các mô hình.
1	Kết quả thực nghiệm cho thấy Random Forest đạt độ chính xác cao hơn so với các mô hình so sánh trên tất cả các tập dữ liệu. Giá trị F1-score của Random Forest ổn định ngay cả khi dữ liệu có mức độ mất cân bằng cao. Trong khi đó, Decision Tree đơn lẻ có xu hướng overfitting và Logistic Regression không khai thác tốt các mối quan hệ phi tuyến. Kiểm định thống kê xác nhận sự khác biệt về hiệu suất giữa Random Forest và các mô hình còn lại là có ý nghĩa ở mức 5%. Điều này cho thấy Random Forest phù hợp cho các bài toán phân loại dữ liệu lớn phức tạp.
1	Nghiên cứu đã chứng minh hiệu quả của thuật toán Random Forest trong bài toán phân loại dữ liệu lớn trên môi trường Apache Spark. Nhờ cơ chế xây dựng tập hợp nhiều cây quyết định, Random Forest có khả năng giảm hiện tượng overfitting và khai thác hiệu quả các mối quan hệ phi tuyến phức tạp trong dữ liệu. So với các mô hình phân loại truyền thống, Random Forest cho kết quả chính xác hơn và ổn định hơn khi làm việc với tập dữ liệu có kích thước lớn và độ đa dạng cao. Kết quả thực nghiệm cho thấy mô hình này phù hợp để triển khai trong các hệ thống phân tích dữ liệu lớn hiện đại, đặc biệt trong các ứng dụng yêu cầu khả năng mở rộng và xử lý song song.
1	Nghiên cứu này đề xuất một cách tiếp cận kết hợp giữa mô hình lý thuyết truyền thống và thuật toán học máy nhằm nâng cao hiệu quả phân tích dữ liệu. Trên cơ sở một mô hình lý thuyết nền tảng, các thuật toán học máy được sử dụng để khai thác các mối quan hệ phi tuyến và giảm sai số dự báo. Dữ liệu nghiên cứu bao gồm các biến định lượng được thu thập trong nhiều giai đoạn khác nhau. Hiệu quả mô hình được đánh giá bằng các chỉ số sai số và kiểm định thống kê. Kết quả thực nghiệm cho thấy mô hình kết hợp cho độ chính xác cao hơn so với mô hình lý thuyết đơn lẻ.
1	Các mô hình lý thuyết truyền thống đóng vai trò quan trọng trong việc giải thích hiện tượng dữ liệu, tuy nhiên thường gặp hạn chế do các giả định chặt chẽ. Trong khi đó, học máy có khả năng khai thác dữ liệu linh hoạt nhưng thiếu tính giải thích. Việc kết hợp hai hướng tiếp cận này được kỳ vọng vừa đảm bảo nền tảng lý thuyết, vừa nâng cao độ chính xác thực nghiệm. Trong bối cảnh khoa học dữ liệu hiện đại, xu hướng kết hợp mô hình ngày càng được quan tâm. Do đó, nghiên cứu này tập trung đánh giá hiệu quả của mô hình kết hợp trong phân tích dữ liệu, qua đó đề xuất một hướng tiếp cận thay thế cho các mô hình truyền thống.
1	Nghiên cứu sử dụng mô hình lý thuyết làm nền tảng nhằm xác định cấu trúc và mối quan hệ ban đầu giữa các biến trong bài toán phân tích dữ liệu. Trên cơ sở đó, các thuật toán học máy được áp dụng để hiệu chỉnh và mở rộng mô hình, giúp tăng khả năng mô tả các quan hệ phi tuyến và phức tạp mà mô hình lý thuyết truyền thống khó nắm bắt. Dữ liệu được chia thành tập huấn luyện và tập kiểm tra để đảm bảo tính khách quan trong đánh giá. Các tham số của mô hình học máy được tối ưu thông qua kỹ thuật xác nhận chéo nhằm giảm hiện tượng quá khớp.
1	Kết quả thực nghiệm cho thấy mô hình kết hợp giữa mô hình lý thuyết và học máy đạt sai số dự báo thấp hơn đáng kể so với mô hình lý thuyết đơn lẻ trên tất cả các chỉ số đánh giá. Cụ thể, các giá trị MAE và RMSE giảm rõ rệt, trong khi hệ số R² của mô hình kết hợp cao hơn, phản ánh khả năng giải thích dữ liệu tốt hơn. Kiểm định thống kê xác nhận rằng sự khác biệt về hiệu quả dự báo giữa hai cách tiếp cận là có ý nghĩa ở mức 5%. Ngoài ra, mô hình kết hợp vẫn duy trì được tính ổn định và độ tin cậy khi dữ liệu chứa nhiễu, cho thấy khả năng khái quát tốt.
1	Nghiên cứu khẳng định rằng việc kết hợp giữa các mô hình lý thuyết và các phương pháp học máy là một hướng tiếp cận hiệu quả trong phân tích dữ liệu hiện đại. Mô hình kết hợp không chỉ giúp cải thiện độ chính xác trong dự báo mà còn duy trì được ý nghĩa lý thuyết, qua đó tăng khả năng giải thích kết quả và độ tin cậy của mô hình. Kết quả thực nghiệm cho thấy cách tiếp cận này có thể tận dụng ưu điểm của cả hai phương pháp, vừa khai thác tốt cấu trúc dữ liệu, vừa giảm thiểu sai lệch trong quá trình dự báo. Những phát hiện này mở ra tiềm năng ứng dụng rộng rãi trong nhiều lĩnh vực như khoa học máy tính, khoa học dữ liệu, kinh tế và kỹ thuật.
1	Nghiên cứu này phân tích hiệu quả của mạng nơ-ron nhân tạo (Artificial Neural Network – ANN) trong bài toán dự báo chuỗi thời gian. Dữ liệu nghiên cứu bao gồm các chuỗi quan sát liên tục theo thời gian, có đặc điểm phi tuyến và nhiễu cao. Mô hình ANN được xây dựng với nhiều tầng ẩn nhằm học các mối quan hệ phức tạp trong dữ liệu. Hiệu quả dự báo của ANN được đánh giá thông qua các chỉ số MAE, RMSE và R², đồng thời so sánh với các mô hình thống kê truyền thống. Kết quả thực nghiệm cho thấy ANN có khả năng dự báo tốt hơn, đặc biệt trong các chuỗi có tính phi tuyến mạnh. Nghiên cứu góp phần khẳng định vai trò của ANN trong phân tích chuỗi thời gian hiện đại.
1	Dự báo chuỗi thời gian là một bài toán quan trọng trong nhiều lĩnh vực của khoa học máy tính và khoa học dữ liệu. Các mô hình thống kê truyền thống thường dựa trên giả định tuyến tính, do đó khó khai thác các mối quan hệ phức tạp trong dữ liệu thực tế. Mạng nơ-ron nhân tạo được đề xuất như một giải pháp thay thế nhờ khả năng xấp xỉ hàm phi tuyến. Với cấu trúc linh hoạt, ANN có thể học từ dữ liệu mà không cần giả định phân phối. Trong bối cảnh dữ liệu ngày càng lớn và phức tạp, việc ứng dụng ANN trong dự báo chuỗi thời gian ngày càng được quan tâm. Do đó, nghiên cứu này tập trung đánh giá hiệu quả của ANN trong bài toán dự báo.
1	Dữ liệu chuỗi thời gian được tiền xử lý bằng cách loại bỏ giá trị thiếu và chuẩn hóa để đảm bảo tính ổn định của mô hình. Mô hình ANN được xây dựng với một lớp đầu vào, hai lớp ẩn và một lớp đầu ra. Số lượng nơ-ron trong các lớp ẩn được xác định thông qua quá trình thử nghiệm và xác nhận chéo. Hàm kích hoạt phi tuyến được sử dụng nhằm tăng khả năng học của mô hình. Dữ liệu được chia thành tập huấn luyện và tập kiểm tra theo thứ tự thời gian. Hiệu quả dự báo của ANN được so sánh với mô hình hồi quy tuyến tính và ARIMA dựa trên các chỉ số sai số.
1	Kết quả thực nghiệm cho thấy ANN đạt giá trị RMSE và MAE thấp hơn đáng kể so với các mô hình truyền thống. Hệ số R² của ANN cao hơn, cho thấy khả năng giải thích biến động dữ liệu tốt hơn. Trong các giai đoạn dữ liệu biến động mạnh, ANN vẫn duy trì được độ ổn định trong dự báo. Kiểm định thống kê xác nhận sự cải thiện hiệu suất của ANN là có ý nghĩa ở mức 5%. Tuy nhiên, ANN đòi hỏi thời gian huấn luyện lớn hơn và phụ thuộc vào việc lựa chọn tham số. Điều này cho thấy cần cân nhắc giữa độ chính xác và chi phí tính toán khi triển khai ANN trong thực tế.
1	Nghiên cứu đã chứng minh rằng mạng nơ-ron nhân tạo (ANN) là một công cụ hiệu quả trong dự báo chuỗi thời gian phi tuyến, đặc biệt trong các bài toán có mối quan hệ phức tạp và khó mô hình hóa bằng phương pháp truyền thống. So với các mô hình thống kê cổ điển, ANN cho kết quả dự báo chính xác hơn và thể hiện tính ổn định cao khi xử lý dữ liệu có nhiễu. Tuy nhiên, hiệu suất của ANN phụ thuộc lớn vào việc lựa chọn cấu trúc mạng, số lớp ẩn, số nơ-ron cũng như các tham số huấn luyện phù hợp. Việc tinh chỉnh các tham số này đòi hỏi kinh nghiệm và chi phí tính toán đáng kể.
1	Nghiên cứu này đánh giá vai trò của Apache Spark trong việc triển khai các thuật toán học máy trên dữ liệu lớn. Spark cung cấp môi trường tính toán phân tán hiệu quả, cho phép xử lý dữ liệu với quy mô lớn và tốc độ cao. Các thuật toán học máy phổ biến được triển khai thông qua Spark MLlib nhằm phân tích hiệu suất và khả năng mở rộng. Hiệu quả hệ thống được đánh giá dựa trên thời gian xử lý, khả năng mở rộng và độ chính xác của mô hình. Kết quả thực nghiệm cho thấy Spark giúp cải thiện đáng kể tốc độ xử lý so với các nền tảng truyền thống. Nghiên cứu khẳng định Spark là công cụ phù hợp cho các ứng dụng học máy trên dữ liệu lớn.
1	Sự gia tăng nhanh chóng của dữ liệu lớn đặt ra yêu cầu mới cho các hệ thống học máy. Các nền tảng xử lý truyền thống gặp khó khăn trong việc mở rộng và xử lý dữ liệu phân tán. Apache Spark được phát triển nhằm khắc phục những hạn chế này thông qua mô hình xử lý trong bộ nhớ. Spark hỗ trợ nhiều ứng dụng, trong đó học máy là một lĩnh vực quan trọng. Với thư viện MLlib, Spark cung cấp các thuật toán học máy được tối ưu cho môi trường phân tán. Do đó, việc đánh giá hiệu quả của Spark trong triển khai học máy là cần thiết nhằm hỗ trợ các hệ thống phân tích dữ liệu hiện đại.
1	Nghiên cứu triển khai các thuật toán học máy như hồi quy tuyến tính, Random Forest và K-Means trên nền tảng Spark MLlib. Dữ liệu được lưu trữ và xử lý theo mô hình phân tán. Hiệu suất hệ thống được đo lường thông qua thời gian huấn luyện, thời gian dự báo và mức sử dụng tài nguyên. Các thí nghiệm được thực hiện với quy mô dữ liệu tăng dần nhằm đánh giá khả năng mở rộng. Kết quả được so sánh với việc triển khai trên môi trường đơn lẻ để làm rõ lợi thế của Spark. Phân tích thống kê được sử dụng để đánh giá sự khác biệt về hiệu suất.
1	Kết quả thực nghiệm cho thấy Spark giảm đáng kể thời gian xử lý khi quy mô dữ liệu tăng. Các thuật toán học máy triển khai trên Spark duy trì được độ chính xác tương đương với môi trường truyền thống nhưng có tốc độ vượt trội. Khả năng mở rộng tuyến tính được quan sát khi tăng số nút tính toán. Tuy nhiên, chi phí thiết lập hệ thống và quản lý tài nguyên là những yếu tố cần xem xét. Kết quả khẳng định Spark phù hợp cho các bài toán học máy yêu cầu xử lý dữ liệu lớn và phức tạp.
1	Nghiên cứu đã chứng minh hiệu quả của Apache Spark trong việc triển khai các thuật toán học máy trên môi trường dữ liệu lớn. Spark không chỉ cải thiện đáng kể tốc độ xử lý nhờ khả năng tính toán phân tán trong bộ nhớ, mà còn đảm bảo độ chính xác của mô hình khi làm việc với tập dữ liệu có kích thước và độ phức tạp cao. Kết quả thực nghiệm cho thấy Spark giúp rút ngắn thời gian huấn luyện và mở rộng quy mô hệ thống một cách hiệu quả, đáp ứng tốt yêu cầu của các bài toán phân tích dữ liệu hiện đại. Những kết quả này gợi ý rằng Spark là một nền tảng phù hợp để xây dựng các hệ thống phân tích dữ liệu lớn trong thực tế.
1	Nghiên cứu này so sánh hiệu quả của các thuật toán học máy trong bài toán dự báo dữ liệu phi tuyến. Các thuật toán được xem xét bao gồm SVR, KNN và Random Forest. Dữ liệu nghiên cứu có đặc điểm nhiễu và không tuân theo phân phối tuyến tính. Hiệu quả mô hình được đánh giá thông qua MAE, RMSE và R². Kết quả thực nghiệm cho thấy SVR và Random Forest cho hiệu suất vượt trội so với KNN. Nghiên cứu cung cấp cơ sở thực nghiệm cho việc lựa chọn thuật toán phù hợp trong các bài toán dự báo phi tuyến.
1	Dữ liệu phi tuyến xuất hiện phổ biến trong nhiều lĩnh vực của khoa học máy tính. Các mô hình tuyến tính truyền thống thường không đủ khả năng khai thác cấu trúc phức tạp của dữ liệu. Học máy cung cấp nhiều thuật toán có khả năng xử lý mối quan hệ phi tuyến. Tuy nhiên, việc lựa chọn thuật toán phù hợp vẫn là một thách thức. Do đó, nghiên cứu này tiến hành so sánh các thuật toán học máy phổ biến nhằm đánh giá ưu và nhược điểm của từng phương pháp trong dự báo dữ liệu phi tuyến.
1	Dữ liệu được tiền xử lý bằng cách loại bỏ ngoại lai và chuẩn hóa. Các thuật toán SVR, KNN và Random Forest được triển khai với các tham số tối ưu thông qua xác nhận chéo. Dữ liệu được chia thành tập huấn luyện và tập kiểm tra. Hiệu quả dự báo của từng mô hình được đánh giá bằng các chỉ số sai số. Kiểm định thống kê được sử dụng để xác định sự khác biệt về hiệu suất giữa các thuật toán. Phân tích này giúp làm rõ khả năng ứng dụng của từng mô hình trong dữ liệu phi tuyến.
1	Kết quả thực nghiệm cho thấy các mô hình SVR và Random Forest đạt sai số dự báo thấp hơn đáng kể so với KNN trong bài toán dự báo dữ liệu phi tuyến. Đặc biệt, Random Forest thể hiện độ ổn định cao nhờ khả năng tổng hợp nhiều cây quyết định, giúp giảm hiện tượng quá khớp và cải thiện tính khái quát của mô hình. Trong khi đó, SVR cho thấy độ chính xác vượt trội khi làm việc với dữ liệu nhiễu, nhờ cơ chế tối ưu hóa biên và hàm kernel linh hoạt. Ngược lại, KNN bộc lộ hạn chế rõ rệt khi số chiều dữ liệu tăng cao, do chịu ảnh hưởng của “lời nguyền chiều dữ liệu”, dẫn đến suy giảm hiệu suất. Kiểm định thống kê cho thấy sự khác biệt về hiệu suất giữa các mô hình là có ý nghĩa ở mức 5%.
1	Nghiên cứu này đã tiến hành so sánh hiệu quả của các thuật toán học máy phổ biến trong dự báo dữ liệu phi tuyến, qua đó khẳng định rằng SVR và Random Forest là những lựa chọn phù hợp và đáng tin cậy hơn so với KNN trong các bài toán dự báo phức tạp. Những kết quả thu được không chỉ cung cấp bằng chứng thực nghiệm rõ ràng mà còn góp phần làm rõ vai trò quan trọng của việc lựa chọn mô hình phù hợp với đặc tính dữ liệu. Điều này mang ý nghĩa thực tiễn cao trong các lĩnh vực như kinh tế, tài chính và kỹ thuật. Trong tương lai, các nghiên cứu tiếp theo có thể mở rộng sang các mô hình học sâu như mạng nơ-ron sâu hoặc LSTM nhằm khai thác tốt hơn các mối quan hệ phi tuyến.
1	Nghiên cứu này tập trung phân tích hiệu quả của các thuật toán học máy trong việc xử lý và dự báo dữ liệu lớn phi cấu trúc. Dữ liệu phi cấu trúc, bao gồm văn bản và chuỗi dữ liệu hỗn hợp, ngày càng phổ biến trong các hệ thống thông tin hiện đại. Các thuật toán học máy được sử dụng nhằm khai thác mối quan hệ tiềm ẩn và giảm nhiễu trong dữ liệu. Nghiên cứu đánh giá hiệu quả dự báo thông qua các chỉ số RMSE, MAE và độ chính xác tổng thể, đồng thời so sánh với các phương pháp xử lý truyền thống. Kết quả thực nghiệm cho thấy các mô hình học máy có khả năng xử lý dữ liệu phi cấu trúc hiệu quả hơn, góp phần nâng cao chất lượng dự báo trong các hệ thống dữ liệu lớn.
1	Sự phát triển nhanh chóng của công nghệ số đã tạo ra một lượng lớn dữ liệu phi cấu trúc từ nhiều nguồn khác nhau. Các phương pháp phân tích dữ liệu truyền thống thường gặp hạn chế khi xử lý loại dữ liệu này do tính phức tạp và không đồng nhất. Học máy được xem là một hướng tiếp cận tiềm năng nhờ khả năng tự học và thích nghi với dữ liệu lớn. Các thuật toán học máy có thể phát hiện các mẫu ẩn và mối quan hệ phi tuyến mà các phương pháp thống kê khó khai thác. Do đó, việc nghiên cứu ứng dụng học máy trong phân tích dữ liệu phi cấu trúc là cần thiết nhằm nâng cao hiệu quả dự báo và hỗ trợ ra quyết định trong các hệ thống thông tin hiện đại.
1	Dữ liệu nghiên cứu bao gồm các tập dữ liệu phi cấu trúc được tiền xử lý thông qua các bước làm sạch, chuẩn hóa và trích xuất đặc trưng. Các thuật toán học máy như SVR, Random Forest và Logistic Regression được triển khai nhằm đánh giá hiệu quả dự báo. Dữ liệu được chia thành tập huấn luyện và tập kiểm tra theo tỷ lệ phù hợp để đảm bảo tính khách quan. Quá trình xác nhận chéo được áp dụng để lựa chọn siêu tham số tối ưu. Hiệu quả mô hình được đo lường thông qua các chỉ số sai số và độ chính xác. Phương pháp nghiên cứu cho phép đánh giá toàn diện khả năng xử lý dữ liệu phi cấu trúc của từng thuật toán.
1	Kết quả thực nghiệm cho thấy các mô hình học máy vượt trội hơn so với phương pháp truyền thống trong việc xử lý dữ liệu phi cấu trúc. SVR và Random Forest cho sai số RMSE và MAE thấp hơn đáng kể, trong khi Logistic Regression thể hiện hiệu suất ổn định với dữ liệu ít nhiễu. Hệ số xác định R² đạt giá trị cao cho thấy khả năng giải thích biến động dữ liệu tốt. Kiểm định thống kê xác nhận sự khác biệt về hiệu suất là có ý nghĩa ở mức 5%. Tuy nhiên, chi phí tính toán tăng khi quy mô dữ liệu lớn là một thách thức cần được cân nhắc trong thực tế.
1	Nghiên cứu đã chứng minh tính hiệu quả của học máy trong phân tích và dự báo dữ liệu lớn phi cấu trúc. Các thuật toán học máy không chỉ cải thiện độ chính xác dự báo mà còn khai thác tốt các mối quan hệ phức tạp trong dữ liệu. Kết quả nghiên cứu gợi ý rằng học máy là công cụ phù hợp cho các hệ thống phân tích dữ liệu hiện đại. Trong tương lai, các nghiên cứu có thể mở rộng bằng cách kết hợp học sâu và xử lý ngôn ngữ tự nhiên nhằm nâng cao hiệu quả phân tích dữ liệu phi cấu trúc.
1	Nghiên cứu này tập trung vào việc ứng dụng mạng nơ-ron Long Short-Term Memory (LSTM) để dự báo chuỗi thời gian tài chính, đặc biệt là giá cổ phiếu. Dữ liệu được thu thập từ các sàn giao dịch chứng khoán lớn, bao gồm các chỉ số như giá mở cửa, đóng cửa, khối lượng giao dịch và các biến kinh tế vĩ mô. Mô hình LSTM được chọn vì khả năng xử lý phụ thuộc dài hạn trong dữ liệu chuỗi, khắc phục hạn chế của các mô hình thống kê truyền thống như ARIMA. Quá trình tiền xử lý dữ liệu bao gồm chuẩn hóa, loại bỏ nhiễu và chia tập huấn luyện/kiểm tra theo tỷ lệ 80/20. Các siêu tham số như số lớp ẩn, kích thước batch và tỷ lệ học được tối ưu hóa qua phương pháp grid search.
1	Trong bối cảnh khoa học máy tính, việc tích hợp học sâu vào phân tích dữ liệu lớn đang trở thành xu hướng quan trọng. Nghiên cứu sử dụng môi trường Python với thư viện TensorFlow và Keras để triển khai mô hình. Dữ liệu chuỗi thời gian được biến đổi thành các cửa sổ trượt để phù hợp với đầu vào của LSTM. Để tránh overfitting, kỹ thuật dropout và early stopping được áp dụng. Phân tích thống kê cho thấy mô hình ổn định trên các tập dữ liệu khác nhau, ngay cả với dữ liệu có nhiễu cao. Kết quả kiểm định Wilcoxon xác nhận sự khác biệt ý nghĩa thống kê giữa LSTM và các mô hình truyền thống ở mức 5%.
1	Nghiên cứu kết luận rằng LSTM là công cụ hiệu quả cho dự báo chuỗi thời gian tài chính, vượt trội so với phương pháp cổ điển nhờ khả năng học tự động các đặc trưng phức tạp. Kết quả gợi ý rằng học sâu có thể bổ trợ cho các chiến lược đầu tư, giúp giảm rủi ro. Trong tương lai, nghiên cứu có thể mở rộng bằng cách kết hợp LSTM với các mô hình hybrid như Attention hoặc Transformer để cải thiện độ chính xác hơn nữa. Ngoài ra, việc tích hợp dữ liệu thời gian thực từ API tài chính sẽ tăng tính ứng dụng thực tế. Những đóng góp này góp phần vào lĩnh vực khoa học dữ liệu, nhấn mạnh nhu cầu phát triển các thuật toán học máy phù hợp với dữ liệu lớn và biến động cao trong khoa học máy tính.
1	Nghiên cứu này đánh giá hiệu quả của thuật toán Random Forest trong bài toán phân loại dữ liệu lớn thuộc lĩnh vực khoa học máy tính. Dữ liệu được thu thập từ các nguồn mở như Kaggle, bao gồm các tập dữ liệu có chiều cao, nhiễu lớn và phân bố không đồng đều. Mô hình Random Forest được triển khai trên nền tảng Apache Spark để tận dụng xử lý song song. Các siêu tham số như số lượng cây, độ sâu tối đa và số đặc trưng ngẫu nhiên được tối ưu hóa qua cross-validation. Hiệu quả được đo lường bằng Accuracy, Precision, Recall và F1-score, so sánh với Decision Tree và Logistic Regression. Kết quả thực nghiệm cho thấy Random Forest vượt trội với F1-score trung bình 0.92, giảm overfitting nhờ cơ chế ensemble. Nghiên cứu khẳng định Random Forest phù hợp cho dữ liệu lớn trong môi trường phân tán.
1	Sự bùng nổ dữ liệu lớn đặt ra thách thức cho các thuật toán phân loại truyền thống, thường gặp vấn đề về hiệu suất và tổng quát hóa. Random Forest khắc phục bằng cách xây dựng nhiều cây quyết định độc lập, giảm phương sai và tăng độ ổn định. Trong nghiên cứu, dữ liệu được tiền xử lý bằng cách loại bỏ giá trị thiếu, chuẩn hóa và mã hóa biến phân loại. Quá trình huấn luyện trên Spark MLlib cho thấy thời gian xử lý giảm 40% so với môi trường đơn lẻ. Phân tích thống kê với kiểm định ANOVA xác nhận sự khác biệt hiệu suất ở mức ý nghĩa 1%. Tuy nhiên, mô hình vẫn nhạy cảm với dữ liệu mất cân bằng, đòi hỏi kỹ thuật resampling.
1	Nghiên cứu kết luận rằng Random Forest là lựa chọn tối ưu cho phân loại dữ liệu lớn, với khả năng mở rộng và độ chính xác cao. Kết quả cung cấp cơ sở thực nghiệm cho việc áp dụng trong các hệ thống thực tế như phát hiện gian lận hoặc phân tích mạng xã hội. Trong tương lai, có thể kết hợp Random Forest với kỹ thuật học sâu để xử lý dữ liệu đa phương thức. Nghiên cứu cũng chỉ ra nhu cầu cải thiện thuật toán để giảm chi phí tính toán trên dữ liệu siêu lớn. Những đóng góp này góp phần vào lĩnh vực khoa học dữ liệu, thúc đẩy phát triển các công cụ học máy hiệu quả hơn trong khoa học máy tính.
1	Nghiên cứu này thực hiện so sánh hiệu quả giữa hai thuật toán học máy phổ biến là Support Vector Regression (SVR) và K-Nearest Neighbors (KNN) trong bài toán dự báo giá cổ phiếu – một dạng chuỗi thời gian tài chính điển hình. Dữ liệu được thu thập từ các công ty niêm yết trên sàn HOSE và HNX trong giai đoạn 5 năm gần nhất, bao gồm các đặc trưng như giá mở cửa, giá đóng cửa, khối lượng giao dịch, biến động chỉ số VN-Index và một số chỉ báo kỹ thuật (RSI, MACD). Quá trình tiền xử lý dữ liệu bao gồm chuẩn hóa Min-Max, xử lý giá trị thiếu và tạo các cửa sổ thời gian (sliding window) để chuyển đổi dữ liệu thành dạng phù hợp cho mô hình học máy.
1	Kết quả thực nghiệm cho thấy SVR tuyến tính đạt hiệu suất vượt trội so với KNN trên hầu hết các tập kiểm tra. Cụ thể, SVR ghi nhận giá trị MAE và RMSE thấp hơn trung bình 18–25% so với KNN, đồng thời ổn định hơn khi dữ liệu có nhiễu cao hoặc giai đoạn thị trường biến động mạnh. KNN thể hiện nhạy cảm rõ rệt với việc lựa chọn tham số k và dễ bị ảnh hưởng bởi “lời nguyền chiều dữ liệu” khi số đặc trưng tăng. Kết quả kiểm định Wilcoxon signed-rank test xác nhận sự khác biệt về hiệu suất giữa hai mô hình là có ý nghĩa thống kê ở mức 5%. Phân tích thêm cho thấy SVR duy trì khả năng tổng quát hóa tốt hơn nhờ cơ chế tối ưu hóa biên và khả năng xử lý phi tuyến qua kernel.
1	Nghiên cứu kết luận rằng Support Vector Regression là lựa chọn hiệu quả và đáng tin cậy hơn so với K-Nearest Neighbors trong bài toán dự báo giá cổ phiếu. Kết quả không chỉ cung cấp bằng chứng thực nghiệm cho các nhà nghiên cứu và nhà đầu tư mà còn làm rõ những hạn chế của KNN khi áp dụng cho dữ liệu chuỗi thời gian có số chiều lớn. Trong tương lai, có thể mở rộng nghiên cứu bằng cách kết hợp SVR với các kỹ thuật học sâu như LSTM hoặc Transformer, hoặc tích hợp thêm dữ liệu phi cấu trúc (tin tức, tâm lý thị trường) để nâng cao độ chính xác dự báo.
1	Nghiên cứu này tập trung vào việc ứng dụng Apache Spark kết hợp với thuật toán Gradient Boosting (GB) để phân tích và dự đoán hành vi người dùng trên các nền tảng mạng xã hội quy mô lớn. Dữ liệu được thu thập từ các nguồn công khai (bao gồm tương tác, thời gian hoạt động, loại nội dung tương tác) với quy mô hàng triệu bản ghi. Apache Spark được sử dụng để xử lý dữ liệu phân tán, thực hiện các bước tiền xử lý như làm sạch, chuẩn hóa và trích xuất đặc trưng trên cụm tính toán. Mô hình Gradient Boosting được triển khai thông qua Spark MLlib với các siêu tham số như learning rate, số lượng cây và độ sâu cây được tối ưu hóa bằng kỹ thuật cross-validation phân tầng theo thời gian.
1	Kết quả thực nghiệm cho thấy mô hình Gradient Boosting trên Spark đạt hiệu suất vượt trội, với F1-score trung bình đạt 0.91 và AUC-ROC 0.94, cao hơn đáng kể so với các mô hình so sánh. Gradient Boosting thể hiện khả năng xử lý tốt các mối quan hệ phi tuyến và tương tác phức tạp giữa các đặc trưng, trong khi Spark giúp giảm thời gian xử lý xuống còn khoảng 35% so với triển khai trên máy đơn lẻ khi quy mô dữ liệu tăng gấp 10 lần. Phân tích tầm quan trọng đặc trưng (feature importance) chỉ ra rằng thời gian hoạt động, tần suất tương tác và loại nội dung là các yếu tố ảnh hưởng mạnh nhất đến hành vi dự đoán.
1	Nghiên cứu kết luận rằng việc tích hợp Apache Spark với Gradient Boosting mang lại lợi thế rõ rệt trong phân tích hành vi người dùng trên mạng xã hội quy mô lớn, cả về tốc độ xử lý lẫn độ chính xác dự báo. Kết quả không chỉ có ý nghĩa thực tiễn trong việc cá nhân hóa nội dung, phát hiện hành vi bất thường hay dự đoán xu hướng lan truyền, mà còn góp phần vào việc phát triển các hệ thống học máy có khả năng mở rộng. Trong tương lai, nghiên cứu có thể mở rộng bằng cách kết hợp thêm các kỹ thuật học sâu (như Graph Neural Networks) hoặc xử lý dữ liệu thời gian thực để nâng cao tính ứng dụng.
0	Trong giám sát sức khỏe kết cấu, đặc biệt là giám sát sức khỏe cầu, nguồn dữ liệu đóng vai trò vô cùng quan trọng. Tuy nhiên trong thực tế, việc mất dữ liệu hoặc dữ liệu bị nhiễu/lỗi là điều khó tránh khỏi. Các nguyên nhân tiềm ẩn rất phức tạp, bao gồm lỗi cảm biến, lỗi truyền dẫn, tương tác vô tuyến, mất điện và lão hóa các bộ phận cảm biến. Dữ liệu đo được có thể bị mất tạm thời hoặc vĩnh viễn. Vấn đề này có thể ảnh hưởng nghiêm trọng đến việc đo lường và đánh giá kết cấu, dẫn đến những kết luận thiếu chính xác hoặc thậm chí có thể sai lệch. Do đó, nghiên cứu tái tạo lại dữ liệu bị mất trong hệ thống giám sát sức khỏe kết cấu là cần thiết.
0	Nghiên cứu này đề xuất một cách tiếp cận dựa trên học sâu (deep learning) – mạng nơ-ron tích chập 1 chiều (1DCNN) để tái tạo lại dữ liệu dao động bị mất hoặc lỗi trong quá trình quan trắc cầu. Thử nghiệm thực tế được thực hiện nhằm thu thập được một bộ dữ liệu dao động đầy đủ của kết cấu cầu. Nguồn dữ liệu này sẽ được sử dụng để đào tạo mạng nơ-ron tích chập. Sau đó, dữ liệu từ một cảm biến sẽ được làm lỗi bằng cách gán cho giá trị 0. Mạng được đào tạo sẽ được sử dụng để xây dựng lại dữ liệu bị lỗi. Kết quả nghiên cứu chứng minh rằng dữ liệu được xây dựng lại phù hợp tốt với dữ liệu thực trong miền thời gian và tần số.
0	Các cảm biến trong hệ thống giám sát sức khỏe kết cấu (SHM) thu thập dữ liệu phục vụ phân tích và đánh giá tình trạng kết cấu [1, 2]. Bằng cách sử dụng các kỹ thuật như phân tích tần số, phân tích thống kê, chúng ta có thể đánh giá độ an toàn và hiệu suất của kết cấu dựa trên các tín hiệu thu thập được [3]. Tuy nhiên, việc không đảm bảo chất lượng dữ liệu trong hệ thống SHM là khó có thể tránh khỏi. Vấn đề này ảnh hưởng trực tiếp đến kết quả đánh giá. Kết quả có thể không chính xác, sai lệch hoặc gây nhầm lẫn, ảnh hưởng đến quá trình giám sát kết cấu. Có rất nhiều nguyên nhân tiềm ẩn, phức tạp gây mất mát dữ liệu.
0	Cảm biến hoạt động dưới tác động của tải trọng và bị ảnh hưởng bởi nhiều yếu tố môi trường khác như mưa, gió, độ ẩm… gây ra các vấn đề hư hỏng hoặc các vấn đề khác liên quan đến lão hóa. Đối với hệ thống theo dõi sức khỏe kết cấu có dây hoặc không dây toàn diện yêu cầu khá nhiều cảm biến, hệ thống truyền dẫn, mô-đun và bộ điều khiển thu thập dữ liệu. Mỗi sự cố phần mềm và phần cứng riêng lẻ có thể chặn tạm thời hoặc vĩnh viễn một số tín hiệu nhất định. Mặc dù việc thay thế các cảm biến bị lỗi là một giải pháp để giải quyết những vấn đề này nhưng nó đòi hỏi một quá trình tốn nhiều công sức và thời gian để phát hiện lỗi, tìm ra nguyên nhân và giải quyết chúng.
0	Trong những năm gần đây, trí tuệ nhân tạo và các thuật toán thông minh nổi lên như một xu hướng giải quyết nhiều bài toán liên quan đến tái tạo dữ liệu. Wan [4] đã đề xuất phương pháp khôi phục dữ liệu của hệ thống SHM trong ứng dụng mạng Bayesian với lập trình Gaussian đa chiều. Hiệu suất của dữ liệu được tái tạo lại đạt được có độ tin cậy cao. Zhang [5] đã sử dụng mối tương quan dữ liệu để khôi phục dữ liệu ứng suất bị mất. Phương pháp này dẫn đến sai số nội suy khoảng 5 - 7%. Với số lượng dữ liệu SHM được lưu trữ trong cơ sở dữ liệu ngày càng tăng, các kỹ thuật học sâu cung cấp các phương pháp đầy hứa hẹn để giải quyết các vấn đề về tái thiết dữ liệu.
0	Lei và cộng sự [6] đã sử dụng mạng đối nghịch tạo tích chập sâu để xây dựng lại dữ liệu bị mất trong SHM. Các nghiên cứu về học sâu (deep learning) được đề xuất có khả năng tái tạo dữ liệu của gia tốc kế và cảm biến biến dạng trên mô hình và trong thực tế với hiệu suất cao. Jiang và cộng sự [7] đã thực hiện khôi phục dữ liệu chưa hoàn chỉnh của hệ thống SHM dài hạn bằng phương pháp học tập không giám sát dựa trên mạng GAN (Global Area Network).
0	Mạng nơ-ron tích chập (CNN) có thể học một cách thích ứng các tính năng cấp cao bằng các bộ lọc tích chập quét qua các vùng cục bộ của một lượng lớn dữ liệu. Phương pháp sử dụng mạng nơ-ron tích chập để khôi phục dữ liệu SHM cho phép phục hồi dữ liệu bị mất tương đối tốt, ngay cả đối với các tín hiệu có tỷ lệ mất dữ liệu nghiêm trọng lên tới 90%. Fan [8] đã tiến hành nghiên cứu và đạt được kết quả tốt trong việc khôi phục dữ liệu phản ứng động bằng cách sử dụng mạng tích chập (CNN) với các kết nối dày đặc giữa các lớp trong mạng. Trong nghiên cứu này, ứng dụng mạng nơ-ron tích chập 1 chiều (1DCNN) trong tái tạo dữ liệu dao động, phục vụ giám sát sức khỏe kết cấu sẽ được thực hiện.
0	Mạng nơ-ron tích chập (Convolutional Neural Network - CNN) là một mô hình tiên tiến được áp dụng nhiều trong lĩnh vực học sâu. CNN là một tập hợp các lớp tích chập (Convolution) chồng lên nhau và sử dụng các hàm kích hoạt phi tuyến (nonlinear activation) như ReLU và pooling để kích hoạt các trọng số [9]. Mỗi một lớp sau khi thông qua các hàm kích hoạt sẽ tạo ra các thông tin trừu tượng hơn cho các lớp tiếp theo. Các lớp liên kết được với nhau thông qua cơ chế tích chập. Lớp tiếp theo là kết quả tích chập từ lớp trước đó, nhờ vậy mà ta có được các kết nối cục bộ. Trong quá trình huấn luyện mạng, CNN sẽ tự học hỏi những giá trị thông qua các lớp lọc (filter layer).
0	Mạng nơ-ron tích chập 1 chiều được phát triển và được ứng dụng tương đối tốt cho các loại dữ liệu chuỗi thời gian [11]. Giống như trong CNN thông thường, lớp đầu vào là lớp thụ động nhận tín hiệu 1D thô và lớp đầu ra là Perceptron nhiều lớp (Multilayer Perceptron - MLP) có số lượng nơ-ron bằng số lượng lớp. Các hoạt động trích xuất đặc điểm và phân loại đều được hợp nhất thành một quy trình có thể được tối ưu hóa để tối đa hóa hiệu suất của mạng. Mạng nơ-ron tích chập 1 chiều được phát triển và được ứng dụng tương đối tốt cho các loại dữ liệu chuỗi thời gian [19]. Giống như trong CNN thông thường, lớp đầu vào là lớp thụ động nhận tín hiệu 1D thô và lớp đầu ra là lớp MLP có số lượng nơ-ron bằng số lượng lớp.
0	Các hoạt động trích xuất đặc điểm và phân loại đều được hợp nhất thành một quy trình có thể được tối ưu hóa để tối đa hóa hiệu suất của mạng. 1DCNN có các tham số sau: (i) Số lượng lớp chập và lớp được kết nối đầy đủ; (ii) Kích thước hạt nhân trong mỗi lớp chập; (iii) Cách chọn tham số gộp và kích hoạt. Lớp tích chập (Convolution layer) được gọi là phương pháp dịch chuyển và tính toán. Giả sử x là đầu vào có độ dài n và w là hạt nhân có độ dài k của lớp chập. Hơn nữa, vị trí của cửa sổ hạt nhân (kernel) thay đổi theo s (số bước) sau mỗi thao tác tích chập.
0	Lớp gộp (Pooling layer) thường được đặt sau lớp chập và được sử dụng để giảm kích thước của bản đồ đặc trưng và tham số mạng. Tổng hợp tối đa là kỹ thuật tổng hợp nổi tiếng nhất, trong đó giá trị tối đa được chọn trong cửa sổ có kích thước f và sau đó được dịch chuyển qua đầu vào với bước s sau mỗi lần tổng hợp. Giả sử rằng đầu ra của các lớp tích chập có độ sâu lớn hơn 1. Trong trường hợp đó, lớp được làm phẳng (Flatten) chuyển đổi đầu ra của các lớp chập thành định dạng có thể được cung cấp làm đầu vào cho các lớp dày đặc. Các lớp được kết nối đầy đủ kết nối tất cả các nút trong các lớp liên tiếp.
0	Ma trận dữ liệu được trải phẳng và đi qua một lớp được kết nối đầy đủ (fully connective) để đạt được yêu cầu mong muốn.Các bước tiền xử lý dữ liệu đóng vai trò quan trọng trong việc huấn luyện thành công mô hình trí tuệ nhân tạo. Quá trình tiền xử lý dữ liệu mang lại cái nhìn tổng quát hơn về dữ liệu. Dữ liệu sẽ được khám phá trong bước này và sẽ có được cái nhìn sâu hơn về dữ liệu. Đồng thời, việc chuẩn bị dữ liệu phù hợp với yêu cầu đầu vào của 1DCNN cũng được thực hiện. Dữ liệu sẽ được chuẩn hóa và phân chia. Trong nghiên cứu này, dữ liệu từ các cảm biến trong hệ thống giám sát được sử dụng để huấn luyện 1DCNN.
0	Dữ liệu thu được từ các cảm biến sẽ được thiết lập tương ứng với độ nhạy của từng cảm biến trong chương trình đo. Những giá trị này sẽ không có cùng tiêu chuẩn. Việc chuẩn hóa dữ liệu nhằm mục đích đưa tất cả dữ liệu cảm biến về cùng một phạm vi giá trị từ 0 đến 1. Bước tiếp theo, đầu vào và đầu ra của mạng 1DCNN được xác định. Đầu vào ở đây là dữ liệu từ các cảm biến thông thường. Đầu ra là dữ liệu của cảm biến được coi là có lỗi. Dữ liệu đầu vào sẽ có dạng m×200×t (m là số lượng cảm biến thông thường, t là bước thời gian), dữ liệu đầu ra sẽ có dạng 1×200×t. Dữ liệu được chia thành tập huấn luyện và tập kiểm tra.
0	Sau khi chia tập dữ liệu, tập huấn luyện trước tiên được sử dụng để huấn luyện 1DCNN, để mạng có thể tìm hiểu mối quan hệ ánh xạ phi tuyến giữa cảm biến thông thường và dữ liệu cảm biến bị hỏng. Sau khi quá trình huấn luyện mạng hoàn tất, tập kiểm tra sẽ được đưa vào mạng thần kinh đã được huấn luyện để tái tạo lại dữ liệu bị mất. Trong nghiên cứu này, nghiên cứu trường hợp được sử dụng là cầu Đoan Hùng (Hình 3.1) nằm ở địa phận huyện Đoan Hùng, tỉnh Phú Thọ. Cầu bắc qua sông Chảy tại Km111+300 QL2 và phục vụ giao thông trên tuyến Hà Nội - Hà Giang. Tại thời điểm kiểm tra, toàn bộ cầu có cáp dự ứng lực bên ngoài.
0	Một bộ dữ liệu thực tế gồm 5 cảm biến rung được thu thập từ cầu Đoan Hùng. Một lưới đo được thiết kế để triển khai với mục tiêu thu thập dữ liệu rung động của 5 điểm trên cầu. Dao động tại các điểm được đo dưới tác nhân kích thích ngẫu nhiên (gió, dòng chảy, tải trọng xung quanh, phương tiện qua cầu...). Các cảm biến được lắp đặt theo phương thẳng đứng. Sau khi bố trí, lắp đặt các thiết bị đo và cảm biến, hệ thống thu thập bắt đầu thu thập dữ liệu. Phần dữ liệu đầu tiên sẽ bị loại bỏ. Dữ liệu sau khi tín hiệu ổn định sẽ được ghi lại và lưu lại.
0	Quy trình đo được điều khiển bởi máy tính xách tay, máy tính này cũng tập hợp và lưu các phản hồi động. Bộ dữ liệu thực tế bao gồm dữ liệu từ 5 cảm biến trong sơ đồ đo. Mỗi tập hợp con dữ liệu đại diện cho một đặc tính trong mô hình học máy. Trong trường hợp tái tạo dữ liệu kênh đơn, đầu vào của 1DCNN là dữ liệu của 4 cảm biến (cảm biến số 1, 2, 3 và 4). Đầu ra là dữ liệu của cảm biến số 5. Quá trình tiền xử lý dữ liệu được thực hiện. Tập dữ liệu sẽ được chia thành 2 phần: Đào tạo và xác nhận. Tỷ lệ của hai phần này lần lượt là 80 và 20%.
0	Sau khi mạng 1DCNN được đào tạo, sự mất mát chênh lệch giữa giá trị thực và giá trị dự đoán trong quá trình đào tạo được hiển thị trong Hình 3.3. Khi kết thúc quá trình đào tạo, giá trị mất mát đã hội tụ gần bằng 0 trong cả tập huấn luyện và xác nhận. Mạng được huấn luyện thành công. Để minh họa rõ hơn hiệu suất của phương pháp được đề xuất, trực quan hóa dữ liệu của dữ liệu thực và dữ liệu được tái tạo được hiển thị trong Hình 3.4. Mạng được đào tạo đã tái tạo lại một cách hiệu quả thông tin phản hồi và rung động bị thiếu cho cảm biến số 5. Phân tích phổ tần số cũng được thực hiện trên cả dữ liệu thực và dữ liệu được tái tạo.
0	Mặc dù hiệu suất tái thiết của mạng được đào tạo là tốt nhưng dữ liệu được tái tạo không khớp hoàn toàn với dữ liệu thực. Điều này là do sự hiện diện của nhiễu và sự kích thích môi trường yếu trong quá trình thí nghiệm. Tuy nhiên, khi phân tích dữ liệu miền tần số, dữ liệu được tái tạo thu được có thể đại diện hoàn toàn cho dữ liệu thực. Nhìn chung, 1DCNN được đề xuất đã học và tái tạo dữ liệu trong trường hợp có nhiều kênh bị tổn hao. Mặc dù dữ liệu được xây dựng lại không khớp chính xác với dữ liệu thực nhưng dữ liệu được xây dựng lại vẫn có thể được sử dụng để theo dõi tình trạng cấu trúc.
0	Xem xét rằng tần số tự nhiên và các chế độ rung là các đặc tính rung cơ bản quan trọng nhất, mạng được đề xuất trong nghiên cứu này đã xác định và học chúng một cách hiệu quả để nắm bắt các mẫu trong dữ liệu rung. Nghiên cứu này đề xuất sử dụng phương pháp tiếp cận dựa trên học sâu, cụ thể là 1DCNN, để tái tạo lại dữ liệu rung động bị mất nhằm quan trắc sức khỏe kết cấu cầu. Dưới đây là một số kết luận chính: Trong các hệ thống giám sát sức khỏe kết cấu, các trường hợp mất dữ liệu hoặc lỗi dữ liệu có thể thường xuyên xảy ra. Điều này xảy ra do nhiều lý do, bao gồm hư hỏng cảm biến, đường truyền bị gián đoạn, mất điện và tác động đến môi trường.
0	Cần có biện pháp để tái tạo lại dữ liệu. Dữ liệu cảm biến gia tốc có thể được xây dựng lại bằng mạng 1DCNN. Hiệu quả của phương pháp được thể hiện qua trường hợp nghiên cứu từ số liệu thực tế cầu Đoan Hùng. Trường hợp tái tạo dữ liệu đơn kênh được thực hiện cho kết quả tốt. Trước khi huấn luyện mạng, việc xử lý trước dữ liệu là rất quan trọng để đạt được hiệu suất tốt. Dữ liệu cần được tiền xử lý để phù hợp với mạng 1DCNN. Đồng thời, việc chuẩn hóa dữ liệu giúp đạt được kết quả tốt hơn và thời gian thực hiện nhanh hơn.
0	Thực phẩm nhiễm kim loại nặng gây ra nhiều hậu quả nghiêm trọng cho sức khỏe con người, luôn được các cơ sở giám sát chất lượng an toàn thực phẩm (ATTP) kiểm tra bằng các quy trình và phương pháp cẩn thận, thậm chí tốn kém từ việc lấy mẫu đến ước lượng thành phần chất gây hại được tích trong đó. Trong nghiên cứu này, chúng tôi hướng đến sử dụng sóng siêu âm để đánh giá mức độ liên quan với sự tích lũy kim loại nặng trong khoai lang. Sóng siêu âm an toàn với thực phẩm và vì thế không ảnh hưởng đến chất lượng thực phẩm của mẫu kiểm tra.
0	Cùng với dữ liệu thu được, chúng tôi sử dụng mạng học sâu như một bộ phân lớp hiệu quả và đang được quan tâm của nhiều nhà nghiên cứu hiện nay cho việc nhận biết sự khác nhau giữa các mẫu khoai trước và sau khi cho qua dung dịch chì sunfat. Với 31 bộ dữ liệu siêu âm về các mẫu khoai thu được và sử dụng hai mạng nơ-ron (NN) và mạng Deep Boltzmann Machine (DBM) để nhận dạng hai nhóm mẫu khoai lang trước và sau khi cho qua nhiễm chì sunfat. Kết quả cho thấy việc nhận dạng giữa hai nhóm mẫu chưa cho nhiễm và cho nhiễm chì sunfat của mạng NN là 62% với dữ liệu huấn luyện và 55% với dữ liệu kiểm tra, và của mạng DBM là 68% với dữ liệu huấn luyện và 65% với kiểm tra.
0	Nhóm bệnh gây ra bởi thực phẩm không an toàn đang là một gánh nặng cho xã hội và kinh tế của mỗi quốc gia (WHO, 2017; Young & cs., 2016), và các nước trên thế giới đang ngày càng quan tâm chú trọng đến vấn đề này. Nâng cao mức ATTP vì thế là vấn đề chính yếu cho sự ổn định an ninh, xã hội và sự phát triển kinh tế của mỗi nước. Do tình trạng ô nhiễm môi trường ngày càng nghiêm trọng hơn làm cho thực phẩm có thể nhiễm bị độc ở bất cứ khâu nào từ sản xuất đến tiêu dùng.
0	Thêm vào đó, sự phát triển mạnh mẽ của hệ thống vận tải toàn cầu mà thực phẩm có thể được vận chuyển trên thị trường và giữa các quốc gia một cách nhanh chóng, vì thế việc kiểm tra, phát hiện thực phẩm có nguy cơ gây hại đến sức khỏe là một việc quan trọng và cần được giám sát chặt chẽ ở các khâu của chuỗi cung ứng thực phẩm. Thực phẩm nhiễm độc bởi các kim loại nặng thuộc nhóm cần quan tâm nhất cho việc bảo vệ sức khỏe như đã được chỉ ra trong các báo cáo (WHO, 2017; Rahman & cs., 2014), bởi vì kim loại nặng tích lũy trong thực phẩm có tác động nghiêm trọng đến sức khỏe con người.
0	Ước lượng mức độ nhiễm kim loại nặng tích lũy trong thực phẩm trước khi sử dụng là một yêu cầu quan trọng cho vấn đề an toàn thực phẩm (ATTP). Thực phẩm có thể bị nhiễm kim loại từ các nguồn như đất, nước, không khí cho đến các khâu chế biến và chuẩn bị trước khi ăn (Toth & cs., 2016; Gan & cs., 2017). Thông thường, phân tích thành phần hóa chất sẽ chỉ ra các nguyên tố kim loại nặng có chứa trong thực phẩm. Tuy nhiên, vai trò đảm nhiệm phân tích chỉ được thực hiện ở các cơ quan chức năng đặt tại một số vùng trung tâm ở Việt Nam.
0	Trong báo cáo gần đây của Ngân hàng Thế giới về quản lý nguy cơ an toàn thực phẩm tại Việt Nam (WB, 2017) đã chỉ ra rằng vấn đề an toàn thực phẩm (ATTP) nằm trong mối quan tâm rất lớn của người tiêu dùng và người làm chính sách an toàn. Sẽ không thể giải quyết bài toán ATTP nếu không có sự kiểm soát chặt chẽ và một hệ thống giám sát an toàn chuỗi thực phẩm đầy đủ và hiệu quả. Báo cáo cũng nhấn mạnh những công việc chính cần làm để nâng cao mức an toàn thực phẩm (ATTP). Đặc biệt, cần thiết lập một chương trình đồng bộ từ bộ phận tư nhân đến nhà nước cho việc nâng cao hệ thống giám sát ATTP tại Việt Nam.
0	Ở địa phương, theo báo cáo về ATTP và chương trình hành động trong năm 2018 của tỉnh Quảng Ninh (Quangninh, 2017), thực phẩm nhiễm kim loại nặng nằm trong danh sách những nhiệm vụ quan tâm giám sát để đảm bảo ATTP trong phạm vi tỉnh trong giai đoạn 2016–2020. Hệ thống giám sát ATTP hiện tại của tỉnh cần thiết lập mạnh hơn nữa để kiểm soát sự phân phối thực phẩm không an toàn, đồng thời nâng cao nhận thức của người tiêu dùng về vấn đề giữ gìn vệ sinh ATTP. Về công việc hàng ngày, hệ thống giám sát ATTP của tỉnh thu thập những mẫu thực phẩm và gửi mẫu đi phân tích các thành phần gây hại cho sức khỏe tại trung tâm kiểm định ATTP của vùng theo chỉ định của Nhà nước.
0	Rõ ràng công việc này cần thời gian vận chuyển mẫu mà không thể thông báo ngay kết quả chất lượng mẫu tại nơi lấy mẫu được, việc đó không thể đáp ứng ngay được với yêu cầu của thị trường phân phối thực phẩm cần xử lý nhanh. Thêm nữa là, tình trạng ô nhiễm kim loại có nhiều nguy cơ tiềm ẩn trên địa bàn tỉnh Quảng Ninh. Quảng Ninh hiện đang có rất nhiều mỏ than, thậm chí cả các công trường mỏ đang khai thác lộ thiên. Sự khai thác mỏ là nguyên nhân chủ yếu dẫn đến ô nhiễm nguồn nước, đất và không khí. Với nền công nghiệp khai thác mỏ phủ rộng trên địa bàn tỉnh, các vấn đề hệ quả chứng hẳn như thực phẩm nhiễm độc đang là mối quan tâm rất lớn.
0	Cho đến nay, ô nhiễm kim loại trên địa bàn tỉnh thu hút nhiều quan tâm. Nhiều nghiên cứu như đánh giá mức nhiễm kim loại và nguy cơ cho sức khỏe dân cư trên địa bàn tỉnh (Ha & cs., 2018) hoặc nghiên cứu đánh giá sự ảnh hưởng của ô nhiễm kim loại đến tăng trưởng của lúa gạo trồng gần khu vực mỏ Quảng Ninh (Marquez & cs., 2018) đã được tiến hành. Nghiên cứu đánh giá về vấn đề này, ở trong nước có nhiều nghiên cứu quan tâm đến ô nhiễm kim loại nặng và nguy cơ đến sức khỏe người dân như nghiên cứu của Ha & cs. (2018). Nghiên cứu này đã chỉ ra vấn đề ô nhiễm không khí có chứa kim loại nặng gây ra những nguy cơ ảnh hưởng đến sức khỏe người dân.
0	Hay nghiên cứu của Marquez & cs. (2018) còn xem xét sự tích lũy của kim loại nặng ảnh hưởng đến sự sinh trưởng của cây lúa gạo. Vấn đề ô nhiễm kim loại nặng còn nằm trong mối quan tâm rất lớn của các cơ quan quản lý, điều đó thể hiện ở các báo cáo hàng năm (WB, 2017; Quangninh, 2017) ở cả trung ương và địa phương. Sóng siêu âm là kỹ thuật được ứng dụng trong nhiều thiết bị bảo quản thực phẩm vì sự an toàn của nó đối với sức khỏe con người. Trong nghiên cứu của Danh & cs. (2017), nhóm nghiên cứu đã sử dụng sóng siêu âm trong việc rửa rau củ quả và vệ sinh an toàn thực phẩm.
0	Liên quan đến sử dụng sóng siêu âm trích xuất thành phần thực phẩm, Ly & cs. (2018) đã ứng dụng sóng siêu âm để xác định thành phần protein trong gạo.Trong kiểm soát an toàn thực phẩm (ATTP), sóng siêu âm đã được dùng để phân tích tương quan giữa các tính chất lý – sinh của thực phẩm trong các nghiên cứu của Young & cs. (2016), Awad & cs. (2012) và Sheshrao & cs. (2018). Tuy nhiên, dữ liệu thu được để phân tích là rất phức tạp và cần có phương pháp phân tích hiệu quả để tìm ra mối tương quan giữa thành phần hóa chất trong thức ăn với các mẫu sóng siêu âm quan sát được. Điều này đã được nghiên cứu và đánh giá trong các nghiên cứu trên.
0	Gần đây, các phương pháp của học máy và mạng học sâu có nhiều cải tiến cùng với công nghệ hỗ trợ tính toán đã có nhiều ứng dụng hiệu quả trong việc phân tích dữ liệu lớn và đang thu hút nhiều sự quan tâm (Bengio, 2016). Trong các nghiên cứu của Kuong & cs. (2017, 2018a, 2018b), đã sử dụng phương pháp học máy để nhận dạng các mẫu sóng siêu âm trong y tế. Với nhiều phương pháp hiệu quả trong khoa học máy tính được nghiên cứu và chứng minh gần đây nhằm giải quyết các bài toán có dữ liệu lớn, điều đó hướng chúng tôi đến việc sử dụng sóng siêu âm và phương pháp học sâu cho bài toán nhận biết mức độ tích lũy thành phần kim loại nặng trong khoai lang trong phòng thí nghiệm.
0	Trong nghiên cứu gần đây của Roro & cs. (2019), đã xem xét mức độ tích lũy của kim loại nặng tích trong khoai lang ở vùng mà hệ thống thủy lợi có sự ảnh hưởng của công nghiệp dệt may và những hệ quả đến sức khỏe của người dân xung quanh. Nghiên cứu đó cho thấy sự tồn dư kim loại nặng gây hại như Crôm (Cr), Đồng (Cu), Chì (Pb), Kẽm (Zn) trong khoai lang sau thu hoạch. Tương tự, ở Việt Nam, sự phát triển của các khu công nghiệp, nhà máy xí nghiệp dệt may trong những năm gần đây đã được đánh giá trong các nghiên cứu của Ghaly & cs. (2014) và Duy & cs. (2019) về tình hình ô nhiễm từ nền công nghiệp dệt may.
0	Điều đó khiến chúng tôi đi đến xem xét nghiên cứu bước đầu những mẫu khoai lang với sự tích lũy kim loại gây hại này. Trong nghiên cứu này, chúng tôi tiến hành lấy dữ liệu sóng siêu âm truyền qua các mô của khoai lang bằng máy siêu âm Chison Eco. Thiết bị siêu âm sẽ phát ra sóng siêu âm và nhận tín hiệu phản hồi lại từ các mô khoai. Trên mỗi mẫu, dữ liệu chúng tôi thu được là các sóng siêu âm phản hồi lại như vậy. Mục đích của chúng tôi là nhận biết sự khác biệt của các mẫu sóng siêu âm này giữa các mẫu khoai lang trước và sau khi cho hấp thụ chì sunfat (ngâm mẫu khoai lang trong dung dịch chì sunfat).
0	Để xác định nồng độ của dung dịch chì sunfat cho phù hợp, chúng tôi dựa theo các nghiên cứu của Thanh & cs. (2019) và Huong & cs. (2007). Các nghiên cứu đó đánh giá mức độ ô nhiễm của các thành phần hóa chất gây hại trong nguồn nước ở một số khu vực Hà Nội. Theo kết quả đó, hàm lượng dung dịch của chì (Pb) trong nguồn nước sông được đánh giá ở quanh khu vực Hà Nội lớn hơn mức 1 mg/l, thậm chí có thời điểm đến hơn 6 mg/l. Trong thí nghiệm này, chúng tôi lấy dung dịch chì sunfat với nồng độ 4 mg/l và cho các mẫu khoai ngâm trong dung dịch đó.
0	Sau khoảng thời gian 24 giờ, chúng tôi đưa ra thu dữ liệu sóng siêu âm để xem xét khả năng khác biệt của các mẫu sóng trước và sau khi cho hấp thụ với dung dịch chì sunfat. Dữ liệu các sóng siêu âm trên mỗi mẫu, sau khi tiền xử lý loại bỏ các vùng gần chịu cộng hưởng của thiết bị, được biểu diễn để quan sát dưới dạng ảnh B-mode. Hình 1 biểu diễn ảnh B-mode của 3 trường hợp trong số các mẫu chúng tôi xem xét. Với mục đích sử dụng mạng học sâu như là bộ phân lớp mục tiêu cho việc nhận biết các mẫu siêu âm, trong phần này chúng tôi trình bày về mạng nơ-ron truyền thẳng (Hagan & cs., 1996), được xem như mạng học sâu cơ bản dành cho việc học có giám sát đã được phát triển từ lâu.
0	Mạng Deep Boltzmann Machine (DBM) là sự xếp chồng của nhiều RBM (LeCun & cs., 2015). Với thuật toán hiệu quả CD-k, mô hình cho phép tầng ẩn tham gia vào việc học phân phối của dữ liệu đầu vào, đồng thời tầng này lại đóng vai trò như một đầu vào cho tầng ẩn tiếp theo. Đó là cơ sở để phát triển mạng học sâu.Trong nghiên cứu này, chúng tôi sử dụng mạng DBM với 2 tầng ẩn, trong đó tầng ẩn thứ hai có sự tham gia của mạng classRBM, nghĩa là việc huấn luyện mạng tầng thứ hai là học có giám sát, kết hợp với nhãn để nhận diện các đoạn mẫu âm. Mô hình mạng DBM được sử dụng trong nghiên cứu này được mô tả ở Hình 5.
0	Trong nghiên cứu này, chúng tôi thu thập được 31 mẫu dữ liệu siêu âm, bao gồm dữ liệu của các mẫu khoai chưa cho hấp thụ qua dung dịch chì sunfat và sau khi ngâm qua dung dịch chì sunfat với thời gian 24 giờ và 48 giờ. Tất cả các mẫu sau khi ngâm được chúng tôi xếp vào một lớp làm đối chứng với các mẫu chưa cho qua dung dịch.Trên mỗi ảnh dữ liệu thô, chúng tôi lấy ảnh có kích thước 200 × 200 ở giữa tâm làm ảnh thuộc tính cho đánh giá phân lớp. Hai mạng phân lớp được chúng tôi sử dụng là mạng nơ-ron và mạng DBM, trong đó chúng tôi cấu hình cả hai mạng đều có 2 tầng ẩn với kích thước lần lượt là 300 và 100.
0	Nghĩa là tầng ẩn thứ nhất có 300 nơ-ron tham gia và tầng ẩn thứ hai có 100 nơ-ron tham gia. Với mạng nơ-ron, chúng tôi sử dụng thuật toán backpropagation (Hagan & cs., 1996). Với mạng DBM, giữa tầng input x và tầng ẩn h₁ là mạng RBM được huấn luyện bằng thuật toán CD-1, như giới thiệu bởi Hinton (2012). Giữa tầng ẩn h₁ và tầng ẩn h₂ có sự tham gia của lớp nhãn y, hay còn gọi là mạng classRBM, và được huấn luyện bằng thuật toán học có giám sát của classRBM, được trình bày bởi Hugo & cs. (2012). Ở đây, các tham số được khởi tạo ban đầu rất nhỏ, trong khoảng ±10⁻⁵.
0	Chúng tôi sử dụng phương pháp 5-fold cho việc phân chia dữ liệu huấn luyện và dữ liệu kiểm tra. Để đánh giá và so sánh kết quả huấn luyện và kiểm tra của hai mạng, chúng tôi sử dụng độ chính xác, nghĩa là tỷ lệ phần trăm giữa số mẫu được nhận dạng đúng trên tổng số mẫu đầu vào. Kết quả biểu diễn khả năng học của cả hai bộ phân lớp mục tiêu được thể hiện ở Bảng 1. Nghiên cứu này nhằm sử dụng phương pháp học máy trong việc nhận biết các mẫu sóng siêu âm để phát hiện dư lượng kim loại nặng tích lũy trong khoai lang. Nghiên cứu phản ánh tính cần thiết trong điều kiện các vấn đề an toàn thực phẩm (ATTP) đang được xã hội quan tâm, đồng thời gắn liền với tình trạng ô nhiễm môi trường.
0	Mặc dù dữ liệu cho nghiên cứu còn hạn chế, tuy nhiên kết quả của nghiên cứu cho thấy khả năng ứng dụng các phương pháp của mạng học sâu với dữ liệu sóng siêu âm trong việc trợ giúp giám sát sự tích lũy kim loại gây hại trong rau củ. Việc sử dụng mạng DBM trong nghiên cứu này cũng đã chứng tỏ khả năng nhận dạng tốt hơn so với mạng nơ-ron truyền thống. Cho dù vậy, việc so sánh với các phương pháp phân lớp khác cũng là cần thiết cho sự phát triển tiếp theo, hướng tới cài đặt các phương pháp phân lớp hiệu quả, nhằm nâng cao độ chính xác trong việc nhận dạng các mẫu.
0	Hệ thiết bị gia công vật liệu kim loại bằng bức xạ laser kết hợp với cánh tay robot 6 trục được sử dụng rộng rãi trong công nghiệp để gia công các sản phẩm kim loại có hình dạng 3D phức tạp. Để đánh giá các chỉ tiêu kỹ thuật của một hệ robot 6 trục cắt vật liệu kim loại bằng bức xạ laser, cần tiến hành phân tích và đánh giá thông số của từng khối, bộ phận trong hệ thống. Đối với robot 6 trục, một trong những thông số quan trọng nhất quyết định chất lượng làm việc của cánh tay robot là độ chính xác quãng đường dịch chuyển trong quá trình điều khiển chuyển động.
0	Trong bài báo này, chúng tôi trình bày phương pháp đánh giá độ chính xác dịch chuyển của cánh tay robot bằng thị giác máy, thông qua hai thông số chính: tọa độ dịch chuyển và quãng đường dịch chuyển. Phương pháp này cho độ chính xác cao, bố trí thí nghiệm nhanh gọn, và phù hợp với điều kiện phòng thí nghiệm tại Việt Nam. Kết quả đo thực nghiệm, khi so sánh đối chứng với số liệu do nhà sản xuất cung cấp, cho thấy giải pháp sử dụng thị giác máy hoàn toàn đáp ứng yêu cầu về độ chính xác. Kết quả cũng khẳng định rằng bộ lập trình tự động ghi nhận tọa độ dịch chuyển của cánh tay robot một cách chính xác và tức thời, đồng thời có đối chứng với hệ thang/bảng tọa độ gốc, phù hợp với yêu cầu của bài toán đặt ra.
0	Cánh tay robot IRB 4600 là loại robot 6 trục (hay còn gọi là 6 bậc tự do), thuộc nhóm cánh tay robot khớp nối (articulated robotic arm) [1]. Một cánh tay robot 6 trục điển hình có cấu trúc khớp nối theo chiều dọc, bao gồm 7 liên kết được kết nối thành chuỗi thông qua 6 khớp nối (trục). Mỗi trục là một khớp cho phép robot thực hiện các chuyển động khác nhau như xoay, xoắn, v.v. Các chuyển động này được gọi là các bậc tự do. Robot 6 trục có tính linh hoạt cao, khả năng chịu tải tốt và phạm vi làm việc rộng, đáp ứng yêu cầu của hầu hết các ứng dụng công nghiệp, đặc biệt trong tự động hóa gia công vật liệu như cắt, hàn, xử lý vật liệu, sơn, v.v.
0	Để điều khiển chuyển động của cánh tay robot, mỗi trục được trang bị bộ phân giải (encoder), là thiết bị phản hồi vị trí góc của trục robot. Thông tin vị trí này được sử dụng để tính toán sai lệch vị trí, sau đó đưa vào bộ điều chỉnh PID của hệ điều khiển servo, nhằm đảm bảo độ chính xác và ổn định trong quá trình vận hành. Ngay cả khi đầu gia công của robot được lập trình lặp đi lặp lại tại cùng một vị trí, trong thực tế, đầu gia công vẫn có thể không đạt đến cùng một vị trí vật lý hoàn toàn giống nhau. Hiện tượng này có thể xảy ra ngay cả khi tất cả các bộ phân giải (encoder) đều trả về giá trị phản hồi chính xác.
0	Nguyên nhân là do không thể đo lường tuyệt đối độ lệch có thể phát sinh giữa các bộ phân giải, và các sai lệch này có thể xuất hiện do giãn nở nhiệt, độ linh hoạt của cánh tay robot, hoặc đặc tính làm việc của hộp số (Hình 2). Tuy nhiên, đối với các cánh tay robot được chế tạo với chất lượng cao, trong điều kiện làm việc ổn định, các sai lệch vị trí thường ở mức rất nhỏ [2, 3]. Để đánh giá độ sai lệch vị trí dịch chuyển của một cánh tay robot, người ta thường sử dụng các tiêu chí sau đây:
0	Phương pháp thủ công truyền thống đã được sử dụng từ lâu và bao gồm việc di chuyển TCP của robot đến các điểm tham chiếu đã xác định trên đối tượng gia công. Bằng cách sử dụng các công cụ thuôn nhọn, các cạnh của công cụ được đặt trùng hoặc tiếp xúc lần lượt với các cạnh của vật thể, từ đó xác định vị trí của đối tượng. Tuy nhiên, phương pháp hiệu chuẩn thủ công tỏ ra kém hiệu quả trong các ứng dụng yêu cầu độ chính xác cao. Ưu điểm của phương pháp này là đơn giản, chi phí thấp. Nhược điểm là tốn nhiều thời gian và độ chính xác không cao.
0	Phương pháp Bull’s Eye được sử dụng để xác định TCP (Tool Center Point) thông qua việc thiết lập điểm gốc của hệ tọa độ công cụ tại điểm gốc của hệ tọa độ cổ tay robot. Việc xác định TCP được thực hiện bằng thiết bị hiệu chuẩn Bull’s Eye, dựa trên nguyên lý chiếu thẳng của tia laser. Cụ thể, bằng cách di chuyển công cụ của robot đi qua chùm tia laser, có thể xác định chiều rộng vật lý của các phần đồng tâm của công cụ. Khi đã biết hình dạng danh nghĩa của công cụ, việc đo chiều rộng các lát cắt mà công cụ đi qua cho phép xác định chính xác đường tâm của công cụ, từ đó suy ra TCP.
0	Phương pháp Force Control (kiểm soát lực) được thiết kế như một tiện ích bổ sung cho robot được trang bị gói chức năng kiểm soát lực của ABB, cho phép giám sát các lực tác dụng lên vật thể thông qua cảm biến lực bên ngoài.  Laser LAB là một công nghệ hiệu chuẩn dựa trên laser. Phương pháp này bao gồm một thiết bị đo có tên là “laser LAB” và một quả cầu đo. Thiết bị đo laser LAB bao gồm năm cảm biến laser riêng lẻ. Các cảm biến laser được đặt dưới dạng hình ngũ giác trong thiết bị và được căn chỉnh sao cho năm tia laser sẽ giao nhau tại một điểm chung. Bằng cách định vị quả cầu trong thiết bị laser, các vị trí trên bề mặt quả cầu có thể được xác định theo ba chiều.
0	Độ chính xác tuyệt đối là phương pháp do hãng ABB phát triển dựa trên kỹ thuật laser [6]. Bằng cách sử dụng thiết bị theo dõi laser, một số vị trí tọa độ được xác định trong khu vực làm việc của robot. Bằng cách so sánh các vị trí lý thuyết trong bộ điều khiển robot và vị trí thực tế của mặt bích lắp của robot, có thể xác định được một tập hợp hiệu chỉnh các tham số chuyển đổi từ trục robot sang bộ điều khiển servo. Hệ thống PosEye® đo vị trí và hướng của cảm biến so với hệ thống tham chiếu bao gồm các mẫu đã biết. Nguyên lý đo lường tương tự như hệ thống định vị toàn cầu.
0	Hiệu chuẩn robot thường được thực hiện bằng cách định vị các điểm hiệu chuẩn sử dụng robot làm thiết bị đo lường. Điều hướng là một kỹ thuật tự động, rất chính xác và độc lập với người dùng, thay thế các bước thủ công.Các phương pháp đo và hiệu chuẩn độ chính xác dịch chuyển cánh tay robot được trình bày ở trên có nhiều ưu điểm là đo với độ chính xác cao, một số phương pháp ít nhiều có nhược điểm ở các khía cạnh khác nhau (do đặc thù của phương pháp). Điểm chung của các phương pháp đo là có độ chính xác cao nhưng giải pháp này đi kèm thiết bị thường rất đắt tiền; yêu cầu thiết bị bổ sung đặc thù, giai đoạn thiết lập tốn thời gian và cần có kỹ năng đặc biệt.
0	Với phương pháp hiệu chuẩn thủ công thì độ chính xác không cao, cần nhiều thời gian và phụ thuộc rất nhiều vào người vận hành. Trong bài báo này, chúng tôi đề xuất một phương pháp mới để đánh giá độ chính xác của cánh tay robot bằng cách sử dụng thị giác máy: Đánh giá độ chính xác của vị trí và độ chính xác lặp lại vị trí, sử dụng hai camera để theo dõi vị trí của TCP so với vị trí tham chiếu. Giá trị nhận được từ camera (x, z) và (y, z) sẽ qua xử lý của máy tính để được kết quả tức thời. Đánh giá độ chính xác đường tuyến tính và độ chính xác lặp lại đường tuyến tính, sử dụng một camera để theo dõi độ lệch vị trí của TCP so với hình tham chiếu.
0	Giá trị nhận được từ camera (dx, dy) sẽ qua xử lý của máy tính để được kết quả tức thời. Một bàn cơ khí kích thước 620x620 mm, trên đó lắp 2 thiết bị camera độ phân giải FullHD. Bố trí camera vuông góc với nhau, hướng vào một tấm bia (hình 5). Khoảng cách từ ống kính camera đến bia khoảng 400 mm, kết hợp góc mở, độ phóng đại cho kết quả hình ảnh kích thước khoảng 60x40 mm.Đầu ra từ camera được nối vào máy tính, xử lý, thu hình ảnh vào khu vực cần theo dõi với kích thước 20x20 mm (hình 6).
0	Bố trí camera theo dõi như ở hình 7. Kết quả theo dõi được ghi lại bởi camera ở phân giải chiều ngang 1920 pixel tương đương 60 mm (hình 8). Camera thuộc loại square pixel, vì vậy ta có độ phân giải tương ứng 32 pixels/mm. Như vậy ta có độ sai số khi tính theo pixel là 1/32 mm ≅0,033 mm. Rapid là ngôn ngữ chương trình được sử dụng để điều khiển robot ABB [3]. Chương trình cho phép điều khiển chuyển động của trục robot, tính toán toán học, tín hiệu I/O... Cú pháp của chương trình tương tự như ngôn ngữ Basic. Với Rapid, vị trí đường đi trong các lệnh điều khiển chuyển động nhanh được biên soạn và chuyển đến bộ nội suy và kế hoạch đường đi của robot.
0	Tiêu chuẩn ISO 9283:1998 [7]: Thao tác với robot công nghiệp - Tiêu chí hiệu suất và các phương pháp thử nghiệm liên quan, mô tả các thử nghiệm để đánh giá hiệu suất của robot công nghiệp. Trong số những thứ khác, nó cung cấp các quy trình để đo đúng độ chính xác của vị trí robot, độ lặp lại và độ chính xác đường dẫn. Theo ISO 9283:1998, tất cả các thử nghiệm nên được thực hiện bên trong cái gọi là khối kiểm tra ISO, là khối lập phương lớn nhất có thể phù hợp bên trong không gian làm việc của robot; Ngoài ra, độ chính xác của vị trí và độ lặp lại nên được đo ở năm cấu hình khác nhau, mỗi cấu hình tối thiểu 30 lần.
0	Với khuyến cáo trên, thiết lập đối với IRB 4600 được lựa chọn là: Khối lập phương 1 M với tất cả sáu trục trong chuyển động; Tốc độ của cánh tay robot là 100, 150 và 200 mm/giây; Việc xác định độ chính xác vị trí và độ chính xác lặp lại cần được thực hiện tối thiểu với cấu hình MoveL, MoveC. Thử nghiệm độ chính xác lặp lại được tiến hành với các điểm: aPAccHome,  aPAccPreDest,  aPAccRealDest,  aCurveStart, aCurveMidle. Thử nghiệm với 2 cấu hình như sau:Chuyển động thẳng MoveL (hình 9): Lập trình để robot di chuyển theo lộ trình aPAccHome đến aPAccPreDest rồi đến aPAccRealDest bằng lệnh MoveL.
0	Đến aPAccRealDest robot dừng để đo số liệu rồi quay về aPAccHome.Chuyển  động  cong  MoveC  (hình  10):  Lập  trình  để robot di chuyển theo lộ trình aPAccHome đến aCurveStart, từ  đó  đến  aPAccRealDest  bằng  lệnh  MoveC  qua  điểm aCurveMidle. Đến aPAccRealDest robot dừng để đo số liệu rồi đi qua aPAccPreDest quay về aPAccHome.Trong quá trình di chuyển, áp dụng vùng z15 ở điểm trung  gian  và  là  FINE  ở  điểm  aPAccRealDest.  Tốc  độ vHome= [250,100,40,40] khi di chuyển trung gian và vDest khi đến đích aPAccRealDest. vDest được đặt ở ba giá trị V= 100, 150 và 200 (mỗi giá trị đo 3 ngày và thu thập 200 số liệu). Kết quả đo tại phòng thí nghiệm của Trung tâm Công nghệ Laser - Viện Ứng dụng Công nghệ thể hiện ở bảng 1.
0	Trên cơ sở nhu cầu cần đo, kiểm tra các thông số dịch chuyển của cánh tay robot 6 bậc tự do: độ chính xác và độ lặp lại vị trí dịch chuyển cánh tay robot; độ chính xác đường tuyến tính và độ chính xác lặp lại đường tuyến tính. Chúng tôi đã đề xuất phương phápsử dụng thị giác máy để đánh giá độ chính xác chuyển động của cánh tay robot IRB 4600 đã được đầu tư tại Phòng Thí nghiệm của Viện Ứng dụng Công nghệ. Trên cơ sở đó, xây dựng phương pháp đo độ chính xác dịch chuyển của các loại cánh tay robot công nghiệp bằng các trang thiết bị phù hợp với điều kiện sẵn có tại Viện mà vẫn cho các giá trị đáng tin cậy.
0	Kết quả thu được có độ chính xác nằm trong ngưỡng tham khảo từ hãng ABB [7]:Sử dụng phương pháp này sẽ đem lại các lợi ích sau: Yêu cầu thiết bị bổ sung là phù hợp với các thiết bị, máy móc sẵn có trên thị trường hiện nay. Điều này cũng có nghĩa là giai đoạn thiết lập và vận hành nhanh hơn. Dễ dàng vận hành, không cần phải có năng lực đặc biệt. Ví dụ, việc hiệu chuẩn có thể được thực hiện bởi người vận hành robot. Thời gian khởi động linh hoạt và giảm thiểu. Việc hiệu chỉnh lại các đối tượng công việc có thể được tiến hành theo khoảng thời gian đã lên lịch. Bằng cách hiệu chỉnh lại các đối tượng làm việc trong khoảng thời gian đã đặt, độ chính xác vị trí liên tục có thể được đảm bảo.
1	Học máy đã cách mạng hóa lĩnh vực xử lý hình ảnh, cho phép các hệ thống tự động nhận diện và phân tích dữ liệu hình ảnh với độ chính xác cao. Báo cáo này khảo sát các tiến bộ gần đây trong các thuật toán học máy, tập trung vào mạng nơ-ron tích chập (CNN) và các mô hình học sâu như ResNet và Vision Transformer. Chúng tôi khám phá ứng dụng trong y tế, ô tô tự lái và giám sát an ninh, đồng thời đánh giá hiệu suất qua các chỉ số như độ chính xác, tốc độ xử lý và khả năng chống nhiễu.
1	Dữ liệu từ các bộ dữ liệu chuẩn như ImageNet và COCO được sử dụng để minh họa sự cải thiện từ 70% độ chính xác ban đầu lên hơn 95% ở các mô hình hiện đại. Báo cáo cũng thảo luận về thách thức như thiếu dữ liệu đa dạng và vấn đề đạo đức trong nhận diện khuôn mặt. Kết luận đề xuất hướng nghiên cứu tương lai, nhấn mạnh tích hợp học máy với edge computing để ứng dụng thực tế. Khoa học máy tính, đặc biệt là lĩnh vực học máy, đã chứng kiến sự phát triển vượt bậc trong xử lý hình ảnh nhờ sự bùng nổ của dữ liệu lớn và sức mạnh tính toán. Xử lý hình ảnh liên quan đến việc trích xuất thông tin ý nghĩa từ dữ liệu hình ảnh, từ nhận diện đối tượng đến phân đoạn ngữ nghĩa.
1	Trong bối cảnh cách mạng công nghiệp 4.0, các ứng dụng thực tế như chẩn đoán bệnh qua hình ảnh y tế hoặc hệ thống xe tự lái đòi hỏi thuật toán hiệu quả và đáng tin cậy. Báo cáo này nhằm mục tiêu phân tích các tiến bộ chính trong học máy cho xử lý hình ảnh, bao gồm sự chuyển dịch từ phương pháp truyền thống dựa trên đặc trưng thủ công sang các mô hình học sâu tự động hóa. Chúng tôi sẽ khảo sát lịch sử phát triển, từ AlexNet năm 2012 đến các mô hình hiện đại như EfficientNet. Ngoài ra, báo cáo nhấn mạnh tầm quan trọng của dữ liệu huấn luyện đa dạng để giảm thiên kiến và tăng tính tổng quát hóa.
1	Cuối cùng, chúng tôi đề xuất các hướng nghiên cứu để vượt qua hạn chế hiện tại, góp phần vào sự phát triển bền vững của ngành. Các nghiên cứu trước đây về học máy trong xử lý hình ảnh bắt đầu từ những năm 1980 với các thuật toán cơ bản như bộ lọc cạnh và phân loại dựa trên đặc trưng SIFT. Tuy nhiên, bước ngoặt lớn xảy ra với sự ra đời của CNN bởi LeCun vào năm 1989, được phổ biến rộng rãi qua cuộc thi ImageNet năm 2012. Krizhevsky et al. (2012) với AlexNet đã giảm lỗi phân loại từ 25% xuống còn 15%, mở đường cho các mô hình sâu hơn như VGGNet (Simonyan & Zisserman, 2014) và GoogLeNet (Szegedy et al., 2015).
1	Gần đây, Vision Transformer (Dosovitskiy et al., 2020) áp dụng cơ chế attention từ xử lý ngôn ngữ vào hình ảnh, đạt hiệu suất vượt trội trên các bộ dữ liệu lớn. Các tài liệu cũng chỉ ra thách thức như overfitting và nhu cầu dữ liệu lớn, dẫn đến các kỹ thuật như data augmentation và transfer learning (Pan & Yang, 2010). Trong ứng dụng y tế, Howard et al. (2019) với MobileNet chứng minh khả năng triển khai trên thiết bị di động. Tổng hợp, tài liệu khảo sát cho thấy sự tiến hóa từ mô hình tĩnh sang động, với trọng tâm vào hiệu quả tính toán và độ chính xác.
1	Phương pháp của báo cáo này dựa trên phân tích so sánh các thuật toán học máy cho xử lý hình ảnh. Chúng tôi sử dụng các bộ dữ liệu chuẩn như MNIST cho nhận diện chữ số, CIFAR-10 cho phân loại hình ảnh nhỏ, và MS COCO cho nhận diện đối tượng phức tạp. Các mô hình được đánh giá bao gồm CNN cơ bản, ResNet-50 với residual blocks để giải quyết vanishing gradient, và Vision Transformer với multi-head attention. Quá trình huấn luyện sử dụng framework TensorFlow và PyTorch, với optimizer Adam và learning rate 0.001. Để đo lường hiệu suất, chúng tôi áp dụng các metrics như accuracy, precision, recall, và F1-score, đồng thời thực hiện cross-validation 5-fold để đảm bảo tính tổng quát. Thử nghiệm được chạy trên GPU NVIDIA RTX 3080, với batch size 32 và epochs 50.
1	Ngoài ra, chúng tôi tích hợp kỹ thuật regularization như dropout (0.5) và L2 regularization để giảm overfitting. Phương pháp cũng bao gồm phân tích nhạy cảm với nhiễu Gaussian và adversarial attacks để đánh giá độ bền vững của mô hình. Toàn bộ mã nguồn được lưu trữ trên GitHub để tái tạo. Kết quả thí nghiệm cho thấy ResNet-50 đạt độ chính xác 92% trên CIFAR-10, cao hơn CNN cơ bản (85%), trong khi Vision Transformer đạt 95% nhưng yêu cầu thời gian huấn luyện dài hơn 30%. Trên MS COCO, mô hình phát hiện đối tượng với mAP (mean Average Precision) 0.45 cho ResNet và 0.52 cho Transformer, chứng tỏ khả năng xử lý hình ảnh phức tạp tốt hơn.
1	Khi thêm nhiễu Gaussian (sigma=0.1), độ chính xác giảm trung bình 10%, nhưng với data augmentation như rotation và flipping, sự giảm sút chỉ còn 5%. Transfer learning từ ImageNet giúp cải thiện tốc độ hội tụ, giảm epochs cần thiết từ 50 xuống 30. Trong ứng dụng thực tế, mô hình trên thiết bị di động (MobileNet variant) đạt FPS (frames per second) 20, phù hợp cho ô tô tự lái. Biểu đồ so sánh cho thấy Transformer vượt trội ở dữ liệu lớn nhưng kém hiệu quả ở tài nguyên hạn chế. Tổng thể, kết quả khẳng định tiến bộ trong học sâu, nhưng vẫn tồn tại khoảng cách giữa hiệu suất lý thuyết và thực tế.
1	Các kết quả nhấn mạnh rằng học sâu đã nâng cao đáng kể xử lý hình ảnh, nhưng vẫn đối mặt với thách thức như nhu cầu dữ liệu lớn và thiên kiến. Ví dụ, mô hình huấn luyện trên dữ liệu phương Tây có thể kém chính xác với hình ảnh từ châu Á, đòi hỏi dataset đa dạng hơn. So với tài liệu, kết quả của chúng tôi phù hợp với Howard et al. (2019), nhưng bổ sung phân tích adversarial, cho thấy Transformer bền vững hơn ResNet trước tấn công. Về ứng dụng, trong y tế, nhận diện ung thư qua X-quang có thể cứu sống, nhưng vấn đề đạo đức như quyền riêng tư dữ liệu cần được giải quyết qua federated learning.
1	Hạn chế của nghiên cứu bao gồm chưa thử nghiệm trên dữ liệu thực thời gian lớn, và chỉ tập trung vào hình ảnh tĩnh. Tương lai, tích hợp với edge AI có thể giảm độ trễ, mở rộng ứng dụng cho IoT. Thảo luận này gợi ý rằng tiến bộ học máy không chỉ kỹ thuật mà còn cần cân nhắc xã hội và đạo đức để triển khai bền vững.Tóm lại, báo cáo đã làm rõ các tiến bộ trong học máy cho xử lý hình ảnh, từ CNN truyền thống đến Transformer hiện đại, với ứng dụng rộng rãi trong nhiều lĩnh vực. Kết quả chứng minh sự cải thiện độ chính xác và hiệu quả, nhưng cũng chỉ ra nhu cầu vượt qua thách thức như thiếu dữ liệu và an ninh.
1	Nghiên cứu này góp phần vào kho kiến thức khoa học máy tính, khuyến khích các nhà nghiên cứu tập trung vào mô hình hybrid kết hợp sức mạnh của nhiều kiến trúc. Để phát triển hơn, cần hợp tác quốc tế xây dựng dataset mở và tiêu chuẩn đạo đức. Cuối cùng, học máy không chỉ là công cụ kỹ thuật mà còn là động lực thay đổi xã hội, từ y tế đến giao thông. Chúng tôi hy vọng báo cáo này sẽ truyền cảm hứng cho các nghiên cứu tiếp theo, thúc đẩy sự đổi mới trong ngành khoa học máy tính.
1	Xử lý ngôn ngữ tự nhiên (NLP) là một nhánh quan trọng của khoa học máy tính, tập trung vào việc cho phép máy tính hiểu và tạo ra ngôn ngữ con người. Báo cáo này khảo sát các tiến bộ gần đây trong NLP, đặc biệt là các mô hình dựa trên Transformer như BERT và GPT. Chúng tôi phân tích ứng dụng trong dịch máy, tóm tắt văn bản và chatbot, đánh giá qua các chỉ số như BLEU score và ROUGE. Dữ liệu từ các bộ như GLUE và SQuAD cho thấy độ chính xác tăng từ 70% lên hơn 90% ở các mô hình hiện đại. Báo cáo cũng thảo luận thách thức như xử lý ngôn ngữ đa dạng và vấn đề thiên kiến.
1	Kết luận đề xuất hướng nghiên cứu tích hợp NLP với multimodal learning để cải thiện hiệu suất trong môi trường thực tế. Trong kỷ nguyên số, NLP đã trở thành trụ cột của khoa học máy tính, hỗ trợ các ứng dụng từ tìm kiếm thông tin đến trợ lý ảo. NLP liên quan đến việc phân tích cú pháp, ngữ nghĩa và ngữ dụng của ngôn ngữ, giúp máy tính tương tác tự nhiên với con người. Với sự phát triển của dữ liệu lớn và học sâu, các mô hình như RNN và LSTM đã được thay thế bởi Transformer (Vaswani et al., 2017).
1	Báo cáo này nhằm khám phá các tiến bộ chính, từ word embedding như Word2Vec đến large language models (LLM). Chúng tôi nhấn mạnh vai trò của NLP trong kinh doanh, giáo dục và y tế, đồng thời chỉ ra nhu cầu dữ liệu đa ngôn ngữ để giảm khoảng cách văn hóa. Cuối cùng, báo cáo đề xuất các chiến lược nghiên cứu để nâng cao độ tin cậy và đạo đức trong NLP. Lịch sử NLP bắt đầu từ những năm 1950 với máy dịch tự động, nhưng tiến bộ lớn xảy ra với học máy thống kê vào thập niên 1990. Mikolov et al. (2013) giới thiệu Word2Vec, mở đường cho embedding vector. Transformer của Vaswani et al. (2017) cách mạng hóa với cơ chế attention, dẫn đến BERT (Devlin et al., 2018) cho nhiệm vụ downstream.
1	Gần đây, GPT-3 (Brown et al., 2020) chứng minh khả năng zero-shot learning. Tài liệu cũng thảo luận thách thức như ambiguity và context, với giải pháp từ fine-tuning và reinforcement learning (Lewis et al., 2019). Trong ứng dụng, Radford et al. (2019) áp dụng cho generation, đạt hiệu suất cao. Tổng hợp, khảo sát cho thấy sự chuyển dịch từ rule-based sang data-driven, tập trung vào scale và generalization. Phương pháp dựa trên đánh giá so sánh các mô hình NLP trên bộ dữ liệu chuẩn như WikiText cho language modeling và CoNLL cho NER. Các mô hình bao gồm BERT-base, RoBERTa với robust training, và T5 cho text-to-text. Sử dụng Hugging Face Transformers để huấn luyện, với optimizer AdamW và learning rate scheduler. Metrics bao gồm perplexity, F1-score và accuracy, với 5-fold cross-validation.
1	Thử nghiệm trên GPU cluster, batch size 16, epochs 10-20. Áp dụng augmentation như back-translation để tăng đa dạng dữ liệu. Phân tích robustness qua adversarial examples từ TextAttack. Mã nguồn công khai trên GitHub cho reproducibility. Kết quả cho thấy BERT đạt F1-score 89% trên CoNLL, RoBERTa cải thiện lên 92%, trong khi T5 đạt 95% ở multitask. Perplexity giảm từ 50 ở LSTM xuống 10 ở Transformer-based. Với dữ liệu đa ngôn ngữ, accuracy giảm 8% nếu thiếu fine-tuning, nhưng augmentation giúp phục hồi. Thời gian inference: BERT 0.5s/sentence, phù hợp cho real-time. Biểu đồ so sánh minh họa Transformer vượt trội ở long-context. Tổng thể, kết quả khẳng định NLP đang tiến gần human-level, nhưng vẫn cần cải thiện ở low-resource languages.
1	Kết quả nhấn mạnh sức mạnh của Transformer trong NLP, nhưng thiên kiến từ dữ liệu huấn luyện (ví dụ: gender bias) vẫn là vấn đề lớn. So với tài liệu, nghiên cứu bổ sung phân tích multilingual, phù hợp với Bender et al. (2021). Ứng dụng như chatbot trong dịch vụ khách hàng có thể tăng hiệu quả, nhưng privacy cần bảo vệ qua differential privacy. Hạn chế: chưa thử nghiệm trên dữ liệu noisy từ mạng xã hội. Tương lai, hybrid model với knowledge graphs có thể nâng cao reasoning. Thảo luận gợi ý cân bằng giữa performance và ethics cho NLP bền vững.
1	Tóm lại, báo cáo đã làm rõ quá trình phát triển của lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) từ các phương pháp embedding truyền thống đến các mô hình ngôn ngữ lớn (LLM) hiện đại, cùng với phạm vi ứng dụng ngày càng rộng trong nhiều lĩnh vực khác nhau. Các kết quả nghiên cứu chứng minh rằng hiệu suất mô hình được cải thiện đáng kể về độ chính xác, khả năng tổng quát hóa và hiểu ngữ cảnh. Tuy nhiên, vẫn tồn tại nhiều thách thức quan trọng như thiên lệch dữ liệu (bias), khả năng mở rộng (scalability), chi phí tính toán và vấn đề đạo đức.
1	An ninh mạng là lĩnh vực cốt lõi của khoa học máy tính, sử dụng học máy để phát hiện và ngăn chặn tấn công. Báo cáo khảo sát tiến bộ trong detection models như anomaly-based và signature-based, tập trung vào deep learning như Autoencoder và GAN. Ứng dụng trong IDS, malware analysis và phishing detection được phân tích, đánh giá qua metrics như TPR và FPR. Dữ liệu từ KDD Cup và NSL-KDD cho thấy accuracy từ 80% lên 98%. Thảo luận thách thức như zero-day attacks và adversarial ML. Kết luận đề xuất tích hợp ML với blockchain cho an ninh bền vững.
1	An ninh mạng đối mặt với mối đe dọa ngày càng phức tạp, từ ransomware đến APT, đòi hỏi công cụ tiên tiến từ khoa học máy tính. Học máy cung cấp khả năng học từ dữ liệu để dự đoán tấn công. Báo cáo khám phá tiến bộ, từ rule-based đến ML-based như SVM và DNN. Chúng tôi nhấn mạnh vai trò trong bảo vệ infrastructure, đặc biệt IoT và cloud. Cũng chỉ ra nhu cầu dữ liệu real-time để cải thiện. Đề xuất hướng nghiên cứu cho robust ML chống evasion. (92 từ – mở rộng: Báo cáo này nhằm phân tích các mô hình chính và ứng dụng thực tế, góp phần vào an ninh số toàn cầu.
1	Nghiên cứu trong lĩnh vực an ninh mạng ban đầu tập trung chủ yếu vào các cơ chế phòng thủ truyền thống như firewall và hệ thống kiểm soát truy cập dựa trên luật tĩnh. Tuy nhiên, từ những năm 1990, machine learning (ML) đã dần nổi lên thông qua các hệ thống phát hiện xâm nhập (IDS) nhằm tự động nhận diện các hành vi bất thường trong mạng. Sommer & Paxson (2010) chỉ ra rằng các phương pháp dựa trên signature gặp nhiều hạn chế trước các mối đe dọa mới và biến thể chưa từng xuất hiện, từ đó thúc đẩy sự phát triển của các kỹ thuật anomaly detection, tiêu biểu là One-Class SVM. Gần đây, Mirsky et al. (2018) đề xuất sử dụng Autoencoder để phát hiện tấn công trong môi trường IoT, nơi dữ liệu lớn và tính dị biệt cao.
1	Phương pháp so sánh models trên datasets như CIC-IDS2017. Models: SVM, Random Forest, LSTM cho sequence data. Sử dụng Scikit-learn và Keras, optimizer SGD. Metrics: accuracy, precision, recall, ROC-AUC với 10-fold CV. Thử nghiệm trên CPU/GPU, batch 64, epochs 30. Augmentation với SMOTE cho imbalanced data. Phân tích adversarial qua FGSM. Mã trên GitHub. (78 từ – mở rộng: Bao gồm feature engineering từ network flows. SVM đạt 90% accuracy trên KDD, LSTM cải thiện 95% ở time-series. FPR giảm từ 5% xuống 1% với ensemble. Adversarial giảm accuracy 10%, nhưng robust training phục hồi 80%. Thời gian detection: 0.1s/packet. Biểu đồ cho thấy ML vượt traditional methods. Kết quả khẳng định hiệu quả nhưng cần real-world validation. (78 từ – mở rộng: Trong phishing, precision 98%.
1	Kết quả nhấn mạnh ML nâng cao detection, nhưng adversarial là thách thức lớn. Phù hợp tài liệu Carlini & Wagner (2017). Ứng dụng trong enterprise security tiết kiệm chi phí, nhưng privacy cần xem xét. Hạn chế: datasets outdated. Tương lai, federated learning cho distributed systems. Thảo luận nhấn mạnh balance security và usability.  Tóm lại, báo cáo làm rõ ML trong an ninh mạng, từ detection đến prevention. Kết quả chứng minh tiến bộ, nhưng cần vượt adversarial. Góp phần khoa học máy tính, khuyến khích hybrid security. Hợp tác xây dựng datasets mới. An ninh mạng là nền tảng xã hội số. Truyền cảm hứng nghiên cứu.
1	Trí tuệ nhân tạo (AI) đang cách mạng hóa lĩnh vực y tế bằng cách cải thiện chẩn đoán, điều trị và quản lý bệnh nhân. Báo cáo này khảo sát các tiến bộ gần đây trong AI, tập trung vào học sâu và học tăng cường, với các mô hình như CNN cho hình ảnh y tế và RL cho lập kế hoạch điều trị. Chúng tôi phân tích ứng dụng trong chẩn đoán ung thư, dự đoán dịch bệnh và y tế cá nhân hóa, đánh giá qua chỉ số như AUC-ROC và specificity. Dữ liệu từ bộ như MIMIC-III và TCGA cho thấy độ chính xác tăng từ 80% lên 96% ở mô hình hiện đại. Báo cáo thảo luận thách thức như bảo mật dữ liệu và thiên kiến thuật toán.
1	Kết luận đề xuất hướng nghiên cứu tích hợp AI với wearable devices để theo dõi sức khỏe thời gian thực, góp phần vào y tế bền vững. Y tế là một trong những lĩnh vực hưởng lợi lớn nhất từ khoa học máy tính, đặc biệt là AI, giúp xử lý lượng dữ liệu khổng lồ từ hồ sơ bệnh án, hình ảnh và gen. AI cho phép dự đoán sớm bệnh tật, tối ưu hóa nguồn lực và cá nhân hóa điều trị, giảm chi phí và tăng hiệu quả. Trong bối cảnh đại dịch COVID-19, AI đã chứng minh giá trị qua mô hình dự báo lây lan. Báo cáo này khám phá tiến bộ từ machine learning cơ bản đến deep learning phức tạp, bao gồm GAN cho tạo dữ liệu giả và transformer cho phân tích văn bản bệnh án.
1	Chúng tôi nhấn mạnh nhu cầu dữ liệu đa dạng để tránh thiên kiến chủng tộc hoặc giới tính. Ngoài ra, báo cáo đánh giá tác động xã hội, như giảm bất bình đẳng y tế ở các nước đang phát triển. Cuối cùng, chúng tôi đề xuất khung nghiên cứu đạo đức để đảm bảo AI phục vụ con người một cách công bằng và an toàn. Nghiên cứu AI trong y tế bắt đầu từ những năm 1970 với hệ chuyên gia như MYCIN cho chẩn đoán nhiễm khuẩn. Tuy nhiên, bước ngoặt là sự ra đời của deep learning, với Esteva et al. (2017) sử dụng CNN để chẩn đoán ung thư da với độ chính xác ngang bác sĩ. Rajkomar et al. (2018) áp dụng Google’s DeepMind cho dự đoán tử vong từ dữ liệu EHR.
1	Gần đây, transformer-based models như BioBERT (Lee et al., 2019) cải thiện xử lý văn bản y khoa. Tài liệu cũng chỉ ra thách thức như thiếu dữ liệu nhãn, dẫn đến federated learning (Rieke et al., 2020) để bảo vệ quyền riêng tư. Trong dự đoán dịch bệnh, Wu et al. (2020) dùng graph neural networks cho COVID-19. Tổng hợp, khảo sát cho thấy sự tiến hóa từ supervised learning sang unsupervised và multimodal, với trọng tâm vào interpretability qua techniques như SHAP (Lundberg & Lee, 2017). Phương pháp dựa trên phân tích so sánh các mô hình AI trên bộ dữ liệu y tế chuẩn như ChestX-ray14 cho hình ảnh phổi và UK Biobank cho gen. Các mô hình bao gồm ResNet cho classification, U-Net cho segmentation và DQN cho reinforcement learning trong điều trị.
1	Sử dụng framework PyTorch và TensorFlow, với optimizer Adam và early stopping để tránh overfitting. Metrics: accuracy, sensitivity, specificity, và Dice coefficient cho segmentation, kết hợp 10-fold cross-validation. hử nghiệm trên cluster GPU với batch size 64, epochs 100. Tích hợp data augmentation như mixup và cutmix để tăng robustness. Phân tích nhạy cảm với noise và adversarial attacks qua libraries như Foolbox. Ngoài ra, áp dụng explainable AI với LIME để giải thích quyết định mô hình. Mã nguồn được chia sẻ trên GitHub, đảm bảo reproducibility và khuyến khích cộng đồng đóng góp. Kết quả cho thấy ResNet đạt AUC-ROC 0.95 trên ChestX-ray14, vượt trội so với baseline SVM (0.85), trong khi U-Net đạt Dice 0.92 cho segmentation tumor. Trong RL, DQN tối ưu hóa liều thuốc giảm 20% side effects so với phương pháp truyền thống.
1	Với dữ liệu multimodal, fusion models cải thiện accuracy 8%. Adversarial testing giảm performance 12%, nhưng robust training phục hồi 90%. Thời gian inference: dưới 1s/image trên edge devices. Biểu đồ so sánh minh họa transformer vượt trội ở text-based tasks với F1-score 0.94. Tổng thể, kết quả khẳng định AI nâng cao y tế, nhưng vẫn cần validation lâm sàng để chuyển giao công nghệ từ lab sang bệnh viện. Kết quả nhấn mạnh tiềm năng AI trong y tế, nhưng thách thức như black-box nature đòi hỏi explainability để bác sĩ tin tưởng. So với tài liệu, nghiên cứu bổ sung multimodal analysis, phù hợp với Huang et al. (2020).
1	Ứng dụng như telemedicine có thể cứu sống ở vùng sâu, nhưng privacy risks cần GDPR-compliant frameworks. Hạn chế: dữ liệu chủ yếu từ phương Tây, dẫn đến bias; đề xuất global datasets. Tương lai, integration với genomics qua AlphaFold (Jumper et al., 2021) có thể cách mạng hóa drug discovery. Thảo luận này nhấn mạnh rằng AI không thay thế bác sĩ mà hỗ trợ, yêu cầu quy định đạo đức để tránh misuse như trong predictive policing y tế. Tóm lại, báo cáo đã làm rõ tiến bộ AI trong y tế, từ chẩn đoán đến điều trị, với kết quả chứng minh hiệu quả cao. Tuy nhiên, cần vượt qua bias và privacy để triển khai rộng rãi. Nghiên cứu góp phần vào khoa học máy tính, khuyến khích collaborative AI với human-in-the-loop. Hợp tác quốc tế xây dựng standards sẽ thúc đẩy innovation.
1	Blockchain là công nghệ phân tán cốt lõi của khoa học máy tính, cung cấp tính minh bạch và an toàn cho dữ liệu. Báo cáo khảo sát tiến bộ trong blockchain, tập trung vào consensus algorithms như PoW, PoS và ứng dụng trong DeFi, NFT. Phân tích hiệu suất qua metrics như TPS (transactions per second) và energy consumption. Dữ liệu từ Ethereum và Hyperledger cho thấy TPS tăng từ 15 lên 1000 ở layer-2 solutions. Thảo luận thách thức như scalability và regulatory issues. Kết luận đề xuất hybrid blockchain với AI để nâng cao smart contracts, góp phần vào nền kinh tế số.
1	Blockchain đã vượt qua vai trò ban đầu trong tiền điện tử để trở thành nền tảng cho các hệ thống phân tán an toàn trong khoa học máy tính. Nó sử dụng cryptography và distributed ledger để đảm bảo tính bất biến và loại bỏ trung gian. Trong bối cảnh số hóa, blockchain hỗ trợ supply chain, voting systems và data sharing. Báo cáo này khám phá tiến bộ từ Bitcoin (Nakamoto, 2008) đến blockchain 3.0 với interoperability. Chúng tôi nhấn mạnh ứng dụng trong IoT và cloud computing, đồng thời chỉ ra nhu cầu giảm energy footprint. Ngoài ra, báo cáo đánh giá tác động kinh tế, như giảm chi phí giao dịch 80%. Cuối cùng, đề xuất nghiên cứu hướng tới quantum-resistant blockchain để chống lại threats tương lai, đảm bảo tính bền vững và mở rộng.
1	Blockchain bắt nguồn từ Nakamoto (2008) với Bitcoin, giới thiệu PoW. Buterin (2013) mở rộng với Ethereum cho smart contracts. Gần đây, Polkadot (Wood, 2016) giải quyết interoperability qua parachains. Tài liệu thảo luận scalability trilemma, với giải pháp sharding (Zamani et al., 2018) và layer-2 như Lightning Network (Poon & Dryja, 2016). Trong ứng dụng, Swan (2015) khám phá DeFi, đạt TVL hàng tỷ USD. Thách thức như 51% attacks được giải quyết qua PoS (King & Nadal, 2012). Tổng hợp, khảo sát cho thấy sự chuyển dịch từ centralized sang decentralized, với trọng tâm vào privacy qua zero-knowledge proofs (Goldwasser et al., 1985).
1	Phương pháp nghiên cứu dựa trên việc mô phỏng các mạng blockchain trong môi trường kiểm soát, sử dụng các công cụ phổ biến như Ganache và Hyperledger Fabric nhằm đánh giá hiệu năng và độ an toàn của các cơ chế đồng thuận khác nhau. Nghiên cứu tiến hành so sánh các thuật toán PoW, PoS và DPoS dựa trên các chỉ số quan trọng như thông lượng giao dịch (TPS), độ trễ (latency) và mức độ bảo mật (security). Dữ liệu được thu thập từ các testnet với quy mô lên tới 1000 nút, trong đó các giao dịch được sinh tự động bằng các script Python để đảm bảo tính nhất quán. Geth được sử dụng cho các mô phỏng Ethereum, kết hợp với các kỹ thuật tối ưu hóa nhằm tinh chỉnh cơ chế đồng thuận.
1	Kết quả cho thấy PoS đạt TPS 500, cao hơn PoW (15), với latency giảm 70%. Energy consumption PoS thấp hơn 99% so PoW. Trong interoperability, Polkadot-style bridges giảm failure rate 90%. Smart contracts audit phát hiện 20% vulnerabilities, fixed qua formal verification. Biểu đồ minh họa scalability cải thiện với sharding. Tổng thể, kết quả khẳng định blockchain sẵn sàng enterprise, nhưng cần optimization cho real-world adoption. Kết quả nhấn mạnh blockchain cải thiện an ninh, nhưng scalability vẫn hạn chế ở public chains. Phù hợp tài liệu Ben-Sasson et al. (2014) về zk-SNARKs. Ứng dụng trong supply chain giảm fraud 50%, nhưng regulatory hurdles chậm adoption. Hạn chế: simulations không capture real attacks. Tương lai, integration với 5G cho IoT. Thảo luận nhấn mạnh ethical use, tránh centralization in DeFi. (92 từ – mở rộng: Cũng xem xét environmental impact.
1	Tóm lại, báo cáo đã làm rõ những tiến bộ quan trọng của công nghệ blockchain, từ các cơ chế đồng thuận cốt lõi đến việc mở rộng các ứng dụng trong nhiều lĩnh vực khác nhau như tài chính, chuỗi cung ứng và quản lý dữ liệu. Các kết quả thực nghiệm chứng minh hiệu quả của blockchain trong việc tăng cường tính minh bạch, bảo mật và khả năng chống giả mạo. Tuy nhiên, công nghệ này vẫn cần vượt qua thách thức của “blockchain trilemma”, bao gồm khả năng mở rộng, tính phi tập trung và mức độ an toàn. Nghiên cứu đóng góp tích cực cho lĩnh vực khoa học máy tính, đồng thời khuyến khích sự phát triển của các ứng dụng phi tập trung (decentralized applications).
1	Dữ liệu lớn (Big Data) kết hợp với học máy đang định hình khoa học máy tính bằng cách xử lý volume, velocity và variety dữ liệu. Báo cáo khảo sát tiến bộ trong frameworks như Hadoop và Spark, tập trung vào ML algorithms cho clustering và prediction. Phân tích ứng dụng trong marketing, finance và climate modeling, đánh giá qua scalability và accuracy. Dữ liệu từ Twitter API và NOAA cho thấy processing time giảm từ hours xuống minutes. Thảo luận thách thức như data quality và privacy. Kết luận đề xuất edge analytics để real-time insights, góp phần vào data-driven decisions.
1	Big Data đại diện cho thách thức và cơ hội lớn trong khoa học máy tính, yêu cầu công cụ xử lý dữ liệu khổng lồ từ sensors, social media và transactions. Học máy nâng cao giá trị bằng cách trích xuất patterns và predictions. Báo cáo khám phá tiến bộ từ MapReduce (Dean & Ghemawat, 2004) đến stream processing với Kafka. Chúng tôi nhấn mạnh ứng dụng trong predictive maintenance và fraud detection, đồng thời chỉ ra nhu cầu ethical data usage. Ngoài ra, báo cáo đánh giá tác động kinh tế, như tăng doanh thu 20% qua personalized recommendations. Cuối cùng, đề xuất nghiên cứu hướng tới AI-augmented analytics để tự động hóa insights, đảm bảo competitiveness trong era data.
1	Big Data bắt đầu phát triển mạnh mẽ với sự ra đời của Hadoop (White, 2009), cho phép xử lý và lưu trữ dữ liệu phân tán trên các cụm máy tính quy mô lớn, mở ra khả năng khai thác dữ liệu vượt xa hệ thống truyền thống. Tiếp theo, Zaharia et al. (2010) giới thiệu Apache Spark với cơ chế xử lý in-memory, giúp cải thiện đáng kể tốc độ tính toán và hỗ trợ cả batch processing lẫn stream processing. Gần đây, TensorFlow Extended (TFX) do Baylor et al. (2017) đề xuất đã tích hợp toàn bộ pipeline học máy, từ tiền xử lý dữ liệu, huấn luyện đến triển khai mô hình, góp phần tự động hóa quy trình phân tích dữ liệu lớn.
1	Phương pháp dựa trên benchmarking frameworks trên datasets lớn như Common Crawl. So sánh Spark MLlib với Scikit-learn cho tasks như K-means và regression. Sử dụng AWS EMR cho distributed runs, metrics: throughput, fault tolerance. Data ingestion qua Kafka, processing với PySpark. Áp dụng hyperparameter tuning với GridSearchCV. Phân tích privacy qua differential privacy libraries. Mã trên GitHub. Kết quả cho thấy Spark xử lý 1TB data trong 5 minutes, nhanh hơn Hadoop 10x. ML models đạt accuracy 92% ở prediction, với clustering silhouette score 0.8. Real-time streaming giảm latency 90%. Biểu đồ minh họa scalability linear với nodes. Tổng thể, kết quả khẳng định Big Data ML hiệu quả cho enterprise.
1	Kết quả nghiên cứu nhấn mạnh vai trò then chốt của tốc độ xử lý trong các hệ thống Big Data, cho phép phân tích khối lượng dữ liệu lớn trong thời gian ngắn và hỗ trợ ra quyết định kịp thời. Tuy nhiên, các vấn đề về chất lượng dữ liệu, chẳng hạn như dữ liệu nhiễu, không đầy đủ hoặc không nhất quán, có thể dẫn đến việc xây dựng các mô hình thiếu ổn định và kém chính xác, phù hợp với nhận định của Fan et al. (2014). Trong các ứng dụng thực tiễn, Big Data đã chứng minh hiệu quả rõ rệt, đặc biệt trong dự báo khí hậu, nơi các mô hình dựa trên dữ liệu lớn giúp cải thiện độ chính xác của các dự báo dài hạn.
1	Tóm lại, báo cáo đã làm rõ vai trò của Big Data kết hợp với học máy, từ các kỹ thuật xử lý dữ liệu quy mô lớn đến việc trích xuất tri thức và hỗ trợ ra quyết định dựa trên dữ liệu. Các kết quả nghiên cứu chứng minh giá trị thực tiễn của Big Data trong việc nâng cao hiệu suất phân tích và dự báo, tuy nhiên vẫn cần giải quyết nhiều thách thức liên quan đến chất lượng dữ liệu, khả năng mở rộng, chi phí hạ tầng và vấn đề đạo đức. Nghiên cứu đóng góp cho lĩnh vực khoa học máy tính, đồng thời khuyến khích việc phát triển và sử dụng các công cụ mã nguồn mở nhằm thúc đẩy đổi mới sáng tạo.
1	Điện toán lượng tử đang chuyển mình từ giai đoạn thử nghiệm sang ứng dụng thực tiễn, với các đột phá về qubit logic và error correction. Báo cáo này khảo sát tiến bộ năm 2025-2026, tập trung vào các nền tảng như superconducting (IBM Nighthawk), neutral-atom (QuEra) và photonic. Chúng tôi phân tích roadmap đạt quantum advantage vào cuối 2026 và fault-tolerant computing vào 2029, đánh giá qua metrics như số gate (lên đến 10,000) và logical qubits. Dữ liệu từ IBM, QuEra và Fujitsu cho thấy qubit tăng từ 256 lên hơn 1,000, giảm noise đáng kể. Báo cáo thảo luận thách thức như scalability và energy consumption. Kết luận đề xuất tích hợp hybrid quantum-classical để giải quyết vấn đề thực tế trong hóa học và tối ưu hóa.
1	Điện toán lượng tử đại diện cho bước nhảy vọt trong khoa học máy tính, khai thác superposition và entanglement để giải quyết vấn đề vượt quá khả năng máy tính cổ điển. Năm 2025 được công nhận là International Year of Quantum Science, đánh dấu sự chuyển dịch từ lý thuyết sang thực nghiệm với các demonstration vượt classical supercomputers. Báo cáo này khám phá tiến bộ gần đây, từ IBM Quantum Nighthawk (dự kiến 5,000-10,000 gates) đến neutral-atom systems của QuEra với error correction sẵn sàng. Chúng tôi nhấn mạnh ứng dụng trong material science, drug discovery và optimization. Ngoài ra, báo cáo chỉ ra nhu cầu đầu tư lớn (hàng tỷ USD) và hợp tác công-tư để vượt qua thách thức noise và decoherence.
1	Cuối cùng, chúng tôi đề xuất hướng nghiên cứu hybrid systems để đạt quantum advantage bền vững vào 2026-2027, góp phần vào kỷ nguyên lượng tử thứ hai. Nghiên cứu lượng tử bắt đầu từ Feynman (1982) với ý tưởng simulation quantum bằng quantum. Đột phá lớn là Shor's algorithm (1994) và Grover's (1996). Gần đây, IBM (2025) giới thiệu Nighthawk với multi-chip architecture đạt 4,158 qubits. Fujitsu và RIKEN (2025) ra mắt 256-qubit superconducting, hướng tới 1,000 qubits năm 2026. QuEra và Atom Computing tập trung neutral-atom cho error-corrected logical qubits. Tài liệu cũng nhấn mạnh error correction demonstrations (2025), từ surface codes đến color codes. Trong ứng dụng, Google và IBM đã vượt classical ở một số tasks physics. Tổng hợp, khảo sát cho thấy chuyển dịch từ noisy intermediate-scale quantum (NISQ) sang fault-tolerant, với trọng tâm vào hardware-software integration và post-quantum cryptography.
1	Phương pháp dựa trên simulation và benchmarking các nền tảng lượng tử sử dụng Qiskit (IBM) và Cirq (Google). So sánh superconducting, trapped-ion và neutral-atom trên metrics như fidelity, gate depth và coherence time. Dữ liệu từ public benchmarks như Quantum Volume và random circuit sampling. Thử nghiệm trên simulators với noise models, kết hợp real hardware access qua cloud (IBM Quantum, AWS Braket). Áp dụng error mitigation techniques như zero-noise extrapolation. Phân tích scalability qua projections đến 2027. Mã nguồn và circuits được lưu trữ trên GitHub cho reproducibility. Ngoài ra, đánh giá hybrid classical-quantum algorithms như VQE cho chemistry simulations.
1	Kết quả cho thấy IBM Nighthawk đạt fidelity >99% cho two-qubit gates, hỗ trợ 5,000 gates năm 2025, dự kiến 10,000 năm 2027. Neutral-atom systems của QuEra đạt logical qubits với error rate giảm 100x so physical qubits. Trong simulations, hybrid VQE giải quyết molecular energy chính xác hơn classical methods 15-20%. Quantum advantage demonstrations vượt classical ở sampling tasks. Energy efficiency cải thiện ở photonic chips (mới 2026). Biểu đồ so sánh minh họa neutral-atom vượt trội ở scalability. Tổng thể, kết quả khẳng định 2026 là năm then chốt cho quantum advantage thực tế, nhưng vẫn cần thêm progress ở error correction full-scale.
1	Kết quả nhấn mạnh tiến bộ hardware, nhưng thách thức lớn là scaling logical qubits và integration với HPC. So với tài liệu, nghiên cứu bổ sung analysis hybrid applications, phù hợp với predictions DARPA (2025). Ứng dụng như breaking encryption đòi hỏi post-quantum standards. Hạn chế: simulations chưa capture full noise real-world. Tương lai, couplers và interconnects sẽ là key cho modular systems. Thảo luận nhấn mạnh ethical implications, như cybersecurity risks, và cần collaboration để democratize access. Quantum computing không thay thế classical mà bổ sung, yêu cầu investment bền vững. Tóm lại, báo cáo làm rõ tiến bộ điện toán lượng tử hướng tới advantage và fault-tolerance. Kết quả chứng minh khả năng thực tế năm 2026, nhưng cần vượt noise và scalability. Nghiên cứu góp phần khoa học máy tính, khuyến khích hybrid approaches. Hợp tác quốc tế xây dựng ecosystem sẽ thúc đẩy.
1	Edge computing đang cách mạng hóa IoT bằng cách xử lý dữ liệu gần nguồn, giảm latency và tăng privacy. Báo cáo khảo sát xu hướng 2025-2026, tập trung vào AI at the edge, container orchestration và observability. Chúng tôi phân tích ứng dụng trong smart cities, autonomous vehicles và industrial IoT, đánh giá qua metrics như latency (<5ms), throughput và energy efficiency. Dữ liệu từ Forrester và IDC cho thấy 75% enterprise data processed tại edge, thị trường đạt $260 tỷ năm 2025. Thảo luận thách thức như security và resource constraints. Kết luận đề xuất LLM deployment trên edge devices cho real-time intelligence, góp phần vào IoT bền vững.
1	Edge computing di chuyển xử lý từ cloud về periphery, phù hợp với IoT explosion (hàng tỷ devices). Với 5G và AI, edge cho phép real-time decisions ở autonomous systems và smart healthcare. Báo cáo này khám phá tiến bộ như AI inference tại edge, federated learning và containerized deployments. Chúng tôi nhấn mạnh lợi ích giảm bandwidth và tăng security. Ngoài ra, báo cáo đánh giá tác động sustainability, như energy optimization ở green IoT. Cuối cùng, đề xuất nghiên cứu integration LLM với edge để intelligent data analysis, hướng tới ecosystem tự động và an toàn hơn trong 2026.
1	Edge computing bắt đầu với fog computing (Cisco, 2012), nhưng bùng nổ với 5G và AI. Forrester (2025) dự báo top trends: AI at edge, advanced security và sustainability. Scale Computing (2025) nhấn mạnh IaC và container orchestration cho scalable deployments. Trong LLM on edge, nghiên cứu (2025) tập trung resource optimization qua quantization và federated learning. Ứng dụng trong IIoT, autonomous vehicles và healthcare wearables. Tổng hợp, khảo sát cho thấy chuyển dịch từ centralized sang distributed intelligence, với trọng tâm privacy-preserving techniques và hardware accelerators (RISC-V). Phương pháp dựa trên benchmarking frameworks như Kubernetes edge và TensorFlow Lite trên devices. So sánh cloud vs edge processing latency và energy trên datasets IoT (như UCI HAR). Sử dụng simulators và real hardware (Raspberry Pi, Jetson Nano). Metrics: latency, accuracy, power consumption, với stress testing. Áp dụng differential privacy cho federated setups. Mã nguồn PyTorch và Dockerized trên GitHub. Phân tích scalability với varying node numbers.
1	Kết quả cho thấy edge AI giảm latency 90% (dưới 5ms) so cloud, với accuracy duy trì >92% sau quantization. Container orchestration tăng scalability 5x, energy giảm 40% ở inference. Trong IoT simulations, real-time anomaly detection đạt F1-score 0.96. Biểu đồ minh họa edge vượt trội ở bandwidth-limited scenarios. Tổng thể, kết quả khẳng định edge computing là key cho next-gen IoT, sẵn sàng cho LLM lightweight deployment năm 2026. Kết quả nhấn mạnh edge intelligence, nhưng resource constraints và security vẫn là thách thức. Phù hợp Forrester trends (2025). Ứng dụng như smart cities cải thiện traffic 30%, nhưng cần zero-trust models. Hạn chế: datasets chủ yếu simulated. Tương lai, integration 6G và quantum-resistant security. Thảo luận nhấn mạnh balance performance và sustainability cho IoT ethical.
1	Tóm lại, báo cáo đã làm rõ những tiến bộ quan trọng của edge computing khi được kết hợp với trí tuệ nhân tạo trong các hệ thống IoT hiện đại. Các kết quả nghiên cứu chứng minh rằng việc xử lý dữ liệu tại biên giúp giảm đáng kể độ trễ (latency), nâng cao hiệu quả sử dụng tài nguyên và cải thiện khả năng phản hồi thời gian thực so với mô hình cloud truyền thống. Nghiên cứu này đóng góp thiết thực cho lĩnh vực khoa học máy tính, đồng thời khuyến khích phát triển các nền tảng mở nhằm tăng tính linh hoạt và khả năng tương tác giữa các hệ thống. Sự hợp tác giữa học thuật, công nghiệp và các nhà phát triển sẽ đóng vai trò then chốt trong việc thúc đẩy quá trình áp dụng rộng rãi edge computing.
1	Agentic AI, với khả năng tự chủ lập kế hoạch, thực hiện đa bước và tương tác với môi trường, đang chuyển từ giai đoạn hype sang thực tiễn doanh nghiệp năm 2026. Báo cáo này khảo sát tiến bộ gần đây trong các mô hình như reasoning agents (o1-style), multi-agent systems và autonomous workflows. Chúng tôi phân tích ứng dụng trong automation doanh nghiệp, customer service và scientific discovery, đánh giá qua metrics như task success rate, error rate và ROI. Dữ liệu từ benchmarks như GAIA và WebArena cho thấy success rate tăng từ 30% (2025) lên hơn 70% ở các mô hình frontier. Báo cáo thảo luận thách thức như hallucination, reliability ở high-stakes tasks và integration với legacy systems. Kết luận đề xuất hybrid human-agent frameworks và governance để đạt value realization bền vững, góp phần vào kỷ nguyên AI tự chủ.
1	"Năm 2026 đánh dấu sự trưởng thành của AI agents, khi generative AI chuyển từ công cụ hỗ trợ sang các thực thể tự chủ (agentic) có khả năng reasoning, planning và execution đa bước. Theo dự báo từ MIT Sloan và IBM, đây là năm agentic AI thoát khỏi ""trough of disillusionment"" để tập trung vào value creation thực tế, đặc biệt trong enterprise. Báo cáo này khám phá tiến bộ từ single-purpose agents (2024-2025) đến multi-agent orchestration và embodied agents. Chúng tôi nhấn mạnh vai trò trong hyperautomation, nơi AI kết hợp RPA, process mining và LLMs để tự động hóa toàn bộ workflow. Ngoài ra, báo cáo chỉ ra nhu cầu accountability và reliability để tránh errors ở high-money processes."
1	Cuối cùng, chúng tôi đề xuất hướng nghiên cứu hybrid systems với human-in-the-loop, nhằm đảm bảo AI agents trở thành đối tác đáng tin cậy, thúc đẩy productivity và innovation trong doanh nghiệp toàn cầu. Lịch sử agentic AI bắt nguồn từ reinforcement learning agents (Silver et al., 2016) và ReAct framework (Yao et al., 2022). Đột phá lớn là reasoning models như OpenAI o1 (2025), cho phép chain-of-thought dài và self-correction. Tài liệu gần đây (Davenport & Bean, 2026) nhấn mạnh agentic AI vẫn chưa sẵn sàng prime-time do error rates cao ở complex tasks. Các nghiên cứu từ Anthropic và Carnegie Mellon (2025-2026) chỉ ra multi-agent collaboration cải thiện robustness, trong khi IBM (2026) đề xuất factory infrastructure cho scalable deployment.
1	Trong ứng dụng, agents đã chứng minh hiệu quả ở software development (GitHub Copilot Workspace) và scientific discovery (DeepMind's AI lab assistants). Tổng hợp, khảo sát cho thấy chuyển dịch từ experimentation sang production-grade agents, với trọng tâm vào reliability, observability và governance để đạt ROI thực tế. Phương pháp dựa trên benchmarking và case studies các agent frameworks như LangChain, AutoGen và CrewAI trên datasets chuẩn như GAIA (General AI Assistants), WebArena và ToolBench. So sánh single-agent vs multi-agent trên metrics: success rate, steps efficiency, hallucination rate và cost per task. Sử dụng PyTorch và Hugging Face cho fine-tuning reasoning models, kết hợp simulation environments (như BabyAGI variants). Thử nghiệm trên cloud infrastructure với varying complexity levels, áp dụng 5-fold validation cho robustness. Phân tích error patterns qua logging và human evaluation. Ngoài ra, đánh giá integration với enterprise tools (SAP, Salesforce) qua API orchestration.
1	Mã nguồn và benchmarks được công khai trên GitHub để reproducibility và cộng đồng đóng góp.Kết quả cho thấy multi-agent systems đạt success rate 72% trên GAIA benchmark, cao hơn single-agent 45%, với giảm 60% hallucination nhờ self-verification. Reasoning models như o1 variants giảm error ở multi-step tasks từ 40% xuống 12%. Trong enterprise simulations, agents tự động hóa workflow (ví dụ: invoice processing) đạt ROI 3.5x sau 6 tháng, nhưng vẫn fail 15% ở edge cases high-stakes. Thời gian execution giảm 70% so với human-only. Biểu đồ so sánh minh họa scaling laws: performance tăng theo agent count nhưng plateau ở resource limits. Tổng thể, kết quả khẳng định agentic AI sẵn sàng cho một số use cases production năm 2026, nhưng cần mitigation cho reliability ở critical domains.
1	"Kết quả nhấn mạnh tiềm năng agentic AI trong enterprise, nhưng thách thức lớn là reliability và safety ở processes liên quan money hoặc decisions cao rủi ro. So với tài liệu (Davenport & Bean, 2026), nghiên cứu bổ sung multi-agent analysis, phù hợp với predictions về ""factory"" infrastructure. Ứng dụng như autonomous customer support có thể giảm chi phí 50%, nhưng cần zero-trust governance và audit trails. Hạn chế: benchmarks chưa capture full real-world variability (e.g., noisy data, dynamic environments). Tương lai, embodied agents và robotics integration sẽ mở rộng scope. Thảo luận nhấn mạnh ethical aspects: accountability cho agent actions và cần regulations để tránh misuse. Agentic AI không thay thế con người mà augment, yêu cầu redesign workflows và upskilling workforce."
1	Tóm lại, báo cáo đã làm rõ tiến bộ agentic AI năm 2026, từ reasoning capabilities đến multi-agent orchestration, với kết quả chứng minh khả năng value realization ở enterprise. Tuy nhiên, cần vượt qua reliability và governance để triển khai rộng rãi. Nghiên cứu góp phần vào khoa học máy tính, khuyến khích hybrid human-AI systems và open benchmarks. Hợp tác giữa academia, industry và regulators sẽ thúc đẩy adoption bền vững. Agentic AI là bước tiến quan trọng hướng tới AI tự chủ thực sự. Chúng tôi hy vọng báo cáo này truyền cảm hứng cho các nghiên cứu tiếp theo, xây dựng nền tảng AI đáng tin cậy cho tương lai.
1	"Năm 2026 chứng kiến sự bùng nổ ứng dụng agentic AI trong doanh nghiệp, với Gartner dự báo 40% ứng dụng doanh nghiệp sẽ tích hợp AI agents (tăng từ dưới 5% năm 2025). Các case study điển hình bao gồm Amazon sử dụng agents để modernize hàng nghìn ứng dụng legacy Java, giảm thời gian nâng cấp đáng kể; hoặc các công ty như Sanofi tổ chức ""Shark Tank"" nội bộ để nhân viên đề xuất dự án agentic AI cấp doanh nghiệp. Trong lĩnh vực dịch vụ khách hàng, agents tự chủ xử lý end-to-end workflows như zero-click commerce, nơi khách hàng không cần click hay search mà AI tự động mua sắm dựa trên sở thích. Trong lĩnh vực an ninh mạng, agents phát hiện threat theo thời gian thực với accuracy cao hơn."
1	"Mặc dù tiến bộ nhanh chóng, agentic AI năm 2026 vẫn đối mặt với nhiều thách thức lớn, bao gồm ""shadow agents"" (agents không được quản lý, tạo rủi ro bảo mật), hallucination ở high-stakes tasks, và chi phí cao dẫn đến hơn 40% dự án bị hủy theo Gartner. Deloitte nhấn mạnh khoảng cách giữa pilot và production, với chỉ 11-14% tổ chức triển khai thực sự. Giải pháp đề xuất bao gồm xây dựng ""factory infrastructure"" (theo IBM) để quản lý agents như digital workers, với governance frameworks tích hợp observability, audit trails và zero-trust models. Human-in-the-loop (HITL) động kết hợp deterministic guardrails giúp đảm bảo reliability. Ngoài ra, áp dụng continuous monitoring để phát hiện model drift và bias sớm, cùng với training workforce về agent orchestration và escalation paths."
1	Để thúc đẩy agentic AI bền vững, nghiên cứu tương lai nên tập trung vào ba hướng chính: (1) Phát triển open-source reasoning models và protocols interoperability như Model Context Protocol (MCP) và Agent-to-Agent Protocol (A2A) để giảm phụ thuộc vendor; (2) Cải thiện benchmarks như GAIA và WebArena với các task real-world dài hạn hơn, bao gồm multi-agent collaboration và embodied agents trong robotics; (3) Nghiên cứu governance và ethical frameworks, đặc biệt RAI (Responsible AI) practices để xử lý accountability khi agents tự chủ. Các tổ chức nên ưu tiên low-code/no-code agent builders để democratize development, cho phép 50%+ knowledge workers tự tạo agents. Hợp tác giữa academia, industry và regulators sẽ là chìa khóa để xây dựng standards và datasets mở.
0	Hệ thống trợ lái thông minh ADAS (Advanced Driver Assistance Systems) đóng vai trỏ quan trọng trong việc xây dựng một hệ thống giao thông an toàn và hiện đại. Đối với các hệ thống này, yêu cầu hiệu suấtphát hiện chinh xác và tốc độ đáp ứng là rất quan trọng. Tuy thế mà, việcphát hiện cácphương tiện di động đang gặp rất nhiều khó khăn do mật độ phương tiện, bối cảnh nền phức tạp trong thành phố... Ngoài ra, yêu cầu phát hiện và nhận dạng đáp ứng thời gian thực cũng đang là thử thách cho các hệ thống hiện tại. Bài báo này đề xuất mô hình sử dụng thuật toán học sâu và tri tuệ nhân tạo đế tăng độ chính xác và cải thiện tốc độ đáp ứng cho hệ thống trợ lái thông minh.
0	Theo đó, trước tiên bài báo này đề xuất mô hình YOLO (You Only Look One) cùng với tập dữ liệu mẫu được thu thập và phân loại riêng phù họp với giao thông Việt Nam và thuật toán đào tạo của chủng chúng tôi. Các kết quả thí nghiệm sau đó được thực hiện trên máy tính nhúngNVIDIA Jetson TX2. Các kết quả thí nghiệm chỉra rằng, phươngpháp đề xuất đã tăng tốc độ ít nhất 1,6 lần với tỷ lệ phát hiện đạt 90 % cho hệ thống camera tĩnh; và tăng tốc độ ít nhất 1,7 lần với tỷ lệ phát hiện đạt 66,67 % cho hệ thống camera động ở các ảnh có độ phân giải cao 1280x720.
0	Hệ thống trợ lái thông minh ADAS đóng vai trò quan trọng trong việc xây dựng một hệ thống giao thông an toàn và hiện đại. Ngoài việc xây dựng cơ sở hạ tàng thi việc phát triến các công nghệ mới cho các phưcmg tiện di động thông minh là một trong những yếu tố quan trọng nhất cấu thành nên hệ thống này. Việc phát hiện, nhận dạng các phương tiện xung quanh và đưa ra các cảnh báo cho người dùng sẽ giúp cho việc lái xe an toàn hơn. Ngày nay, với nền tảng phần cứng là những bộ máy tính nhúng có kích thước nhỏ gọn, hiệu suất cao, tích hợp nhiều công nghệ hỗ trợ mạnh mẽ cho việc xử lý ảnh.
0	Cùng với các mô hình học máy được phát triển bởi các công ty công nghệ hàng đầu để đào tạo mạng nơron học sâu. Điều này giúp cho các nhà nghiên cứu dễ dàng lựa chọn mô hình học máy phù hợp để thử nghiệm các phương pháp được đề xuất. Các kết quả nghiên cứu ứng dụng thuật toán học sâu và trí tuệ nhân tạo để phát hiện và nhận dạng các phương tiện di động cho các hệ thống trợ lái thông minh ADAS đã được các tác giả chi ra trong [1-3]. Bài báo này đề xuất mô hình sử dụng thuật toán học sâu và trí tuệ nhân tạo để tăng độ chính xác và cải thiện tốc độ đáp ứng cho hệ thống trợ lái thông minh.
0	Cùng với các mô hình học máy được phát triển bởi các công ty công nghệ hàng đầu để đào tạo mạng nơron học sâu. Điều này giúp cho các nhà nghiên cứu dễ dàng lựa chọn mô hình học máy phù hợp để thử nghiệm các phương pháp được đề xuất. Các kết quả nghiên cứu ứng dụng thuật toán học sâu và trí tuệ nhân tạo để phát hiện và nhận dạng các phương tiện di động cho các hệ thống trợ lái thông minh ADAS đã được các tác giả chi ra trong [1-3]. Bài báo này đề xuất mô hình sử dụng thuật toán học sâu và trí tuệ nhân tạo để tăng độ chính xác và cải thiện tốc độ đáp ứng cho hệ thống trợ lái thông minh.
0	Theo đó, trước tiên chúng tôi đề xuất mô hình YOLOv5 [4] cùng với tập dữ liệu mẫu được thu thập và phân loại riêng phù họp với giao thông Việt Nam và thuật toán đào tạo của chúng tôi. Các kết quả thí nghiệm sau đó được thực hiện trên máy tính nhúng NVIDIA Jetson TX2. Các kết quả thí nghiệm chỉ ra rằng, phương pháp đề xuất đã tăng tốc độ ít nhất 1,6 lần với tỷ lệ phát hiện đạt 90 % cho hệ thống camera tĩnh; và tăng tốc độ ít nhất 1,7 lần với tỷ lệ phát hiện đạt 66,67 % cho hệ thống camera động ở các ảnh có độ phân giải cao 1280x720.
0	YOLO [5] là một mô hình mạng nơ-ron tích chập CNN (Convolutional Neural Network) cho phép phát hiện, nhận dạng và phân loại đối tượng. YOLO được tạo ra từ việc kết hợp giữa các lớp tích chập (convolutional layers) và lóp kết nối (connected layers). Trong đó các convolutional layers sẽ trích xuất ra các đặc trưng của ảnh, còn full-connected layers sẽ dự đoán ra xác suất và tọa độ của đối tượng. Trong các thuật toán, các chỉ số luôn được đánh giá so với ground truth (được xác định trước từ bộ dữ liệu thông qua tọa độ (cx,cy,w,h) giúp xác định vật the). Ground truth là vị trí của các đối tượng được cung cấp trong tập dữ liệu đào tạo, được xác thực và kiểm ưa.
0	Đổi với các hệ thống phát hiện đối tượng, ground truth bao gồm hình ảnh, các lớp của đối tượng trong nó và các khung hình bao quanh ground truth (ground truth box) của mỗi đối tượng trong ảnh đó. Như Hình 2 các ground truth box được xác định trên ảnh được huấn luyện và kiểm tra. Giả sử hình ảnh gốc và chú thích cho ground truth giống như ãnh trên. Dữ liệu huấn luyện và kiểm tra tất cả các hình ảnh được chú thích theo cùng một cách. Mô hình sẽ trả về rất nhiều dự đoán nhưng trong số đó hầu hết độ tin cậy rất thấp, do đó chỉ xem xét các dự đoán trên một độ tin cậy nhất định.
0	Hình ảnh gốc qua mô hình và thuật toán phát hiện đối tượng sẽ trả về kết quả vị trí của đối tượng có ttong ảnh theo một ngưỡng của độ tin cậy. Để đánh giá được độ chính xác, đầu tiên cần phải đánh giá tính chính xác của từng dự đoán trên ảnh. Đe tính được độ chính xác của một hộp giới hạn cần sử dụng độ đo loU - Intersection over Union: là tỷ lệ đo lường mức độ giao nhau giữa 2 khung hình (thường là khung hình dự báo và khung hình ground truth) để nhằm xác định 2 khung hình chồng lên nhau không. Tỷ lệ này được tính dựa trên phần diện tích giao nhau giữa 2 khung hình với phần tổng diện tích giao nhau và không giao nhau giữa chúng.
0	Với bài toán phân loại mà tập dữ liệu của các lớp chênh lệch nhau rất nhiều (mất cân bằng) thường sẽ sử dụng phương pháp đánh giá PrecisionRecall [6], Khi đó, Precision được định nghĩa là tỉ lệ số điểm positive mô hình dự đoán đúng trên tổng số điểm mô hình dự đoán là positive. Precision càng cao, tức là số điểm mô hình dự đoán là positive đều là positive càng nhiều. Precision = 1, tức là tất cả số điểm mô hình dự doán là positive đều đúng, hay không có điểm nào có nhãn là negative mà mô hình dự đoán nhầm là positive.
0	Recall được định nghĩa là tỉ lệ số điếm positive mô hình dự đoán đúng trên tổng số điểm thật sự là positive (hay tổng số điểm được gán nhãn là positive ban đầu). Recall càng cao, tức là số điểm là positive bị bỏ sót càng ít. Recall = 1, tức là tất cả số điểm có nhãn là positive đều được mô hình nhận ra [6]. YOLO dự đoán nhiều hộp giới hạn cho mỗi ô lưới. Đe tính lỗi cho các dự đoán đúng, cần xác định một trong số những hộp giới hạn chịu hách nhiệm cho đối tượng. Với mục đích này chọn một cái có loU cao nhất với hộp giới hạn thực sự. Chiến lược này dẫn đến chuyên môn hóa ơong việc dự đoán hộp giới hạn.
0	Sẽ tốt hơn khi dự đoán một số kích thước và tỷ lệ khung hình nhất định. YOLO sử dụng hàm Sum-Squared Error giữa dự đoán và giá trị mong muốn để tính mất mát [5]. Phần thứ hai của hàm mất mát, YOLO tính toán sai số ưong việc dự đoán chiều rộng và chiều cao. Tuy nhiên, độ lớn của lỗi trong các hộp lớn ảnh hưởng đến phương trình trong các hộp nhỏ. Vì cả chiều rộng và chiều cao đều được chuẩn hóa trong khoảng từ 0 đến 1, căn bậc hai của chúng làm tăng nhiều hơn sự khác biệt cho các giá trị nhỏ và giá trị lớn. Kể từ đây, căn bậc hai của chiều rộng và chiều cao của hộp giới hạn được sử dụng thay cho chiều rộng và chiều cao trực tiếp.
0	Bộ dữ liệu sử dụng trong quá trình huấn luyện mô hình và chạy thử nghiệm được trích xuất từ camera hành trình gắn ưên ô tô và camera gắn cố định tại ngã tư. Với bộ dữ liệu huấn luyện mô hình, các video được chuyển đối thành các hình ảnh có độ phân giải 1280x720. Bao gồm 500 ảnh trong đó 450 ảnh được sử dụng cho việc huấn luyện và 50 ảnh cho quá trình kiếm tra độ chính xác mô hình vừa huấn luyện. Bộ dữ liệu hình ảnh huấn luyện mô hình được tiền xử lý bằng makesense.ai và gắn các thông số đi kèm bao gồm: [số thứ tự của lớp] [tọa độ X của tâm đối tượng] [tọa độ y của tâm đối tượng] [chiều rộng] [chiều cao].
0	Dữ liệu được lưu trữ dưới dạng file .txt ứng với từng ảnh. Trong đó ID cho các lớp lần lượt: O-Car, 1-Motorbike, 2-Truck, 3-Bus.Huấn luyện mô hình là một chương trinh hoạt động liên tục, tiêu tốn nhiều tài nguyên như RAM, GPU, CPU vì vậy để đảm bảo quá trình huấn luyện nhanh chóng và chính xác dựa trên những nền tảng phần cứng sẵn có chúng tôi đã sử dụng máy chủ ảo của Google có tên là Google Colab [8].Chúng tôi lựa chọn cấu hình GPU để huấn luyện mô hình cũng như chạy thử nghiệm chương trình phát hiện và nhận diện phương tiện di động bằng mô hình YOLOv5.
0	Đầu tiên, chúng tôi đã thiết lập một môi trường ảo bang Python Virtual Environment [9]. Môi trường ảo được sử dụng để cô lập môi trường của các dự án với nhau. Môi trường ảo cho phép cài đặt và quản lý các gói cài đặt một cách riêng rẽ và không xung đột với trình quản lý gói cài đặt của toàn hệ thống. Các gói thư viện cần cài đặt: Python 2.6.9, Pytorch 1.8.0, Torchvision 0.9.0, OpenCv 4.5.4.60, Matplotlib, Pillow, Pyyaml, Tensorboard, Tqdm, Scipy, Pandas, Seaborn, Numpy. Các bước của phương pháp được đề xuất như sau: Đầu tiên dữ liệu đầu vào video được tách thành các frame và được chuyển về độ phân giải 1280x720, đây là độ phân giải tối ưu cho tốc độ cũng như đủ chất lượng để xác định đối tượng.
0	Dữ liệu sau khi được trích xuất sẽ được so sánh với mô hình được huấn luyện từ trước và đưa vào thuật toán nhận diện đối tượng của YOLOv5. Dữ liệu đầu ra bao gồm tọa độ đối tượng, ID đối tượng sẽ được khoanh vùng và gắn tên tương ứng. Đầu ra của hệ thống là video hiển thị theo thời gian thực cùng với đó sẽ được lưu trữ dưới dạng .mp4. Kết quả chạy thực nghiệm cho thấy, ở môi trường ngã tư gầm cầu, nhiều xe qua lại hệ thống nhận diện đầy đủ các đối tượng. Độ chính xác đạt 90%, tốc độ xử lý khung hình trung bình đạt 16 FPS ở độ phân giải 1280x720. Hệ thống cho thấy sự ổn định và không có hiện tượng chồng kéo khung hình.
0	Ở trong trạng thái xe đang di chuyển trong một môi trường phức tạp với nhiều đối tượng tạo nhiễu như cây cối, tòa nhà, các vật lạ ven đường. Hệ thống đạt độ chính xác 66,67 %, tốc độ xử lý khung hình trung bình đạt 17 FPS ở độ phân giải 1280x720. Các kết quà thí nghiệm ở Bảng 3 chi ra rằng cách tiếp cận của chúng tôi có thể tăng tốc ít nhất 1.6 lần khi so sánh với 2 mô hình đào tạo có sẵn, được dùng phổ biến là: Yolov5s và Yolov5m. Điều này có thể lý giải như sau: Mô hình đào tạo Yolov5s: số kiều đối tượng nhận dạng là 80, mẫu đào tạo ít nên có tốc độ xử lý nhanh nhưng độ chính xác không cao.
0	Mô hình đào tạo Yolov5m: số kiểu đối tượng nhận dạng là 80, mẫu đào tạo nhiều nên có tốc độ xử lý chậm nhưng độ chính xác cao horn. Mô hình đào tạo được đề xuất (proposed model) trong bài báo: Là mô hình được nhóm tác giả tùy biến riêng cho phương tiện giao thông ở Việt Nam với số kiểu đối tượng nhận dạng là 4, mẫu đào tạo phù hợp nên có tốc độ xử lý nhanh và độ chính xác cao. Các kết quả thí nghiệm là được minh họa chi tiết ở các Hình 11-13.
0	Kết quả đạt được khi sử dụng mô hình YOLOv5 trong huấn luyện và nhận dạng đối tượng chứng minh kết quả khả thi và tiềm năng trong việc xây dựng một mô hình hệ thống trợ lái thông minh ADAS. Ngoài ra, việc thực hiện mô hình đề xuất thành công trên thiết bị máy tính nhúng NVIDIA Jetson TX2 mở ra một hướng tiếp cận mới trong việc nhận dạng các đối tượng khác nhau theo thời gian thực trên các thiết bị camera có sẵn trên hệ thống phưorng tiện di động. Phương pháp đề xuất này có thể được ứng dụng cho các xe tự hành thông minh thời gian thực.
0	Những năm gần đây, giám sát sử dụng điện năng đang trở thành yếu tố then chốt trong định hướng tiết kiệm năng lượng và phát triển lưới điện thông minh. Tuy nhiên, để giám sát và theo dõi chi tiết các phụ tải thành phần cần một lượng lớn thiết bị đo đếm, từ đó dẫn tới khó khăn về đầu tư, triển khai và quản lý. Để giải quyết vấn đề này, giải thuật Giám sát tải không xâm nhập (NILM) ứng dụng kỹ thuật học máy (ML) cho phép xác định phụ tải thành phần dựa trên dữ liệu đo tổng tiêu thụ điện, qua đó giảm đáng kể số lượng thiết bị đo đếm và chi phí đầu tư.
0	Trong bài báo này, thông qua ứng dụng các giải thuật Machine learning, nhóm nghiên cứu đã phân tích bộ dữ liệu tiêu thụ điện năng của một căn hộ với các phụ tải tổng và thành phần đa dạng. Kết quả của bài báo đã chỉ ra những khó khăn và tiềm năngtrong việc ứng dụng học máy phân tách phụ tải thành phần từ tổng tiêu thụ điện năng. Nhằm ứng phó với biến đổi khí hậu và sự nóng lên toàn cầu, sử dụng hiệu quả năng lượng ngày càng trở thành một trong các tiêu chí được quan tâm trong các dự án về năng lượng. Những năm gần đây, Chính phủ Việt Nam đã ban hành hàng loạt các quyết định và chương trình quốc gia về sử dụng năng lượng tiết kiệm và hiệu quả.
0	Đặc biệt, các đồng hồ thông minh và hệ thống giám sát năng lượng đóng một vai trò quan trọng trong các mục tiêu về quản lý, hiệu quả năng lượng. Thực tế, việc phản hồi các thông tin về tiêu thụ năng lượng đến người dùng có thể giúp tiết kiệm năng lượng lên đến hơn 14% tổng lượng điện năng tiêu thụ [1]. Tuy nhiên, việc triển khai các hệ thống giám sát năng lượng theo cách truyền thống cần một số lượng các thiết bị giám sát năng lượng, dẫn đến khó khăn trong đầu tư, triển khai và quản lý.
0	Vì vậy, kỹ thuật Giám sát tải trọng không xâm phạm đã nổi lên như một giải pháp tốt nhất để đáp ứng các yếu tố kỹ thuật, đồng thời giải quyết các vấn đề chi phí đầu tư và quản lý nhờ số lượng điểm đo cần triển khai thấp hơn. Giám sát tải không xâm nhập (NILM) là một quá trình phân tích dữ liệu điện năng tiêu thụ tổng và đưa ra các thông tin tiêu thụ điện của từng thiết bị trong hệ thống điện. Trong giai đoạn đào tạo, một mô hình học máy được lựa chọn để đào tạo với bộ dữ liệu bao gồm dữ liệu tiêu thụ của tải thành phần.
0	Sau đó, mô hình đã được đào tạo sẽ sử dụng để đưa ra các dự đoán về tải tiêu thụ thành phần dựa trên dữ liệu tiêu thụ tổng được cung cấp cho mô hình. Trong bài báo này, kỹ thuật giám sát tải không xâm nhập được nghiên cứu để dự đoán tiêu thụ điện của một căn hộ tại Hà Nội. Trong phần đầu tiên của bài báo, phương pháp luận của kỹ thuật giám sát tải không xâm nhập được đưa ra. Sau đó, các mô hình NILM sẽ được xây dựng từ bộ thư viện Scikit-learn với các thuật toán học máy thông dụng và được tối ưu hoá các siêu tham số nhằm gia tăng độ chính xác của kết quả dự đoán. Kết quả thu được cho thấy các thách thức cũng như cơ hội trong việc áp dụng học máy phân tách phụ tải điện.
0	Kỹ thuật NILM được xác định có 4 bước chính, bao gồm: Giám sát tiêu thụ, xác định sự kiện, trích xuất đặc trưng và phân loại tải [2]. Bước đầu tiên của kỹ thuật NILM là thực hiện đo và thu thập dữ liệu. Thông tin được thu thập lý tưởng nhất là thông tin về điện áp và dòng điện tiêu thụ với tần số cao. Tuy nhiên, đa phần thiết bị thương mại cho mục đích giám sát và quản lý năng lượng toà nhà đều có tốc độ lấy mẫu thấp (0.2–1Hz), do vậy các thông tin khác như công suất tác dụng, công suất phản kháng, hệ số công suất hay các đặc tính V-I khác cũng sẽ được xem xét để thu thập.
0	Mỗi khi thiết bị điện trong hệ thống được thay đổi sang một trạng thái khác (chuyển từ trạng thái tắt sang bật hoặc chuyển sang một trạng thái hoạt động mới) sẽ gây ra các thay đổi trong trạng thái điện. Dựa trên mô tả trong Hình 2, kỹ thuật NILM thực hiện xác định sự kiện trên bằng cách phân tích các thay đổi trong các thông số đo đạc được. Thông thường có hai phương pháp được sử dụng để phát hiện sự kiện [6]: Phương pháp phát hiện cạnh là phương pháp xác định sự kiện thông qua những thay đổi trong các thông số điện như công suất, dòng điện, sóng hài,... Phương pháp này đòi hỏi xây dựng những bộ phát hiện sự kiện phức tạp.
0	Ngoài ra bộ dữ liệu tần suất cao được khuyến nghị để tăng mức độ chính xác khi xác định các sự kiện.Phương pháp xác suất là phương pháp đơn giản hơn trong việc xác định các sự kiện. Hệ thống thực hiện ghi lại các thay đổi giá trị công suất và tính toán xác suất của thay đổi đó ứng với từng thiết bị cụ thể. Phương pháp này được khuyến nghị sử dụng cho bộ dữ liệu tần suất thấp.Mỗi một sự kiện được xác định, các thông số đặc trưng của sự kiện sẽ được phân tích và đưa ra. Như mô tả trong Hình 3, thông số đặc trưng được trích xuất gồm hai loại: Thông số đặc trưng cho quá trình quá độ và trạng thái ổn định [7].
0	Các thiết bị điện được cấu tạo từ nhiều thành phần khác nhau và gây ra các thay đổi khác nhau trong tín hiệu điện và tạo ra các thông số đặc trưng cho quá trình quá độ. Để có thể lấy được các thông số này, bộ dữ liệu yêu cầu phải được thu thập ở tần suất cao. Các thông số đặc trưng trạng thái quá độ có thể là công suất quá độ, dạng sóng của dòng điện khởi động hay nhiễu điện áp. Ở mặt khác, các thông số của trạng thái ổn định thì trái ngược lại do có thể thu thập được với bộ dữ liệu tần số thấp. Các thông số bao gồm sự thay đổi về công suất tiêu thụ, dòng điện tiêu thụ, nhiễu điện,...
0	Bước cuối cùng của kỹ thuật giám sát tải không xâm nhập là phân loại ra thiết bị nào đã gây ra sự kiện. Dựa trên các dữ liệu được thu thập và với các đặc trưng xác định được, các thuật toán học máy sẽ thông qua các phương pháp khác nhau để dự đoán được loại thiết bị gây ra các sự kiện. Một ví dụ đơn giản là có thể phân cụm các điểm dữ liệu và phân loại thông qua các cụm dữ liệu đó như Hình 4. Với các ứng dụng của NILM, loại thuật toán học máy thường được sử dụng bao gồm hai dạng: Học tập có giám sát (Supervised learning algorithms) - thường được sử dụng [8].  Học tập không giám sát (Unsupervised learning algorithms) [9].
0	Scikit-learn [10] là một thư viện được viết phần lớn bằng Python, được tích hợp một loạt thuật toán machine learning hiện đại được sử dụng cho các phương pháp học tập có giám sát và học tập không được giám sát ở quy mô trung bình. Bộ thư viện tập trung vào việc đưa machine learning đến những người không phải là chuyên gia bằng cách sử dụng ngôn ngữ bậc cao và đề cao tính dễ sử dụng, hiệu suất, tài liệu hướng dẫn và tính nhất quán của API. Trong nội dung bài báo, thông qua bộ thư viện Scikit-learn, một số thuật toán thông dụng được đưa ra và thông qua các công cụ bổ sung của thư viện để lựa chọn các tham số tối ưu cho từng thuật toán.
0	Kết quả hướng đến xây dựng các mô hình dự đoán cho kết quả tốt nhất. Các thuật toán được lựa chọn bao gồm: RandomForest Regressor, GradientBoostingRegressor, K-Nearest Neighbors Regressor, GaussianProcess Regressor, Multi-layer Perceptron Regressor. Chương trình hoạt động theo các bước được mô tả trong Hình 5. Bộ dữ liệu được chuẩn bị bao gồm dữ liệu tiêu thụ tổng và dữ liệu thành phần. Sau đó sẽ được chia thành hai bộ dữ liệu dùng cho mục đích huấn luyện mô hình và kiểm tra độ chính xác. Bộ dữ liệu dùng để huấn luyện sẽ được đưa vào mô hình và được tiến hành training. Tuy nhiên, trong quá trình huấn luyện, mô hình sẽ được thay đổi các siêu tham số đầu vào nhằm mục đích gia tăng độ chính xác.
0	Tại bước này, chương trình sử dụng công cụ GridSearchCV được cung cấp kèm theo thư viện Scikit-learn. Công cụ này hỗ trợ việc thay lần lượt các tham số trong một bộ tham số được cung cấp từ trước và so sánh độ chính xác với nhau. Mô hình có độ chính xác cao nhất sẽ được lựa chọn cùng với bộ tham số tương ứng. Bộ dữ liệu kiểm tra bao gồm dữ liệu tiêu thụ tổng sẽ được đưa vào để mô hình tiến hành dự đoán và kiểm tra lại với dữ liệu tiêu thụ thành phần, từ đó đánh giá được độ chính xác của mô hình.
0	Để đánh giá độ chính xác trong việc ước tính tiêu thụ điện của từng thiết bị, thông số RMSE được sử dụng để đánh giá sai số giữa giá trị công suất dự đoán được với giá trị công suất thực tế. Ngoài ra, mục đích của kỹ thuật NILM là dự đoán mức tiêu thụ điện của từng thiết bị, do vậy sai số giữa tổng tiêu thụ điện dự đoán và thực tế sẽ được đưa ra để so sánh. Chương trình NILM sẽ được thử nghiệm với bộ dữ liệu tiêu thụ điện thực tế của một căn hộ chung cư tại Hà Nội có diện tích 91m2, gồm 1 phòng khách, 2 phòng ngủ và 1 phòng làm việc. Dựa trên dữ liệu tiêu thụ tổng, mô hình sẽ dự đoán và đưa ra hoạt động của các phụ tải trong căn hộ.
0	Bộ dữ liệu tiêu thụ điện trong 20 ngày được thể hiện như Hình 6. Trong đó bình nóng lạnh, điều hoà và bếp từ là thông tin của thiết bị riêng biệt, còn lại thông tin về chiếu sáng bao gồm các đèn được sử dụng trong căn hộ và ổ cắm là tổ hợp nhiều thiết bị được sử dụng thông qua ổ cắm. Bộ dữ liệu được xây dựng từ toàn bộ dữ liệu tiêu thụ điện của căn chung cư. Dữ liệu tiêu thụ điện tổng sẽ được tính tổng của các tải tiêu thụ thành phần. Mục tiêu của trường hợp thử nghiệm nhằm hướng đến đánh giá độ hiệu quả của chương trình với dữ liệu thực tế và tập trung đi sâu vào phân tích các dạng tải riêng lẻ trong công trình như chiếu sáng, điều hoà không khí, bếp từ và bình nóng lạnh.
0	Riêng dạng tải ổ cắm thuộc dạng dùng hỗn hợp của nhiều tải khác nên nhóm tác giả không kể đến dạng tải này. Dữ liệu tiêu thụ tổng sẽ được tính bằng tổng của các tải được kể trên.Bộ dữ liệu được chia làm ba phần như mô tả trong Hình 7: Dữ liệu từ ngày đầu tiên (ngày 22/7/2020) đến hết ngày 06/08/2020 được sử dụng để làm bộ dữ liệu đào tạo cho các mô hình. Phần dữ liệu tiếp theo đến hết ngày 08/09/2020 (3 ngày) dùng làm dữ liệu kiểm tra phục vụ cho GridSearchCV nhằm tối ưu tham số cho mô hình. Dữ liệu còn lại (1 ngày) dùng để kiểm tra độ chính xác cho mô hình sau khi được tối ưu tham số.
0	Trong phần này, một số kết quả dự đoán đặc trưng của các mô hình sẽ được đưa ra phân tích, qua đó có thể đánh giá mức độ chính xác và hiệu quả của mô hình sau khi tối ưu các tham số. Với kết quả từ mô hình sử dụng thuật toán Random Forest thể hiện trong Hình 8, mô hình mặc dù đã được tối ưu tham số, tuy nhiên kết quả sau khi dự đoán vẫn chưa thực sự chính xác. Trong khoảng thời gian lúc 6:00 và khoảng 13:00–23:00 điều hoà được sử dụng, mô hình cũng đã xác định điều hoà có tiêu thụ điện, tuy nhiên mô hình cũng không dự đoán được các giá trị đỉnh mà chỉ đưa ra được giá trị trung bình trong giai đoạn tiêu thụ.
0	Ngoài ra còn một số thời điểm điều hoà không được sử dụng, tuy nhiên mô hình cũng cho kết luận phụ tải có tiêu thụ.Về cơ bản có thể thấy kết quả dự đoán của mô hình sử dụng thuật toán Gradient Boosting (thể hiện trong Hình 9) cho ra kết quả tương tự mô hình sử dụng thuật toán Random Forest. Ngoài ra, mô hình được cải thiện một số thiếu sót như đã có thể xác định một vài đỉnh tải mặc dù giá trị dự đoán chưa chính xác hoàn toàn. Một số thời điểm xác định tải tiêu thụ bị sai đã giảm nhưng vẫn chưa thể khắc phục hoàn toàn.
0	Kết quả nhận được từ mô hình Gaussian Process Regressor thể hiện trong Hình 10 khả quan hơn rất nhiều so với các mô hình bên trên. Ngoài việc có thể đưa ra các kết quả dự đoán thời điểm tải được sử dụng khá chính xác và đưa ra các giá trị trung bình, các đỉnh tải cũng được mô hình nhận biết và dự đoán ra với kết quả tương đối chính xác so với thực tế. Ngoài ra, các thời điểm dự đoán sai cũng xảy ra ít hơn rất nhiều so với các mô hình trên.
0	Khác với phụ tải điều hòa, phụ tải chiếu sáng có đặc trưng tiêu thụ nhỏ hơn và được sử dụng biến thiên rất nhiều trong quá trình được sử dụng. Kết quả thể hiện trong Hình 11 của mô hình sử dụng thuật toán k-NN cho thấy khoảng thời gian từ 0:00 đến 6:00, khi căn hộ tiêu thụ nhỏ và ít dao động, mô hình cho ra kết quả dự đoán tương đối tốt, gần như bám sát được đường tải tiêu thụ. Các thời điểm khác mô hình cũng dự đoán được việc vận hành của phụ tải. Tuy nhiên, một số thời điểm sử dụng phụ tải có dao động lớn, mô hình vẫn có những sai sót nhất định trong việc dự báo.
0	Cuối cùng là kết quả của mô hình sử dụng thuật toán Multi-layer Perceptron Regressor thể hiện tại Hình 12 cho thấy khả năng dự đoán tải kém hiệu quả hơn so với mô hình sử dụng thuật toán k-NN. Dù mô hình đã có thể dự đoán được tiêu thụ của phụ tải và cũng có thể xác định được tương đối các đỉnh phụ tải. Tuy nhiên, mô hình có một số dự đoán sai khá nghiêm trọng vào thời điểm từ 13:00 đến 23:00 khi dự đoán sai hoàn toàn tiêu thụ của phụ tải. Ngoài ra, các đỉnh tiêu thụ của tải cũng được mô hình dự đoán kém chính xác hơn.
0	Bảng 1 tổng hợp lại kết quả nhận được từ các mô hình nhằm so sánh được mức độ hiệu quả của các mô hình khác nhau. Có thể thấy các dạng phụ tải phức tạp như chiếu sáng hay điều hòa không khí đều gây ra sai số lớn hơn (10–20%) so với các dạng phụ tải có dạng tiêu thụ đơn giản (3–5%). Ngoài ra, một số mô hình mặc dù cho thấy khả năng dự đoán đỉnh tải tốt hơn nhưng xét về tổng quan lại cho sai số tổng thể cao hơn như mô hình Gaussian Process. Tuy nhiên, xuất hiện đột biến khi mô hình này dự đoán tải điều hòa cho kết quả tốt hơn đáng kể (2,9%) so với các mô hình còn lại (~20%).
0	Trong bài báo này, việc dự đoán tiêu thụ của tải thành phần dựa trên dữ liệu đo tổng của một căn hộ chung cư tại Hà Nội đã được triển khai thử nghiệm với việc ứng dụng các thuật toán machine learning thông dụng được cung cấp bởi thư viện scikit-learn. Các thuật toán được lựa chọn để thử nghiệm bao gồm Random Forest, Gradient Boosting Regressor, K-Nearest Neighbors Regressor, Gaussian Process Regressor và Multi-layer Perceptron Regressor.Từ so sánh các kết quả nhận được tiếp tục cho thấy các thuật toán đều có các ưu nhược điểm khác nhau. Các mô hình như Random Forest, Gradient Boosting hoặc k-NN cho kết quả sai số tổng quan khá ấn tượng khi sai số của kết quả thấp hơn so với các thuật toán còn lại, mặc dù việc dự đoán ra các đỉnh tải kém hơn.
0	Mặt còn lại, thuật toán Gaussian Process Regressor và Multi-layer Perceptron Regressor mặc dù cho các kết quả dự đoán được các đỉnh phụ tải tốt hơn, tuy nhiên khi xét về các sai số tổng quan lại cho ra sai số cao hơn các thuật toán còn lại. Như vậy, có thể thấy việc đưa ra các trọng số của kết quả hướng đến sẽ có tính quyết định khá lớn khi muốn lựa chọn thuật toán nào trong việc dự đoán các tải thành phần. Trong các nghiên cứu sâu hơn về sau, các công việc tiếp theo sẽ tập trung vào nâng cao độ chính xác của việc dự đoán bao gồm sai số và việc dự đoán ra các đặc tính phụ tải.
0	Trong những năm gần đây, các botnet đã trở thành một trong các nguy cơ gây mất an toàn thông tin hàng đầu do chúng không ngừng phát triển về cả quy mô và mức độ tinh vi. Nhiều giải pháp phát hiện botnet, như dựa trên phân tích lưu lượng mạng, phân tích truy vấn DNS đã được đề xuất. Trong đó, mô hình phát hiện botnet dựa trên học máy sử dụng dữ liệu tên miền trích xuất từ dữ liệu truy vấn DNS [9] cho kết quả phát hiện DGA botnet khả quan. Mô hình này loại bỏ phần tên miền cấp cao nhất trong mỗi tên miền truy vấn và sử dụng 18 đặc trưng để vector hoá mỗi tên miền cho khâu huấn luyện và phát hiện.
0	Trong bài báo này, chúng tôi tập trung phân tích ảnh hưởng của một số yếu tố lên hiệu quả của mô hình phát hiện đề xuất bởi [9]. Các yếu tố được xem xét phân tích bao gồm (1) vấn đề loại bỏ hoặc giữ nguyên phần tên miền cấp cao nhất và (2) ảnh hưởng của các nhóm đặc trưng huấn luyện. Kết quả khảo sát cho thấy phần tên miền cấp cao nhất giúp tăng đáng kể hiệu quả phát hiện và nhóm đặc trưng 2-gram có ảnh hưởng lớn nhất đến hiệu quả phát hiện.Trong những năm gần đây, botnet được đánh giá là một trong các nguy cơ gây mất an toàn thông tin hàng đầu trong các dạng mã độc (malware) hoạt động trên mạng Internet [1][2].
0	Các botnet không ngừng phát triển trên mạng Internet toàn cầu về cả quy mô và sự tinh vi của các kỹ thuật điều khiển. Mỗi thành viên trong botnet được gọi là bot. Bot là một malware do một nhóm tin tặc (botmaster) tạo ra cho phép chúng điều khiển các hệ thống máy tính bị lây nhiễm từ xa. Các bot khác với các dạng malware khác ở chỗ chúng có tính tự động (autonomy) cao và được trang bị khả năng sử dụng các kênh truyền thông để nhận lệnh và thông báo trạng thái hoạt động của mình đến hệ thống điều khiển. Các hệ thống điều khiển, hay các máy chủ cung cấp lệnh và điều khiển (Command and Control - C&C), là phương tiện trung gian để botmaster gửi các lệnh và bản mã cập nhật đến các bot.
0	Các mạng botnet thường được sử dụng để truyền tải các phần mềm độc hại, gửi thư rác, đánh cắp thông tin nhạy cảm, lừa đảo, sinh click ảo, hoặc nghiêm trọng hơn để thực hiện các cuộc tấn công mạng trên quy mô lớn, như tấn công DDoS. Theo một số báo cáo, hiện nay có khoảng 80% lưu lượng thông tin trên Internet có liên quan đến các hoạt động của các botnet, bao gồm các hoạt động gửi thư rác và tấn công mạng [1][2]. Dịch vụ tên miền (DNS – Domain Name Service) là một dịch vụ thiết yếu trên mạng Internet cho phép phân giải tên máy hoặc tên miền sang địa chỉ IP và ngược lại.
0	Chẳng hạn, mỗi khi trình duyệt máy khách cần truy nhập một trang web, nó trước hết gửi yêu cầu đến hệ thống DNS để tìm địa chỉ IP của máy chủ web, sau đó sử dụng địa chỉ IP tìm được để truy nhập máy chủ web và tải trang web. Như vậy, hầu hết các ứng dụng hợp pháp đều sử dụng dịch vụ DNS khi thực hiện các yêu cầu truy cập các dịch vụ mạng của mình. Tuy nhiên, dịch vụ DNS cũng được các bot trong botnet sử dụng như các ứng dụng hợp pháp. Các bot gửi các yêu cầu truy vấn DNS để tìm địa chỉ IP của máy chủ C&C và khi có địa chỉ IP, chúng truy nhập các máy chủ C&C để nhận các lệnh, cũng như để tải các bản mã bot cập nhật.
0	Để lẩn tránh việc rà quét, phát hiện các máy chủ C&C, botmaster liên tục thay đổi tên và địa chỉ IP của các máy chủ C&C theo các kỹ thuật xác định trước, như DGA (Domain Generation Algorithms) hoặc FF (Fast flux) [3][4][5]. Các thay đổi về tên và IP của các máy chủ C&C liên tục được đẩy lên hệ thống DNS. Các bot cũng được trang bị khả năng sinh tự động tên máy chủ C&C theo các kỹ thuật này. Nhờ vậy, các bot vẫn có thể tìm được địa chỉ IP của máy chủ C&C bằng cách sinh tên máy chủ tự động và truy vấn dịch vụ DNS.
0	Do vậy, việc giám sát và phân tích dữ liệu truy vấn DNS, đặc biệt là các tên miền và kết quả truy vấn có thể tiết lộ sự tồn tại của các hành động độc hại trong hệ thống mạng được giám sát do một phần dữ liệu truy vấn DNS có thể do botnet tạo ra.Phần còn lại của bài báo được cấu trúc như sau: Mục II trình bày các nghiên cứu có liên quan; Mục III giới thiệu mô hình phát hiện DGA botnet dựa trên học máy, giới thiệu khái quát về học máy và vấn đề tiền xử lý dữ liệu; Mục IV là phần thử nghiệm, các kết quả và phần bàn luận; và Mục V là kết luận của bài báo.
0	Như đã đề cập trong Mục I, nhiều botnet hiện nay sử dụng kỹ thuật DGA để sinh và đăng ký nhiều tên miền ngẫu nhiên khác nhau cho máy chủ lệnh và điều khiển của chúng nhằm chống lại việc bị kiểm soát và đưa vào danh sách đen (Blacklist) [3][5]. Lý do chính của việc sử dụng DGA là làm phức tạp việc kiểm soát thu hồi tên miền. Các botnet dạng này còn được gọi là DGA-based botnet, hay ngắn gọn là DGA botnet.Thuật toán DGA có thể sử dụng các phép toán tử kết hợp với các biến luôn thay đổi, chẳng hạn như năm, tháng, ngày, giờ, phút để sinh tên miền ngẫu nhiên. Ví dụ, một dạng của thuật toán DGA được thực hiện bởi một hàm có chứa 16 vòng lặp.
0	Hình 1 biểu diễn cơ chế botnet sử dụng DGA để tự động sinh và đăng ký các tên miền cho máy chủ C&C của mình. Theo đó, máy chủ C&C và các bot sử dụng cùng một thuật toán DGA với cùng một nhân (seed) nên chúng có thể sinh ra cùng một tập các tên miền. DGA botnet thường sử dụng ngày giờ như là nhân để khởi tạo thuật toán sinh tên miền và như vậy, DGA botnet tạo một tập các tên miền trong mỗi ngày nó hoạt động. Để khởi tạo kết nối đến máy chủ C&C, một bot trước hết cần thực thi thuật toán DGA để sinh một tên miền và tên miền này cũng có thể được sinh tự động bởi máy chủ C&C do cả máy chủ C&C và bot sử dụng chung một thuật toán DGA và một nhân.
0	Sau khi tạo được tên miền, bot sử dụng hệ thống DNS để phân giải tên miền thành địa chỉ IP của máy chủ C&C. Nếu quá trình phân giải tên miền không thành công, bot sử dụng DGA để sinh một tên miền mới và lặp lại yêu cầu phân giải địa chỉ. Nếu quá trình phân giải tên miền thành công, bot sử dụng địa chỉ IP để kết nối đến máy chủ C&C để nhận các lệnh và điều khiển từ botmaster. Liên quan đến việc giám sát các truy vấn hệ thống DNS cho phát hiện DGA botnet, một số công bố đề xuất các kỹ thuật phát hiện các tên miền được tạo ra tự động sử dụng các thuật toán, hoặc tên miền được mã độc sử dụng.
0	Theo đó, Yadav và cộng sự [6] đề xuất phương pháp phân biệt các tên miền sinh tự động bằng thuật toán thường được sử dụng trong các botnet với tên miền hợp lệ dựa trên phân tích sự phân bố các ký tự trong tên miền. Cũng với mục đích tương tự, Stalmans và cộng sự [7] sử dụng phương pháp cây quyết định C5.0 và thống kê Baysian để phân loại các tên miền sinh tự động với các tên hợp lệ.Với cách tiếp cận khác, Bilge và cộng sự [8] giới thiệu hệ thống EXPOSURE cho phép giám sát lưu lượng truy vấn DNS trên diện rộng để phát hiện các tên miền có liên quan đến các hành vi độc hại. EXPOSURE sử dụng 15 thuộc tính của tên miền để phân biệt các tên miền đáng ngờ với tên miền hợp lệ.
0	Các kết quả thử nghiệm cho thấy hệ thống có khả năng mở rộng tốt và nhận dạng được các tên miền mới có liên quan đến các hành vi độc hại, như chúng được sử dụng cho máy chủ C&C, cho gửi thư rác và sử dụng cho các website lừa đảo. Nghiên cứu trước đây của chúng tôi [9] đề xuất mô hình phân loại tên miền DGA botnet dựa trên các kỹ thuật học máy sử dụng dữ liệu truy vấn DNS. Các tên miền trước hết được loại bỏ phần tên miền cấp cao nhất và sau đó mỗi tên miền được chuyển đổi thành một vector gồm 18 đặc trưng sử dụng cho các khâu huấn luyện và phát hiện.
0	Mô hình sử dụng tập dữ liệu gồm các tên miền bình thường, hay lành tính và các tên miền botnet đã được gán nhãn cho quá trình huấn luyện và phát hiện. Kết quả thử nghiệm cho thấy độ chính xác phát hiện chung tốt nhất đạt khoảng 90% với thuật toán học máy rừng ngẫu nhiên.Trong bài báo này, chúng tôi tập trung phân tích ảnh hưởng của một số yếu tố lên hiệu quả của mô hình phát hiện đề xuất bởi [9]. Các yếu tố được xem xét, phân tích bao gồm (1) vấn đề sử dụng hoặc loại bỏ phần tên miền cấp cao nhất và (2) ảnh hưởng của các nhóm đặc trưng huấn luyện.
0	Hình 1 biểu diễn mô hình phát hiện DGA botnet dựa trên học máy [9]. Mô hình này được xây dựng trên cơ sở phân tích ở Mục I về việc các bot trong DGA botnet thường xuyên sinh tự động các tên miền và truy vấn hệ thống DNS để tìm địa chỉ IP của các máy chủ C&C. Mô hình phát hiện được triển khai thành 2 giai đoạn: (a) giai đoạn huấn luyện và (b) giai đoạn phát hiện. Trong giai đoạn huấn luyện, dữ liệu truy vấn hệ thống DNS được thu thập, sau đó qua khâu tiền xử lý nhằm tách các tên miền được truy vấn và trích xuất các đặc trưng.
0	Trong khâu huấn luyện, thuật toán học máy cây quyết định được áp dụng để học ra Bộ phân loại. Tập dữ liệu sử dụng cho khâu huấn luyện đã được gán nhãn, gồm (1) tập tên miền bình thường, hay lành tính và (2) tập tên miền botnet. Chi tiết về các tập dữ liệu cho thử nghiệm được mô tả trong Mục IV.A.Trong giai đoạn phát hiện của mô hình, các truy vấn DNS được giám sát và qua quá trình tiền xử lý đến khâu phân loại sử dụng Bộ phân loại từ giai đoạn huấn luyện để xác định một tên miền là bình thường hay tên miền của DGA botnet.
0	Học máy (Machine learning) là một lĩnh vực của khoa học máy tính, liên quan đến việc nghiên cứu và xây dựng các kỹ thuật cho phép máy tính có khả năng tự học dựa trên dữ liệu đưa vào để giải quyết những vấn đề cụ thể [10]. Dựa theo phương pháp học, các kỹ thuật học máy thường được chia làm 3 nhóm chính: học có giám sát, học không giám sát và học bán giám sát [10]. Mỗi phương pháp học có ưu nhược điểm riêng và miền ứng dụng riêng. Trong bài báo này, chúng tôi sử dụng thuật toán học máy Cây quyết định (Decision Tree) trong mô hình phát hiện DGA botnet.
0	Cây quyết định là một thuật toán học máy có giám sát cho độ chính xác phân loại cao, tốc độ xử lý nhanh và được ứng dụng rộng rãi trong nhiều lĩnh vực của Khoa học máy tính [10]. Cây quyết định là một kiểu mô hình dự đoán, nghĩa là một ánh xạ từ các quan sát về một sự vật hoặc hiện tượng tới các kết luận về giá trị mục tiêu của sự vật hoặc hiện tượng. Cây quyết định tạo ra mô hình cho phép phân loại một đối tượng bằng cách tạo ra một bộ các quy tắc quyết định. Các quy tắc này được trích ra dựa trên bộ các đặc trưng của các dữ liệu huấn luyện.
0	Trong cây quyết định, các lá đại diện cho các lớp (nhãn), mỗi nút con trong cây và các nhánh cây của nó biểu diễn sự kết hợp của các đặc trưng để dẫn dắt tới việc phân lớp.Như vậy, việc phân loại một đối tượng sẽ bắt đầu với việc kiểm tra giá trị của nút gốc, sau đó tiếp tục đi xuống dưới theo các nhánh cây tương ứng với các giá trị đó. Quá trình này được thực hiện đệ quy, lặp đi lặp lại đối với từng nút, cho đến khi không thể đi tiếp được nữa và chạm đến nút lá. Để có mô hình tốt nhất, việc quyết định lựa chọn nút gốc và nút con trong khi xây dựng cây quyết định dựa trên các độ đo Information Gain (IG) và Gini Impurity [10]. Thuật toán cây quyết định được sử dụng là C4
0	Như đã đề cập trong mục III.A, hai nhiệm vụ chính của khâu tiền xử lý dữ liệu bao gồm (1) làm sạch dữ liệu truy vấn DNS và (2) trích xuất các đặc trưng của tên miền truy vấn phục vụ cho khâu huấn luyện và phát hiện. Do mô hình phát hiện DGA botnet chỉ sử dụng tên miền truy vấn trong dữ liệu truy vấn DNS, nhiệm vụ (1) làm sạch dữ liệu truy vấn DNS thực hiện việc tách tên miền truy vấn trong yêu cầu truy vấn (DNS Query) mà các máy khách gửi đến hệ thống DNS để tìm địa chỉ IP tương ứng. Các tên miền sau trích xuất được lưu vào tập dữ liệu tên miền cho nhiệm vụ tiền xử lý tiếp theo.
0	Nhiệm vụ (2) trích xuất các đặc trưng của tên miền truy vấn thực hiện việc tính toán các giá trị cho các đặc trưng của tên miền truy vấn và biểu diễn mỗi tên miền thành một vector đặc trưng làm đầu vào cho khâu huấn luyện và phát hiện. Chúng tôi sử dụng 18 đặc trưng đề xuất trong nghiên cứu trước đây của nhóm [9] để vector hoá các tên miền nhằm khảo sát ảnh hưởng của các nhóm đặc trưng lên hiệu quả phát hiện. Các đặc trưng được khảo sát trong 3 nhóm sau:  Nhóm 1 gồm 8 đặc trưng thống kê từ vựng cho các cụm 2-gram (bi-gram) trích xuất từ mỗi tên miền. Các đặc trưng này được ký hiệu là f1, f2, f3, f4, f5, f6, f7, f8.
0	Nhóm 2 gồm 8 đặc trưng thống kê từ vựng cho các cụm 3-gram (tri-gram) trích xuất từ mỗi tên miền. Các đặc trưng này được ký hiệu là f9, f10, f11, f12, f13, f14, f15, f16. Nhóm 3 gồm 2 đặc trưng phân bố nguyên âm của tên miền. Các đặc trưng này được ký hiệu là f17, f18.Bi-gram hay 2-gram là một cụm gồm 2 ký tự kề nhau được trích ra từ một chuỗi ký tự. Ví dụ, với chuỗi “example” gồm các bi-gram: ex, xa, am, mp, pl, le. Một tên miền có thể chứa các ký tự trong tập 26 ký tự chữ cái (a–z), các ký tự số (0–9), ký tự “.” và “-”, do đó tổng số bi-gram có thể có là TS(bi-gram) = 38 × 38 = 1.444.
0	Từ tập hợp các tên miền bình thường trích rút ra danh sách gồm N cụm bi-gram thường xuyên xuất hiện nhất, ký hiệu là DS(bi-gram). DS(bi-gram) được sử dụng cho việc tính toán 8 đặc trưng trong Nhóm 1 cho từng tên miền.Tri-gram hay 3-gram là một cụm gồm 3 ký tự kề nhau được trích ra từ một chuỗi ký tự. Ví dụ, với chuỗi “example” gồm các tri-gram: exa, xam, amp, mpl, ple. Tương tự cách tính tổng số bi-gram, tổng số tri-gram có thể có là TS(tri-gram) = 38 × 38 × 38 = 54.872. Từ tập hợp các tên miền bình thường trích rút ra danh sách gồm M cụm tri-gram có tần suất xuất hiện cao nhất, ký hiệu là DS(tri-gram). DS(tri-gram) được sử dụng cho việc tính toán 8 đặc trưng trong Nhóm 2 cho từng tên miền.
0	Tập dữ liệu tên miền sử dụng trong các thử nghiệm trong bài báo này bao gồm: 80.000 tên miền lành tính, được gán nhãn Normal. Đây là các tên miền có thứ hạng cao nhất được trích xuất từ tập 1 triệu tên miền hàng đầu do Alexa xếp hạng [11]. Các tên miền lành tính được kiểm tra lại tại trang virustotal.com để đảm bảo chúng thực sự là tên miền lành tính. 80.000 tên miền độc hại, được gán nhãn Botnet. Đây là các tên miền được trích xuất từ [12][13], sinh bởi Conficker botnet và DGA botnet. Từ tập dữ liệu trên, chúng tôi trích xuất ngẫu nhiên để sinh 2 tập dữ liệu cho các thử nghiệm: tập TRAIN cho huấn luyện và tập TEST cho kiểm thử. Các tên miền trong tập TRAIN và tập TEST không trùng nhau.
0	Mỗi tập trên đều gồm 40.000 tên miền với thành phần cho như trên Bảng I. Để đánh giá ảnh hưởng của các đặc trưng trích chọn từ các tên miền truy vấn DNS, chúng tôi xây dựng các kịch bản thử nghiệm huấn luyện và phân loại tên miền sử dụng tập huấn luyện TRAIN và tập kiểm thử TEST dựa trên thuật toán cây quyết định C4.5 như sau: Kịch bản 1: Sử dụng đầy đủ 18 đặc trưng như mô tả trong [9] với 2 trường hợp: (1) các tên miền được sử dụng đầy đủ và (2) các tên miền được loại bỏ phần tên miền cấp cao nhất. Kịch bản này nhằm đánh giá ảnh hưởng của tên miền cấp cao nhất lên hiệu quả phân loại.
0	Kịch bản 2: Chỉ sử dụng các đặc trưng 2-gram hoặc các đặc trưng 3-gram với các đặc trưng nguyên âm. Kịch bản này nhằm đánh giá ảnh hưởng của mỗi nhóm đặc trưng n-gram lên hiệu quả phân loại. Kịch bản 3: Loại bỏ các đặc trưng n-gram, chỉ sử dụng các đặc trưng nguyên âm. Kịch bản này nhằm đánh giá ảnh hưởng của các đặc trưng n-gram lên hiệu quả phân loại. Kịch bản 4: Sử dụng nhóm đặc trưng 2-gram và các đặc trưng nguyên âm, lần lượt loại bỏ từng đặc trưng trong nhóm đặc trưng 2-gram. Kịch bản này nhằm đánh giá ảnh hưởng của mỗi đặc trưng 2-gram lên hiệu quả phân loại.
0	Các độ đo phân loại được sử dụng bao gồm PPV (Positive Predictive Value – Độ chính xác dự đoán), FPR (False Positive Rate – Tỷ lệ sai dương), TPR (True Positive Rate – Tỷ lệ đúng dương hay độ nhạy), ACC (Accuracy – Độ chính xác chung) và độ đo F1 (F1-Score). Từ các kết quả thử nghiệm cho trên các bảng kết quả trình bày ở mục IV.C, có thể rút ra một số nhận xét như sau: Bảng II cho thấy việc sử dụng tên miền đầy đủ làm tăng đáng kể độ chính xác phân loại chung (ACC) do tỷ lệ sai dương (FPR) giảm. Tuy nhiên, cần có các thử nghiệm toàn diện trên các tập dữ liệu lớn hơn để có thể khẳng định việc loại bỏ tên miền cấp cao nhất có thể làm hiệu quả phân loại tên miền DGA botnet xấu đi.
0	Bảng III khẳng định các đặc trưng 2-gram có ảnh hưởng lớn hơn so với các đặc trưng 3-gram lên hiệu quả phân loại. Mặc dù việc phân loại sử dụng các đặc trưng 2-gram có độ đo PPV thấp hơn, nhưng độ đo TPR lại cao hơn hẳn so với việc sử dụng các đặc trưng 3-gram, nên độ chính xác phân loại chung (ACC) cao hơn. Bảng IV cho thấy tầm quan trọng của các đặc trưng n-gram với hiệu quả phân loại. Điều này có thể hiểu được, do các đặc trưng n-gram chiếm đa số (16 đặc trưng) trong tổng số 18 đặc trưng sử dụng. Thông qua kết quả cho ở Bảng V có thể thấy, từng đặc trưng 2-gram riêng lẻ có ảnh hưởng không đáng kể lên hiệu quả phân loại.
0	Tuy nhiên, tập hợp của tất cả các đặc trưng n-gram, gồm các đặc trưng 2-gram và 3-gram lại có ảnh hưởng quyết định lên hiệu quả phân loại (như thể hiện ở Bảng IV).Bài báo này khảo sát ảnh hưởng của hai yếu tố đến độ chính xác của mô hình phát hiện DGA botnet dựa trên học máy sử dụng dữ liệu truy vấn DNS. Các yếu tố được khảo sát bao gồm (1) việc sử dụng hay loại bỏ tên miền cấp cao nhất và (2) ảnh hưởng của các nhóm đặc trưng huấn luyện lên độ chính xác phát hiện. Các kết quả trên 4 kịch bản thử nghiệm cho thấy việc sử dụng tên miền cấp cao nhất làm tăng đáng kể độ chính xác phân loại chung (tăng 1,21%).
0	Trong 3 nhóm đặc trưng, nhóm 8 đặc trưng 2-gram có ảnh hưởng lớn nhất đến độ chính xác phân loại chung, xếp sau là nhóm 3-gram và cuối cùng là nhóm các đặc trưng nguyên âm. Kết quả thử nghiệm cũng cho thấy từng đặc trưng 2-gram có ảnh hưởng không đáng kể đến độ chính xác phân loại chung. Dù vậy, tập hợp 16 đặc trưng 2-gram và 3-gram có ảnh hưởng quyết định đến độ chính xác phân loại chung. Trong tương lai, chúng tôi tiếp tục nghiên cứu, đề xuất các đặc trưng mới giúp nâng cao độ chính xác phát hiện của mô hình.
1	Thị giác máy tính là một trong những lĩnh vực cốt lõi của trí tuệ nhân tạo, cho phép máy móc hiểu và phân tích thông tin trực quan từ hình ảnh hoặc video. Trong những năm gần đây, sự phát triển của học sâu, đặc biệt là mạng nơ-ron tích chập (CNN), đã tạo ra bước đột phá trong bài toán nhận dạng đối tượng. Các mô hình học sâu có khả năng tự động trích xuất đặc trưng cấp cao, vượt trội so với các phương pháp truyền thống dựa trên đặc trưng thủ công. Bài báo này tập trung phân tích các kiến trúc CNN phổ biến và đánh giá hiệu quả của chúng trong nhận dạng đối tượng ảnh số, nhằm làm rõ vai trò của học sâu trong việc nâng cao độ chính xác và khả năng tổng quát hóa.
1	Nghiên cứu sử dụng các kiến trúc CNN tiêu biểu như VGG, ResNet và EfficientNet để thực hiện bài toán nhận dạng đối tượng. Dữ liệu đầu vào được tiền xử lý thông qua chuẩn hóa và tăng cường dữ liệu nhằm giảm hiện tượng quá khớp. Quá trình huấn luyện được thực hiện bằng thuật toán tối ưu Adam với hàm mất mát cross-entropy. Các mô hình được đánh giá dựa trên độ chính xác, độ thu hồi và F1-score. Ngoài ra, nghiên cứu còn so sánh hiệu quả giữa mô hình pretrained và mô hình huấn luyện từ đầu để đánh giá lợi ích của học chuyển giao trong thị giác máy tính.
1	Kết quả thực nghiệm cho thấy các mô hình CNN sâu như ResNet và EfficientNet đạt độ chính xác cao hơn đáng kể so với các mô hình nông. Việc sử dụng trọng số pretrained giúp rút ngắn thời gian huấn luyện và cải thiện khả năng hội tụ. Tuy nhiên, các mô hình phức tạp đòi hỏi tài nguyên tính toán lớn, gây khó khăn trong triển khai thực tế. Phân tích lỗi cho thấy mô hình vẫn gặp khó khăn với các đối tượng bị che khuất hoặc có ánh sáng không đồng đều. Điều này cho thấy cần kết hợp thêm các kỹ thuật attention hoặc học đa nhiệm.
1	Bài báo đã chứng minh một cách thuyết phục hiệu quả vượt trội của học sâu (deep learning) trong bài toán nhận dạng đối tượng trên ảnh số, vượt xa các phương pháp truyền thống dựa trên đặc trưng thủ công. Các kiến trúc CNN hiện đại như VGG, ResNet hay EfficientNet cho thấy khả năng tự động học đặc trưng phân cấp, giúp mô hình đạt độ chính xác cao và tính ổn định tốt trên nhiều bộ dữ liệu chuẩn. Đặc biệt, khi kết hợp với học chuyển giao (transfer learning) từ các mô hình pretrained, hiệu suất được cải thiện đáng kể ngay cả trong bối cảnh dữ liệu huấn luyện hạn chế.
1	Bài báo đã chứng minh hiệu quả vượt trội của học sâu (Deep Learning) trong lĩnh vực nhận dạng đối tượng từ ảnh số. Các kiến trúc mạng thần kinh tích chập (CNN) hiện đại, nhờ khả năng trích xuất đặc trưng tự động, đã mang lại độ chính xác đột phá, đặc biệt khi kết hợp với kỹ thuật học chuyển giao (Transfer Learning) trên các bộ dữ liệu lớn như ImageNet.Tuy nhiên, việc triển khai thực tế vẫn đối mặt với thách thức lớn về tài nguyên tính toán và độ trễ khi xử lý trong các môi trường phức tạp hoặc trên thiết bị biên (Edge devices). Do đó, các hướng nghiên cứu tương lai cần tập trung vào việc tối ưu hóa các mô hình nhẹ hơn thông qua kỹ thuật nén mạng hoặc tri thức chưng cất (Knowledge Distillation).
1	Sự phát triển nhanh chóng của mạng máy tính kéo theo sự gia tăng các mối đe dọa an ninh mạng. Các hệ thống phát hiện xâm nhập truyền thống dựa trên chữ ký thường không hiệu quả trước các kiểu tấn công mới. Học máy nổi lên như một giải pháp tiềm năng nhờ khả năng học mẫu và phát hiện bất thường trong dữ liệu mạng. Bài báo này trình bày tổng quan về việc áp dụng các thuật toán học máy trong phát hiện tấn công mạng, tập trung vào khả năng tự động hóa và thích nghi với môi trường mạng động.
1	Nghiên cứu này tập trung áp dụng các thuật toán học máy phổ biến như SVM (Support Vector Machine), Random Forest và Mạng nơ-ron nhân tạo (ANN) trên tập dữ liệu lưu lượng mạng để phân loại hành vi người dùng và phát hiện bất thường. Các đặc trưng đầu vào quan trọng được trích xuất bao gồm số lượng gói tin, thời gian duy trì kết nối và tần suất truy cập trung bình.Để đảm bảo tính khách quan, dữ liệu được phân tách nghiêm ngặt thành tập huấn luyện và kiểm tra, giúp đánh giá chính xác khả năng tổng quát hóa của mô hình trên các kịch bản mạng thực tế.
1	Kết quả thực nghiệm cho thấy Random Forest đạt độ chính xác (Accuracy) vượt trội trong việc phát hiện các hình thức tấn công phổ biến nhờ khả năng xử lý tốt các tập dữ liệu có cấu trúc phân cấp. Trong khi đó, SVM chứng minh thế mạnh đặc biệt khi làm việc với dữ liệu có số chiều lớn, duy trì được ranh giới phân loại tối ưu ngay cả khi không gian đặc trưng bị nhiễu. Mạng nơ-ron nhân tạo thể hiện tiềm năng mạnh mẽ trong việc nhận diện các mẫu tấn công phi tuyến phức tạp, tuy nhiên, chi phí tính toán và thời gian huấn luyện kéo dài vẫn là rào cản lớn cho việc triển khai thực tế.
1	Học máy đã khẳng định vai trò không thể thay thế trong hệ thống phát hiện xâm nhập mạng (IDS) hiện đại, mang lại lớp bảo mật thông minh và chủ động hơn. Mặc dù các mô hình hiện tại đạt hiệu quả cao, việc tối ưu hóa để xử lý dữ liệu quy mô lớn (Big Data) và thích ứng với các biến thể tấn công mới vẫn là nhiệm vụ cấp thiết.Nghiên cứu tương lai nên tập trung vào các kiến trúc Học sâu (Deep Learning) như mạng LSTM để phân tích dữ liệu chuỗi thời gian, kết hợp với chiến lược Học liên tục (Continual Learning). Cách tiếp cận này giúp hệ thống tự cập nhật tri thức mà không cần huấn luyện lại từ đầu, từ đó nâng cao khả năng phản ứng trước các mối đe dọa an ninh mạng luôn biến đổi không ngừng.
1	Trong nghiên cứu này, chúng tôi đề xuất một mô hình phát hiện đối tượng mới dựa trên mạng nơ-ron tích chập (CNN) để cải thiện độ chính xác và tốc độ xử lý hình ảnh thời gian thực. Mô hình kết hợp kiến trúc YOLOv5 với các kỹ thuật tăng cường dữ liệu và tối ưu hóa tham số, nhằm giải quyết vấn đề phát hiện đối tượng trong môi trường phức tạp như giao thông đô thị. Kết quả thí nghiệm trên bộ dữ liệu COCO cho thấy mô hình đạt mAP@0.5 là 0.72, cao hơn 15% so với các phương pháp truyền thống. Nghiên cứu cũng đánh giá hiệu suất trên thiết bị nhúng, chứng minh tính khả thi cho ứng dụng thực tế như xe tự lái.
1	Các đóng góp chính bao gồm thuật toán tối ưu hóa vùng đề xuất và cơ chế học chuyển tiếp từ dữ liệu lớn. Nghiên cứu này mở ra hướng phát triển cho thị giác máy tính trong IoT. Thị giác máy tính đã trở thành lĩnh vực then chốt trong trí tuệ nhân tạo, với ứng dụng rộng rãi từ giám sát an ninh đến y tế. Tuy nhiên, phát hiện đối tượng vẫn gặp thách thức lớn do biến đổi ánh sáng, che khuất và đa dạng đối tượng. Nghiên cứu trước đây chủ yếu dựa vào các mô hình như Faster R-CNN hoặc SSD, nhưng chúng thường chậm và yêu cầu tài nguyên cao. Chúng tôi tập trung vào việc cải thiện tốc độ mà không hy sinh độ chính xác bằng cách tích hợp học sâu và kỹ thuật tối ưu hóa.
1	Mục tiêu chính là phát triển một hệ thống có thể hoạt động trên thiết bị di động với độ trễ thấp. Nghiên cứu này sử dụng dữ liệu thực tế từ camera giao thông để đánh giá, nhằm đóng góp vào sự phát triển của thành phố thông minh. Chúng tôi cũng thảo luận về các vấn đề đạo đức liên quan đến quyền riêng tư dữ liệu hình ảnh.Các nghiên cứu trước đây về phát hiện đối tượng có thể chia thành hai loại: hai giai đoạn như R-CNN và một giai đoạn như YOLO. Girshick et al. (2014) giới thiệu R-CNN, đạt độ chính xác cao nhưng chậm do xử lý vùng đề xuất riêng lẻ.
1	Redmon et al. (2016) đề xuất YOLO, chuyển đổi vấn đề thành hồi quy, tăng tốc độ lên 45 FPS. Gần đây, Bochkovskiy et al. (2020) cải tiến YOLOv5 với CSPNet, cải thiện mAP lên 0.65 trên COCO. Tuy nhiên, các mô hình này vẫn yếu trong môi trường nhiễu. Các công trình khác như DETR (Carion et al., 2020) sử dụng transformer, nhưng yêu cầu dữ liệu huấn luyện lớn. Nghiên cứu của chúng tôi xây dựng trên YOLOv5, thêm module chú ý để tập trung vào đặc trưng quan trọng, khắc phục hạn chế của các phương pháp cũ.
1	Chúng tôi thiết kế mô hình dựa trên kiến trúc YOLOv5, với backbone là CSPDarknet53 để trích xuất đặc trưng. Quá trình huấn luyện sử dụng bộ dữ liệu COCO với 80 lớp đối tượng, kết hợp tăng cường dữ liệu như xoay, phóng to và thay đổi độ sáng. Thuật toán tối ưu hóa sử dụng SGD với learning rate 0.01, và thêm layer chú ý để cải thiện phát hiện đối tượng nhỏ. Quy trình bao gồm: (1) Tiền xử lý hình ảnh; (2) Trích xuất đặc trưng; (3) Dự đoán bounding box và lớp; (4) Non-max suppression để loại bỏ trùng lặp. Mô hình được huấn luyện trên GPU NVIDIA RTX 3090 trong 300 epochs. Để đánh giá, chúng tôi sử dụng metrics như precision, recall và mAP. Các tham số được tinh chỉnh qua grid search để đạt hiệu suất tối ưu.
1	Kết quả thí nghiệm cho thấy mô hình đề xuất đạt mAP@0.5 là 0.72 trên bộ test COCO, cao hơn YOLOv4 (0.65) và SSD (0.58). Tốc độ xử lý đạt 60 FPS trên GPU, phù hợp cho ứng dụng thời gian thực. Trong môi trường giao thông, độ chính xác phát hiện xe hơi là 95%, giảm lỗi che khuất 20% so với baseline. Biểu đồ so sánh cho thấy recall tăng 18% cho đối tượng nhỏ. Trên thiết bị nhúng như Jetson Nano, tốc độ vẫn duy trì 25 FPS với độ chính xác 0.68 mAP. Các thử nghiệm với nhiễu Gaussian cho thấy mô hình robust hơn, với F1-score 0.85. Tổng thể, kết quả chứng minh tính vượt trội của mô hình trong cả độ chính xác và hiệu suất.
1	Mặc dù mô hình đạt kết quả tốt, vẫn tồn tại hạn chế như phụ thuộc vào dữ liệu huấn luyện chất lượng cao và yêu cầu phần cứng mạnh. Trong môi trường thực tế, biến đổi thời tiết có thể giảm độ chính xác xuống 10%. So với các nghiên cứu khác, phương pháp của chúng tôi ưu việt hơn ở tốc độ, nhưng cần cải thiện cho đối tượng hiếm gặp. Các hàm ý thực tiễn bao gồm tích hợp vào hệ thống giám sát thông minh, giúp giảm tai nạn giao thông. Về mặt đạo đức, cần đảm bảo bảo mật dữ liệu để tránh lạm dụng. Hướng nghiên cứu tương lai có thể kết hợp với học liên tục để thích ứng với dữ liệu mới mà không cần retrain toàn bộ.
1	Nghiên cứu này đã phát triển thành công một mô hình phát hiện đối tượng tiên tiến dựa trên kiến trúc YOLOv5, mang lại sự cân bằng tối ưu giữa độ chính xác và tốc độ xử lý vượt trội so với các phương pháp truyền thống. Đóng góp cốt lõi của nghiên cứu nằm ở việc tích hợp module chú ý (Attention Module) giúp mô hình tập trung vào các đặc trưng quan trọng của đối tượng, đồng thời thực hiện các kỹ thuật tối ưu hóa trọng số để thích ứng tốt với các thiết bị nhúng có tài nguyên hạn chế. Kết quả thực nghiệm cho thấy mô hình không chỉ hoạt động ổn định trong các điều kiện ánh sáng thay đổi mà còn mở ra triển vọng ứng dụng rộng lớn trong các hệ thống thị giác máy tính thực tế .
1	Nghiên cứu này khám phá ứng dụng học tăng cường (RL) trong phát triển AI cho trò chơi, tập trung vào mô hình DQN cải tiến để chơi game Atari. Chúng tôi đề xuất thuật toán kết hợp epsilon-greedy với experience replay ưu tiên, nhằm tăng tốc độ học và ổn định. Thí nghiệm trên 10 game Atari cho thấy điểm trung bình cao hơn 30% so với DQN cơ bản. Mô hình đạt hiệu suất gần con người ở các game như Pong và Breakout. Đóng góp bao gồm cơ chế thưởng động và tích hợp mạng nơ-ron convolutional. Nghiên cứu nhấn mạnh tiềm năng của RL trong giải trí và mô phỏng thực tế ảo.
1	Học tăng cường đang cách mạng hóa AI, đặc biệt trong trò chơi nơi môi trường động và không chắc chắn. Các nghiên cứu trước như AlphaGo chứng minh khả năng vượt trội, nhưng áp dụng cho game video vẫn hạn chế do không gian trạng thái lớn. Chúng tôi nhằm cải thiện DQN bằng cách thêm ưu tiên replay để tập trung vào trải nghiệm quan trọng. Mục tiêu là tạo AI học nhanh hơn với ít tài nguyên hơn. Nghiên cứu sử dụng môi trường OpenAI Gym để đánh giá, tập trung vào các game 2D cổ điển. Chúng tôi cũng thảo luận về thách thức như overestimation và cách khắc phục.
1	Mnih et al. (2015) đã tạo ra bước ngoặt với DQN, kết hợp thành công Q-learning cùng mạng CNN để đạt mức độ chơi game Atari vượt xa con người. Để khắc phục các hạn chế cố hữu, các biến thể như Double DQN (Van Hasselt et al., 2016) đã ra đời nhằm giảm thiểu tình trạng ước tính quá mức (overestimation) giá trị Q, trong khi Prioritized Experience Replay (Schaul et al., 2015) giúp tối ưu hóa việc học bằng cách ưu tiên các mẫu dữ liệu có sai số TD (TD-error) lớn. Đáng chú ý nhất là Rainbow (Hessel et al., 2018), một sự kết hợp đa nền tảng các kỹ thuật tân tiến, đã thiết lập những kỷ lục mới về điểm số.
1	Hệ thống được xây dựng trên nền tảng kiến trúc DQN với mạng xương sống (backbone) là CNN 3 lớp tích chập, cho phép trích xuất các đặc trưng không gian trực tiếp từ khung hình game đầu vào. Quy trình thực thi bắt đầu bằng việc khởi tạo mạng Q-network, sau đó tác tử thực hiện lựa chọn hành động thông qua chiến lược epsilon-greedy để cân bằng giữa khai phá và khai thác. Các chuyển đổi trạng thái (transitions) được lưu trữ vào bộ nhớ đệm (buffer) theo cơ chế ưu tiên dựa trên sai số TD-error, giúp mô hình tập trung học từ những mẫu dữ liệu quan trọng nhất. Tiếp theo, một batch dữ liệu sẽ được lấy mẫu để cập nhật trọng số mạng bằng thuật toán tối ưu hóa Adam.
1	Trên trò chơi Pong, mô hình đề xuất đã đạt được mức điểm trung bình ấn tượng là 20 sau 500.000 khung hình huấn luyện, vượt xa mức 15 điểm của phiên bản DQN cơ bản. Kết quả tương tự cũng được ghi nhận đối với trò chơi Breakout, khi mô hình đạt tới 400 điểm so với mức 250 điểm của mô hình đối chứng. Tính tổng thể trên 10 trò chơi thử nghiệm, phương pháp cải tiến đã giúp nâng cao hiệu quả phần thưởng (reward) lên đến 35%, đồng thời rút ngắn thời gian hội tụ khoảng 20%.
1	Mặc dù đạt được những kết quả khả quan, mô hình vẫn tồn tại hạn chế nhất định khi phụ thuộc khá nhiều vào hệ thống siêu tham số (hyperparameters), đòi hỏi quá trình tinh chỉnh thủ công để đạt hiệu suất tối ưu cho từng môi trường cụ thể. Khi đặt bàn cân so với kiến trúc Rainbow, phương pháp của chúng tôi có cấu trúc đơn giản hơn đáng kể nhưng vẫn duy trì được hiệu quả tương đương trên các dòng trò chơi có độ phức tạp thấp và trung bình. Với khả năng phản ứng linh hoạt, mô hình này mở ra tiềm năng ứng dụng thực tiễn rất lớn trong lĩnh vực kiểm thử trò chơi tự động (AI game testing), giúp phát hiện lỗi và đánh giá cân bằng game một cách hiệu quả.
1	Nghiên cứu này đề xuất một thuật toán lập lịch tài nguyên tiên tiến dựa trên học máy nhằm tối ưu hóa chất lượng dịch vụ (QoS) trong hạ tầng mạng 5G vốn đòi hỏi khắt khe về băng thông. Phương pháp cốt lõi là sử dụng thuật toán SVM (Support Vector Machine) để dự đoán chính xác sự biến động của tải mạng theo thời gian thực, từ đó thực hiện phân bổ băng thông động một cách linh hoạt.Thông qua các kịch bản mô phỏng trên nền tảng NS-3, kết quả thực nghiệm cho thấy sự cải thiện rõ rệt với độ trễ giảm 25% và lưu lượng (throughput) tăng 18% so với các phương pháp lập lịch truyền thống.
1	Mạng 5G hứa hẹn mang lại tốc độ truyền tải vượt trội, tuy nhiên việc đảm bảo chất lượng dịch vụ (QoS) vẫn là một thách thức lớn do tính chất đa dạng và biến động không ngừng của lưu lượng mạng. Các phương pháp lập lịch truyền thống như Round-robin thường tỏ ra kém hiệu quả, không thể thích ứng kịp thời với các kịch bản thay đổi nhanh. Trong nghiên cứu này, chúng tôi ứng dụng học máy (Machine Learning) để dự đoán lưu lượng và tối ưu hóa tài nguyên. Mục tiêu trọng tâm là giảm thiểu tỷ lệ mất gói tin (packet loss) và duy trì sự ổn định ngay cả trong môi trường có mật độ kết nối đông đúc.
1	Vào năm 2018, tổ chức 3GPP đã đặt ra những định nghĩa chuẩn hóa về QoS cho hạ tầng 5G. Tiếp nối định hướng đó, các nghiên cứu của Li et al. (2019) đã khai thác học sâu (Deep Learning) để dự báo lưu lượng, trong khi Akyildiz et al. (2014) đề xuất các giải pháp lập lịch dựa trên mạng điều khiển bằng phần mềm (SDN). Tuy nhiên, hầu hết các tiếp cận hiện nay vẫn tách biệt giữa trí tuệ nhân tạo và hạ tầng điều khiển. Nghiên cứu của chúng tôi thực hiện một bước tiến mới bằng cách kết hợp thuật toán SVM với kiến trúc SDN để tạo ra một hệ thống điều phối linh hoạt và thông minh hơn.
1	Quy trình thực hiện bao gồm ba giai đoạn chính: Trước hết, hệ thống thu thập dữ liệu lưu lượng thời gian thực từ các trạm phát sóng. Tiếp theo, mô hình SVM được huấn luyện để dự đoán các đỉnh tải (traffic peaks) và phân loại ưu tiên dịch vụ. Cuối cùng, các quyết định phân bổ tài nguyên được thực thi thông qua bộ điều khiển (SDN Controller) để tối ưu hóa băng thông động.Toàn bộ quá trình thử nghiệm được triển khai trên simulator NS-3 với quy mô 100 nút mạng để mô phỏng chính xác các tương tác phức tạp trong môi trường thực tế.
1	Kết quả thực nghiệm cho thấy hệ thống đề xuất đạt được những cải thiện rõ rệt về mặt hiệu năng kỹ thuật so với phương pháp truyền thống. Cụ thể, độ trễ trung bình của mạng đã giảm đáng kể từ 50 ms xuống còn 37 ms, tương đương mức cải thiện khoảng 26%, qua đó nâng cao đáng kể chất lượng trải nghiệm người dùng đối với các ứng dụng thời gian thực như truyền thông đa phương tiện, thực tế ảo và các dịch vụ điều khiển từ xa. Bên cạnh đó, lưu lượng xử lý của hệ thống (throughput) ghi nhận mức tăng ấn tượng, đạt 1.2 Gbps, cho thấy khả năng khai thác tài nguyên mạng hiệu quả hơn khi áp dụng mô hình dự báo dựa trên học máy.
1	Mặc dù đạt được hiệu năng vượt trội, nghiên cứu vẫn tồn tại một số hạn chế nhất định cần được xem xét. Trước hết, chi phí tính toán của mô hình học máy có xu hướng tăng nhanh khi quy mô mạng mở rộng, đặc biệt trong các kịch bản mật độ người dùng cao hoặc lưu lượng biến động mạnh. Ngoài ra, độ trễ phản hồi của thuật toán có thể trở thành nút thắt cổ chai khi hệ thống phải xử lý lượng lớn dữ liệu theo thời gian thực tại trung tâm điều khiển. Do đó, trong các nghiên cứu tiếp theo, một hướng phát triển đầy tiềm năng là tích hợp kiến trúc tính toán biên (Edge Computing), cho phép xử lý và ra quyết định ngay tại các nút gần nguồn phát dữ liệu.
1	Nghiên cứu này đã chứng minh rằng việc kết hợp các kỹ thuật học máy với kiến trúc mạng định nghĩa bằng phần mềm (SDN) là một giải pháp hiệu quả và đầy triển vọng nhằm cải thiện chất lượng dịch vụ (QoS) trong mạng 5G. Thông qua khả năng dự báo và điều phối tài nguyên thông minh, hệ thống không chỉ giảm độ trễ, tăng thông lượng mà còn duy trì độ tin cậy cao trong quá trình truyền dữ liệu. Những kết quả đạt được cho thấy giải pháp đề xuất hoàn toàn có thể đáp ứng các yêu cầu khắt khe của hạ tầng viễn thông hiện đại.
1	Nghiên cứu này đề xuất một quy trình phân tích cảm xúc văn bản chuyên sâu dựa trên kiến trúc BERT (Bidirectional Encoder Representations from Transformers), được tinh chỉnh (fine-tuning) đặc biệt cho ngôn ngữ tiếng Việt. Bằng cách khai thác sức mạnh của việc học biểu diễn ngôn ngữ hai chiều, mô hình có khả năng hiểu sâu sắc ngữ cảnh và các sắc thái biểu cảm phức tạp trong câu. Kết quả thực nghiệm trên bộ dữ liệu thu thập từ Twitter (X) cho thấy mô hình đạt độ chính xác ấn tượng lên tới 92%. Đóng góp trọng tâm của nghiên cứu là quy trình tiền xử lý và thích ứng mô hình cho các ngôn ngữ có nguồn tài nguyên thấp (low-resource languages) như tiếng Việt.
1	Trong kỷ nguyên số, phân tích cảm xúc (Sentiment Analysis) đã trở thành một công cụ không thể thiếu trong lĩnh vực marketing, chăm sóc khách hàng và giám sát dư luận xã hội. Việc hiểu được thái độ của người tiêu dùng giúp doanh nghiệp điều chỉnh chiến lược kịp thời và phản ứng nhanh trước các cuộc khủng hoảng truyền thông. Mặc dù kiến trúc BERT của Devlin et al. (2019) đã thiết lập những tiêu chuẩn mới về hiệu suất trong xử lý ngôn ngữ tự nhiên, việc áp dụng nguyên bản vào tiếng Việt vẫn gặp nhiều khó khăn do sự khác biệt về cấu trúc từ vựng và ngữ pháp.
1	Lịch sử của phân tích cảm xúc đã trải qua nhiều giai đoạn, từ các phương pháp dựa trên từ điển (lexicon-based) như VADER của Hutto (2014) vốn gặp hạn chế khi xử lý các câu có cấu trúc đảo ngữ hoặc mỉa mai, đến các mô hình học sâu hiện đại. Sự ra đời của các mô hình ngôn ngữ tiền huấn luyện (pre-trained models) đã thay đổi hoàn toàn cục diện nghiên cứu. Trong bối cảnh Việt Nam, PhoBERT của Nguyen et al. (2020) là một bước tiến quan trọng khi sử dụng phương pháp RoBERTa để huấn luyện trên lượng dữ liệu tiếng Việt khổng lồ.
1	Quy trình thực hiện bắt đầu bằng việc thu thập và gán nhãn cho 10.000 mẫu văn bản tiếng Việt từ các nền tảng mạng xã hội. Dữ liệu sau đó trải qua các bước tiền xử lý nghiêm ngặt như chuẩn hóa bảng mã, tách từ (word segmentation) và loại bỏ các ký tự đặc biệt không mang thông tin cảm xúc. Chúng tôi thực hiện fine-tune mô hình BERT cơ sở bằng cách thêm một lớp phân loại (Linear layer) lên trên đầu ra của token [CLS]. Thuật toán tối ưu hóa AdamW được sử dụng với tốc độ học (learning rate) nhỏ để tránh hiện tượng mất các tri thức đã học trước đó.
1	Kết quả thực nghiệm khẳng định ưu thế vượt trội của mô hình đề xuất với độ chính xác đạt 92% và chỉ số F1-score duy trì ở mức 0.90. Khi đặt trong phép so sánh đối chiếu với các kiến trúc truyền thống như LSTM (Long Short-Term Memory) vốn chỉ đạt khoảng 85%, mô hình dựa trên BERT cho thấy khả năng nắm bắt ngữ cảnh dài và các quan hệ từ vựng phức tạp tốt hơn hẳn. Đặc biệt, trong các kịch bản có sự xuất hiện của các từ ngữ phủ định hoặc cấu trúc câu phức, mô hình vẫn duy trì được độ nhạy cao. Điều này chứng minh rằng việc sử dụng các biểu diễn ngôn ngữ được huấn luyện trước mang lại giá trị rất lớn cho các bài toán phân tích cảm xúc ở mức độ chi tiết.
1	Mặc dù đạt kết quả khả quan, nghiên cứu vẫn ghi nhận một số hạn chế liên quan đến sự thiên kiến (bias) của dữ liệu huấn luyện, khi các mẫu văn bản chủ yếu tập trung vào các chủ đề công nghệ và tiêu dùng. Trong tương lai, chúng tôi định hướng mở rộng mô hình sang hướng tiếp cận đa ngôn ngữ (multi-lingual) để có thể phân tích các văn bản chứa cả tiếng Việt và tiếng Anh vốn rất phổ biến hiện nay. Tổng kết lại, nghiên cứu đã xây dựng thành công một giải pháp phân tích cảm xúc hiệu quả, đóng góp một công cụ hữu ích cho cộng đồng nghiên cứu ngôn ngữ học máy tại Việt Nam và khẳng định tính khả thi của việc thích ứng các mô hình ngôn ngữ lớn cho mục đích thực tiễn.
1	Nghiên cứu này trình bày một giải pháp toàn diện sử dụng hệ sinh thái dữ liệu lớn, bao gồm Hadoop và Apache Spark, để phân tích khối lượng dữ liệu y tế khổng lồ nhằm mục tiêu dự đoán sớm các bệnh lý về tim mạch. Với khả năng xử lý song song và tính toán trên bộ nhớ (in-memory computing), hệ thống cho phép trích xuất các đặc trưng quan trọng từ các hồ sơ bệnh án điện tử một cách nhanh chóng. Kết quả thử nghiệm trên tập dữ liệu tiêu chuẩn từ Kaggle đã đạt được độ chính xác 88%.
1	Cuộc cách mạng dữ liệu lớn trong lĩnh vực y tế đang mở ra những cơ hội chưa từng có trong việc chẩn đoán sớm và cá nhân hóa lộ trình điều trị. Tuy nhiên, sự bùng nổ về khối lượng (Volume), tốc độ (Velocity) và sự đa dạng (Variety) của dữ liệu y tế cũng đặt ra những thách thức kỹ thuật không nhỏ. Các phương pháp lưu trữ và xử lý truyền thống thường bị quá tải khi phải đối mặt với hàng triệu bản ghi chứa các thông tin sinh hiệu phức tạp. Nghiên cứu này đặt ra mục tiêu vượt qua các rào cản đó bằng cách tận dụng sức mạnh của tính toán phân tán.
1	Nền tảng của xử lý dữ liệu lớn bắt đầu từ mô hình MapReduce của Dean et al. (2008), cho phép xử lý dữ liệu phân tán trên các cụm máy tính thông thường. Sau đó, sự ra đời của Apache Spark (Meng et al., 2016) với thư viện MLlib đã tạo nên một bước đột phá, cho phép thực hiện các thuật toán học máy với tốc độ nhanh hơn nhiều lần so với MapReduce nhờ cơ chế lưu trữ dữ liệu trung gian trên RAM. Trong lĩnh vực y tế, nhiều nghiên cứu trước đây đã cố gắng áp dụng các mô hình học máy đơn lẻ, nhưng thường gặp khó khăn trong việc mở rộng quy mô.
1	Pipeline của nghiên cứu được thiết kế qua ba giai đoạn chặt chẽ. Đầu tiên, dữ liệu thô được đưa vào quy trình ETL (Extract-Transform-Load) sử dụng Spark SQL để làm sạch, xử lý các giá trị thiếu và chuẩn hóa đơn vị đo lường. Tiếp theo, chúng tôi áp dụng kỹ thuật lựa chọn đặc trưng để giữ lại các chỉ số có ý nghĩa lâm sàng cao nhất như chỉ số huyết áp, cholesterol và tần số tim. Cuối cùng, một mô hình Random Forest được triển khai trên môi trường phân tán để thực hiện việc phân loại. Việc sử dụng thuật toán Random Forest giúp mô hình có khả năng chống nhiễu tốt và hạn chế tình trạng quá khớp, điều vốn rất quan trọng khi làm việc với dữ liệu y tế nhạy cảm và không đồng nhất.
1	Thực nghiệm cho thấy mô hình đạt được độ chính xác 88%, một con số rất khả quan trong các bài toán dự đoán y khoa. Đặc biệt, chỉ số Recall (độ nhạy) đạt 0.85, điều này có ý nghĩa cực kỳ quan trọng vì nó cho thấy khả năng phát hiện đúng các bệnh nhân thực sự có nguy cơ mắc bệnh, giảm thiểu tỷ lệ bỏ sót các ca bệnh nguy hiểm. Tốc độ xử lý của Spark giúp hệ thống có thể phân tích hàng triệu dòng dữ liệu chỉ trong vài phút, chứng minh tính hiệu quả vượt trội so với các mô hình chạy trên máy đơn lẻ. Các biểu đồ đánh giá cũng cho thấy sự ổn định của hệ thống khi quy mô dữ liệu tăng dần, khẳng định khả năng mở rộng linh hoạt của kiến trúc đề xuất.
1	Bên cạnh những thành công về mặt kỹ thuật, nghiên cứu cũng thảo luận về các vấn đề cấp thiết liên quan đến bảo mật dữ liệu và quyền riêng tư của bệnh nhân (Privacy issues) khi triển khai trên các cụm máy tính lớn. Việc áp dụng các kỹ thuật mã hóa và kiểm soát truy cập là bắt buộc để đảm bảo an toàn thông tin. Hướng nghiên cứu tương lai sẽ tập trung vào việc xây dựng hệ thống phân tích thời gian thực (Real-time analytics), cho phép cảnh báo tức thời cho bệnh nhân thông qua các thiết bị đeo thông minh. Kết luận lại, sự kết hợp giữa công nghệ Big Data và học máy đã chứng minh được giá trị thực tiễn to lớn trong y tế hiện đại, mang lại hy vọng về một hệ thống chăm sóc sức khỏe thông minh và hiệu quả hơn.
1	Nghiên cứu này đề xuất một phương pháp tiếp cận mới trong việc nhận diện hành vi bất thường dựa trên kiến trúc Vision Transformer (ViT), nhằm khắc phục những hạn chế về khả năng nắm bắt ngữ cảnh toàn cục của các mạng CNN truyền thống. Bằng cách chia nhỏ khung hình thành các mảng (patches) và áp dụng cơ chế chú ý (Self-Attention), mô hình có thể hiểu được mối quan hệ không gian và thời gian giữa các đối tượng trong một cảnh quay phức tạp. Kết quả thực nghiệm trên bộ dữ liệu UCF-Crime cho thấy độ chính xác vượt trội, đồng thời giảm thiểu tỷ lệ báo động giả trong các môi trường ánh sáng yếu.
1	Sự bùng nổ của các hệ thống camera an ninh tại các đô thị hiện đại đã tạo ra một lượng dữ liệu video khổng lồ, vượt quá khả năng giám sát thủ công của con người. Điều này đặt ra nhu cầu cấp thiết về các giải pháp thị giác máy tính tự động có khả năng phát hiện sớm các hành vi gây rối, tai nạn hoặc đột nhập trái phép. Tuy nhiên, các thách thức về sự thay đổi góc nhìn, mật độ đám đông và điều kiện ngoại cảnh khắc nghiệt vẫn là rào cản lớn đối với các thuật toán hiện nay. Nghiên cứu này hướng tới việc xây dựng một hệ thống nhận diện thông minh không chỉ dựa trên hình dáng đối tượng mà còn phân tích sự biến đổi của quỹ đạo chuyển động.
1	Trong những thập kỷ qua, lĩnh vực nhận diện hành vi đã chuyển dịch từ các phương pháp thủ công sang học sâu (Deep Learning) với các kiến trúc tiêu biểu như C3D và Two-Stream CNN. Mặc dù các mô hình này đạt được hiệu suất khá tốt, chúng thường gặp khó khăn trong việc duy trì tính nhất quán khi xử lý các chuỗi video dài do giới hạn về trường thu thụ (receptive field). Gần đây, sự xuất hiện của Transformer trong thị giác máy tính đã mở ra một kỷ nguyên mới, cho phép mô hình hóa các phụ thuộc xa một cách hiệu quả hơn.
1	Quy trình thực hiện bao gồm bốn giai đoạn chính: Thu thập dữ liệu, Tiền xử lý, Huấn luyện và Đánh giá. Đầu tiên, các đoạn video từ bộ dữ liệu được trích xuất thành các chuỗi khung hình và chuẩn hóa về kích thước 224x224. Tiếp theo, mô hình Vision Transformer được cấu hình với cơ chế chú ý đa đầu để trích xuất các đặc trưng không gian. Để xử lý yếu tố thời gian, chúng tôi áp dụng mạng LSTM (Long Short-Term Memory) phía sau encoder của Transformer nhằm liên kết thông tin giữa các khung hình kế tiếp. Quá trình huấn luyện sử dụng hàm mất mát Cross-Entropy kết hợp với bộ tối ưu hóa Adam, với tỷ lệ học (learning rate) được điều chỉnh theo chiến lược giảm dần để đảm bảo mô hình hội tụ ổn định và tránh tình trạng quá khớp trên các tập dữ liệu nhỏ.
1	Kết quả thực nghiệm cho thấy mô hình đề xuất đạt độ chính xác (Accuracy) lên tới 94,5% trên tập kiểm tra, cao hơn khoảng 5% so với kiến trúc ResNet-50 truyền thống. Đặc biệt, chỉ số AUC (Area Under Curve) đạt mức 0,92, chứng minh khả năng phân loại hiệu quả giữa hành vi bình thường và bất thường ngay cả trong các kịch bản có độ nhiễu cao. Thời gian phản hồi của hệ thống trung bình chỉ mất 30ms cho mỗi khung hình, đáp ứng tiêu chuẩn xử lý thời gian thực trên các GPU hiện đại. Biểu đồ ma trận nhầm lẫn (Confusion Matrix) chỉ ra rằng mô hình hoạt động đặc biệt tốt trong việc phát hiện các hành vi xâm nhập, tuy nhiên vẫn còn một số sai sót nhỏ đối với các hành động có tốc độ di chuyển quá nhanh hoặc bị che khuất một phần.
1	Mặc dù đạt được những con số ấn tượng, nghiên cứu vẫn ghi nhận hạn chế về khả năng xử lý trên các thiết bị nhúng có tài nguyên hạn chế do độ phức tạp tính toán của cơ chế Self-Attention. Trong tương lai, chúng tôi định hướng tối ưu hóa mô hình thông qua kỹ thuật nén mạng (Model Pruning) để có thể triển khai trực tiếp trên các camera thông minh. Tổng kết lại, việc ứng dụng Transformer vào thị giác máy tính đã chứng minh tính hiệu quả vượt trội trong việc hiểu ngữ cảnh phức tạp của hành vi con người. Nghiên cứu này đóng góp một giải pháp tiềm năng cho công nghệ thành phố thông minh, giúp nâng cao tính chủ động trong việc phòng chống tội phạm và quản lý trật tự xã hội một cách khoa học và minh bạch hơn.
1	Nghiên cứu này tập trung giải quyết bài toán dự báo rời bỏ dịch vụ (Churn Prediction) của khách hàng trong ngành viễn thông bằng cách sử dụng thuật toán Random Forest. Một thách thức phổ biến trong lĩnh vực này là hiện tượng mất cân bằng dữ liệu, khi số lượng khách hàng rời bỏ thường chiếm tỷ lệ rất nhỏ so với khách hàng ở lại. Để khắc phục, chúng tôi áp dụng kỹ thuật SMOTE (Synthetic Minority Over-sampling Technique) nhằm tạo ra các mẫu dữ liệu giả lập cho nhóm thiểu số. Kết quả cho thấy mô hình không chỉ đạt độ chính xác tổng thể cao mà còn cải thiện đáng kể chỉ số Recall cho nhóm khách hàng có nguy cơ rời bỏ.
1	Trong thị trường viễn thông cạnh tranh khốc liệt hiện nay, chi phí để thu hút một khách hàng mới thường cao gấp nhiều lần so với chi phí duy trì một khách hàng hiện tại. Do đó, việc xác định sớm những khách hàng có ý định ngừng sử dụng dịch vụ là yếu tố sống còn để doanh nghiệp duy trì lợi thế cạnh tranh. Các phương pháp thống kê truyền thống thường không thể nắm bắt được những thay đổi nhỏ trong hành vi của khách hàng từ lượng dữ liệu giao dịch khổng lồ. Học máy, với khả năng tự học từ dữ liệu lịch sử, mang lại một phương thức dự báo chính xác và linh hoạt hơn.
1	Nghiên cứu về dự báo rời bỏ dịch vụ đã trải qua nhiều giai đoạn phát triển, từ các mô hình hồi quy Logistic đơn giản đến các thuật toán phức tạp hơn như SVM và Gradient Boosting. Các công trình trước đây của Verbeke et al. đã chỉ ra rằng việc lựa chọn đặc trưng đóng vai trò quyết định đến hiệu suất mô hình. Tuy nhiên, vấn đề mất cân bằng dữ liệu (Class Imbalance) thường bị bỏ qua hoặc xử lý chưa triệt để, dẫn đến việc mô hình thiên kiến về phía khách hàng không rời bỏ. Nghiên cứu của chúng tôi kế thừa các thành tựu về lấy mẫu lại dữ liệu (Resampling) và khai thác sức mạnh của mô hình ensemble (tập hợp) như Random Forest để nâng cao tính bền vững (robustness) trước các nhiễu động dữ liệu.
1	Quy trình thực hiện bắt đầu bằng việc thu thập dữ liệu từ hồ sơ của 15.000 khách hàng, bao gồm các thông tin về cước phí hàng tháng, loại hợp đồng, thời gian gắn bó và tần suất sử dụng dịch vụ. Dữ liệu sau đó được tiền xử lý để loại bỏ các giá trị thiếu và chuẩn hóa các biến định tính. Chúng tôi áp dụng kỹ thuật SMOTE để cân bằng tỷ lệ giữa hai nhóm khách hàng rời bỏ và ở lại theo tỷ lệ 1:1. Mô hình Random Forest được huấn luyện với 500 cây quyết định, kết hợp với kỹ thuật kiểm chéo (K-fold cross-validation) để đảm bảo tính khách quan.
1	"Kết quả thực nghiệm cho thấy mô hình đạt độ chính xác (Accuracy) 91,2% và chỉ số F1-score đạt 0,89 trên tập kiểm tra độc lập. Đặc biệt, chỉ số Recall cho nhóm khách hàng rời bỏ (nhóm đích) đã tăng từ 65% (khi chưa dùng SMOTE) lên mức 87%, chứng minh tính hiệu quả vượt trội của kỹ thuật cân bằng dữ liệu. Phân tích tầm quan trọng của các đặc trưng chỉ ra rằng ""loại hợp đồng"" và ""tổng cước phí"" là hai yếu tố có ảnh hưởng mạnh nhất đến quyết định rời bỏ của khách hàng. Biểu đồ đường cong ROC (Receiver Operating Characteristic) với diện tích dưới đường cong (AUC) đạt 0,94 cho thấy mô hình có khả năng phân biệt cực kỳ tốt, hỗ trợ đắc lực cho việc ra quyết định của các cấp quản lý trong thực tế."
1	Mặc dù đạt được hiệu suất cao, nghiên cứu vẫn cần được mở rộng để xem xét các yếu tố định tính như phản hồi trực tiếp của khách hàng qua tổng đài hoặc các tương tác trên mạng xã hội. Việc tích hợp các dữ liệu phi cấu trúc này có thể giúp mô hình hiểu sâu hơn về tâm lý khách hàng. Tổng kết lại, nghiên cứu đã khẳng định rằng sự kết hợp giữa kỹ thuật tiền xử lý dữ liệu thông minh và thuật toán học máy mạnh mẽ là chìa khóa để giải quyết bài toán dự báo kinh doanh. Hướng nghiên cứu tương lai sẽ tập trung vào việc triển khai mô hình theo dạng học trực tuyến (Online Learning) để cập nhật liên tục theo sự thay đổi của thị trường, giúp doanh nghiệp luôn chủ động trong chiến lược chăm sóc khách hàng.
1	Mạng cảm biến không dây (WSN) đóng vai trò nền tảng trong hạ tầng Internet vạn vật (IoT), tuy nhiên chúng thường xuyên đối mặt với các nguy cơ bảo mật như tấn công hố đen (blackhole) hoặc tấn công giả mạo. Nghiên cứu này đề xuất một giao thức định tuyến dựa trên mức độ tin cậy (Trust-based Routing) để nâng cao tính an toàn và hiệu quả truyền tải dữ liệu. Bằng cách gán chỉ số tin cậy cho mỗi nút mạng dựa trên lịch sử chuyển tiếp gói tin, hệ thống có thể tự động loại bỏ các nút có hành vi bất thường khỏi đường truyền chính. Thử nghiệm trên nền tảng Cooja cho thấy giao thức này giúp tăng tỷ lệ chuyển tiếp gói tin thành công lên 20% và giảm tiêu thụ năng lượng khoảng 15%.
1	Trong các ứng dụng IoT hiện đại như nông nghiệp thông minh hay giám sát môi trường, các nút cảm biến thường được triển khai ở những khu vực xa xôi và vận hành bằng năng lượng pin hạn chế. Do đặc thù môi trường mở, các mạng này rất dễ bị xâm nhập bởi các nút độc hại nhằm đánh cắp thông tin hoặc làm gián đoạn luồng dữ liệu. Các phương pháp bảo mật truyền thống sử dụng mã hóa phức tạp thường không phù hợp vì tiêu tốn quá nhiều tài nguyên tính toán và năng lượng. Do đó, việc phát triển các cơ chế định tuyến nhẹ nhàng nhưng vẫn đảm bảo tính an toàn là một thách thức lớn.
1	Các nghiên cứu về định tuyến trong mạng WSN đã chuyển dịch từ việc tối ưu hóa đường dẫn ngắn nhất (như giao thức AODV hay DSR) sang việc tích hợp các yếu tố về năng lượng và bảo mật. Các công trình của Karlof et al. đã chỉ ra nhiều lỗ hổng nghiêm trọng trong các giao thức định tuyến truyền thống. Gần đây, các mô hình quản lý tin cậy (Trust Management) đã bắt đầu được áp dụng, trong đó các nút mạng tự giám sát lẫn nhau để đưa ra điểm số uy tín. Tuy nhiên, hầu hết các nghiên cứu hiện tại vẫn gặp khó khăn trong việc cân bằng giữa độ chính xác của việc đánh giá tin cậy và chi phí quản lý (overhead).
1	Giao thức đề xuất được thiết lập dựa trên ba thành phần chính: Cơ chế giám sát, Mô hình tính toán tin cậy và Thuật toán chọn đường. Trong giai đoạn giám sát, mỗi nút sẽ lắng nghe các hoạt động của hàng xóm để kiểm tra xem gói tin có được chuyển tiếp đúng kế hoạch hay không. Mức độ tin cậy được cập nhật định kỳ bằng công thức toán học kết hợp giữa tin cậy trực tiếp (tự quan sát) và tin cậy gián tiếp (phản hồi từ các nút khác). Khi một nút cần gửi dữ liệu, nó sẽ ưu tiên chọn các nút trung gian có điểm tin cậy cao nhất thay vì chỉ dựa vào khoảng cách vật lý.
1	Kết quả mô phỏng cho thấy sự cải thiện rõ rệt về hiệu năng mạng khi áp dụng giao thức định tuyến tin cậy. Tỷ lệ chuyển tiếp gói tin (Packet Delivery Ratio - PDR) duy trì ở mức trên 92% ngay cả khi có sự hiện diện của các nút độc hại, trong khi giao thức AODV thông thường bị giảm xuống dưới 70%. Độ trễ trung bình của mạng cũng được kiểm soát ổn định do tránh được việc truyền lại gói tin nhiều lần. Đặc biệt, phân tích về mức tiêu thụ năng lượng cho thấy giao thức đề xuất giúp kéo dài tuổi thọ mạng thêm 200 giờ hoạt động nhờ giảm thiểu các hoạt động chuyển tiếp vô ích qua các nút không tin cậy.
1	Nghiên cứu đã khẳng định rằng việc tích hợp yếu tố tin cậy vào quy trình định tuyến là một hướng đi đúng đắn cho các mạng WSN hiện đại. Mặc dù giao thức hoạt động tốt, thách thức lớn nhất vẫn là việc đối phó với các cuộc tấn công có tính toán cao, nơi các nút độc hại có thể giả vờ hành xử tốt để tích lũy điểm tin cậy trước khi tấn công. Trong các giai đoạn tiếp theo, chúng tôi dự kiến ứng dụng lý thuyết trò chơi (Game Theory) để mô hình hóa các chiến lược tấn công phức tạp hơn và tăng cường khả năng dự báo của hệ thống.
1	Nghiên cứu này đề xuất một kiến trúc dịch máy mới nhằm nâng cao chất lượng chuyển đổi ngôn ngữ cho các cặp ngôn ngữ thiếu hụt dữ liệu huấn luyện (low-resource language pairs). Thay vì chỉ dựa vào phương pháp học có giám sát truyền thống, chúng tôi tích hợp cơ chế Học tăng cường (Reinforcement Learning) với hàm phần thưởng dựa trên chỉ số ngữ nghĩa và độ trôi chảy của câu dịch. Mô hình sử dụng mạng Transformer làm cốt lõi, kết hợp với kỹ thuật Back-translation để tự động tạo ra các cặp câu giả lập từ dữ liệu đơn ngữ. Kết quả thực nghiệm cho thấy điểm số BLEU tăng đáng kể so với các mô hình nền tảng.
1	Trong bối cảnh toàn cầu hóa, dịch máy đóng vai trò cầu nối quan trọng trong việc xóa bỏ rào cản ngôn ngữ. Tuy nhiên, trong khi các cặp ngôn ngữ phổ biến như Anh-Pháp hay Anh-Trung đã đạt được độ chính xác rất cao, các ngôn ngữ ít phổ biến hơn vẫn gặp khó khăn do thiếu các bộ ngữ liệu song ngữ chất lượng cao. Việc thiếu dữ liệu dẫn đến mô hình thường xuyên tạo ra các câu dịch rời rạc, sai ngữ pháp hoặc mất đi sắc thái văn hóa đặc trưng. Nghiên cứu này đặt mục tiêu giải quyết vấn đề đó bằng cách cho phép mô hình tự học thông qua quá trình thử và sai trong môi trường học tăng cường.
1	"Các nghiên cứu về dịch máy đã tiến hóa mạnh mẽ từ phương pháp dựa trên luật (Rule-based) sang dịch máy thống kê (SMT) và gần đây là dịch máy mạng nơ-ron (NMT). Sự ra đời của kiến trúc Transformer bởi Vaswani et al. đã tạo nên một cuộc cách mạng về hiệu suất nhờ cơ chế chú ý (Attention). Tuy nhiên, các mô hình này cực kỳ ""đói"" dữ liệu và thường hoạt động kém khi tập huấn luyện nhỏ. Các công trình của Sennrich et al. về Back-translation đã mở ra hướng đi mới bằng cách tận dụng dữ liệu đơn ngữ. Nghiên cứu của chúng tôi tiếp tục phát triển hướng đi này bằng cách áp dụng thuật toán Policy Gradient để tinh chỉnh quá trình giải mã (decoding)."
1	Quy trình thực hiện bao gồm ba bước chính: Xây dựng bộ ngữ liệu, Huấn luyện sơ bộ và Tinh chỉnh bằng học tăng cường. Đầu tiên, chúng tôi thu thập dữ liệu từ các nguồn tài liệu mở và thực hiện tiền xử lý bằng kỹ thuật BPE (Byte Pair Encoding) để xử lý các từ hiếm. Sau đó, mô hình Transformer được huấn luyện sơ bộ bằng phương pháp Maximum Likelihood Estimation (MLE) để nắm bắt cấu trúc ngôn ngữ cơ bản. Ở giai đoạn then chốt, chúng tôi sử dụng mô hình này như một tác tử (agent) trong môi trường học tăng cường. Hàm phần thưởng được thiết kế là sự kết hợp giữa điểm BLEU truyền thống và điểm tương đồng vector (Cosine similarity) từ các mô hình nhúng ngôn ngữ lớn.
1	Kết quả thực nghiệm trên tập dữ liệu thử nghiệm cho thấy mô hình đề xuất đạt được mức tăng 4.5 điểm BLEU so với mô hình Transformer tiêu chuẩn. Các đánh giá định tính cũng chỉ ra rằng độ trôi chảy (fluency) và tính nhất quán về mặt thuật ngữ của các câu dịch được cải thiện rõ rệt. Đặc biệt, hệ thống thể hiện khả năng xử lý tốt hơn đối với các câu dài phức tạp, vốn là điểm yếu cố hữu của các phương pháp cũ. Phân tích lỗi cho thấy tỷ lệ dịch sai ngữ nghĩa đã giảm 18%, đồng thời khả năng xử lý các thành ngữ đặc thù cũng chính xác hơn. Các biểu đồ học tập minh chứng rằng việc tích hợp học tăng cường giúp mô hình ổn định nhanh hơn và đạt được mức trần hiệu suất cao hơn trong môi trường dữ liệu nghèo nàn.
1	"Mặc dù đạt được những kết quả khả quan, việc huấn luyện mô hình học tăng cường đòi hỏi tài nguyên tính toán rất lớn và thời gian huấn luyện kéo dài. Một hạn chế khác là hàm phần thưởng đôi khi vẫn có thể bị ""đánh lừa"" bởi các câu có cấu trúc tốt nhưng sai lệch nhỏ về ý nghĩa logic. Trong tương lai, chúng tôi định hướng tích hợp thêm các mô hình ngôn ngữ lớn (LLMs) làm bộ đánh giá phần thưởng để nâng cao tính chính xác của quá trình phản hồi. Tổng kết lại, nghiên cứu đã chứng minh rằng học tăng cường là một công cụ mạnh mẽ để vượt qua giới hạn về dữ liệu trong dịch máy."
1	Tấn công từ chối dịch vụ phân tán (DDoS) vẫn là một trong những mối đe dọa nghiêm trọng nhất đối với tính khả dụng của các dịch vụ điện toán đám mây. Nghiên cứu này đề xuất một giải pháp phát hiện xâm nhập thông minh bằng cách chuyển đổi lưu lượng mạng thành các biểu diễn hình ảnh và sử dụng Mạng nơ-ron tích chập (CNN) để nhận diện mẫu tấn công. Bằng cách quan sát cấu trúc không gian của các gói tin trong một khoảng thời gian ngắn, mô hình có khả năng phân biệt chính xác giữa lưu lượng truy cập đột biến hợp lệ (Flash events) và lưu lượng tấn công DDoS phức tạp. Kết quả thử nghiệm trên bộ dữ liệu CICDDoS2019 cho thấy độ chính xác đạt 98,7% với thời gian phát hiện gần như tức thời.
1	Điện toán đám mây đã trở thành hạ tầng cốt lõi cho hầu hết các hoạt động kinh tế xã hội, tuy nhiên tính tập trung của nó cũng biến đây thành mục tiêu hàng đầu của các cuộc tấn công mạng. Tấn công DDoS với cường độ cao có thể làm tê liệt toàn bộ hệ thống, gây thiệt hại kinh tế khổng lồ và làm gián đoạn các dịch vụ thiết yếu. Các phương pháp phát hiện truyền thống dựa trên ngưỡng (threshold-based) thường gặp khó khăn trước các kỹ thuật tấn công hiện đại có cường độ thấp hoặc giả mạo lưu lượng người dùng thật. Vì vậy, nhu cầu về một hệ thống phát hiện dựa trên trí tuệ nhân tạo có khả năng học hỏi và tự thích nghi là cực kỳ cấp thiết.
1	Lịch sử phòng chống DDoS đã chứng kiến sự phát triển từ các thiết bị tường lửa cứng đến các giải pháp phần mềm thông minh. Các nghiên cứu ban đầu của Moore et al. đã đặt nền móng cho việc phân tích lưu lượng dựa trên thống kê. Sau đó, các thuật toán học máy như SVM và Random Forest được đưa vào sử dụng và mang lại hiệu quả nhất định. Tuy nhiên, các mô hình này đòi hỏi quy trình trích xuất đặc trưng (feature engineering) rất phức tạp và tốn thời gian. Gần đây, xu hướng sử dụng học sâu (Deep Learning) đã mở ra tiềm năng tự động hóa hoàn toàn quy trình này.
1	Quy trình nghiên cứu bao gồm: Thu thập dữ liệu, Chuyển đổi dữ liệu thành hình ảnh và Huấn luyện mô hình. Đầu tiên, chúng tôi sử dụng bộ dữ liệu lưu lượng mạng chứa các cuộc tấn công DDoS đa dạng như UDP flood, SYN flood và DNS amplification. Mỗi bản ghi lưu lượng mạng gồm các thuộc tính như địa chỉ IP, cổng, giao thức và kích thước gói tin sẽ được chuẩn hóa. Sau đó, chúng tôi áp dụng kỹ thuật ánh xạ để chuyển đổi các khối dữ liệu này thành các hình ảnh xám (grayscale images) kích thước 32x32. Hình ảnh này sau đó được đưa vào mạng CNN với nhiều lớp tích chập và lớp gộp (pooling layers) để trích xuất các đặc trưng phân loại.
1	Mô hình CNN đề xuất đã đạt được kết quả ấn tượng với độ chính xác tổng thể là 98,7% và chỉ số F1-score đạt 0,98. Quan trọng hơn, tỷ lệ dương tính giả (False Positive Rate) được duy trì ở mức cực thấp, chỉ 0,2%, điều này rất quan trọng trong môi trường đám mây để tránh việc ngăn chặn nhầm các khách hàng hợp lệ. Khi so sánh với các thuật toán học máy truyền thống, CNN cho thấy tốc độ xử lý nhanh hơn 30% nhờ khả năng tận dụng phần cứng GPU để tính toán song song. Biểu đồ Confusion Matrix cho thấy mô hình nhận diện gần như tuyệt đối đối với các cuộc tấn công SYN flood, trong khi các cuộc tấn công lớp ứng dụng (Layer 7) cũng đạt độ chính xác trên 95%.
1	"Mặc dù mô hình đạt hiệu suất cao, thách thức lớn nhất vẫn là việc đối phó với các cuộc tấn công ""Zero-day"" chưa từng xuất hiện trong tập huấn luyện. Ngoài ra, việc lưu trữ và chuyển đổi dữ liệu thành hình ảnh trong thời gian thực đòi hỏi một hạ tầng lưu trữ đệm mạnh mẽ. Trong tương lai, chúng tôi dự kiến kết hợp CNN với mạng RNN để tận dụng thêm các đặc trưng thời gian dài hạn, giúp tăng cường khả năng phát hiện các cuộc tấn công kéo dài và âm thầm. Tổng kết lại, nghiên cứu đã xây dựng thành công một lá chắn an ninh mạng thông minh cho môi trường điện toán đám mây."
0	Nghiên cứu giới thiệu phương pháp dự báo độ rỗng bằng phương pháp truyền thống và sử dụng mạng neuron nhân tạo (Artificial Neural Network - ANN). Phương pháp nội suy truyền thống Kriging sẽ được áp dụng để tìm ra mối quan hệ trong không gian của thông số độ rỗng thông qua các mô hình 2D. Nghiên cứu cũng ứng dụng công cụ “nnstart” của phần mềm Matlab thông qua các lý thuyết về ANN và áp dụng vào việc dự báo độ rỗng cho giếng nghiên cứu. Kết quả cho thấy phương pháp sử dụng ANN đã giúp tối ưu công tác dự báo độ rỗng cho một giếng khoan từ tài liệu địa cơ học cho trước.
0	Độ rỗng là thông số quan trọng trong việc mô hình hóa đặc trưng thành hệ, có ảnh hưởng lớn đến tính toán trữ lượng và quyết định sự phát triển của một mỏ dầu hoặc khí. Mục đích của nghiên cứu này là sử dụng phương pháp truyền thống địa thống kê Kriging trong các nghiên cứu thông số độ rỗng đồng thời so sánh với các kết quả tính toán sử dụng ANN. Variogram là công cụ để định lượng tính ổn định/liên tục hoặc sự tương quan không gian của đối tượng nghiên cứu bằng cách nghiên cứu các giá trị bình phương trung bình của hiệu giữa 2 giá trị cách nhau một khoảng cách “h” theo một hướng xác định.
0	Kriging là nhóm phương pháp địa thống kê dùng để nội suy số liệu của một trường ngẫu nhiên tại một điểm chưa biết giá trị (không lấy được mẫu phân tích) từ những giá trị đã biết ở các điểm lân cận. Tính chất của Kriging là chất lượng của mẫu tốt thì giá trị xác định sẽ tốt. Kriging nội suy dựa trên quy luật BLUE - Best Linear Unbiased Estimator. ANN ra đời xuất phát từ ý tưởng mô phỏng bộ não con người. Giống như con người, ANN được học bởi kinh nghiệm, lưu những kinh nghiệm đó và sử dụng trong tình huống phù hợp (Hình 1, 2).
0	Học có giám sát (supervised learning) Học có giám sát là nhóm thuật toán dự đoán đầu ra (output) của dữ liệu mới (new input) dựa trên các cặp dữ liệu đã biết trước. Cặp dữ liệu này còn được gọi là dữ liệu - nhãn (data - label). Đây là nhóm phổ biến nhất trong các thuật toán học máy. Theo toán học, học có giám sát là khi có một tập hợp “n” biến đầu vào X = {x1, x2, ..., xn} và một tập hợp “n” nhãn tương ứng Y = {y1, y2, ..., yn}. Các cặp dữ liệu biết trước (xi, yi) được gọi là tập dữ liệu huấn luyện (training data). Từ tập huấn luyện này cần tạo ra một hàm số ánh xạ mỗi phần tử từ tập X sang một phần tử (xấp xỉ) tương ứng của tập Y.
0	Nhóm thuật toán học có giám sát gồm các bài toán chính sau: Phân loại (classification): Các nhãn của dữ liệu đầu vào được chia thành các nhóm hữu hạn. Hồi quy (regression): Nhãn là một giá trị thực cụ thể. Ở nghiên cứu này, nhóm tác giả đã áp dụng bài toán hồi quy để dự báo phân bố độ rỗng của vỉa. - Học không giám sát (unsupervised learning) Trong thuật toán này không biết trước được đầu ra hay nhãn của tập dữ liệu đầu vào, chỉ dựa vào cấu trúc của dữ liệu để thực hiện công việc như: phân nhóm (clustering) hoặc giảm số chiều của dữ liệu (dimension reduction) để thuận tiện trong việc lưu trữ và tính toán.
0	Học không giám sát là khi chỉ có dữ liệu đầu vào X mà không biết nhãn Y tương ứng. Học bán giám sát (semi-supervised learning): Các bài toán khi có một lượng lớn dữ liệu X nhưng chỉ có một phần được gán nhãn được gọi là học bán giám sát. Những bài toán thuộc nhóm này nằm giữa 2 nhóm trên. Học củng cố (reinforcement learning): Học củng cố giúp hệ thống tự động xác định hành vi dựa trên hoàn cảnh để đạt được lợi ích cao nhất (maximising the performance). Gradient descent (GD) là một thuật toán tối ưu dùng để tìm cực tiểu của hàm số. Thuật toán sẽ khởi tạo một điểm ngẫu nhiên trên hàm số và sau đó điểm này sẽ được di chuyển theo chiều giảm của đạo hàm cho đến khi đạt đến điểm cực tiểu.
0	Tốc độ học tập là tham số quan trọng (hyper parameter), được dùng để kiểm soát số lượng vòng lặp trong quá trình Gradient descent. Khi tham số này nhỏ, thuật toán sẽ cần nhiều bước lặp để hàm số có thể đạt tới điểm cực tiểu. Ngược lại, nếu tham số này lớn, thuật toán sẽ cần ít vòng lặp hơn, tuy nhiên khi đó, có thể hàm số sẽ bỏ qua điểm cực tiểu và không thể hội tụ được (Hình 3). Hàm kích hoạt hay còn gọi là hàm truyền có chức năng chuyển đổi thông số đầu vào sang một khoảng giá trị khác. Mạng cần có các hàm truyền để quyết định có nên truyền tiếp dữ liệu hay không và truyền với cường độ bao nhiêu.
0	Nhóm tác giả giới thiệu và sử dụng mạng neuron thông thường (Regular Neural Network) để thực hiện vì phương thức tính toán đơn giản, dễ tiếp cận. Một ANN thường tổ chức các neuron thành từng lớp và mỗi lớp chịu trách nhiệm cho một công việc cụ thể. ANN thường có 3 lớp: lớp nhập hay lớp đầu vào, lớp ẩn và lớp xuất. Lớp nhập (input layer) cung cấp cho mạng các số liệu cần thiết. Số lượng neuron trong lớp nhập tương ứng với số lượng thông số đầu vào được cung cấp cho mạng và các thông số đầu vào này được giả thiết ở dạng vector.
0	Lớp ẩn (hidden layer) chứa các neuron ẩn giúp kết nối giá trị đầu vào đến giá trị đầu ra. Một mạng neuron có thể có một hoặc nhiều lớp ẩn chịu trách nhiệm chính cho việc xử lý các neuron của lớp nhập và đưa các thông tin đến neuron của lớp xuất. Các neuron này thích ứng với việc phân loại và nhận diện mối liên hệ giữa thông số đầu vào và thông số đầu ra. Lớp xuất (output layer) chứa các neuron đầu ra nhằm chuyển thông tin đầu ra của các tính toán từ ANN đến người dùng. Một ANN có thể được xây dựng để có nhiều thông số đầu ra. Số neuron của lớp nhập và lớp xuất sẽ do bài toán quyết định, số neuron lớp ẩn và số lớp ẩn sẽ do người nhập quyết định.
0	Tuy nhiên, việc chọn loại và số lượng của thông số đầu vào có ảnh hưởng lớp đến chất lượng của mạng. Bản chất nguyên lý hoạt động của ANN truyền thẳng chính là quá trình huấn luyện mạng (training). Cụ thể, quá trình huấn luyện thường sử dụng giải thuật lan truyền ngược để tìm đạo hàm cho từng tham số trong mạng [1, 5, 9]. Giai đoạn lan truyền thẳng [9]: Bước 1: Vector thông số đầu vào được nhập vào các neuron ở lớp nhập. a(0) = x Bước 2: Tại neuron lớp ẩn thứ j, giá trị tín hiệu nhận từ lớp nhập sẽ được tính tổng trọng số hóa của tất cả các dữ liệu được nhập bằng cách cộng tất cả tích của mỗi dữ liệu đầu vào và trọng số liên kết giữa lớp ẩn và lớp nhập.
0	Bước 3: Sau đó, hàm kích hoạt (hàm truyền) sẽ được sử dụng để chuyển giá trị được nhận thành giá trị đầu ra. Tiếp theo, giá trị đầu ra tại neuron lớp ẩn j tiếp tục được truyền đến neuron lớp xuất k giống với phương thức từ lớp nhập đến lớp ẩn. Sau đó, hàm truyền lại được sử dụng để tính giá trị đầu ra của neuron tại lớp xuất. Lúc này, giai đoạn lan truyền thẳng đến đây kết thúc, mạng sẽ chuyển đến giai đoạn lan truyền ngược. Bước 4: Trong giai đoạn nhập, số liệu nhập gồm cả số liệu đầu vào và giá trị thực tế. Từ đó, với mỗi bộ số liệu tính được từng sai số đầu ra tương ứng, giá trị này được gọi là hàm mất mát (Cost Function - J)
0	Bước 5: Từ hàm cost function vừa tìm được, tính đạo hàm của hàm này theo trọng số giữa lớp ẩn - lớp ra và trọng số giữa lớp nhập - lớp ẩn. Bước 6: Kế tiếp, giá trị trọng số liên kết giữa lớp ẩn và lớp xuất cũng như giá trị trọng số liên kết giữa lớp nhập và lớp ẩn được hiệu chỉnh lại đồng thời. Overfitting là hiện tượng mô hình tìm được quá khớp với dữ liệu huấn luyện. Việc này sẽ gây ra hậu quả lớn nếu trong tập dữ liệu huấn luyện có nhiễu. Khi đó, mô hình không thực sự mô tả tốt dữ liệu ngoài tập huấn luyện. Overfitting đặc biệt xảy ra khi lượng dữ liệu huấn luyện quá nhỏ hoặc độ phức tạp của mô hình quá cao.
0	Một mô hình được coi là tốt (fit) nếu cả training error và test error đều thấp. Nếu training error thấp nhưng test error cao, mô hình bị overfitting. Nếu training error cao và test error cao, mô hình bị underfitting, còn đối với việc training error cao - test error thấp thì xác suất xảy ra rất nhỏ. Để có được mô hình tốt, cần tránh hiện tượng overfitting thông qua kỹ thuật sau: - Validation là kỹ thuật lấy từ tập huấn luyện (training data set) ra một tập con nhỏ và thực hiện việc đánh giá mô hình trên tập con này. Tập con này được gọi là tập validation và tập huấn luyện mới của mô hình là phần còn lại của tập huấn luyện ban đầu.
0	Regularisation là kỹ thuật làm thay đổi mô hình một ít, giảm độ phức tạp của mô hình, từ đó tránh được hiện tượng overfitting. Ở bài báo này, nhóm tác giả sẽ kiểm tra hiện tượng overfitting qua kỹ thuật validation. Các số liệu độ rỗng theo quỹ đạo thu thập từ 3 giếng X1, X5, X9. Trong nghiên cứu này số liệu của 3 giếng offset chọn cùng trong một đối tượng để đảm bảo bộ số liệu ổn định dừng bậc 2 phục vụ cho tính toán địa thống kê. Các dữ liệu khác như bản đồ tướng và thuộc tính địa chấn không nằm trong phạm vi của nghiên cứu này. Các số liệu lựa chọn đã được kiểm tra từ các nghiên cứu tài liệu địa vật lý.
0	Từ các thông số trong Bảng 1, nhập vào Gs+ và thu được quỹ đạo 2D của 3 giếng theo từng phương riêng biệt Tây - Đông (Hình 9). Biểu đồ Variogram theo độ rỗng trên được xây dựng theo mô hình hàm mũ (Exponential) và có hệ số tương quan là 0,772. Sau khi tìm được mô hình Variogram cho thông số độ rỗng, nhóm tác giả tiến hành bước kiểm tra chéo (Cross-validate) để đánh giá độ chính xác trước khi đưa vào tính toán. Để có được mô hình Variogram tốt nhất, có thể loại bỏ hoặc chỉnh sửa các giá trị ngoại lai (do sai số trong quá trình đo đạc). Với mô hình Variogram trên, nhóm tác giả đã tìm được hệ số hồi quy cho mô hình (Hình 10).
0	Kết quả cho ra tốt (0,968), do đó mô hình trên sẽ được dùng để nội suy độ rỗng. Tiến hành nội suy độ rỗng theo quỹ đạo giếng X11 từ thông số độ rỗng của các giếng lân cận trên thông qua mô hình xây dựng được. Hình 12 cho thấy độ chính xác của thuật toán Kriging tương đối chính xác, với hệ số hồi quy của dữ liệu dự báo là trên 60%. Nguyên nhân dẫn đến hệ số hồi quy này chỉ đạt 60% là do chỉ áp dụng thuật toán Kriging 1 thông số. Nếu muốn tăng độ chính xác của thuật toán lên có thể áp dụng Cokriging, từ đó có thông số phụ hỗ trợ cho việc dự đoán thông số chính.
0	Từ các thông số đầu vào như độ bền nén đơn trục (UCS), áp suất lỗ rỗng (Pore Pressure) và độ sâu theo phương thẳng đứng (TVD), mong muốn dự báo được số liệu đầu ra là độ rỗng tương ứng với các thành phần trên. Các thông số đầu vào này phải có tính ổn định, tính phổ biến và có liên quan mật thiết đến thông số đầu ra. Ở bộ số liệu thu thập được từ 3 giếng trong khu vực, nhóm tác giả đã chia thành 2 bộ con chính: - Bộ 1 (Dữ liệu huấn luyện - Training Data) bao gồm 266 giá trị bộ mẫu đầu vào (UCS, Pore Pressure, độ sâu) và 266 giá trị độ rỗng tương ứng.
0	Từ đây, công cụ xây dựng ANN của Matlab sẽ tiếp tục chia thành 3 nhóm nhỏ: Luyện mạng (training) chiếm 70% bộ số liệu ứng với 186 bộ mẫu đầu vào. Các giá trị này được sử dụng liên tục trong quá trình luyện mạng và mạng neuron sẽ được tinh chỉnh dựa trên sai số mạng. Kiểm tra chéo (validation) chiếm 15% bộ số liệu ứng với 40 bộ mẫu đầu vào. Chúng dùng để kiểm tra mạng có xảy ra hiện tượng quá khớp hay là không. Kiểm tra mạng (testing) chiếm 15% bộ số liệu ứng với 40 bộ mẫu đầu vào, kiểm tra mức độ hiệu quả của mạng trong và sau khi luyện mạng.
0	Bộ 2 (Dữ liệu kiểm tra mạng - Testing data) gồm 30 giá trị bộ mẫu đầu vào và 30 giá trị độ rỗng tương ứng, xác định độ tin cậy của mạng vừa mới huấn luyện để từ đó có thể dự đoán cho giếng lân cận. Ngoài ra, để dự báo độ rỗng cho giếng X11, nhóm tác giả đã chuẩn bị 1 bộ số liệu của giếng X11 gồm độ sâu theo phương thẳng đứng, độ bền nén đơn trục, áp suất ỗ rỗng. Thông qua mạng neuron vừa được huấn luyện, nhóm tác giả sẽ suy được độ rỗng tương ứng.
0	Với yêu cầu bài toán là dự đoán độ rỗng từ tài liệu địa cơ học, vì thế lớp đầu vào chứa các giá trị địa cơ học thu thập được và lớp đầu ra chứa giá trị độ rỗng từ mạng. Sau đó, tiến hành quá trình xây dựng mạng hay chọn cấu trúc mạng thông qua việc chọn số lớp và số neuron ẩn trong từng lớp cho mạng. Thông thường, việc thiết kế ANN sẽ bắt đầu với một lớp ẩn. Số neuron trong lớp ẩn đó sẽ được điều chỉnh tăng dần cho đến khi đạt được kết quả sai số đầu ra của mạng và giá trị đầu ra mong muốn là chấp nhận được.
0	Nếu số neuron quá lớn (hơn 50) mà sai số vẫn chưa chấp nhận được thì tăng lớp ẩn thành 2. Quá trình này được lặp đi lặp lại cho đến khi đạt được sai số và đầu ra mong muốn. Trong bài báo này, nhóm tác giả sẽ xây dựng mạng với số lớp ẩn là 1 và số neuron của lớp ẩn này lần lượt là 10, 20. Với cấu trúc mạng được chọn ở bước 2, tiếp tục tiến hành bước huấn luyện mạng. Thực chất, quá trình huấn luyện mạng chính là quá trình điều chỉnh trọng số liên kết (weights). Các giá trị trọng số liên kết này sẽ được mặc định ngẫu nhiên khi bắt đầu xây dựng mạng, sau đó, trong suốt quá trình luyện mạng, các thuật toán của mạng sẽ điều chỉnh các giá trị trên.
0	Kết quả quá trình luyện mạng sẽ hiển thị sai số toàn phương trung bình (MSE) và hệ số tương quan (R) của 3 bộ số liệu nhỏ được chia từ bộ 1, đó là sai số bộ số liệu luyện mạng, sai số kiểm tra chéo và sai số kiểm tra mạng. Để tránh hiện tượng quá khớp và đánh giá mạng được luyện ở bước 3 phải kiểm tra độ chính xác của mạng. Thực tế, mạng sau khi được huấn luyện sẽ sử dụng phần số liệu kiểm tra mạng để kiểm tra mức độ hiệu quả của mạng. Tiếp đến, mạng sẽ sử dụng dữ liệu trung gian hay số liệu kiểm tra chéo để tính toán sai số nhằm đảm bảo hiện tượng quá khớp không xảy ra.
0	Việc quan sát đồ thị thể hiện (Performance) qua xây dựng mạng được dùng để đánh giá mạng và xem xét hiện tượng quá khớp. Các giá trị MSE và R ở bước 3 sẽ hiển thị tại vị trí vòng lặp (Epoch) cho hiệu quả tốt nhất của quá trình xây dựng mạng. Nếu độ tin cậy của mạng sau khi kiểm tra không đạt kết quả mong muốn, sẽ thực hiện một trong 2 cách sau: Tiếp tục luyện lại mạng để có được kết quả tốt hơn. Quay lại bước 2, tiến hành điều chỉnh số neuron ở lớp ẩn hoặc cấu trúc mạng, sau đó luyện mạng lại.
0	Một ANN hoạt động tốt sẽ cho ra các kết quả sau: Sai số luyện mạng, kiểm tra chéo, kiểm tra mạng thấp; Sai số luyện mạng ở những vòng lặp cuối ổn định; Mức độ quá khớp không đáng kể; Kiểm ra mạng bằng bộ số liệu khác (bộ 2) cho kết quả tốt. Sử dụng mạng để dự báo các giá trị độ rỗng cần tìm với bộ số liệu là các thông số cơ học đá đã nhập để xây dựng mạng. Các giá trị được dự báo sẽ được so sánh với giá trị độ rỗng thực của giếng X11. Từ đó, tiến hành tính toán MSE để đưa ra những kết luận về phương pháp ANN.
0	Với bài toán trên, nhóm tác giả sẽ xây dựng các mạng sau để dự báo độ rỗng: Mạng 2-10-1 (mạng A) với các thông số đầu vào gồm áp suất lỗ rỗng và UCS, 10 neuron trong lớp ẩn. Mạng 2-20-1 (mạng B) với các thông số đầu vào gồm áp suất lỗ rỗng và UCS, 20 neuron trong lớp ẩn. Mạng 3-10-1 (mạng C) với các thông số đầu vào gồm áp suất lỗ rỗng, UCS và độ sâu theo phương thẳng đứng, 10 neuron trong lớp ẩn. Mạng 3-20-1 (mạng D) với các thông số đầu vào gồm áp suất lỗ rỗng, UCS và độ sâu theo phương thẳng đứng, 20 neuron trong lớp ẩn.
0	Đối với mạng A: Với 2 thông số đầu vào là áp suất lỗ rỗng và độ bền nén đơn trục, nhóm tác giả đã xây dựng được mạng A như Hình 13. Bảng 2 thể hiện kết quả huấn luyện mạng, kiểm tra chéo và kiểm tra mạng A ở vòng lặp thứ 7 và đồ thị thể hiện quá trình huấn luyện trong 13 vòng lặp. Ngoài ra, nhóm tác giả tiếp tục sử dụng số liệu gồm 30 mẫu ở bộ 2 để đánh giá sự chính xác của mạng vừa được huấn luyện. Kết quả Bảng 3 thể hiện làm tăng độ tin cậy của mạng A.
0	Sau khi dùng mạng A để dự báo độ rỗng từ bộ số liệu X11, nhóm tác giả đã được một bộ số liệu độ rỗng 113 giá trị. Từ đó, nhóm tác giả đã đem so sánh với bộ số liệu độ rỗng thực tế của X11 cũng gồm 113 giá trị. Với 2 bộ số liệu độ rỗng thực tế và dự báo có được, nhóm tác giả tiến hành đưa 2 thông số này vào Excel và tính toán MSE = 142,533 × 10-7 và hệ số hồi quy R = 0,9959. Tương tự đối với 3 mạng còn lại: Ở mạng B, nhóm tác giả cũng tiến hành tương tự các bước ở mạng A và thu được kết quả cuối cùng sau khi nhập vào Excel, MSE = 143,171 × 10-7 và hệ số hồi quy R = 0,9955.
0	Kết quả ở mạng C, MSE = 1428,875 × 10-7 và hệ số hồi quy R = 0,9624 (Hình 17). Với kết quả vẫn ra hệ số hồi quy lớn (trên 90%) cho thấy mạng neuron đã huấn luyện thành công và cho ra một hàm xấp xỉ gần với quy luật trong thực tế. Kết quả ở mạng D, MSE = 138,968 × 10-7 và hệ số hồi quy R = 0,9955. Mạng A với 2 thông số đầu vào (áp suất lỗ rỗng và độ bền nén đơn trục) được khởi tạo cùng với 10 neuron ở lớp ẩn, sau đó thu được kết quả dự báo rất khả quan với độ chính xác lên đến 99,59% so với số liệu thực tế.
0	Tuy nhiên, ở mạng B, mạng chỉ thay đổi số neuron của lớp ẩn từ 10 lên 20 neuron thì độ chính xác của mạng đạt 99,55. Như vậy, từ mạng A và B cho thấy với 10 neuron lớp ẩn thì chỉ cần 2 thông số đầu vào, mạng đã đạt giá trị dự báo ở mức tối đa. Đối với mạng C, mạng được thêm thông số độ sâu TVD, kết quả cho thấy khi thêm thông số đầu vào (input) mạng sẽ giảm độ chính xác (đạt 96,24% so với thực tế). Muốn khắc phục hiện tượng trên chỉ cần tăng số lượng neuron ở lớp ẩn lên. Trong nghiên cứu này, nhóm tác giả đã tăng lên 20 để mạng C chuyển đổi thành mạng D.
0	Khử nhiễu hình ảnh luôn là một vấn đề được quan tâm hàng đầu trong xử lý ảnh. Trong bài viết này, chúng tôi đề xuất sử dụng kỹ thuật bộ mã tự động kết hợp mạng nơ-ron tích chập để giải quyết bài toán xử lý ảnh nhiễu đồng thời cho thấy hiệu quả của phương pháp này đối với các loại nhiễu thường gặp trong việc số hóa hình ảnh. Ngày nay với sự phát triển mạnh mẽ của công nghệ số hóa, thông tin số đặc biệt là hình ảnh số đã trở nên vô cùng phổ biến. Nhiễu ảnh luôn tồn tại trong các bức ảnh kỹ thuật số do quá trình mã hóa, truyền phát, thu nhận hình ảnh. Do đó khử nhiễu cho hình ảnh từ lâu đã trở thành vấn đề luôn được quan tâm đặc biệt trong lĩnh vực Thị giác máy tính.
0	Có rất nhiều phương pháp để khử nhiễu ảnh đã được đề xuất như biến đổi wavelets (Coifman và cộng sự, 1995), partial differential equations (PDEs) (Perona và cộng sự, 1990). Hiện nay, với sự phát triển mạnh mẽ của công nghệ Học máy (Machine Learning), việc khử nhiễu hình ảnh có thể được thực hiện bằng các thuật toán Machine Learning và thu được những kết quả khả quan. Trong đó có thể kể đến thuật toán Bộ mã tự động (Autoencoder), một mạng nơ-ron được phát triển và ứng dụng trong những năm gần đây. Được đánh giá là một phương pháp hiệu quả và không bị giới hạn bởi loại nhiễu cần xử lý, Autoencoder được sử dụng rộng rãi trong lĩnh vực này (Gondara, 2016; Vincent và cộng sự, 2008, Xie và cộng sự, 2012).
0	Trong khuôn khổ bài báo này, chúng tôi sử dụng một phiên bản của Autoencoder là Bộ mã tự động tích chập để tiến hành loại nhiễu trên tập hình ảnh MNIST. Sau khi được xây dựng, bộ mã tự động này sẽ chạy thử trên các loại nhiễu thường gặp trong xử lý ảnh. Autoencoder là mạng nơ-ron tự động mã hóa, một thuật toán máy học không giám sát áp dụng sự lan truyền ngược, đặt các giá trị đích bằng với các đầu vào. Mã tự động được sử dụng để giảm kích thước đầu vào thành một biểu diễn nhỏ hơn. Kiến trúc của Autoencoder tạo ra một nút thắt cổ chai vì vậy những đặc trưng đại diện được giữ lại.
0	Một Autoencoder gồm 3 phần chính (Hình 1): Encoder: mã hóa dữ liệu đầu vào thành một phần dữ liệu với số chiều nhỏ hơn dữ liệu ban đầu. Cũng có thể nói, đây là bước nén dữ liệu đầu vào. Code: được xem như phần thắt nút cổ chai của kiến trúc, đại diện cho phần dữ liệu đã nén để chuẩn bị đưa vào Decoder. Decoder: có nhiệm vụ tái tạo phần dữ liệu đã nén sao cho giống dữ liệu đầu vào nhất có thể. Như đã nói ở trên, mục tiêu của autoencoder là tái tạo lại dữ liệu hình ảnh ở đầu vào. Tuy nhiên ở bài toán khử nhiễu, đầu vào sẽ là hình ảnh có nhiễu và mục tiêu của mô hình được mở rộng ra là khôi phục lại hình ảnh ban đầu (trước khi thêm nhiễu).
0	Khi đó, tiêu chí để đánh giá mô hình không còn là sự giống nhau giữa hình ảnh đầu vào và đầu ra nữa, mà sẽ là sự giống nhau giữa hình ảnh đầu ra và hình ảnh trước nhiễu (Hình 2). Bộ mã tự động tích chập CAE (Convolutional Autoencoder) được xây dựng trên cơ sở của kiến trúc autoencoder với mạng nơ-ron tích chập (convolutional neural network) tại bộ encoder và decoder. Bộ mã tự động tích chập được đánh giá là ưu việt hơn so với bộ mã tự động truyền thống nhờ tích hợp được những thế mạnh của mạng nơ-ron tích chập.
0	Mạng nơ-ron tích chập (Convolutional Neural Network - CNN) (LeCun, 1989) là một loại mạng thần kinh chuyên biệt để xử lý dữ liệu có cấu trúc liên kết giống như lưới, ví dụ như dữ liệu trên miền thời gian, được xem như là lưới 1D của các mẫu được lấy trong từng khoảng thời gian nhất định, hay dữ liệu hình ảnh, có thể xem là lưới 2D các giá trị pixel. Mạng tích chập đã rất thành công trong các ứng dụng thực tế. Cái tên “mạng tích chập” ý nói đến phép toán được sử dụng trong mạng nơ-ron là phép toán tích chập. Mạng nơ-ron tích chập có thể hiểu đơn giản là mạng nơ-ron sử dụng tích chập thay cho phép nhân ma trận tổng quát trong ít nhất một lớp của mạng.
0	Về cơ bản, có 3 loại lớp (layer) cấu thành nên một mạng nơ-ron tích chập:  Convolutional Layers, Pooling Layers, Dense Layers hay còn gọi là Fully Connected Layers. Để huấn luyện mô hình CAE khử nhiễu, ở đây chúng tôi sử dụng bộ dữ liệu mã nguồn mở MNIST, gồm 60.000 bức ảnh trắng đen của các chữ số viết tay, có kích thước 28x28, thường được dùng trong huấn luyện các hệ thống xử lý ảnh khác nhau. Mục tiêu của việc huấn luyện là tạo ra 1 mô hình CAE có khả năng khử nhiễu hình ảnh, do đó các hình ảnh trong bộ MNIST được thêm nhiễu và đưa vào mô hình để tiến hành huấn luyện (Hình 3, 4).
0	Kiến trúc của mô hình bao gồm 2 phần chính: encoder và decoder, trong đó mỗi phần đều có các lớp cơ bản của một mạng nơ-ron tích chập. Các thông số cụ thể về từng lớp trong mạng nơ-ron được thể hiện ở Hình 5. Hình 6 biểu diễn kết quả huấn luyện qua từng epoch. Việc huấn luyện mô hình này được thực hiện bằng laptop ASUS Vivobook (Ryzen7 3700U, RAM 8GB, không có GPU) trên nền tảng thư viện Keras, một thư viện mã nguồn mở chuyên dùng trong lĩnh vực Học máy. Loss function được sử dụng ở đây là hàm Mean-squared error (MSE). Quá trình huấn luyện với 30 epochs và kích thước của 1 batch là 1875, diễn ra trong 30 - 35 phút.
0	Tiến hành thử mô hình đã huấn luyện trên bức ảnh bị nhiễu, ta thu được kết quả ở đầu ra như Hình 7. Như đã đề cập ở trên, trong khuôn khổ bài báo này, chúng tôi không chỉ trình bày về một kỹ thuật khử nhiễu hình ảnh mới, convolutional autoencoder, mà còn thử nghiệm mô hình đã huấn luyện trên các loại nhiễu thường gặp trong xử lý ảnh như nhiễu Gaussian, nhiễu muối tiêu, ... Để thêm các loại nhiễu này vào ảnh, chúng tôi sử dụng hàm random_noise trong thư viện scikit-image của Python. Scikit-image (trước đây là scikits.image) là một thư viện xử lý ảnh mã nguồn mở cho ngôn ngữ lập trình Python.
0	Thư viện này bao gồm các thuật toán cho phân đoạn, biến đổi hình học, thao tác không gian màu, phân tích, lọc, hình thái học, phát hiện tính năng và hơn thế nữa. Giống với các thư viện khác của Python như Scikit-learn, Scikit-image cũng được thiết kế để tương thích với NumPy và SciPy. Hình 8 là kết quả khử nhiễu của mô hình được xây dựng. Để dễ so sánh, các kết quả khử nhiễu sẽ được đặt cạnh hình ảnh ban đầu (đã được thêm nhiễu).Bảng 1 So sánh kết quả lọc nhiễu bằng CAE trên các loại nhiễu khác nhau với thông số MSE (Mean Squared Error) giữa ảnh ban đầu và ảnh sau khi lọc nhiễu.
0	Với việc ứng dụng kỹ thuật autoencoder kết hợp với mạng nơ-ron tích chập, chúng tôi đã tạo ra một mô hình khử nhiễu có thể áp dụng trên các loại nhiều khác nhau. Từ những kết quả thu được, có thể thấy mô hình hoạt động tốt nhất đối với loại nhiễu speckle, nhiễu Gaussian. Riêng đối với nhiễu salt & pepper, nhiễu Poisson kết quả vẫn chưa được tốt. Tuy nhiên, so với cách làm truyền thống sử dụng các bộ lọc như mean filter, Gaussian filter... thường chỉ có tác dụng với một loại nhiễu nhất định, thì đây là một phương án khả quan và có tiềm năng phát triển. 4.2.
0	Hướng phát triển Trong tương lai, để nâng cao hiệu suất khử nhiễu, có thể nghiên cứu kết hợp thêm các phương pháp để tiền xử lý hình ảnh, cải thiện chất lượng đầu vào trước khi tiến hành lọc nhiễu. Ngoài ra, việc ứng dụng autoencoder không chỉ dừng lại ở lọc nhiễu ảnh mà còn có thể ứng dụng trong việc phát hiện bất thường (anomaly detection), nén ảnh... Thêm vào đó, việc kết hợp các ưu điểm của autoencoder với các mạng neuro thường gặp như Convolutional Neural Network (CNN), Long-Short Term Memory (LSTM) hứa hẹn là giải pháp hữu hiệu để giải quyết các bài toán trong lĩnh vực Học máy, Học sâu.
0	Khai thác mỏ Tê Giác Trắng đang đối mặt với thách thức lớn do thiếu hụt dữ liệu địa vật lý giếng khoan tại một số tầng chứa tiềm năng như ULBH/ILBH5.1, ảnh hưởng đến độ chính xác khi đánh giá trữ lượng và tối ưu hóa chiến lược khai thác. Để giải quyết vấn đề này, nhóm tác giả ứng dụng trí tuệ nhân tạo (AI) thông qua phương pháp học máy (machine learning) nhằm dự báo các đường cong địa vật lý quan trọng như độ rỗng neutron (NEU) và mật độ đất đá (RHOB) dựa trên dữ liệu gamma ray (GR) và điện trở suất (RD). Quá trình huấn luyện và kiểm chứng mô hình được thực hiện nghiêm ngặt bằng phương pháp giếng kiểm tra mù, đạt độ tin cậy lên đến 70% tại các vỉa đã thực hiện bắn mở vỉa.
0	Nghiên cứu cũng tích hợp phân tích dữ liệu khí thành phần để xác định tính chất chất lưu của vỉa chứa, từ đó hỗ trợ việc lựa chọn khoảng bắn vỉa tối ưu. Kết quả ứng dụng thực tế tại mỏ Tê Giác Trắng cho thấy phương pháp này đã xác định chính xác các tầng chứa dầu tiềm năng, góp phần gia tăng đáng kể sản lượng khai thác với chi phí tối ưu. Nghiên cứu khẳng định việc ứng dụng AI và học máy có thể cải thiện hiệu quả khai thác dầu khí trong điều kiện dữ liệu hạn chế, mở ra hướng tiếp cận mới trong quản lý và vận hành mỏ dầu tại Việt Nam và trên thế giới.
0	Mỏ Tê Giác Trắng ở Lô 16-1, bể Cửu Long, thềm lục địa Việt Nam. Các vỉa chứa chính của mỏ Tê Giác Trắng gồm nhiều tập cát chứa dầu xếp chồng lên, chủ yếu nằm trong hệ tầng Bạch Hổ, Miocene dưới (tập Bl) và hệ tầng Trà Tân, Oligocene trên (tập C). Mỏ được chia thành các khối đứt gãy riêng biệt (H1, H2, H3, H4 và H5) và nằm dọc theo xu hướng nếp lồi nghiêng về phía Nam, bị cắt ngang bởi các đứt gãy hướng Đông - Tây. Hiện tại, mỏ Tê Giác Trắng có 3 giàn đầu giếng (H1, H4 và H5-WHP) với 56 giếng khai thác và 1 giếng bơm ép nước.
0	Trong giai đoạn đầu phát triển mỏ, ULBH/ILBH5.1 không được đưa vào đối tượng tiềm năng khai thác, do vậy không có kế hoạch đo đạc số liệu, dẫn đến thiếu số liệu địa vật lý giếng khoan cơ bản như neutron, density, sonic và điện trở suất. Việc này gây khó khăn trong đánh giá trữ lượng cũng như đề xuất bắn mở vỉa để gia tăng sản lượng khai thác tại các giếng khoan hiện có. Hoang Long JOC luôn tìm kiếm các giải pháp cho việc tối ưu hóa sản lượng khai thác. Bắn vỉa bổ sung (add perf) là phương pháp can thiệp giếng hiệu quả, có khả năng gia tăng sản lượng đáng kể và được quan tâm nghiên cứu.
0	Theo thống kê, việc áp dụng bắn vỉa bổ sung đã giúp gia tăng đáng kể sản lượng khai thác với lưu lượng dầu ban đầu từ 1,9 - 10,8 nghìn thùng dầu/ngày đêm. Tuy nhiên, hiệu quả bắn vỉa bổ sung phụ thuộc trực tiếp vào việc đánh giá chính xác khoảng mở vỉa tiềm năng, thường dựa trên phân tích tài liệu địa vật lý giếng khoan. Trong thực tế, không phải tất cả các vỉa mà giếng khoan qua đều được thu thập đầy đủ dữ liệu địa vật lý giếng khoan. Điều này dẫn đến việc thiếu thông tin cần thiết để đánh giá thông số vỉa chứa, gây khó khăn cho việc ra quyết định bắn vỉa bổ sung.
0	Để khắc phục tình trạng này, giải pháp thường được sử dụng là đo log độ bão hòa nước sau ống chống. Tuy nhiên, các vỉa này đều nằm sau 2 lớp ống chống, các thiết bị đo có chiều sâu nghiên cứu hạn chế, không chạm được đến thành hệ, chất lượng xi măng sau ống chống kém nên độ tin cậy của tài liệu đo không đáp ứng được yêu cầu kỹ thuật để đánh giá thành hệ. Trong bối cảnh đó, việc cập nhật trữ lượng khai thác (2P) cho các tầng chứa thuộc đối tượng ULBH/ILBH5.1 tại mỏ Tê Giác Trắng trở nên cấp thiết, nhằm tối ưu hóa việc tận thu tài nguyên từ các giếng hiện có.
0	Đồng thời, rà soát và đánh giá các vỉa thiếu dữ liệu địa vật lý giếng khoan, kết hợp với chiến dịch bắn vỉa để duy trì sản lượng khai thác ổn định, đặc biệt đối với các mỏ ở giai đoạn cận biên. Tuy nhiên, việc thực hiện những mục tiêu này với chi phí tối ưu đặt ra nhiều thách thức cho công tác vận hành và quản lý mỏ Tê Giác Trắng. Để giải quyết bài toán thiếu hụt dữ liệu địa vật lý giếng khoan, Hoang Long JOC đã nghiên cứu ứng dụng AI dự báo khoảng mở vỉa trong điều kiện dữ liệu địa vật lý giếng khoan hạn chế, đảm bảo tính kinh tế và tối ưu hóa sản lượng khai thác.
0	Hoang Long JOC đã thay đổi phương pháp minh giải tài liệu truyền thống, ứng dụng học máy (trí tuệ nhân tạo) để dự báo các đặc tính vỉa (như độ rỗng, độ thấm) ngay cả khi không có đầy đủ dữ liệu địa vật lý giếng khoan cơ bản. Phương pháp này dựa trên dữ liệu thu thập trong quá trình khoan gồm gamma ray, biểu hiện chỉ số khí thành phần và khí tổng (mudlog), sắc ký khí (gas chromatography), cùng với thông tin địa chất từ các giếng lân cận. Với độ chính xác đạt 70% tại các vỉa đã thực hiện bắn mở vỉa thành công ra dầu từ 12 giếng trên tổng số 17 giếng, cách tiếp cận này đã được chứng minh về mặt phương pháp luận và mang lại hiệu quả kinh tế đáng kể cho mỏ Tê Giác Trắng.
0	Phương pháp trí tuệ nhân tạo (học máy) đã được áp dụng để dự đoán các đường cong độ rỗng neutron (NEU), mật độ đất đá (RHOB) từ các đường cong gamma ray tự nhiên (GRN) và điện trở suất (RD) đã chuẩn hóa dựa trên phân bố Min - Moslikely - Max, cho các giếng không đo đạc tại mỏ Tê Giác Trắng. Để đảm bảo chất lượng đầu vào tốt nhất và tính đại diện cho các khu vực/khối khác nhau, việc lựa chọn cẩn thận các giếng huấn luyện và kiểm tra mù (blind test) đã được thực hiện.
0	Nhiều mô hình học máy đã được xây dựng riêng biệt cho từng khối, nhằm phản ánh sự biến đổi địa chất theo phương ngang. Quá trình xác thực và kiểm chứng mô hình là bước quan trọng để tránh tình trạng quá khớp (overfitting), vấn đề thường gặp trong mô hình học máy phi tham số (Hình 1). Nghiên cứu lựa chọn phương pháp rừng ngẫu nhiên (random forest) sau khi thử nghiệm với nhiều thuật toán học máy khác nhau. Phương pháp này hoạt động bằng cách xây dựng một tập hợp các cây quyết định trong quá trình huấn luyện, sau đó tổng hợp dự đoán dựa trên giá trị trung bình của các cây cá thể (Hình 2).
0	Dữ liệu đầu vào của phương pháp hồi quy ngẫu nhiên bao gồm:- Tập huấn luyện: Một tập hợp các mẫu dữ liệu, mỗi mẫu gồm một vector đặc trưng giá trị liên tục, tương ứng là dữ liệu đường cong Gamary (GR) là chính kết hợp điện trở (RD) có cho bài toán này là 15.- Số lượng cây quyết định (N_trees): Một siêu tham số quyết định số lượng cây trong rừng. Đối với bài toán này số lượng cây quyết định được lựa chọn là 70. Phương pháp này khắc phục nhược điểm của cây quyết định đơn lẻ, bằng cách tạo ra các cây không tương quan với nhau thông qua kỹ thuật lấy mẫu khởi động (bootstrap) từ dữ liệu huấn luyện.
0	Do số lượng đường cong đầu vào hạn chế, kỹ thuật tạo thêm các đặc trưng mới từ các đặc trưng hiện có đã được áp dụng, nhằm phân tách dữ liệu tốt hơn trong không gian đặc trưng tăng cường. Kỹ thuật này cũng giúp mở rộng số lượng đặc trưng đầu vào, ví dụ như thêm các tính chất phi tuyến như căn bậc hai (√(x)), bình phương (x2) và các số hạng tương tác bậc hai (x/y, x × y). Kết quả dự đoán được thể hiện trên log plot của các giếng huấn luyện và giếng kiểm tra mù (Hình 3).
0	Mô hình học máy cho thấy sự phù hợp tốt với dữ liệu đầu vào, đặc biệt là đối với các vỉa cát. Tuy nhiên, có một số biến đổi nhỏ của RHOB đối với các vỉa sét. Giao thoa NEU-RHOB phù hợp với vỉa cát có GR thấp, cho thấy khả năng dự đoán các vỉa cát mỏng của mô hình. Phổ dữ liệu đầu ra nằm trong phạm vi thông thường của dữ liệu đầu vào từ các giếng huấn luyện. Mặc dù giá trị NEU tại các vỉa sét sạch có xu hướng thấp hơn, song điều này không ảnh hưởng nhiều khi sử dụng NEU để tính toán thể tích sét và độ rỗng.
0	Mô hình học máy này đã được áp dụng thành công để dự đoán NEU và RHOB từ GRN và RD (hoặc chỉ GRN khi không có RD), trong trường hợp không có mối quan hệ vật lý mạnh mẽ. Mô hình đã được huấn luyện kỹ lưỡng và hoạt động tốt trên các giếng kiểm tra mù, mang lại độ tin cậy cao cho dữ liệu dự đoán. Việc đánh giá kỹ lưỡng các đường cong mô phỏng cho thấy kết quả đầu ra phù hợp về mặt thống kê với dữ liệu đầu vào và phản ánh sự thay đổi địa chất theo chiều ngang và chiều dọc.
0	Do đó, kết quả dự đoán đường cong NEU và RHOB có độ tin cậy cao và có thể được sử dụng để tính toán hàm lượng sét và độ rỗng. Sai số của mô hình được định lượng bằng sai số tuyệt đối giữa các đường cong đo và mô phỏng trên các giếng kiểm tra mù. Phương pháp này sử dụng dữ liệu khí thành phần C1-C5 từ carota khí để tính toán các chỉ số quan trọng, bao gồm tỷ lệ độ ẩm của khí (gas wetness ratio - GWR), tỷ lệ LHR (light to heavy ratio - LHR), dấu hiệu hydrocarbon (hydrocarbon flag - HCF) và bộ định tính đặc tính dầu (oil character qualifier - OCQ). Các chỉ số này kết hợp với kết quả thực tế của các khoảng đã mở vỉa giúp nâng cao tính chính xác dự đoán tính chất của chất lưu vỉa.
0	Kết quả phân tích khí thành phần được áp dụng để xác định vị trí bắn vỉa cho giếng T-MLP, nơi thiếu dữ liệu địa vật lý giếng khoan cơ bản cho tầng ULBH/ILBH5.1. Dựa trên kết quả bắn vỉa PH1 của tầng 5.2U, biểu đồ phân tích khí thành phần cho thấy dầu tại mỏ Tê Giác Trắng có đặc điểm riêng biệt với GWR khoảng 45 - 60. Kết quả bắn mở vỉa PH1 (Hình 4), được xác nhận bởi tài liệu mặt cắt dòng (PLT), cho thấy sản lượng dầu rất tốt. Biểu đồ phân tích khí thành phần này được sử dụng làm cơ sở để tìm kiếm các vỉa dầu khác mà không có dữ liệu địa vật lý giếng khoan cơ bản tại mỏ Tê Giác Trắng.
0	Các khoảng mở vỉa (PH3) không có dữ liệu địa vật lý giếng khoan được đề xuất dựa trên phân tích khí thành phần, với GWR tương tự như PH1 (Hình 5), được thể hiện trên cùng biểu đồ. Kết quả dự đoán đường cong độ rỗng cho tầng chứa ULBH/ILBH5.1 (thiếu dữ liệu địa vật lý giếng khoan cơ bản) bằng sử dụng phương pháp học máy, được thể hiện trên giếng điển hình T-MLP để đề xuất bắn mở vỉa PH3 (Hình 6). Kết quả dự đoán này đã được kiểm chứng bằng tài liệu đo mặt cắt dòng (PLT) vào năm 2019, sau khi bắn mở vỉa PH3 thành công. Dựa trên kết quả phân tích khí thành phần và kết quả tính toán hàm lượng sét và độ rỗng từ mô hình học máy cho các giếng khoan có dữ liệu hạn chế tại tập ULBH/ILBH5.1.
0	Các vỉa chứa tiềm năng, kể cả các vỉa rất mỏng, đã được đề xuất để bắn mở vỉa bổ sung. Ưu tiên hàng đầu là các giếng đang khai thác có hàm lượng nước (WCT) từ 90% trở lên, được đề xuất bắn vỉa bổ sung cho các vỉa thuộc tập ULHB/ILBH5.1. Các giếng này đã được đưa vào chiến dịch bắn vỉa bổ sung giai đoạn 2019 - 2023 (Bảng 2). Cách tiếp cận sử dụng AI dự đoán khoảng mở vỉa trong điều kiện dữ liệu địa vật lý giếng khoan hạn chế đã chứng minh hiệu quả: việc bắn các tập vỉa ULBH & ILBH5.1, vốn thiếu dữ liệu địa vật lý giếng khoan, đã giúp gia tăng đáng kể lưu lượng khai thác của giếng (Bảng 3).
0	Nghiên cứu này ứng dụng trí tuệ nhân tạo (AI), đặc biệt là phương pháp học máy, để dự báo các thông số địa vật lý giếng khoan trong điều kiện thiếu hụt dữ liệu. Bằng cách tận dụng dữ liệu sẵn có từ quá trình khoan (gamma ray, điện trở suất), kết hợp với phân tích khí thành phần (mudlog, gas chromatography), mô hình đã cung cấp dự đoán có độ chính xác cao về độ rỗng và tính chất chất lưu của vỉa chứa, từ đó hỗ trợ quá trình ra quyết định lựa chọn khoảng bắn mở vỉa tối ưu.
1	Học máy đã cách mạng hóa lĩnh vực y tế bằng cách áp dụng các thuật toán để chẩn đoán sớm bệnh ung thư phổi, giúp cải thiện tỷ lệ sống sót cho bệnh nhân. Nghiên cứu này tập trung vào việc sử dụng mô hình học sâu dựa trên mạng nơ-ron tích chập (CNN) để phân tích hình ảnh X-quang và CT scan. Dữ liệu từ hơn 10.000 bệnh nhân tại các bệnh viện ở Mỹ và châu Âu cho thấy độ chính xác chẩn đoán đạt 92%, cao hơn 15% so với phương pháp truyền thống của bác sĩ.
1	Các thuật toán như ResNet-50 và DenseNet-121 được huấn luyện trên bộ dữ liệu LUNA16, với tỷ lệ dương tính giả giảm xuống còn 5%. Trong đời sống hàng ngày, ứng dụng này có thể tích hợp vào các ứng dụng di động, cho phép người dùng tải lên hình ảnh chụp và nhận kết quả sơ bộ chỉ trong vài giây. Nghiên cứu cũng đánh giá tác động xã hội, với chi phí chẩn đoán giảm 30% nhờ tự động hóa, giúp tiếp cận y tế dễ dàng hơn cho cộng đồng nông thôn. Kết quả cho thấy học máy không chỉ nâng cao hiệu quả mà còn giảm gánh nặng cho hệ thống y tế toàn cầu, với dự báo đến năm 2030, 70% các ca chẩn đoán ung thư sẽ sử dụng AI.
1	Ung thư phổi là một trong những nguyên nhân gây tử vong hàng đầu trên thế giới, với hơn 2,2 triệu ca mới được chẩn đoán hàng năm theo Tổ chức Y tế Thế giới (WHO). Trong cuộc sống hàng ngày, việc phát hiện sớm bệnh này thường bị hạn chế bởi chi phí cao và thiếu chuyên gia, dẫn đến tỷ lệ sống sót chỉ khoảng 18% ở giai đoạn muộn. Học máy, đặc biệt là học sâu, cung cấp giải pháp bằng cách phân tích dữ liệu hình ảnh với tốc độ và độ chính xác vượt trội. Nghiên cứu này khám phá cách các mô hình ML như SVM và Random Forest được áp dụng để dự đoán nguy cơ dựa trên dữ liệu lâm sàng và hình ảnh.
1	Ví dụ, một nghiên cứu tại Đại học Stanford sử dụng dữ liệu từ 50.000 bệnh nhân, đạt độ nhạy 94% trong việc phát hiện nốt phổi ác tính. Trong đời sống, các ứng dụng như IBM Watson Health đã được triển khai, giúp bác sĩ gia đình chẩn đoán nhanh chóng, giảm thời gian chờ đợi từ 2 tuần xuống còn 1 ngày. Hơn nữa, tích hợp ML vào thiết bị đeo như smartwatch có thể theo dõi hô hấp thời gian thực, cảnh báo sớm với độ chính xác 85%. Nghiên cứu nhấn mạnh vai trò của ML trong việc dân chủ hóa y tế, với số liệu từ châu Á cho thấy tỷ lệ sử dụng tăng 40% trong đại dịch COVID-19.
0	Để đánh giá ứng dụng học máy trong chẩn đoán ung thư phổi, chúng tôi thu thập dữ liệu từ bộ dữ liệu công khai như NLST (National Lung Screening Trial) với hơn 53.000 hình ảnh CT từ 26.000 bệnh nhân. Các mô hình được huấn luyện sử dụng framework TensorFlow, với dữ liệu chia thành 80% huấn luyện, 10% xác thực và 10% kiểm tra. Thuật toán CNN được tối ưu hóa bằng cách sử dụng augmentation dữ liệu, tăng kích thước bộ dữ liệu lên gấp đôi để tránh overfitting. Tham số học tập bao gồm learning rate 0.001 và batch size 32, huấn luyện qua 50 epochs.
1	Đánh giá hiệu suất sử dụng các chỉ số như AUC-ROC đạt 0.96 và F1-score 0.91. Trong ứng dụng thực tế, mô hình được triển khai trên cloud AWS, cho phép tích hợp vào ứng dụng di động với thời gian xử lý dưới 5 giây. Chúng tôi cũng thực hiện cross-validation 5-fold để đảm bảo tính tổng quát, với độ lệch chuẩn dưới 2%. Số liệu từ thử nghiệm thực tế tại 5 bệnh viện ở Việt Nam cho thấy tỷ lệ chẩn đoán đúng tăng 25% so với phương pháp thủ công. Kết quả từ mô hình học máy cho thấy độ chính xác tổng thể đạt 93% trên bộ dữ liệu kiểm tra, với tỷ lệ phát hiện ung thư giai đoạn sớm tăng từ 45% lên 78%.
1	Trong 1.000 ca thử nghiệm, mô hình xác định đúng 850 ca dương tính, giảm dương tính giả xuống còn 7%. So sánh với bác sĩ, ML vượt trội ở tốc độ: xử lý 100 hình ảnh trong 10 phút so với 2 giờ thủ công. Dữ liệu từ ứng dụng thực tế tại Mỹ cho thấy chi phí chẩn đoán giảm 35%, từ 500 USD xuống 325 USD mỗi ca. Hơn nữa, tích hợp vào đời sống hàng ngày qua app như Lung Cancer Detector, đã có hơn 100.000 lượt tải về, với phản hồi người dùng cho thấy 85% hài lòng về độ chính xác. Trong nhóm bệnh nhân cao tuổi, tỷ lệ sống sót cải thiện 20% nhờ chẩn đoán sớm. Các biểu đồ ROC cho thấy AUC trung bình 0.95, chứng minh tính mạnh mẽ của mô hình.
1	Mặc dù học máy mang lại lợi ích lớn trong chẩn đoán ung thư phổi, vẫn tồn tại thách thức như bias dữ liệu, với bộ dữ liệu chủ yếu từ dân số phương Tây dẫn đến độ chính xác thấp hơn 10% ở châu Á. Trong đời sống, việc áp dụng rộng rãi đòi hỏi tích hợp đạo đức AI, đảm bảo quyền riêng tư dữ liệu theo GDPR. Số liệu từ nghiên cứu cho thấy, khi kết hợp với bác sĩ, độ chính xác tăng lên 97%, chứng minh mô hình hybrid là tối ưu. Tương lai, ML có thể mở rộng sang theo dõi liên tục qua wearable devices, dự báo nguy cơ với độ chính xác 88% dựa trên dữ liệu sinh trắc.
1	Tuy nhiên, chi phí huấn luyện mô hình cao, khoảng 10.000 USD cho mỗi lần, cần hỗ trợ từ chính phủ. Nghiên cứu này góp phần vào việc giảm tử vong toàn cầu, với dự báo đến 2025, 50% bệnh viện sẽ sử dụng ML. Tóm lại, học máy đã chứng minh hiệu quả trong việc áp dụng chẩn đoán ung thư phổi trong đời sống hàng ngày, với số liệu cho thấy cải thiện đáng kể về độ chính xác và tốc độ. Từ dữ liệu nghiên cứu, tỷ lệ sống sót có thể tăng 25% nếu triển khai rộng rãi. Các khuyến nghị bao gồm đầu tư vào dữ liệu đa dạng và đào tạo chuyên gia để tích hợp ML. Trong tương lai, công nghệ này sẽ trở thành tiêu chuẩn, giúp hàng triệu người tiếp cận y tế chất lượng cao mà không phụ thuộc vào vị trí địa lý.
1	Học máy đang thay đổi cách quản lý giao thông đô thị bằng cách dự đoán và tối ưu hóa dòng xe, giảm ùn tắc và tai nạn. Nghiên cứu này sử dụng mô hình học tăng cường (Reinforcement Learning) như DQN để điều khiển đèn tín hiệu thông minh. Dữ liệu từ camera giao thông tại New York và London với hơn 1 triệu giờ ghi hình cho thấy thời gian chờ đợi giảm 40%, và lượng khí thải CO2 giảm 25%. Các thuật toán như LSTM dự đoán lưu lượng xe với độ chính xác 90%. Trong đời sống, ứng dụng này tích hợp vào Google Maps, giúp người dùng tránh ùn tắc, tiết kiệm thời gian trung bình 15 phút mỗi chuyến.
1	Nghiên cứu đánh giá tác động kinh tế, với tiết kiệm 2 tỷ USD hàng năm cho các thành phố lớn. Đến năm 2030, 60% hệ thống giao thông sẽ sử dụng ML, cải thiện chất lượng cuộc sống đô thị. Giao thông đô thị là vấn đề lớn ở các thành phố lớn, với ùn tắc gây thiệt hại kinh tế lên đến 160 tỷ USD hàng năm tại Mỹ theo báo cáo INRIX. Học máy cung cấp giải pháp bằng cách phân tích dữ liệu thời gian thực từ sensor và GPS. Nghiên cứu này khám phá ứng dụng ML trong dự đoán lưu lượng xe, sử dụng mô hình như ARIMA và Neural Networks. Ví dụ, tại Singapore, hệ thống ML đã giảm thời gian di chuyển 20% cho 5 triệu cư dân.
1	Trong cuộc sống hàng ngày, các app như Waze sử dụng crowd-sourced data để dự báo tai nạn với độ chính xác 85%. Hơn nữa, ML hỗ trợ xe tự lái, như Tesla Autopilot, giảm tai nạn 40% theo dữ liệu NHTSA. Nghiên cứu nhấn mạnh vai trò của ML trong giảm ô nhiễm, với số liệu từ châu Âu cho thấy lượng khí thải giảm 30% khi áp dụng hệ thống thông minh. Chúng tôi thu thập dữ liệu từ API giao thông mở như TomTom và camera tại 10 nút giao thông ở Hà Nội, với hơn 500.000 bản ghi. Mô hình RL được huấn luyện bằng Gym environment, với reward function dựa trên thời gian chờ và lưu lượng.
1	Tham số bao gồm epsilon 0.1 cho exploration và discount factor 0.99. Huấn luyện qua 1.000 episodes, đạt convergence sau 500. Đánh giá sử dụng simulation SUMO, so sánh với hệ thống truyền thống. Trong ứng dụng thực tế, mô hình triển khai trên edge computing với latency dưới 1 giây. Cross-validation cho thấy variance dưới 3%. Số liệu thử nghiệm cho thấy giảm ùn tắc 35% ở giờ cao điểm. (118 từ) – Mở rộng: Thêm chi tiết về dữ liệu, chúng tôi cũng tích hợp dữ liệu thời tiết từ NOAA để cải thiện dự đoán, tăng độ chính xác lên 92%.
1	Kết quả cho thấy mô hình RL giảm thời gian chờ trung bình từ 120 giây xuống 72 giây tại các nút giao. Trong 500 giờ thử nghiệm, lưu lượng xe tăng 25%, và tai nạn giả định giảm 30%. So sánh với hệ thống timer cố định, ML hiệu quả hơn 45%. Dữ liệu từ ứng dụng di động cho thấy 70% người dùng tiết kiệm nhiên liệu 10%. Tại các thành phố như Los Angeles, triển khai tương tự tiết kiệm 1,5 tỷ USD/năm. Biểu đồ lưu lượng cho thấy peak reduction 40%. (102 từ) – Mở rộng: Thêm phân tích, với AUC cho dự đoán tai nạn đạt 0.93, chứng minh tính đáng tin cậy. (Bây giờ 118 từ) – Tiếp tục mở rộng: Nghiên cứu cũng ghi nhận phản hồi từ 1.000 người dùng, 85% đánh giá cao tính năng cảnh báo thời gian thực.
1	Mặc dù hiệu quả, ML trong giao thông đối mặt với vấn đề dữ liệu riêng tư và an ninh mạng. Bias trong dữ liệu có thể dẫn đến dự đoán sai lệch 15% ở khu vực nông thôn. Trong đời sống, cần tích hợp với chính sách đô thị để tối ưu. Số liệu cho thấy hybrid với con người tăng hiệu quả 50%. Tương lai, ML sẽ hỗ trợ xe bay, giảm ùn tắc 60%. Chi phí triển khai khoảng 100.000 USD/nút, nhưng ROI cao trong 2 năm. Nghiên cứu góp phần vào thành phố thông minh. (108 từ) – Mở rộng: Thêm ví dụ từ Trung Quốc, nơi hệ thống ML tại Bắc Kinh giảm ô nhiễm 28%.
1	Học máy đã chứng minh giá trị vượt trội trong việc tối ưu hóa giao thông đô thị, với các nghiên cứu thực nghiệm cho thấy khả năng giảm ùn tắc lên đến 40% tại nhiều thành phố lớn trên thế giới. Công nghệ này hoạt động thông qua việc phân tích dữ liệu lưu lượng giao thông theo thời gian thực, dự đoán các điểm nghẽn tiềm ẩn, và tự động điều chỉnh hệ thống đèn tín hiệu giao thông để tối ưu hóa luồng xe cộ. Các thuật toán học sâu còn có khả năng học từ các mô hình di chuyển của người dân, từ đó đề xuất các tuyến đường thay thế hiệu quả hơn và phân bổ nguồn lực giao thông công cộng một cách thông minh.
1	Học máy hỗ trợ nông nghiệp bằng cách dự đoán năng suất và phát hiện bệnh cây trồng, tăng hiệu quả sản xuất. Nghiên cứu sử dụng mô hình CNN để phân tích hình ảnh drone từ 1.000 hecta ruộng lúa ở Việt Nam và Ấn Độ, đạt độ chính xác 95%. Dữ liệu cho thấy năng suất tăng 20%, giảm sử dụng thuốc trừ sâu 30%. Thuật toán như XGBoost dự báo thời tiết với AUC 0.94. Trong đời sống, app như FarmBeats giúp nông dân theo dõi cây trồng thời gian thực, tiết kiệm nước 25%. Tác động kinh tế: tăng thu nhập 15% cho hộ nhỏ. Đến 2030, 50% nông nghiệp sẽ dùng ML, giảm mất mùa 40%.
1	Nông nghiệp đối mặt với biến đổi khí hậu, với mất mùa lên đến 20% hàng năm theo FAO. Học máy cung cấp giải pháp chính xác bằng cách phân tích dữ liệu vệ tinh và sensor. Nghiên cứu khám phá ứng dụng trong dự đoán bệnh cây, sử dụng mô hình SVM. Ví dụ, tại Brazil, ML tăng năng suất cà phê 18%. Trong cuộc sống hàng ngày, nông dân sử dụng drone tích hợp ML để quét ruộng, phát hiện sâu bệnh sớm với độ chính xác 90%. Hơn nữa, ML tối ưu hóa tưới tiêu, giảm lãng phí nước 35% ở California. Nghiên cứu nhấn mạnh vai trò trong an ninh lương thực, với số liệu từ châu Phi cho thấy tăng sản lượng 25%.
1	Nghiên cứu sử dụng bộ dữ liệu đa nguồn quy mô lớn bao gồm hình ảnh từ vệ tinh Sentinel-2 của Cơ quan Vũ trụ Châu Âu (ESA) và dữ liệu thu thập từ thiết bị bay không người lái (drone), với tổng cộng 50.000 hình ảnh độ phân giải cao. Sự kết hợp giữa dữ liệu vệ tinh cung cấp phạm vi phủ sóng rộng với dữ liệu drone mang lại chi tiết ở mức độ cục bộ đã tạo nên một tập dữ liệu toàn diện và đa dạng, phù hợp cho việc phát triển các mô hình học máy mạnh mẽ.
1	Mô hình học máy đã thể hiện khả năng phát hiện bệnh cây trồng ở giai đoạn sớm với độ chính xác lên đến 95% trường hợp, một bước tiến đột phá so với phương pháp quan sát thủ công truyền thống. Việc phát hiện sớm này cho phép nông dân can thiệp kịp thời, ngăn chặn sự lây lan của dịch bệnh trước khi gây thiệt hại nghiêm trọng. Nhờ khả năng dự báo và phòng ngừa chủ động, năng suất cây trồng đã tăng 22% so với mức trung bình trước đây, đánh dấu sự cải thiện đáng kể trong hiệu quả sản xuất nông nghiệp.
1	Mặc dù mang lại nhiều lợi ích, việc triển khai công nghệ học máy trong nông nghiệp vẫn đối mặt với những rào cản đáng kể. Chi phí đầu tư ban đầu cho thiết bị drone và các cảm biến chất lượng cao vẫn còn ở mức khá cao, dao động từ vài nghìn đến hàng chục nghìn đô la tùy thuộc vào quy mô và tính năng. Điều này tạo ra rào cản gia nhập đối với các nông hộ nhỏ lẻ hoặc ở các vùng kinh tế kém phát triển. Thêm vào đó, vấn đề thiếu dữ liệu huấn luyện chất lượng ở các vùng sâu, vùng xa là một trở ngại lớn.
1	Học  máy đang tạo nên một cuộc cách mạng công nghệ toàn diện trong lĩnh vực nông nghiệp, thay đổi căn bản cách thức con người canh tác và quản lý sản xuất nông sản. Khác với những tiến bộ từng bước của quá khứ, công nghệ AI mang đến sự chuyển đổi đột phá với khả năng phân tích khối lượng dữ liệu khổng lồ, dự đoán chính xác và đưa ra quyết định tối ưu trong thời gian thực. Các hệ thống thông minh có thể giám sát sức khỏe cây trồng từng ngày, điều chỉnh lượng nước tưới dựa trên độ ẩm đất và dự báo thời tiết, phát hiện sâu bệnh khi mới xuất hiện triệu chứng ban đầu, và thậm chí dự đoán thời điểm thu hoạch tối ưu để đạt chất lượng cao nhất.
1	Bệnh tim mạch hiện là nguyên nhân gây tử vong hàng đầu trên toàn cầu, đòi hỏi các phương pháp chẩn đoán nhanh chóng và chính xác hơn so với quy trình truyền thống. Nghiên cứu này tập trung vào việc phát triển một hệ thống hỗ trợ quyết định dựa trên thuật toán Mạng thần kinh nhân tạo (ANN) để phân tích các tín hiệu điện tâm đồ (ECG) phức tạp. Mục tiêu chính là phát hiện sớm các dấu hiệu loạn nhịp tim và suy tim sung huyết ngay từ giai đoạn khởi phát. Việc tự động hóa quy trình này không chỉ giúp giảm tải cho các bác sĩ chuyên khoa mà còn hạn chế tối đa các sai sót do yếu tố chủ quan của con người trong quá trình đọc kết quả.
1	Nghiên cứu hướng tới việc tích hợp mô hình này vào các thiết bị đeo thông minh để theo dõi sức khỏe liên tục cho bệnh nhân tại nhà.Chúng tôi đã sử dụng bộ dữ liệu lớn gồm 45.000 mẫu ECG từ cơ sở dữ liệu PhysioNet để huấn luyện mô hình. Thuật toán được lựa chọn là Mạng nơ-ron tích chập (CNN) kết hợp với bộ nhớ dài-ngắn (LSTM) để xử lý dữ liệu dạng chuỗi thời gian. Quy trình bắt đầu bằng bước tiền xử lý để loại bỏ nhiễu từ các hoạt động cơ bắp và nhiễu điện lưới 50Hz. Sau đó, các đặc trưng quan trọng như khoảng cách R-R và độ dốc của đoạn ST được trích xuất tự động. Chúng tôi chia dữ liệu theo tỷ lệ 80:10:10 cho các giai đoạn huấn luyện, kiểm thử và xác thực.
1	"Việc sử dụng kỹ thuật ""Dropout"" và ""Batch Normalization"" đã được áp dụng chặt chẽ nhằm tránh hiện tượng quá khớp (overfitting), đảm bảo mô hình có khả năng tổng quát hóa cao trên các tập dữ liệu thực tế hoàn toàn mới. Kết quả thực nghiệm cho thấy mô hình đạt độ chính xác tổng thể (Accuracy) lên tới 96,4% trong việc phân loại 5 loại nhịp tim khác nhau. Cụ thể, độ nhạy (Sensitivity) đối với triệu chứng rung tâm nhĩ đạt 94,8%, trong khi độ đặc hiệu (Specificity) đạt mức 97,2%. So với các phương pháp chẩn đoán lâm sàng thông thường vốn có độ chính xác dao động khoảng 82-85%, hệ thống này cho thấy sự vượt trội đáng kể."
1	Sự gia tăng nhanh chóng của phương tiện cá nhân tại các đô thị lớn đang gây ra tình trạng ùn tắc nghiêm trọng, dẫn đến tổn thất kinh tế và ô nhiễm môi trường. Nghiên cứu này đề xuất một giải pháp điều khiển tín hiệu đèn giao thông thông minh dựa trên thuật toán Học tăng cường sâu (Deep Reinforcement Learning - DRL). Thay vì sử dụng chu kỳ đèn cố định, hệ thống sẽ tự động điều chỉnh thời gian đèn xanh dựa trên mật độ xe thực tế được ghi nhận qua camera cảm biến. Mục tiêu của nghiên cứu là tối thiểu hóa thời gian chờ đợi của phương tiện tại các nút giao cắt và giảm lượng khí thải CO2 do quá trình dừng chờ lâu.
1	Nghiên cứu sử dụng môi trường mô phỏng SUMO (Simulation of Urban MObility) để tái hiện mạng lưới giao thông phức tạp với 10 nút giao trọng điểm. Tác nhân (Agent) trong mô hình DRL được thiết kế để nhận đầu vào là trạng thái độ dài hàng đợi và vận tốc trung bình của dòng xe. Hàm phần thưởng (Reward function) được định nghĩa là sự sụt giảm tổng thời gian trễ của tất cả phương tiện trong khu vực quan sát. Chúng tôi áp dụng thuật toán Double Deep Q-Network (DDQN) để cải thiện tính ổn định của quá trình học. Qua hơn 5.000 vòng lặp huấn luyện, tác nhân đã học được các chiến lược linh hoạt như ưu tiên làn đường có mật độ cao hoặc kéo dài đèn xanh khi có đoàn xe dài đang di chuyển, giúp dòng lưu thông trở nên mượt mà hơn.
1	Dữ liệu thực nghiệm sau mô phỏng cho thấy tổng thời gian di chuyển của các phương tiện trong giờ cao điểm giảm 22,5% so với hệ thống điều khiển cố định truyền thống. Đặc biệt, số lượng phương tiện phải dừng lại quá 2 lần tại một nút giao đã giảm mạnh 35,2%. Về khía cạnh môi trường, lượng tiêu thụ nhiên liệu trung bình giảm 12,8%, tương đương với việc cắt giảm khoảng 15,5% lượng khí thải carbon tại khu vực thử nghiệm. Các con số này chứng minh rằng việc áp dụng AI vào quản lý hạ tầng không chỉ giải quyết vấn đề giao thông mà còn đóng góp trực tiếp vào mục tiêu phát triển bền vững. Nghiên cứu đề xuất triển khai thí điểm tại các trục đường huyết mạch trước khi mở rộng quy mô toàn thành phố để đánh giá khả năng tương thích hạ tầng.
1	Trong bối cảnh biến đổi khí hậu diễn biến phức tạp, việc dự báo chính xác năng suất cây trồng đóng vai trò sống còn đối với an ninh lương thực và hoạch định chính sách xuất khẩu. Nghiên cứu này tập trung vào việc phát triển mô hình dự báo năng suất lúa tại khu vực Đồng bằng sông Cửu Long bằng cách kết hợp dữ liệu vệ tinh đa phổ và thuật toán học máy. Trước đây, việc dự báo thường dựa trên báo cáo thủ công hoặc các mô hình thống kê tuyến tính đơn giản vốn không thể phản ánh hết sự biến động của thời tiết và sâu bệnh.
1	Bằng cách sử dụng sức mạnh của học sâu, nghiên cứu kỳ vọng cung cấp một công cụ dự báo có độ tin cậy cao, giúp nông dân và các nhà quản lý đưa ra quyết định canh tác và thu mua hợp lý nhất. Dữ liệu sử dụng trong nghiên cứu bao gồm chỉ số thực vật NDVI trích xuất từ vệ tinh Sentinel-2, kết hợp với các biến khí tượng như lượng mưa, nhiệt độ và độ ẩm trong giai đoạn từ năm 2018 đến 2023. Chúng tôi lựa chọn kiến trúc mạng nơ-ron Long Short-Term Memory (LSTM) vì khả năng ghi nhớ các phụ thuộc dài hạn trong dữ liệu chuỗi thời gian của các mùa vụ.
1	"Mô hình được huấn luyện để nhận biết mối quan hệ giữa các chỉ số sinh trưởng ở giai đoạn làm đòng và năng suất cuối cùng khi thu hoạch. Chúng tôi cũng tích hợp thêm kỹ thuật ""Attention Mechanism"" để giúp mô hình tập trung vào những thời điểm nhạy cảm của cây lúa đối với thời tiết, từ đó tăng cường khả năng dự báo trong các điều kiện cực đoan như hạn mặn hoặc lũ lụt. Kết quả đánh giá trên tập dữ liệu kiểm thử độc lập cho thấy sai số bình quân phương căn (RMSE) của mô hình chỉ ở mức 0,32 tấn/ha, thấp hơn đáng kể so với mức 0,85 tấn/ha của các mô hình truyền thống."
1	Độ chính xác dự báo trước thời điểm thu hoạch 1 tháng đạt mức 91,2%, cho phép các doanh nghiệp lúa gạo chủ động trong khâu hậu cần và tìm kiếm thị trường tiêu thụ. Nghiên cứu cũng phát hiện ra rằng dữ liệu độ ẩm đất là biến số quan trọng thứ hai sau chỉ số thực vật, ảnh hưởng trực tiếp đến sai số dự báo. Trong tương lai, việc tích hợp thêm dữ liệu từ các trạm cảm biến IoT đặt trực tiếp tại cánh đồng sẽ giúp nâng cao hơn nữa độ phân giải của mô hình, hướng tới nông nghiệp chính xác bền vững tại Việt Nam.
1	"Trong kỷ nguyên số hóa, các cuộc tấn công mạng ngày càng trở nên tinh vi, đặc biệt là hình thức lừa đảo qua các trang web giả mạo (Phishing) nhằm đánh cắp thông tin cá nhân và tài sản tài chính. Các phương pháp truyền thống dựa trên ""danh sách đen"" (Blacklist) thường có độ trễ lớn và không thể đối phó với các tên miền mới được tạo ra hàng giờ. Nghiên cứu này tập trung vào việc xây dựng một mô hình phân loại thông minh có khả năng nhận diện các dấu hiệu bất thường trong cấu trúc URL và nội dung trang web ngay tại thời điểm người dùng truy cập."
1	Việc phát triển công cụ này không chỉ bảo vệ người dùng cuối mà còn góp phần duy trì sự tin tưởng vào hệ thống giao dịch trực tuyến và các dịch vụ hành chính công điện tử hiện nay. Chúng tôi đã xây dựng một bộ dữ liệu thực tế bao gồm 50.000 URL hợp lệ từ Alexa Ranking và 50.000 URL lừa đảo từ PhishTank. Nghiên cứu thực hiện trích xuất 30 đặc trưng khác nhau, chia làm ba nhóm chính: đặc trưng dựa trên cấu trúc URL (độ dài, số lượng dấu chấm, sự xuất hiện của ký tự đặc biệt), đặc trưng dựa trên tên miền (tuổi đời tên miền, thông tin đăng ký WHOIS), và đặc trưng dựa trên nội dung HTML.
1	Thuật toán Rừng ngẫu nhiên (Random Forest) kết hợp với Gradient Boosting được lựa chọn để huấn luyện mô hình. Chúng tôi sử dụng kỹ thuật trích xuất đặc trưng tự động giúp hệ thống có thể hoạt động mà không cần sự can thiệp thủ công, đảm bảo tính khách quan và tốc độ xử lý dữ liệu ở quy mô lớn trong môi trường thực tế. Mô hình đạt độ chính xác (Accuracy) ấn tượng lên đến 98,2% trên tập dữ liệu kiểm thử. Đáng chú ý, tỷ lệ dương tính giả (False Positive) – tức là các trang web an toàn bị nhận nhầm là lừa đảo – chỉ dừng lại ở mức 0,8%, một con số cực kỳ quan trọng để đảm bảo trải nghiệm người dùng không bị gián đoạn.
1	Tỷ lệ sinh viên bỏ học hoặc chậm tiến độ là một thách thức lớn đối với các cơ sở giáo dục đại học, gây lãng phí nguồn lực của cả gia đình và xã hội. Nghiên cứu này hướng tới việc xây dựng một hệ thống cảnh báo sớm dựa trên dữ liệu lịch sử của sinh viên để dự đoán kết quả học tập và xác định những cá nhân có nguy cơ cao cần được hỗ trợ. Thay vì chỉ dựa vào điểm số cuối kỳ, mô hình phân tích sự tương tác của sinh viên trên các hệ thống quản lý học tập (LMS), dữ liệu rèn luyện và hồ sơ kinh tế xã hội.
1	Mục tiêu cốt lõi là giúp các nhà quản lý giáo dục đưa ra những chính sách can thiệp kịp thời, cá nhân hóa lộ trình học tập để cải thiện chất lượng đào tạo và tăng tỷ lệ tốt nghiệp đúng hạn.Nghiên cứu thu thập dữ liệu từ hơn 12.000 sinh viên tại một trường đại học kỹ thuật trong vòng 4 năm. Dữ liệu bao gồm các biến số như điểm trung bình (GPA) các học kỳ, tần suất truy cập bài giảng trực tuyến, thời gian hoàn thành bài tập và sự tham gia vào các hoạt động ngoại khóa.
1	Chúng tôi áp dụng thuật toán XGBoost kết hợp với mạng nơ-ron sâu (Deep Neural Network) để xử lý các mối quan hệ phi tuyến tính phức tạp giữa các biến. Một bước quan trọng trong quy trình là xử lý mất cân bằng dữ liệu (Imbalanced Data) bằng kỹ thuật SMOTE, vì số lượng sinh viên bỏ học thường ít hơn rất nhiều so với sinh viên tiếp tục học. Mô hình được thiết kế theo cấu trúc đa tầng để cung cấp dự báo ở nhiều thời điểm khác nhau trong suốt khóa học. Kết quả thực nghiệm cho thấy mô hình có khả năng dự báo chính xác nguy cơ bỏ học trước khi kỳ học kết thúc 4 tuần với độ chính xác 89,5%.
1	"Chỉ số F1-score (cân bằng giữa độ chính xác và độ triệu hồi) đạt 0,87, minh chứng cho sự ổn định của hệ thống trên tập dữ liệu thực tế. Đặc biệt, nghiên cứu phát hiện ra rằng tần suất nộp bài tập muộn và sự sụt giảm đột ngột trong tương tác trực tuyến là những ""biến số vàng"" có khả năng dự báo cao nhất. Hệ thống này khi triển khai thực tế đã giúp giảm 15% tỷ lệ sinh viên thôi học tại khoa thử nghiệm thông qua các buổi tư vấn tâm lý và học thuật kịp thời. Đây là nền tảng quan trọng để xây dựng mô hình đại học thông minh dựa trên dữ liệu (Data-driven University)."
1	Trong ngành bán lẻ hiện đại, việc duy trì sự cân bằng giữa hàng tồn kho và nhu cầu thực tế là bài toán sống còn để tối ưu hóa lợi nhuận. Tồn kho quá nhiều dẫn đến đọng vốn và hư hỏng, trong khi tồn kho quá ít làm mất cơ hội bán hàng và giảm lòng trung thành của khách hàng. Nghiên cứu này áp dụng các thuật toán học máy tiên tiến để dự báo nhu cầu tiêu dùng hàng ngày cho các chuỗi siêu thị quy mô lớn. Khác với các mô hình thống kê cũ, phương pháp này tích hợp cả các yếu tố ngoại biên như chương trình khuyến mãi, sự kiện thời tiết, biến động giá đối thủ và các ngày lễ truyền thống.
1	"Mục tiêu là tạo ra một quy trình cung ứng tự động hóa, chính xác và linh hoạt trước các biến động bất ngờ của thị trường. Chúng tôi sử dụng dữ liệu giao dịch của 50 cửa hàng với hơn 2.000 danh mục sản phẩm trong thời gian 3 năm. Kiến trúc mô hình bao gồm sự kết hợp giữa Prophet (của Meta) để xử lý tính mùa vụ và mạng LSTM (Long Short-Term Memory) để bắt kịp các xu hướng ngắn hạn. Dữ liệu đầu vào được làm sạch, xử lý các giá trị ngoại lai và thực hiện trích xuất đặc trưng như ""ngày trong tuần"", ""khoảng cách tới ngày nhận lương"" và ""chỉ số giá tiêu dùng"". Chúng tôi áp dụng chiến lược huấn luyện song song cho các nhóm sản phẩm có đặc tính tương đồng để tăng tốc độ xử lý."
1	"Việc kiểm soát lỗi được thực hiện thông qua chỉ số WMAPE (Weighted Mean Absolute Percentage Error) để đảm bảo các mặt hàng giá trị cao được dự báo chính xác nhất. Sau khi áp dụng mô hình vào thực tế, sai số dự báo nhu cầu đã giảm từ 28% xuống còn 14,5% so với các phương pháp quản lý thủ công trước đây. Hiệu quả rõ rệt nhất được ghi nhận ở nhóm hàng tươi sống với tỷ lệ hao hụt giảm 22%, giúp tiết kiệm hàng tỷ đồng mỗi năm cho đơn vị thử nghiệm. Đồng thời, chỉ số ""On-shelf availability"" (sẵn có trên kệ) tăng từ 85% lên 94%, trực tiếp thúc đẩy doanh thu tăng thêm 7,8%."
1	Ô nhiễm không khí, đặc biệt là bụi mịn PM2.5, đang trở thành mối đe dọa nghiêm trọng đối với sức khỏe cộng đồng tại các đô thị lớn như Hà Nội và TP.HCM. Tuy nhiên, số lượng các trạm quan trắc tiêu chuẩn còn hạn chế và không thể cung cấp dữ liệu chi tiết theo từng khu vực nhỏ. Nghiên cứu này đề xuất một giải pháp kết hợp giữa các trạm cảm biến IoT giá thành thấp và mô hình học sâu để bản đồ hóa chất lượng không khí theo thời gian thực. Bằng cách dự báo sớm nồng độ ô nhiễm trong 24-48 giờ tới, hệ thống có thể cung cấp các khuyến nghị sức khỏe cá nhân hóa và hỗ trợ chính quyền trong việc điều tiết giao thông hoặc hoạt động công nghiệp để giảm thiểu tác động xấu đến môi trường.
1	Nghiên cứu tích hợp dữ liệu từ 100 trạm cảm biến IoT đo nồng độ PM2.5, nhiệt độ, độ ẩm kết hợp với dữ liệu vệ tinh về mật độ xây dựng và dữ liệu mật độ giao thông từ Google Maps. Chúng tôi xây dựng mô hình mạng nơ-ron tích chập đồ thị (Graph Convolutional Networks - GCN) để khai thác các mối quan hệ không gian giữa các khu vực lân cận, kết hợp với mạng GRU (Gated Recurrent Unit) để xử lý biến động theo thời gian. Quy trình huấn luyện đặc biệt chú trọng vào việc hiệu chỉnh dữ liệu (Calibration) từ các cảm biến giá rẻ so với trạm đo tiêu chuẩn của Chính phủ để đảm bảo độ tin cậy.
1	Kết quả thực nghiệm cho thấy mô hình đạt độ tương quan (R-squared) 0,88 so với dữ liệu từ các trạm quan trắc chuẩn quốc gia. Độ chính xác dự báo trong 24 giờ tới đạt 86%, giúp người dân có thể chủ động kế hoạch sinh hoạt ngoài trời. Đặc biệt, hệ thống đã phát hiện thành công quy luật phát tán ô nhiễm từ các trục đường chính vào khu dân cư sâu phía trong với độ trễ từ 2-3 tiếng, một thông tin cực kỳ giá trị cho công tác quy hoạch đô thị. Việc triển khai ứng dụng di động đi kèm đã thu hút hơn 10.000 người dùng thử nghiệm, cung cấp các cảnh báo đỏ khi nồng độ bụi vượt ngưỡng an toàn. Nghiên cứu mở ra hướng đi mới trong việc quản lý môi trường thông minh với chi phí thấp nhưng hiệu quả cao.
1	Tỷ lệ thất thoát nước sạch (Non-Revenue Water) tại nhiều đô thị hiện vẫn ở mức cao, gây tổn thất kinh tế lớn và lãng phí tài nguyên thiên nhiên quý giá. Các vụ rò rỉ ngầm trong lòng đất thường rất khó phát hiện bằng mắt thường và chỉ được nhận biết khi đã xảy ra sự cố nghiêm trọng. Nghiên cứu này phát triển một mô hình học máy phân tích dữ liệu áp lực và lưu lượng từ các đồng hồ thông minh (Smart Meters) để phát hiện sớm các điểm rò rỉ tiềm năng. Mục tiêu là tự động hóa quá trình giám sát mạng lưới, khoanh vùng sự cố trong phạm vi hẹp nhất có thể và điều chỉnh áp lực bơm một cách linh hoạt để giảm thiểu rủi ro vỡ đường ống trong giờ thấp điểm.
1	"Dữ liệu được thu thập từ mạng lưới cấp nước của một khu đô thị với hơn 500 điểm đo lưu lượng và áp suất. Chúng tôi áp dụng thuật toán Phát hiện bất thường (Anomaly Detection) dựa trên mô hình Isolation Forest và mạng nơ-ron tự mã hóa (Autoencoders). Hệ thống sẽ học ""hành vi"" tiêu thụ nước bình thường của khu vực theo từng khung giờ và ngày trong tuần. Khi có sự thay đổi đột ngột trong dòng chảy hoặc sụt giảm áp suất không giải thích được, mô hình sẽ kích hoạt cảnh báo. Chúng tôi cũng tích hợp thuật toán học tăng cường (Reinforcement Learning) để điều khiển các van giảm áp một cách tự động, đảm bảo áp lực nước luôn ở mức tối ưu, vừa đủ phục vụ nhu cầu vừa giảm căng thẳng lên hệ thống ống dẫn lâu năm."
1	Trong giai đoạn thử nghiệm 6 tháng, hệ thống đã phát hiện chính xác 92% các vụ rò rỉ lớn nhỏ, bao gồm cả những điểm rò rỉ âm thầm kéo dài nhiều tháng. Thời gian trung bình từ khi phát hiện đến khi xử lý sự cố giảm từ 48 giờ xuống còn dưới 6 giờ, giúp giảm lượng nước thất thoát khoảng 18,5% cho toàn khu vực. Ngoài ra, nhờ việc tối ưu hóa áp lực bơm, điện năng tiêu thụ tại các trạm tăng áp cũng giảm được 12% mỗi tháng. Kết quả này không chỉ mang lại lợi ích trực tiếp về kinh tế cho đơn vị vận hành mà còn góp phần bảo vệ nguồn tài nguyên nước bền vững.
1	Trong bối cảnh chuyển đổi sang năng lượng tái tạo, việc dự báo chính xác nhu cầu tiêu thụ điện (phụ tải) trở thành yếu tố then chốt để đảm bảo tính ổn định của hệ thống lưới điện quốc gia. Sự biến động của các nguồn năng lượng như điện mặt trời và điện gió thường không ổn định, gây áp lực lớn lên việc điều tiết của các nhà máy điện truyền thống. Nghiên cứu này đề xuất sử dụng kiến trúc mạng Transformer – vốn nổi tiếng trong xử lý ngôn ngữ tự nhiên – để phân tích các quy luật tiêu thụ điện năng phức tạp theo thời gian thực.
1	Mục tiêu là dự báo chính xác nhu cầu phụ tải trong ngắn hạn và trung hạn, từ đó giúp các đơn vị vận hành lưới điện tối ưu hóa việc phân phối, giảm thiểu tình trạng quá tải cục bộ và hạn chế tối đa việc lãng phí năng lượng dư thừa không cần thiết. Chúng tôi sử dụng bộ dữ liệu khổng lồ từ các công ty điện lực tại khu vực đô thị, bao gồm dữ liệu tiêu thụ theo từng giờ của hơn 200.000 hộ gia đình và cơ sở công nghiệp trong 5 năm. Mô hình dự báo dựa trên cơ chế Tự chú ý (Self-Attention) của Transformer để xác định các mối tương quan dài hạn giữa các biến số như nhiệt độ, độ ẩm, các ngày lễ và hoạt động kinh tế.
1	Khác với các mô hình RNN truyền thống, kiến trúc này cho phép xử lý song song dữ liệu và nắm bắt tốt hơn các biến động đột ngột trong nhu cầu năng lượng. Ngoài ra, nghiên cứu còn tích hợp thêm một lớp học máy phụ trợ để hiệu chỉnh sai số dựa trên các kịch bản thời tiết cực đoan, giúp mô hình duy trì độ ổn định ngay cả trong những điều kiện môi trường biến động mạnh nhất. Kết quả cho thấy sai số phần trăm tuyệt đối trung bình (MAPE) của mô hình Transformer chỉ ở mức 2,45%, thấp hơn đáng kể so với mức 5,12% của các mô hình thống kê ARIMA truyền thống.
1	Độ chính xác dự báo trong 24 giờ tới đạt mức 96,8%, cho phép các nhà máy điện giảm được 12% lượng nhiên liệu dự phòng cần thiết, tương đương với việc cắt giảm hàng nghìn tấn khí thải CO2 mỗi tháng. Đặc biệt, hệ thống đã giúp giảm thiểu được 18% các sự cố quá tải tại các trạm biến áp trung thế thông qua việc điều tiết phụ tải thông minh (Demand Response). Những con số này minh chứng rằng việc áp dụng AI vào quản lý năng lượng không chỉ mang lại lợi ích kinh tế to lớn mà còn là chìa khóa để hiện thực hóa mục tiêu phát triển bền vững trong tương lai.
1	Ung thư là một căn bệnh có tính cá thể hóa rất cao; cùng một loại bệnh nhưng mỗi bệnh nhân lại có phản ứng khác nhau với các loại hóa trị hoặc liệu pháp miễn dịch. Nghiên cứu này tập trung vào việc phát triển một hệ thống hỗ trợ quyết định lâm sàng bằng cách sử dụng Machine Learning để phân tích dữ liệu giải trình tự gen thế hệ mới (NGS). Thay vì áp dụng một phác đồ chung cho tất cả mọi người, hệ thống sẽ xác định các đột biến gen cụ thể và dự đoán mức độ nhạy cảm của tế bào ung thư đối với các loại thuốc khác nhau.
1	Mục tiêu của nghiên cứu là tối ưu hóa hiệu quả điều trị, giảm thiểu tác dụng phụ không mong muốn và kéo dài thời gian sống cho bệnh nhân thông qua phương tiếp cận y học chính xác dựa trên bằng chứng dữ liệu sinh học. Chúng tôi thu thập dữ liệu biểu hiện gen và hồ sơ lâm sàng của hơn 15.000 bệnh nhân từ kho lưu trữ The Cancer Genome Atlas (TCGA). Phương pháp nghiên cứu sử dụng Mạng nơ-ron đồ thị (Graph Neural Networks - GNN) để mô hình hóa các tương tác phức tạp giữa các protein và gen trong mạng lưới sinh học của con người. Thuật toán Học sâu đa nhiệm (Multi-task Deep Learning) được triển khai để đồng thời dự báo tỷ lệ sống sót và khả năng kháng thuốc của khối u.
1	Một bước quan trọng là việc sử dụng kỹ thuật học chuyển đổi (Transfer Learning) để áp dụng các tri thức từ các loại ung thư phổ biến sang các loại ung thư hiếm gặp hơn, nơi dữ liệu mẫu thường bị hạn chế, nhằm đảm bảo tính toàn diện và khả năng dự báo mạnh mẽ của hệ thống trên nhiều loại bệnh lý khác nhau. Kết quả thực nghiệm trên các mô hình tế bào và dữ liệu thực tế cho thấy mô hình đạt độ chính xác 88,4% trong việc dự đoán phản ứng của thuốc, cao hơn 20% so với các phương pháp dự đoán lâm sàng hiện hành.
1	"Thời gian để đưa ra một phác đồ gợi ý đã được rút ngắn từ vài tuần xuống còn dưới 48 giờ, giúp các bác sĩ tận dụng ""thời gian vàng"" trong điều trị ung thư giai đoạn muộn. Về mặt kinh tế, hệ thống giúp giảm thiểu khoảng 30% chi phí điều trị vô ích do sử dụng các loại thuốc không phù hợp với đặc điểm di truyền của bệnh nhân. Nghiên cứu này không chỉ mang lại hy vọng mới cho hàng triệu người bệnh mà còn đặt nền móng cho một hệ thống y tế thông minh, nơi mọi quyết định y khoa đều được cá nhân hóa một cách tuyệt đối."
1	Các thảm họa thiên tai như lũ quét và sạt lở đất đang có xu hướng gia tăng về tần suất và cường độ do tác động của biến đổi khí hậu toàn cầu. Việc dự báo các hiện tượng này cực kỳ khó khăn do tính chất bất ngờ và phụ thuộc vào nhiều yếu tố địa chất, thủy văn phức tạp. Nghiên cứu này đề xuất một giải pháp cảnh báo sớm dựa trên sự kết hợp giữa mô hình Học sâu và dữ liệu quan trắc đa nguồn từ vệ tinh, radar và trạm đo mưa mặt đất.
1	Mục tiêu là xây dựng một hệ thống có khả năng nhận diện các dấu hiệu nguy hiểm trước khi thiên tai xảy ra từ 6 đến 12 tiếng, giúp chính quyền địa phương và người dân có đủ thời gian để sơ tán, từ đó giảm thiểu tối đa thiệt hại về người và tài sản tại các khu vực xung yếu. Nghiên cứu sử dụng mạng nơ-ron tích chập (CNN) để phân tích đặc điểm địa hình, thảm thực vật từ ảnh vệ tinh Sentinel-1, kết hợp với mạng GRU (Gated Recurrent Unit) để xử lý dữ liệu lượng mưa tích lũy theo thời gian. Chúng tôi xây dựng một bản đồ số độ cao (DEM) độ phân giải cao để mô phỏng dòng chảy mặt và độ ổn định của các mái dốc.
1	Thuật toán được huấn luyện trên dữ liệu của 1.000 vụ sạt lở đã xảy ra trong quá khứ để học các ngưỡng kích hoạt thiên tai dựa trên độ ẩm đất và cường độ mưa. Hệ thống cũng tích hợp cơ chế học trực tuyến (Online Learning) để liên tục cập nhật dữ liệu từ các cảm biến đo độ nghiêng và mực nước tại hiện trường, giúp điều chỉnh các tham số dự báo theo điều kiện thực tế của từng địa phương cụ thể. Trong quá trình thử nghiệm tại các tỉnh miền núi phía Bắc Việt Nam, mô hình đạt độ chính xác dự báo (Accuracy) lên tới 91,5% và tỷ lệ cảnh báo sai (False Alarm) thấp hơn 7%.
1	Hệ thống đã phát hiện thành công các nguy cơ sạt lở tiềm tàng trước 8 giờ với sai số định vị không gian trong bán kính 50 mét. So với các mô hình thủy văn truyền thống, giải pháp này cho phép xử lý dữ liệu nhanh hơn gấp 10 lần, cung cấp các bản đồ nguy cơ theo thời gian thực với độ chi tiết cao. Việc triển khai hệ thống cảnh báo qua tin nhắn di động và loa phát thanh cộng đồng đã giúp nâng cao đáng kể năng lực ứng phó tại chỗ của người dân. Nghiên cứu chứng minh rằng AI chính là công cụ hữu hiệu nhất để bảo vệ cộng đồng trước những biến động khắc nghiệt của thiên nhiên.
1	Giao hàng chặng cuối là mắt xích tốn kém và phức tạp nhất trong toàn bộ chuỗi cung ứng, chiếm tới hơn 50% tổng chi phí vận chuyển của các doanh nghiệp thương mại điện tử. Thách thức lớn nhất nằm ở việc lập kế hoạch lộ trình cho hàng nghìn đơn hàng mỗi ngày trong điều kiện giao thông đô thị biến động liên tục và yêu cầu về thời gian giao hàng ngày càng khắt khe của khách hàng. Nghiên cứu này tập trung vào việc phát triển một hệ thống điều phối lộ trình thông minh dựa trên thuật toán Học tăng cường sâu (Deep Reinforcement Learning).
1	"Mục tiêu là tự động hóa việc phân bổ đơn hàng cho tài xế và tìm ra con đường ngắn nhất, hiệu quả nhất, giúp giảm thiểu quãng đường di chuyển không cần thiết, tiết kiệm nhiên liệu và nâng cao năng suất làm việc của đội ngũ nhân viên giao hàng. Chúng tôi mô hình hóa bài toán giao hàng như một bài toán tối ưu hóa tổ hợp phức tạp, nơi các ""tác nhân"" (Agent) học cách đưa ra quyết định dựa trên trạng thái hiện tại của mạng lưới giao thông và vị trí các điểm giao hàng. Thuật toán được sử dụng là Proximal Policy Optimization (PPO), một kỹ thuật học tăng cường mạnh mẽ giúp mô hình ổn định trong quá trình huấn luyện."
1	Dữ liệu đầu vào bao gồm tọa độ khách hàng, khung giờ nhận hàng mong muốn, sức chứa của phương tiện và dữ liệu mật độ giao thông thời gian thực từ API bản đồ. Hệ thống thực hiện hàng triệu vòng mô phỏng để học các chiến lược giao hàng tối ưu, chẳng hạn như gom các đơn hàng trong cùng một tòa nhà hoặc ưu tiên các tuyến đường tránh kẹt xe vào giờ cao điểm một cách linh hoạt. Kết quả thực nghiệm tại một công ty chuyển phát nhanh cho thấy hệ thống giúp giảm tổng quãng đường di chuyển của đội xe lên tới 21,4% và tiết kiệm trung bình 15% chi phí nhiên liệu hàng tháng.
1	Quan trọng hơn, tỷ lệ giao hàng đúng hẹn (On-time Delivery) tăng từ 82% lên mức ấn tượng 94,6%, góp phần nâng cao đáng kể mức độ hài lòng của khách hàng. Thời gian để lập kế hoạch lộ trình cho 1.000 đơn hàng giảm từ 30 phút xuống chỉ còn 15 giây, cho phép doanh nghiệp phản ứng tức thì với các đơn hàng phát sinh đột xuất trong ngày. Nghiên cứu kết luận rằng việc ứng dụng AI vào logistics không chỉ đơn thuần là bài toán công nghệ mà còn là yếu tố chiến lược để tăng năng lực cạnh tranh và giảm thiểu tác động môi trường của ngành vận tải đô thị.
1	Trong môi trường sản xuất công nghiệp hiện đại, việc đảm bảo chất lượng sản phẩm đầu ra với độ chính xác tuyệt đối là yếu tố sống còn đối với uy tín của thương hiệu. Tuy nhiên, phương pháp kiểm tra thủ công bởi con người thường gặp nhiều hạn chế như sự mệt mỏi, tính chủ quan và tốc độ xử lý chậm, không thể đáp ứng được các dây chuyền sản xuất hàng loạt tốc độ cao. Nghiên cứu này đề xuất một hệ thống kiểm tra ngoại quan tự động sử dụng công nghệ Thị giác máy tính (Computer Vision) kết hợp với các mô hình Học sâu tiên tiến.
1	Mục tiêu là phát hiện các lỗi nhỏ nhất trên bề mặt sản phẩm như vết trầy xước, nứt vỡ hoặc sai lệch kích thước ngay trên dây chuyền sản xuất, giúp loại bỏ sản phẩm lỗi kịp thời và giảm tỷ lệ phế phẩm trong quy trình sản xuất. Chúng tôi triển khai mô hình YOLOv8 (You Only Look Once) được tinh chỉnh đặc biệt để nhận diện các khuyết tật nhỏ trên các linh kiện điện tử và sản phẩm cơ khí chính xác. Hệ thống sử dụng các camera công nghiệp độ phân giải cao kết hợp với hệ thống chiếu sáng chuyên dụng để thu thập hình ảnh sản phẩm từ nhiều góc độ khác nhau.
1	"Quy trình huấn luyện sử dụng kỹ thuật tăng cường dữ liệu (Data Augmentation) để giúp mô hình có khả năng nhận diện lỗi trong các điều kiện ánh sáng và góc chụp biến đổi. Một mạng nơ-ron tự mã hóa (Autoencoder) cũng được tích hợp để phát hiện các lỗi ""lạ"" chưa từng xuất hiện trong tập huấn luyện, giúp hệ thống có khả năng tự thích nghi và cảnh báo các bất thường mới phát sinh trong quá trình sản xuất thực tế. Kết quả triển khai tại một nhà máy sản xuất linh kiện điện thoại cho thấy hệ thống đạt độ chính xác phát hiện lỗi (Precision) lên tới 99,2%, với tỷ lệ bỏ sót lỗi (Recall) chỉ ở mức 0,5%."
1	Tốc độ kiểm tra đạt 60 sản phẩm mỗi phút, nhanh gấp 5 lần so với kiểm tra bằng mắt thường của công nhân lành nghề. Việc áp dụng hệ thống đã giúp nhà máy giảm 40% chi phí nhân công cho khâu kiểm tra chất lượng và giảm 12% tỷ lệ hàng trả về từ phía khách hàng. Ngoài ra, dữ liệu về các loại lỗi thường gặp được hệ thống thống kê tự động, cung cấp thông tin quý giá cho các kỹ sư để điều chỉnh máy móc sản xuất, ngăn chặn lỗi hệ thống ngay từ gốc. Nghiên cứu khẳng định AI là công cụ không thể thiếu để nâng tầm nền sản xuất thông minh theo tiêu chuẩn toàn cầu.
1	Năng lượng gió ngoài khơi đang trở thành trụ cột trong chiến lược năng lượng tái tạo toàn cầu, nhưng chi phí vận hành và bảo trì (O&M) thường chiếm tới 30% tổng chi phí vòng đời của dự án. Do điều kiện môi trường khắc nghiệt và vị trí xa xôi, việc sửa chữa các bộ phận như hộp số hoặc vòng bi khi đã xảy ra sự cố là cực kỳ tốn kém và mất nhiều thời gian. Nghiên cứu này tập trung vào việc xây dựng một hệ thống bảo trì dự báo dựa trên dữ liệu SCADA (Supervisory Control and Data Acquisition) sẵn có mà không cần lắp đặt thêm cảm biến đắt tiền.
1	Mục tiêu là phát hiện sớm các dấu hiệu thoái hóa của linh kiện trước khi chúng dẫn đến hỏng hóc hoàn toàn, cho phép các đơn vị vận hành lập kế hoạch bảo trì chủ động, tối ưu hóa việc điều động tàu sửa chữa và giảm thiểu thời gian dừng máy không kế hoạch. Chúng tôi sử dụng bộ dữ liệu SCADA lịch sử trong 3 năm từ một trang trại điện gió với tần suất lấy mẫu 10 phút một lần, bao gồm hơn 50 biến số như tốc độ gió, nhiệt độ vòng bi, công suất đầu ra và độ rung của trục chính.
1	"Phương pháp nghiên cứu áp dụng mô hình lai giữa Convolutional Neural Network (CNN) để trích xuất đặc trưng không gian và Gated Recurrent Unit (GRU) để nắm bắt các biến động theo chuỗi thời gian. Chúng tôi xây dựng một ""bản sao số"" (Digital Twin) dựa trên dữ liệu để mô phỏng trạng thái hoạt động bình thường của tuabin. Khi sai lệch giữa dữ liệu thực tế và dự báo của mô hình vượt quá một ngưỡng xác định (Threshold) trong một khoảng thời gian liên tục, hệ thống sẽ kích hoạt cảnh báo lỗi. Kỹ thuật phân tích thành phần chính (PCA) cũng được áp dụng để giảm chiều dữ liệu, giúp tăng tốc độ xử lý mà vẫn giữ được thông tin quan trọng nhất."
1	Kết quả thực nghiệm cho thấy mô hình có khả năng phát hiện các lỗi nghiêm trọng ở hộp số trước khi chúng xảy ra từ 15 đến 22 ngày với độ chính xác (Accuracy) đạt 94,2%. Tỷ lệ báo động giả (False Alarm) chỉ chiếm 4,8%, giúp tránh các chuyến kiểm tra thực địa không cần thiết. Theo tính toán mô phỏng, việc áp dụng hệ thống này giúp giảm chi phí vận hành hàng năm khoảng 18-20%, tương đương với việc tiết kiệm hàng triệu USD cho mỗi trang trại gió quy mô lớn. Ngoài ra, tính khả dụng của tuabin tăng thêm 5,5%, trực tiếp đóng góp vào việc gia tăng sản lượng điện sạch cung cấp cho lưới điện quốc gia.
1	Quản lý rác thải đô thị đang là một trong những thách thức môi trường lớn nhất thế giới, khi khối lượng rác thải tăng nhanh vượt quá khả năng xử lý của các phương pháp thủ công. Việc phân loại rác tại nguồn không hiệu quả dẫn đến tỷ lệ tái chế thấp và gây ô nhiễm chéo giữa các loại vật liệu. Nghiên cứu này đề xuất một hệ thống phân loại rác tự động tích hợp trên băng chuyền, sử dụng công nghệ Thị giác máy tính (Computer Vision) để nhận diện và phân loại rác thành 5 nhóm: nhựa, kim loại, giấy, thủy tinh và rác không tái chế.
1	Mục tiêu là thay thế lao động thủ công trong môi trường độc hại, tăng độ tinh khiết của nguyên liệu tái chế và hỗ trợ xây dựng nền kinh tế tuần hoàn, nơi rác thải được coi là nguồn tài nguyên quý giá cần được khai thác tối đa. Nghiên cứu xây dựng một bộ dữ liệu hình ảnh rác thải thực tế với hơn 80.000 mẫu vật trong các tình trạng khác nhau (biến dạng, bị che khuất hoặc dính bẩn). Chúng tôi sử dụng kiến trúc mạng nơ-ron EfficientDet để đạt được sự cân bằng tối ưu giữa độ chính xác và tốc độ xử lý thời gian thực.
1	Hình ảnh từ camera độ phân giải cao được xử lý qua mô hình để xác định tọa độ và chủng loại vật liệu, sau đó gửi tín hiệu điều khiển đến cánh tay robot Delta hoặc hệ thống thổi khí nén để tách rác vào các thùng chứa tương ứng. Chúng tôi áp dụng kỹ thuật học tăng cường (Reinforcement Learning) để tối ưu hóa quỹ đạo chuyển động của cánh tay robot, giúp tăng mật độ gắp rác trên mỗi phút mà không gây va chạm. Toàn bộ hệ thống được triển khai trên nền tảng Edge Computing để đảm bảo tính ổn định và tốc độ phản hồi cực nhanh.
1	Trong môi trường thử nghiệm công nghiệp, hệ thống đạt độ chính xác phân loại trung bình 96,5% đối với các loại nhựa PET và nhôm. Tốc độ phân loại đạt mức 3.200 vật thể mỗi giờ, nhanh gấp 3 lần so với một nhân công lành nghề và có khả năng vận hành liên tục 24/7 mà không suy giảm hiệu suất. Quan trọng hơn, độ tinh khiết của các lô hàng tái chế sau khi qua hệ thống tự động tăng từ 70% lên 98%, làm tăng giá trị thương mại của nguyên liệu tái chế lên đáng kể. Nghiên cứu cũng chỉ ra rằng việc triển khai hệ thống này có thể giúp các đô thị giảm tới 25% lượng rác thải phải chôn lấp, góp phần trực tiếp vào việc giảm phát thải khí metan từ các bãi rác.
0	Tốc độ tăng trưởng kinh tế là một chỉ số kinh tế vĩ mô có ý nghĩa rất quan trọng đối với các nhà điều hành kinh tế và các doanh nghiệp trong việc xem xét sức khỏe nền kinh tế, là cơ sở đưa ra các quyết định về cơ chế chính sách, kế hoạch phát triển kinh tế - xã hội tại một quốc gia hoặc địa phương. Việc dự báo tốc độ tăng trưởng kinh tế Thành phố Hồ Chí Minh (TP.HCM) theo quý sẽ giúp lãnh đạo thành phố đánh giá được khả năng hoàn thành mục tiêu tăng trưởng kinh tế đề ra trong năm.
0	Nghiên cứu này trình bày việc phân tích và dự báo dữ liệu chuỗi thời gian của chỉ tiêu tốc độ tăng trưởng kinh tế TP.HCM theo quý giai đoạn 2011-2024, bằng cách sử dụng mô hình học máy mạng nơron nhân tạo (ANN) và một số mô hình kinh tế lượng nhằm lựa chọn mô hình phù hợp. Kết quả cho thấy mô hình ANN cho độ chính xác vượt trội so với các mô hình khác. Tốc độ tăng trưởng kinh tế là một chỉ số kinh tế vĩ mô có ý nghĩa rất quan trọng đối với các nhà điều hành kinh tế và các doanh nghiệp trong việc xem xét sức khỏe nền kinh tế, là cơ sở đưa ra các quyết định về cơ chế chính sách, kế hoạch phát triển kinh tế - xã hội tại một quốc gia hoặc địa phương.
0	Trong bối cảnh có không ít các biến động, dự báo càng đóng vai trò quan trọng hơn nhằm phục vụ đánh giá cũng như quản lý vĩ mô nền kinh tế. Trong những năm gần đây, cùng với sự phát triển của khoa học máy tính, nhiều phương pháp dự báo mới trên cơ sở ứng dụng máy học đã ra đời. Cùng với đó, các nhà nghiên cứu cũng bắt đầu vận dụng các phương pháp này nhiều hơn trong lĩnh vực kinh tế. Theo Urrutia và các cộng sự [1], nhóm tác giả sử dụng đường trung bình động tích hợp tự động phục hồi (ARIMA) và Mạng thần kinh nhân tạo Bayes (BANN) để dự báo nhập khẩu và xuất khẩu của Philippines và việc so sánh hai mô hình là một trong những mục tiêu chính của nghiên cứu này.
0	Dữ liệu được thu thập từ Cơ quan thống kê Philippines với tổng số 100 quan sát từ quý đầu tiên từ năm 1993 đến quý 4 năm 2017. Hơn nữa, trong nghiên cứu này có thể xác định mức độ phù hợp nhất trong số các mô hình dự báo nhập khẩu và xuất khẩu của Philippines và các nhà nghiên cứu sẽ đưa ra giá trị dự báo của xuất nhập khẩu từ quý đầu tiên từ năm 2018 đến quý 4 năm 2022 sử dụng mô hình được trang bị phù hợp nhất. Các nhà nghiên cứu đã tiến hành một bài kiểm tra Thống kê để xây dựng và so sánh các mô hình thống kê của ARIMA và BANN cho nhập khẩu và xuất khẩu.
0	Sau đó áp dụng độ chính xác của dự báo chẳng hạn như Trung bình bình phương sai số (MSE), Trung bình bình phương sai số chuẩn hóa (NMSE), Trung bình sai số tuyệt đối (MAE), Căn bậc 2 của trung bình bình phương sai số (RMSE) và Trung bình phần trăm sai số tuyệt đối (MAPE) để so sánh hiệu suất của hai mô hình. Bằng cách so sánh các kết quả, các nhà nghiên cứu kết luận rằng Mạng nơron nhân tạo Bayes là mô hình phù hợp nhất trong việc dự báo nhập khẩu và xuất khẩu của Philippines.
0	Khi sử dụng phép thử T ghép đôi, giá trị p cho cả nhập khẩu và xuất khẩu đều lớn hơn mức ý nghĩa (α = 0,01) có nghĩa là không có sự khác biệt đáng kể giữa giá trị thực tế và giá trị dự đoán cho cả nhập khẩu và xuất khẩu của Philippines. Nghiên cứu này có thể giúp ích cho nền kinh tế của Philippines bằng cách xem xét các dự báo về Nhập khẩu và Xuất khẩu có thể được sử dụng trong việc phân tích thâm hụt thương mại của nền kinh tế. Theo Urrutia và các cộng sự [2], mục tiêu của nhà nghiên cứu là dự báo Tổng sản phẩm quốc nội (GDP) của Philippines từ Quý 1 năm 2018 đến Quý 4 năm 2022.
0	Hơn nữa, nghiên cứu này xác định mô hình phù hợp nhất giữa Đường trung bình động tích hợp tự động và Mạng thần kinh nhân tạo Bayes có thể dự báo GDP của Philippines. Nhà nghiên cứu đã sử dụng dữ liệu từ Quý 1 năm 1990 đến Quý 4 năm 2017 với tổng số 112 quan sát. Thử nghiệm thống kê được thực hiện trong nghiên cứu để có thể hình thành và so sánh mô hình thống kê ARIMA và Bayesian ANN. Trong nghiên cứu này, kết luận rằng ARIMA (1,1,1) và Bayesian ANN có thể dự báo GDP của Philippines.
0	Nhà nghiên cứu sử dụng độ chính xác của Dự báo như MSE, NMSE, MAE, RMSE và MAPE để so sánh hiệu suất của hai mô hình. Trong bài viết này, mô hình phù hợp nhất thu được là Bayesian ANN. Phép thử T ghép đôi kết luận rằng không có sự khác biệt đáng kể giữa giá trị thực tế và giá trị dự đoán. Nghiên cứu này giúp kinh tế học cụ thể trong việc dự báo kinh tế và phân tích kinh tế. Smalter và Cook [3] sử dụng mạng lưới thần kinh dựa trên một chỉ số kinh tế vĩ mô duy nhất - độ trễ hàng tháng của tỷ lệ thất nghiệp dân sự để dự báo tỷ lệ thất nghiệp 1-, 3-, 6- và 12 tháng tới bằng cách sử dụng số liệu thất nghiệp nhiều năm.
0	Mô hình của họ hoạt động tốt hơn SPF (Survey of Professional Forecators) trong khoảng thời gian ngắn (trước 1 và 3 tháng), nhưng không vượt trội hơn trong khoảng thời gian dài hơn 6 và 12 tháng. Stone [4] chỉ ra tính chất nhất quán của ước lượng KNN phi tham số. Mô hình này được sử dụng rộng rãi cho các tác vụ phân loại như nhận dạng đối tượng và do dễ triển khai và dễ giải thích nên nó cũng được sử dụng trong các ứng dụng như tính toán dữ liệu bị thiếu [5] và giảm tập dữ liệu đào tạo [6] để có thể nhận dạng tốt hơn các đối tượng tương tự.
0	KNN có thể nhận dạng các mẫu lặp lại trong chuỗi thời gian và vì lý do này, nó được áp dụng cho mô hình chuỗi thời gian tài chính như trong nghiên cứu của Ban và cộng sự [7]. Al-Qahtani và Crone [8] sử dụng KNN để dự báo nhu cầu điện của Vương quốc Anh và thấy rằng KNN có hiệu suất dự báo tốt hơn các mô hình chuẩn khác. Rodríguez-Vargas [9] thấy rằng KNN cũng vượt trội hơn hai mô hình học máy của đối thủ cạnh tranh là rừng ngẫu nhiên và tăng cường độ dốc cực độ, về độ chính xác trong việc dự đoán lạm phát. Nhìn chung, KNN đã được coi là một trong mười thuật toán hàng đầu trong khai thác dữ liệu [10].
0	Hơn nữa, KNN đặc biệt phù hợp với những trường hợp không có nhiều quan sát trong quá khứ, tức là rất ít thông tin trong quá khứ. Như đã chỉ ra trong nghiên cứu của Wauters và Vanhoucke [6], các phương pháp trí tuệ nhân tạo yêu cầu số lượng quan sát tối thiểu để hoạt động bình thường trong khi đối với KNN, hạn chế này không quá nghiêm ngặt mặc dù yêu cầu số lượng quan sát tối thiểu [11]. Có thể thấy, nhiều nghiên cứu ứng dụng các thuật toán học máy khác nhau để dự báo các chuỗi thời gian vĩ mô.
0	Tuy nhiên, đối với một số chỉ tiêu vĩ mô như GDP, xuất nhập khẩu, doanh thu du lịch, lạm phát, tỷ lệ thất nghiệp…, mô hình ANN cho kết quả dự báo chính xác hơn các mô hình học máy khác cũng như các mô hình kinh tế lượng khác. Các nghiên cứu khám phá tiềm năng của máy học trong lĩnh vực dự báo, mà điển hình, ANN có sự khác biệt giữa giá trị dự báo và giá trị thực tế không lớn. Nó không yêu cầu bất kỳ thông tin trước nào về phân phối và xác suất của dữ liệu, có thể học hỏi từ kinh nghiệm trong quá khứ và làm việc với dữ liệu không hoàn hảo và phi tuyến tính.
0	Tuy nhiên, hầu hết các nghiên cứu hiện nay đều xây dựng mô hình ANN ở cấp quốc gia và nghiên cứu cho cả nước cũng như các dự báo thường được sử dụng chuỗi dữ liệu theo năm. Như vậy, nghiên cứu ứng dụng mô hình ANN cho công tác dự báo một số chỉ tiêu kinh tế cho thành phố Hồ Chí Minh, đặc biệt là chỉ tiêu tốc độ tăng trưởng kinh tế theo quý vẫn chưa nhận được sự quan tâm của các nhà nghiên cứu. Việc lựa chọn mô hình có độ chính xác vượt trội để dự báo chỉ tiêu tốc độ tăng trưởng kinh tế thành phố theo quý có ý nghĩa thực tiễn rất cao vì dự báo theo niên độ quý sẽ giúp lãnh đạo thành phố đánh giá khả năng đạt mục tiêu tăng trưởng kinh tế đã đề ra trong năm.
0	Từ đó sẽ có những phản ứng chính sách kịp thời để thúc đẩy các động lực tăng trưởng kinh tế, nhằm hướng đến đạt mục tiêu tăng trưởng kinh tế theo như kế hoạch. Dữ liệu tốc độ tăng trưởng GRDP TP.HCM được lấy theo chu kỳ 3 tháng, 6 tháng, 9 tháng và 12 tháng của các năm trong giai đoạn 2011-2024. Dữ liệu được sử dụng để vận dụng mô hình được thu thập đến 9 tháng năm 2024. Dữ liệu tốc độ tăng trưởng kinh tế 12 tháng đã được Tổng cục Thống kê ước tính và được dùng để kiểm định nội mẫu.
0	Theo Bảng 1, có thể thấy dữ liệu chuỗi thời gian này không dừng. Do đó cần tiến hành lấy sai phân bậc 1 của chuỗi dữ liệu này để xem xét tính dừng của chuỗi sai phân bậc 1. Việc lấy sai phân của chuỗi thời gian sẽ giúp loại bỏ các tính xu thế, thời vụ hoặc chu kì của dữ liệu giúp dữ liệu xoay quanh giá trị trung bình của chuỗi. Ngoài ra, có thể sử dụng đồ thị ACF để xác định tính dừng của chuỗi dữ liệu. Đối với các chuỗi thời gian dừng, ACF sẽ giảm về giá trị 0 tương đối nhanh, trong khi đối với các chuỗi không dừng (nonstationary) quá trình giảm về 0 diễn ra một cách chậm chạp.
0	Hơn nữa, đối với các chuỗi không dừng giá trị tự tương quan r1 thường dương và lớn. Kết quả thống kê kiểm định Augmented Dickey-Fuller (ADF) của các chuỗi sai phân bậc 1 của biến quan tâm được thực hiện. Kết quả kiểm định thấy giá trị p nhỏ hơn 0,05, do đó chúng ta có thể bác bỏ giả thuyết H0. Điều này ngụ ý rằng chuỗi thời gian dừng. Nói một cách đơn giản, chúng ta có thể nói rằng chuỗi sai phân bậc 1 của tốc độ tăng trưởng GRDP của TP.HCM có cấu trúc không phụ thuộc vào thời gian và sở hữu phương sai không thay đổi theo thời gian.
0	Dữ liệu được chia thành hai tập: 80% cho huấn luyện mô hình và 20% còn lại để kiểm định khả năng dự báo của mô hình. Điều này giúp đảm bảo tính tổng quát của mô hình khi áp dụng vào thực tiễn. Trong nghiên cứu này, mô hình ANN được chúng tôi thiết kế để dự báo tốc độ tăng trưởng GRDP gồm 1 lớp đầu vào, 1 lớp ẩn và 1 đầu ra. Việc xây dựng mô hình ANN với 1 lớp ẩn, ở mức tối giản, giúp mô hình tổng quát tốt hơn trên dữ liệu chưa quan sát được. Đồng thời, thiết kế này giúp tránh các vấn đề quá khớp (overfitting).
0	Trong lớp ẩn này, số nơron xác định theo công thức của Fang và Ma [14], cụ thể: 𝑘 = log2 𝑛. Trong đó, n là số nơron trong lớp đầu vào, ở đây là 3. Như vậy, k sẽ nhận giá trị là 1,6. Tuy nhiên, để đảm bảo tính khách quan, chúng tôi sẽ cho số nơron trong lớp ẩn dao động trong đoạn [1,4] và lựa chọn mô hình có mức độ dự báo chính xác nhất. Cuối cùng, mô hình ANN có 1 lớp đầu ra là biến tăng trưởng GRDP của Thành phố Hồ Chí Minh. Lớp ẩn sử dụng hàm kích hoạt để tối ưu hóa quá trình học tập của mô hình.
0	Kết quả nghiên cứu cũng cho thấy rằng, trong bối cảnh có cú sốc kinh tế như khủng hoảng kinh tế, dịch bệnh, xung đột chính trị,… diễn ra như những năm gần đây, các mô hình dự báo truyền thống sẽ không thể phát huy hiệu quả do sự thay đổi hành vi bất thường của các chủ thể trong nền kinh tế. Mặt khác, mô hình ANN sẽ đưa ra các dự báo tốt hơn trong những trường hợp có cú sốc kinh tế do được phát triển không dựa trên các lý thuyết kinh tế và có thể được điều chỉnh theo bối cảnh thực tế. Bên cạnh đó, một ưu điểm vượt trội của mô hình ANN là khả năng tự học và điều chỉnh các kết quả dự báo phù hợp với thực tế mà không phụ thuộc vào ý kiến chủ quan.
0	Các phương pháp dự báo kinh tế vĩ mô đã được cải thiện đáng kể trong những năm qua. Các phương pháp hiện tại dựa trên các mô hình kinh tế lượng có khả năng tạo ra những dự báo tương đối chính xác về các chỉ tiêu kinh tế vĩ mô. Nhưng những cách tiếp cận này mang lại nhiều loại lỗi không mong muốn khác nhau, từ độ nhạy cao đối với đặc tả mô hình đến yêu cầu dữ liệu cao. Ngoài ra, các phương pháp dự báo dựa trên mô hình kinh tế lượng yêu cầu phải dựa trên mô hình lý thuyết của nền kinh tế được viết ra và được sử dụng làm cơ sở để hình thành một dự báo thực nghiệm.
0	Tuy nhiên, trong bối cảnh hiện nay, dữ liệu phục vụ cho công tác dự báo chưa đầy đủ, trong những tình huống có cú sốc kinh tế như khủng hoảng kinh tế, dịch bệnh, các mô hình dự báo truyền thống sẽ không thể phát huy hiệu quả do sự thay đổi hành vi bất thường của các chủ thể trong nền kinh tế. Nghiên cứu đã mô hình hóa và xây dựng mạng nơron nhân tạo, vận dụng một số mô hình kinh tế lượng, gồm mô hình Single Exponential Smooth, mô hình SARIMA, mô hình Single Moving Average, mô hình Holt-Winter’s Additive trong việc dự báo tốc độ tăng trưởng kinh tế thành phố theo định kỳ 3 tháng, 6 tháng, 9 tháng và 12 tháng để kiểm định độ tin cậy, tính chính xác của kết quả dự báo.
0	Kết quả nghiên cứu cho thấy mô hình ANN cho giá trị dự báo chính xác hơn. Với kết quả này, nghiên cứu sẽ cung cấp cơ sở cho lãnh đạo thành phố xem xét khả năng hoàn thành mục tiêu tăng trưởng đề ra trong năm, từ đó có những phản ứng chính sách phù hợp, đề ra các giải pháp để thúc đẩy các động lực tăng trưởng kinh tế nhằm đạt được mục tiêu tăng trưởng kinh tế theo kế hoạch. Mặc dù đã đạt được các mục tiêu mong muốn đề ra trong việc thiết kế và ước lượng mô hình ANN phục vụ cho mục đích dự báo tốc độ tăng trưởng GRDP của TP.HCM, tuy nhiên, nghiên cứu này không thể tránh khỏi một số hạn chế khách quan.
0	Việt Nam được đánh giá là quốc giá có tiềm năng lớn về năng lượng mặt trời. Đây là nguồn năng lượng thân thiện với môi trường và tái tạo trong thiên nhiên. Trong những năm gần đây, với nhiều cơ chế khuyến khích, các dự án điện năng lượng mặt trời phát triển mạnh ở nước ta, góp phần đảm bảo an ninh năng lượng quốc gia. Tuy nhiên với sự phát triển nhanh kể cả về số lượng dự án, tổng công suất lắp đặt và sản lượng điện của các nhà máy điện mặt trời nên công tác vận hành hệ thống điện gặp khó khăn.
0	Để có cơ sở phối hợp vận hành hợp lý các nguồn điện trong hệ thống cần có công tác dự báo công suất cũng như sản lượng điện của các nhà máy điện mặt trời. Nghiên cứu này đã ứng dụng mạng nơron nhân tạo dự báo sản lượng điện của nhà máy điện mặt trời dựa trên các yếu tồ về thời tiết. Nghiên cứu được ứng dụng dự báo điện lượng ngày của nhà máy điện mặt trời công suất 752KWp tại tỉnh Hưng Yên cho kết quả tin cậy. Kết quả nghiên cứu cung cấp một phương pháp hữu ích trong dự báo sản lượng điện của các nhà máy điện mặt trời, góp phần xây dựng chế độ vận hành hợp lý cho hệ thống điện.
0	Những năm qua, dưới tác động nghiêm trọng từ biến đổi khí hậu, Việt Nam đã chuyển hướng phát triển mạnh ngành năng lượng tái tạo. Các nguồn năng lượng gió và năng lượng mặt trời có nhiều cơ chế khuyến khích để phát triển. Đến thời điểm hiện tại, tổng công suất lắp đặt về điện mặt trời trên cả nước đã đạt tới 19.400 MWp (trong đó có gần 9.300 MWp là điện mặt trời mái nhà), tương ứng 16.500 MW, chiếm khoảng 25% tổng công suất lắp đặt nguồn điện của hệ thống điện quốc gia. Theo dự thảo Quy hoạch điện 8, đến năm 2030 tổng công suất lắp đặt điện mặt trời khoảng 22.040 MW và đến năm 2045 công suất lắp đặt điện mặt trời đạt khoảng 63.640 MW (Dự thảo Quy hoạch phát triển điện quốc gia, 2021).
0	Các dự án điện mặt trời đã đóng góp lớn vào việc đảm bảo an ninh năng lượng quốc gia, góp phần phát triển đất nước. Tuy nhiên với sự phát triển nhanh kể cả về số lượng dự án, tổng công suất lắp đặt và sản lượng điện nên công tác vận hành hệ thống điện gặp khó khăn. Để có cơ sở phối hợp vận hành hợp lý các nguồn điện trong hệ thống cần có công tác dự báo công suất cũng như sản lượng điện của các nhà máy điện trong hệ thống, đặc biệt là công tác dự báo ngắn và trung hạn về sản lượng điện của các nhà máy điện mặt trời.
0	Công suất phát và điện lượng của nhà máy điện mặt trời phụ thuộc vào bức xạ chiếu xuống bề mặt tấm pin năng lượng. Sự biến thiên bức xạ chiếu xuống tấm pin phụ thuộc rất lớn vào các yếu tố khí hậu như nhiệt độ không khí, chế độ mưa, chế độ về mây phủ, số giờ nắng …vv. Do đó có thể dự báo điện năng của nhà máy điện mặt trời dựa trên các yếu tố về thời tiết. Các số liệu về thời tiết như nhiệt độ không khí, số giờ mưa, chế độ mây phủ được lưu trữ ở các trạm khí tượng hoặc các trang web dự báo uy tín.
0	Trong nghiên cứu này, các tác giả sử dụng phương pháp mạng nơ ron nhân tạo (ANN) dự báo sản lượng điện của nhà máy điện mặt trời dựa trên số liệu về nhiệt độ không khí, số giờ mưa, số giờ mây phủ. Nghiên cứu được áp dụng dự báo sản lượng điện cho nhà máy điện mặt trời áp mái công suất 752,4kWp tại huyện Tiên Lữ, tỉnh Hưng Yên. Đối với công trình đã lắp đặt, sản lượng điện phát được trong thời đoạn i phụ thuộc vào tổng xạ chiếu xuống bề mặt tấm pin. Tổng xạ này phụ thuộc rất nhiều vào điều kiện thời tiết như điều kiện về nắng, mưa, sương mù vv… nên công suất phát và sản lượng điện của nhà máy điện mặt trời cũng phụ thuộc vào các yếu tố thời tiết.
0	Từ đó có thể sử dụng mạng nơron nhân tạo để dự báo sản lượng điện phát được trong ngày của nhà máy điện mặt trời dựa trên các yếu tố thời tiết được dự báo.Mạng nơron nhân tạo, Artificial Neural Network (ANN), gọi tắt là mạng nơron là một mô hình xử lý thông tin phỏng theo cách thức xử lý thông tin của các hệ nơron sinh học. Nó được tạo nên từ một số lượng lớn các phần tử (gọi là phần tử xử lý hay nơron) kết nối với nhau thông qua các liên kết (gọi là trọng số liên kết) làm việc như một thể thống nhất để giải quyết một vấn đề cụ thể.
0	Trong những thập niên gần đây, trên thế giới cũng như nước ta đã ứng dụng mô hình toán ANN vào giải quyết các bài toán dự báo cho kết quả đáng tin cậy. Mạng thần kinh nhân tạo có ưu điểm là khả năng học và xử lý chuỗi số liệu đầu vào, tận dụng triệt để được các thông tin dữ liệu đo đạc hiện có trong quá khứ, dựa vào mối quan hệ giữa các dữ liệu đầu vào để từ đó đưa ra các dự báo. Trong các mô hình ANN thì mô hình Mạng thần kinh nhân tạo truyền thẳng nhiều lớp (Multilayer Perceptron - MLP) được sử dụng phổ biến để giải quyết các bài toàn phi tuyến, phức tạp, khi mà mối quan hệ giữa các quá trình không dễ thiết lập một cách tường minh.
0	Cấu trúc của mạng ANN truyền thẳng MPL gồm lớp biến đầu vào input, lớp kết quả đầu ra output và các lớp ẩn hidden. (Vũ Hữu Tiệp, 2018). Các trọng số liên kết giữa các nơron của các lớp trong ANN được xác định qua quá trình luyện mạng (học) từ dữ liệu quá khứ. Quá trình học là quá trình cập nhật trọng số liên kết sao cho giá trị hàm lỗi (sai số) là nhỏ nhất. Một mạng nơron được huấn luyện sao cho với một tập các vec-tơ đầu vào X, mạng có khả năng tạo ra tập các vec-tơ đầu ra mong muốn Y của nó.
0	Tập X được sử dụng cho huấn luyện mạng được gọi là tập huấn luyện (training set). Các phần tử x thuộc X được gọi là các mẫu huấn luyện (training example). Quá trình huấn luyện bản chất là sự thay đổi các trọng số liên kết của mạng. Trong quá trình này, các trọng số của mạng sẽ hội tụ dần tới các giá trị sao cho với mỗi vec-tơ đầu vào x từ tập huấn luyện, mạng sẽ cho ra vec-tơ đầu ra y có sái số so với giá trị thực đo nhỏ nhất. Có nhiều thuật giải để xác định các trọng số liên kết trong đó thuật giải lan truyền ngược (back-propagation algorithm) được ứng dụng rất phổ biến.
0	Sau khi xác định được các trọng số, mạng ANN sẽ được sử dụng dự báo trên tập số liệu kiểm định (testing set). Độ chính xác của kết quả dự báo điện lượng được đánh giá thông qua trị số sai số quân phương (RMSE) và phần trăm sai số tuyệt đối trung bình (MAPE). Sử dụng mạng nơron nhân tạo dự báo sản lượng điện trung bình ngày dựa trên yêu tố thời tiết là nhiệt độ trung bình ngày, số giờ mưa và số giờ mây phủ trong ngày của nhà máy điện mặt trời áp mái công suất 752,4kWp tại huyện Tiên Lữ, tỉnh Hưng Yên.
0	Nhà máy có thông số cơ bản như sau: Thông số chung của nhà máy: Công suất lắp máy 752,4kWp; Cường độ bức xạ trung bình: 3,83 kWh/m²/ngày.; Điện lượng trung bình năm: 857.500 kWh/năm.; Hiệu suất hệ thống (PR): 80,1%. Pin năng lượng mặt trời: Loại pin mặt trời: Đơn c-Si. Công suất tấm pin: 440 Wp. Số lượng tấm pin: 1710 Tấm. Hiệu suất tấm pin: 19,8%. Inverter: Loại máy inverter: String. Số lượng inverter: 06. Công suất inverter: 110kWac. Hiệu suất inverter: 98,7%. Trạm biến áp: Công suất máy biến áp: 750kVA. Điện áp: 0,4/22kV. Số liệu sử dụng là số liệu quan sát từ ngày 01/01/2021 đến ngày 25/8/2021 của nhiệt độ trung bình ngày, số giờ mưa ngày, số giờ mây phủ trong ngày và sản lượng điện trung bình ngày.
0	Theo quan sát, nhà máy điện mặt trời bắt đầu phát điện từ khoảng 6 giờ sáng đến 6 giờ chiều nên để tránh gây nhiễu cho mô hình, các số liệu sử dụng tính toán chỉ lấy trong khoảng thời gian từ 6 giờ sáng đến 6 giờ chiều. Do đặc điểm khí hậu miền Bắc rất khác nhau giữa các mùa nên trong tính toán luyện mô hình ANN, các số liệu đầu vào cũng được phân theo mùa để tính. Số liệu về nhiệt độ trung bình ngày, số giờ mưa, số giờ có mây phủ từ ngày 01/01/2021 đến tháng 25/8/2021 được lấy từ website https://www.wunderground.com/. Điện lượng trung bình ngày từ ngày 01/01/2021 đến ngày 25/8/2021 được lấy từ số liệu vận hành của nhà máy có trong website: https://www.isolarcloud.com.
0	Sử dụng các số liệu thời tiết và điện lương từ tháng 1 đến tháng 7 để luyện mạng ANN bằng ngôn ngữ R (Bradley Boehmke, 2020) với cấu trúc mạng gồm lớp đầu vào là các thông số về nhiệt độ trung bình ngày, số giờ mưa, số giờ có mây phủ; lớp đầu ra là điện lượng phát được trong ngày; 02 lớp ẩn. Kết quả tính toán các trọng số liên kết cho trong hình 7. Ứng dụng mô hình trên dự báo điện lượng ngày cho nhà máy điện mặt trời 752.4kWp tại tỉnh Hưng Yên để kiểm định kết quả tính toán của mô hình. Bộ dữ liệu tính toán kiểm định là dữ liệu về thời tiết và điện lượng của 25 ngày đầu tháng 8 năm 2021.
0	Kết quả dự báo điện lượng có: Sai số quân phương: RMSE = 0.29MWh, Phần trăm sai số tuyệt đối trung bình: MAPE = 7,7%. Kết quả tính toán cho thấy giá trị dự báo và giá trị thực tế của điện lượng sai số không lớn, có thể dùng kết quả dự báo phục vụ công tác vận hành của nhà máy điện mặt trời cũng như hệ thống điện. Ứng dụng mạng nơron nhân tạo dự báo sản lượng điện trung bình ngày thông qua yếu tố dự báo thời tiết là nhiệt độ trung bình ngày, số giờ mưa, số giờ mây phủ đồng thời kể đến yếu tố ảnh hưởng của thời tiết theo mùa cho kết quả phù hợp.
0	Do nhà máy điện mặt trời công suất 752,4KWp tại tỉnh Hưng Yên mới đi vào hoạt động từ tháng 1 năm 2021 nên bộ số liệu sử dụng để dự báo còn khá ít. Trong quá trình làm việc của các nhà máy điện mặt trời, bộ dữ liệu dùng để dự báo sản lượng điện thường xuyên được cập nhật nên kết quả dự báo sẽ ngày càng tin cậy. Dự báo sản lượng điện của các nhà máy điện mặt trời ứng dụng mạng thần kinh nhân tạo có ý nghĩa quan trọng, góp phần cải thiện chế độ vận hành các nhà máy điện mặt trời cũng như toàn bộ hệ thống điện của nước ta.
0	Ung thư vú là bệnh ung thư phổ biến nhất trên nữ giới và là nguyên nhân gây tử vong hàng đầu do ung thư trên toàn thế giới. Cơ sở chính để điều trị ung thư vú là chẩn đoán mô bệnh học, việc chẩn đoán này quyết định hướng điều trị và tiên lượng bệnh. Những tiến bộ trong trí tuệ nhân tạo (AI) cùng với việc vận dụng giải phẫu bệnh kỹ thuật số đã đưa ra một cách tiếp cận đầy hứa hẹn trong việc chẩn đoán, phân loại ung thư vú, đáp ứng được nhu cầu trong thực tế lâm sàng. Trong bài viết này, chúng tôi tổng quan về ứng dụng AI trong chẩn đoán ung thư vú dựa trên ảnh kỹ thuật số hóa giải phẫu bệnh, đồng thời phác thảo những tiềm năng ứng dụng tại Việt Nam.
0	Ung thư vú là một trong những bệnh ung thư phổ biến nhất được chẩn đoán ở phụ nữ trên toàn thế giới và nó là nguyên nhân chính gây tử vong ở phụ nữ. Ở các nước thu nhập thấp và thu nhập trung bình, tỷ lệ tử vong tương đối cao so với các quốc gia phát triển. Theo báo cáo năm 2018 của Tổ chức Nghiên cứu Quốc tế về Ung thư (IARC), hơn 2 triệu phụ nữ trong năm 2018 được chẩn đoán mắc bệnh ung thư vú trên toàn thế giới. Đây được coi là khoảng 11,6% của tất cả các bệnh ung thư được chẩn đoán trong cùng một năm với 626 679 trường hợp tử vong được báo cáo.
0	Nhiều nghiên cứu đã ước tính rằng vào năm 2025 sẽ có 19,3 triệu ca ung thư mới. Hơn nữa, ở các nước đang phát triển bao gồm cả Việt Nam, dân số đông đúc và người bệnh còn hiểu biết hạn chế về các triệu chứng bệnh và tìm kiếm sự tư vấn y khoa khi đã ở giai đoạn muộn dẫn đến tỷ lệ tử vong cao hơn. Ngoài ra, sự thiếu hụt các chuyên gia y tế, đặc biệt là các chuyên gia ở khu vực nông thôn, vùng sâu, vùng xa làm tăng thêm vấn đề chẩn đoán sớm và chính xác ung thư vú và góp phần gây ra tỷ lệ tử vong cao hơn.
0	Do vậy việc sử dụng công nghệ thông tin và dữ liệu y tế để xây dựng hệ thống thông minh hay trí tuệ nhân tạo (AI) có thể bắt chước suy luận của bác sĩ là một giải pháp để có thể phát hiện sớm ung thư vú và từ đó làm tăng cơ hội điều trị và giảm tỷ lệ mắc bệnh. Chẩn đoán dựa trên đánh giá mô bệnh học là phương pháp xác định ung thư chính xác nhất. Biện giải kết quả chẩn đoán hình ảnh phụ thuộc vào chuyên gia nhất định và đòi hỏi tính chuyên môn cao, do đó việc sử dụng công nghệ thông tin là cần thiết để tăng tốc và nâng cao tính chính xác của chẩn đoán, cung cấp ý kiến thứ hai cho các bác sĩ chuyên môn.
0	Với lý do nêu trên, chúng tôi tiến hành tổng quan về vận dụng AI trong chẩn đoán ung thư vú trên thế giới và qua đó đề xuất về tiềm năng xây dựng một hệ thống thông minh hỗ trợ chẩn đoán ung thư vú bằng ảnh giải phẫu bệnh tại Việt Nam. Các phương pháp hiện đại để điều trị ung thư vú đòi hỏi phải phân tầng chẩn đoán cẩn thận bệnh nhân và dự đoán khả năng điều trị phù hợp. Sự phân tầng điều trị này chủ yếu dựa trên việc đọc và giải thích thủ công các mẫu lam kính (slide) giải phẫu bệnh (GPB) – đây là một quá trình mất khá nhiều thời gian với độ biến thiên đáng kể đối với kết quả đọc giữa các bác sĩ trực tiếp đọc.
0	Xu hướng số hóa trong lĩnh vực giải phẫu bệnh học mở ra cơ hội cho các giải pháp phân tích hình ảnh dựa trên máy tính có tiềm năng có thể đưa ra được đánh giá slide GPB một cách khách quan hơn và mang tính định lượng. Trong nhiều thập kỷ qua, do những tiến bộ về thuật toán, sức mạnh tính toán dễ tiếp cận hơn và việc sắp xếp các bộ dữ liệu lớn, các kỹ thuật máy học đã đưa ra định nghĩa tiên tiến trong nhiều nhiệm vụ thị giác máy tính - bao gồm nhiều ứng dụng chăm sóc sức khỏe.
0	Đồng thời, giải phẫu bệnh kỹ thuật số hóa đã và đang nổi lên như một phương pháp nhằm tái tạo ảnh và xử lý hình ảnh phóng đại lớn của các slide GPB – từ áp dụng ban đầu cho mục đích nghiên cứu nhưng ngày càng có xu hướng hoàn thiện trở thành một công cụ hữu ích trên lâm sàng. Gần đây, hai lĩnh vực này đã giao thoa giữa các nhà khoa học máy tính và nhà nghiên cứu bệnh học và họ đã cùng nhau áp dụng các kỹ thuật AI mới nhất cho vấn đề giải phẫu bệnh lý cho các mục đích chẩn đoán Ung thư, và sâu hơn nữa là tiên lượng, tiên đoán điều trị và các mục đích lâm sàng khác, bên cạnh việc ứng dụng trực tiếp công nghệ cao để cải thiện năng suất và hiệu quả của quy trình chẩn đoán.
0	Nhiều vấn đề trong bệnh học ung thư vú liên quan đến việc đánh giá các đặc điểm hình thái của mô. Tuy nhiên, điều này thường không đơn giản và nghiên cứu quan trọng đã đi vào việc cải thiện độ tin cậy và giảm sự biến thiên của việc đánh giá. Vấn đề liên quan tới độ tin cậy và tính biến thiên có tiềm năng được giải quyết triệt để bằng các phương pháp dùng thuật toán trên máy tính. Sau khi được huấn luyện (training), các thuật toán luôn cho kết quả thống nhất khi có cùng một dữ liệu đầu vào (input) được cung cấp.
0	Vào giữa những năm 1990, những tiến bộ trong hệ thống tạo hình ảnh và phần mềm dành cho kính hiển vi để có thể lưu trữ, phục vụ và xem được hình ảnh kích thước lớn (hình ảnh toàn bộ slide trung bình được quét ở độ phóng đại 40x có dung lượng lớn hơn 1 GB) dẫn đến sự phát triển của kỹ thuật tạo hình ảnh toàn bộ slide (WSI-Whole Slide Image). Những kỹ thuật này cho phép toàn bộ slide (chứ không phải các trường nhìn riêng lẻ) được số hóa và kiểm tra ở độ phân giải tương đương với kính hiển vi trường sáng.
0	Những tiến bộ sâu hơn trong những thập kỷ tiếp theo đã đưa giải phẫu bệnh kỹ thuật số hóa từ một chủ đề nghiên cứu chật hẹp để tiến đến trở thành một khía cạnh quan trọng được chấp nhận áp dụng trong thực hành lâm sàng. Từ khá sớm đã có một so sánh quy mô lớn về hiệu suất chẩn đoán giữa sử dụng giải phẫu bệnh kỹ thuật số hóa và sử dụng kính hiển vi thông thường được tiến hành bởi Mukhopadhyay và cộng sự và bao gồm các mẫu bệnh phẩm từ năm 1992 bệnh nhân với các loại khối u khác nhau được đọc bởi 16 bác sĩ bệnh học [2].
0	Nghiên cứu cho thấy hiệu suất chẩn đoán với các WSI được số hóa gần bằng với các phương pháp dựa trên kính hiển vi truyền thống (với tỷ lệ chênh lệch chính từ tiêu chuẩn tham chiếu là 4,9% đối với WSI và 4,6% đối với phương pháp vi mô). Nghiên cứu này được sử dụng làm nghiên cứu cốt lõi cho sự chấp thuận của FDA đối với hệ thống giải phẫu bệnh kỹ thuật số hóa của hãng Philips. Tương tự, để theo đuổi sự chấp thuận của FDA cho hệ thống Wper Aperio AT2 DX, Leica Biosystems đã tiến hành một thử nghiệm lâm sàng trên năm địa điểm nghiên cứu liên quan đến hơn 16.000 slide và tìm thấy 97,9% sự phù hợp trong hệ thống (tức là chấp thuận giữa đọc kết quả từ lam kính và đọc hình ảnh kỹ thuật số tại bất kỳ một trung tâm nào).
0	Tác giả Williams và cộng sự đã thực hiện xác nhận lâm sàng việc chẩn đoán ung thư vú từ các ảnh slide kỹ thuật số và tìm thấy sự phù hợp hoàn toàn giữa kết quả đọc trên lam kính và đọc kỹ thuật số trong 98,8% của 694 trường hợp bởi các bác sĩ giải phẫu bệnh chuyên khoa vú đã được học một khóa đào tạo ngắn về giải phẫu bệnh kỹ thuật số hóa. Và chính nhóm tác giả đó đã thực hiện một phân tích hệ thống với cỡ mẫu là 8069 so sánh giữa việc đọc trên lam kính và đọc bằng kỹ thuật số và đã chỉ ra các chẩn đoán trái ngược trong 335 trường hợp (4%) [3].
0	Một chủ đề nhất quán trong các đánh giá về bệnh lý kỹ thuật số, cả về độ chính xác và hiệu quả chẩn đoán, là việc triển khai thành công phụ thuộc vào thiết kế huấn luyện phù hợp và tích hợp với quy trình công việc hiện có. Ngoài các tác động trực tiếp đến hiệu quả về mặt thời gian trên mỗi lần đọc slide, các lợi ích tiềm năng khác của việc áp dụng giải phẫu bệnh kỹ thuật số hóa. Bao gồm giảm nguy cơ xác định sai bệnh nhân và slide, giảm nguy cơ mất mô hoặc hỏng-mất chất lượng, theo dõi bệnh nhân và phân bổ khối lượng công việc tốt hơn.
0	Dễ dàng lấy lại các trường hợp đã được lưu trữ, và cải thiện đánh giá giải phẫu bệnh từ xa, bên cạnh việc tạo điều kiện cho việc hỗ trợ chéo giữa các bệnh viện khác nhau trong việc chẩn đoán ban đầu, báo cáo được thực hiện từ xa tới các phòng labo giải phẫu bệnh được tập trung hóa. Tuy nhiên, một trong những lợi thế quan trọng nhất của WSI đối với chẩn đoán chính là khả năng áp dụng các thuật toán dựa trên AI khác nhau trong quy trình chẩn đoán thông thường.
0	Trong chẩn đoán ung thư, việc bác sĩ giải phẫu bệnh đánh giá bệnh lý dựa trên mô bệnh học phẫu thuật thông qua các ảnh giải phẫu bệnh để được coi là một tiêu chuẩn vàng. Việc chẩn đoán này rất tốn thời gian. Hơn nữa, nó phụ thuộc nhiều vào đánh giá chủ quan, và do đó có sự khác biệt đáng kể giữa các bác sĩ giải phẫu bệnh trong việc đánh giá chẩn đoán. Nghiên cứu của Ozkan và cộng sự báo cáo rằng hai bác sĩ giải phẫu bệnh không thống nhất về sự hiện diện của ung thư ở 31 trong số 407 sinh thiết kim lõi và sự phù hợp tổng thể của điểm Gleason được đánh giá chỉ là 51,7%, mô tả những thách thức trong chẩn đoán ung thư tuyến tiền liệt một cách nhất quán [4].
0	Do đó, việc phát triển các công cụ có sự hỗ trợ của máy tính là rất quan trọng để tiết kiệm thời gian, tăng độ chính xác và nâng cao tiêu chuẩn hóa trong chẩn đoán cho các nhà giải phẫu bệnh. Đã có sự nỗ lực và thành tựu đáng kể trong việc phát triển xử lý hình ảnh kỹ thuật số và các phương pháp dựa trên máy học để phân tích tự động các hình ảnh bệnh lý để thực hiện phân loại mô và phân loại bệnh, cũng như dự đoán kết quả bệnh và nâng cao độ chính xác của y học. Những tiến bộ gần đây trong nghiên cứu máy học liên quan đến mạng nơ-ron sâu, tức là học sâu (deep learning), đã làm tăng thành công hiệu suất của các phân tích như vậy.
0	Tuy nhiên, các mô hình học sâu được đề xuất thường yêu cầu lượng dữ liệu chú thích đáng kể để được đào tạo thành công. Vì kích thước nhóm thuần tập (cohort) có thể nhỏ và việc chú thích hình ảnh mô bệnh học rất mất thời gian, một khái niệm được gọi là học chuyển giao (transfer learning), tức là đào tạo mạng nơ-ron với tập dữ liệu bên ngoài và sau đó tinh chỉnh mô hình với tập dữ liệu có sẵn, có thể chứng tỏ mang lại lợi ích. Cách tiếp cận như vậy để tinh chỉnh một mô hình được đào tạo trước đã được chứng minh là có hiệu quả tốt hơn việc đào tạo cùng một kiến trúc mạng nơ-ron từ đầu trong các nghiên cứu liên quan đến phân tích hình ảnh bệnh lý kỹ thuật số.
0	Học chuyển giao có nhiều thuận lợi đối với việc tương thích khi mà các hình ảnh thu được bằng loại kính hiển vi hoặc quy trình nhuộm khác nhau. Một trong những thách thức chính trong giải phẫu bệnh kỹ thuật số là kích thước tuyệt đối của WSI. Một hình ảnh thu được ở mức phóng đại 20X có thể chứa vài tỷ pixel, trong khi vùng quan tâm (Region of Interest - ROI) có thể nhỏ chỉ vài nghìn pixel. Để áp dụng bộ phân loại học sâu, WSI phải được chia thành nhiều nghìn ô, với bộ phân loại (classifier) sau đó được áp dụng độc lập trên mỗi ô.
0	Bước đầu tiên quan trọng trong quá trình chẩn đoán nghi ngờ ung thư vú là phát hiện các tế bào khối u xâm lấn, đặc tính của khối u và định lượng mức độ khối u (tumor). Cruz-Roa và cộng sự đã xây dựng mô hình mạng nơ-ron tích chập (CNN - convolutional neural networks) để phân loại các mảng hình ảnh trích ra từ WSI của ung thư vú có chứa ung thư loại ống xâm nhập hay không [5]. Nhóm tác giả đã sử dụng việc gán nhãn vùng chú thích thủ công đối với 400 mẫu slide từ nhiều cơ sở khác nhau để huấn luyện mô hình của mình và xác thực hiệu suất của nó trên 200 slide với chú thích tương tự từ The Cancer Genome Atlas.
0	Tác giả báo cáo chỉ số F1 score (một loại chỉ số đo lường độ chính xác của mô hình tương tự ROC và AUC) ở mức pixel là 75,86%. Han và cộng sự [6] đã sử dụng bộ dữ liệu BreaKHis để huấn luyện một bộ phân loại có thể phân biệt giữa tám loại khối u vú lành tính và ác tính với độ chính xác 93,2%. Mô hình của họ được tiền xử lý trên imagenet và họ sử dụng dữ liệu mở rộng để ngăn chặn việc mô hình quá khớp (overfitting) của dữ liệu. Một trong những ứng dụng chẩn đoán sử dụng học máy (machine learning) nổi bật cho bệnh ung thư vú là chẩn đoán di căn hạch.
0	Bejnori và cộng sự báo cáo về hiệu suất của bảy thuật toán học sâu được phát triển như một phần của cuộc cạnh tranh đầy thách thức; các thuật toán đã được tìm thấy vượt trội hơn một nhóm gồm 11 chuyên gia GPB trong một bối cảnh chẩn đoán hạn chế về thời gian được mô phỏng[7]. Trên cơ sở dữ liệu huấn luyện bao gồm 270 hình ảnh từ hai trung tâm có (n = 110) và không có (n = 160) di căn hạch, và đánh giá trên một bộ ảnh độc lập gồm 129 hình ảnh (49 có và 80 không có di căn), AUC của thuật toán tốt nhất là 0,99, trong khi hiệu suất tốt nhất của một chuyên gia GPB đạt được AUC là 0,88.
0	Trong một nghiên cứu tương tự có 6 chuyên gia GPB đã xem xét 70 slide được số hóa có và không có sự trợ giúp học máy với thời gian xem xét được sử dụng làm một kết điểm chính, thời gian xem xét trung bình ngắn hơn đáng kể với có hỗ trợ hơn là không có hỗ trợ của học máy cho ra kết luận hình ảnh vi di căn (nhanh hơn 1,9 lần) và hình ảnh mà không có bất kỳ di căn nào (nhanh hơn 1,2 lần). Ngoài việc xác định khối u, các phương pháp AI và học máy đã được sử dụng để mô tả sự xâm lấn của khối u vú và phân loại mô học của ung thư vú (do mức độ phù hợp tương đối thấp đối với các đánh giá mang tính chủ quan mặc dù nó có giá trị tiên lượng quan trọng).
0	Các đặc điểm khác nhau ở cấp độ tế bào và cấp độ mô có thể được xác định để mô tả cấu trúc hình thái của các đối tượng để phân biệt giữa các thành phần mô học, ví dụ, tế bào khối u/biểu mô (đối với sự hình thành ống) và tế bào phân bào (cho đếm số lượng phân bào). Tuy nhiên, hầu hết các công việc trong lĩnh vực này tập trung vào phát hiện sự phân bào, đây là cái có giá trị tiên lượng nhất nhưng cũng tiêu tốn nhiều công sức nhất. Năm 2013, Veta và cộng sự đề xuất một thách thức phát hiện sự phân bào tại cộng đồng với một bộ dữ liệu chứa bộ 12 slide huấn luyện (training), 11 slide để kiểm tra (testing) và khoảng 1000 số liệu phân bào đã được chú thích.
0	Người chiến thắng trong thử thách này đã sử dụng mạng CNN với 10 lớp để đạt được 0,61 điểm F1 tổng (F1 score - một chỉ số đánh giá độ tin cậy hay được sử dụng trong học máy) so với sự đồng thuận của các chuyên gia GPB, trong khi đó, chuyên gia GPB đạt được > 0,75 điểm F1 tổng. Năm 2019, Veta và cộng sự đã công bố kết quả một thử thách tiếp theo (TUPAC16 challenge) tập trung vào đánh giá sự phân bào trên slide GPB (liên quan tới đánh giá độ mô học của khối u)[8].
0	Hệ thống chiến thắng trong thử thách này đã đạt được mức kappa Cohen (chỉ số đánh giá mức độ tương đồng) là 0,56 so với kết quả các chuyên gia GPB và đạt được điểm F1 = 0,65 về chẩn đoán độ biệt hóa của khối u. Do bản chất của việc mất nhiều công sức trong việc đếm số lượng phân bào có thể dẫn đến mức độ tương đồng thấp giữa các chuyên gia. Nhuộm Phospho-Histone H3 (PHH3), công đoạn giúp phát hiện phân bào ở độ nhạy cao, là phương pháp hóa mô miễn dịch đã được áp dụng để giải quyết bài toán này.
0	Tellez và cộng sự sử dụng máy scan kết nối tới các mẫu nhuộm PHH3 và nhuộm H&E để tạo chú thích (annotation) cho hệ CNN. Vì là làm với mẫu PHH3, các nhà nghiên cứu đã phải thu thập lên tới hơn 22.000 chú thích từ khoảng 100 slide. Hệ CNN này đã không đạt chiến thắng trên TUPAC16 challenge, nguyên nhân là có thể gây ra bởi sự biến thiên của chú thích. Tuy nhiên, công việc tiếp theo của các tác giả này cho thấy rằng việc sử dụng hệ CNN để phát hiện phân bào giống như như một trợ lý hỗ trợ có thể giúp cải thiện mức độ tương đồng kết quả chẩn đoán giữa các chuyên gia GPB [9].
0	Sự hình thành vi ống và cấp độ hạt nhân là hai thành phần quan trọng khác trong phân loại mô bệnh học của ung thư vú. Tuy nhiên, các phương pháp hoàn toàn tự động cho hai nhiệm vụ này vẫn đang được phát triển. Công việc được công bố hiện tại tập trung vào phân tích cấu trúc mô có thể được sử dụng cho các nhiệm vụ này. Romo-Bucheli và cộng sự đã huấn luyện một mô hình CNN để phát hiện các hạt nhân vi ống và tính toán số liệu thống kê về các hạt nhân để dự đoán các phân loại rủi ro Oncotype DX. Veta và cộng sự đề xuất một loạt các thuật toán phi CNN để phân đoạn và phát hiện hạt nhân. Những phân đoạn này sau đó được sử dụng để phát hiện hạt nhân để phân tích hình thái sâu hơn nữa [9].
1	Sự phát triển mạnh mẽ của học máy đã mở ra nhiều cơ hội mới trong lĩnh vực y tế, đặc biệt là chẩn đoán bệnh sớm và hỗ trợ quyết định lâm sàng. Nghiên cứu này tập trung phân tích hiệu quả của các mô hình học máy giám sát, bao gồm Random Forest, Support Vector Machine và mạng nơ-ron sâu, trong bài toán chẩn đoán bệnh tim mạch dựa trên dữ liệu lâm sàng. Tập dữ liệu gồm 12.000 hồ sơ bệnh nhân được thu thập từ ba bệnh viện lớn trong giai đoạn 2019–2023. Kết quả cho thấy mô hình mạng nơ-ron sâu đạt độ chính xác trung bình 92,4%, vượt trội so với các phương pháp truyền thống. Nghiên cứu khẳng định tiềm năng của học máy trong việc nâng cao chất lượng chăm sóc sức khỏe và giảm tải cho đội ngũ y tế.
1	Bệnh tim mạch là một trong những nguyên nhân gây tử vong hàng đầu trên thế giới, chiếm khoảng 32% tổng số ca tử vong toàn cầu theo báo cáo của WHO năm 2022. Việc chẩn đoán sớm đóng vai trò then chốt trong việc giảm tỷ lệ tử vong và chi phí điều trị. Tuy nhiên, các phương pháp chẩn đoán truyền thống phụ thuộc nhiều vào kinh nghiệm bác sĩ và các xét nghiệm tốn kém thời gian. Trong bối cảnh đó, học máy nổi lên như một công cụ hiệu quả để khai thác dữ liệu y tế lớn và phát hiện các mẫu tiềm ẩn mà con người khó nhận ra. Nghiên cứu này được thực hiện nhằm đánh giá khả năng ứng dụng thực tế của các mô hình học máy trong hỗ trợ chẩn đoán bệnh tim mạch.
1	Dữ liệu sử dụng trong nghiên cứu bao gồm 12.000 bản ghi bệnh nhân với 18 thuộc tính, như tuổi, huyết áp, cholesterol, điện tâm đồ và tiền sử bệnh lý. Dữ liệu được tiền xử lý bằng cách loại bỏ giá trị thiếu, chuẩn hóa theo z-score và cân bằng lớp bằng kỹ thuật SMOTE. Ba mô hình học máy được huấn luyện và so sánh gồm Random Forest với 200 cây, SVM sử dụng kernel RBF và mạng nơ-ron sâu gồm 4 lớp ẩn. Quá trình huấn luyện được thực hiện trên tập TRAIN chiếm 80% dữ liệu, phần còn lại dùng để kiểm thử. Các chỉ số đánh giá bao gồm độ chính xác, độ nhạy, độ đặc hiệu và AUC-ROC.
1	Kết quả thực nghiệm cho thấy mô hình mạng nơ-ron sâu đạt hiệu năng cao nhất với độ chính xác 92,4%, độ nhạy 91,1% và AUC-ROC đạt 0,95. Trong khi đó, Random Forest đạt độ chính xác 88,7% và SVM đạt 86,9%. Đặc biệt, mô hình học sâu cho thấy khả năng phát hiện tốt các trường hợp bệnh ở giai đoạn sớm, giảm tỷ lệ bỏ sót xuống dưới 6%. Thời gian suy luận trung bình của mô hình chỉ khoảng 45 ms cho mỗi bệnh nhân, phù hợp cho ứng dụng thời gian thực. Những kết quả này cho thấy học máy không chỉ nâng cao độ chính xác chẩn đoán mà còn cải thiện hiệu quả vận hành trong môi trường y tế.
1	Mặc dù đạt được kết quả tích cực, nghiên cứu vẫn tồn tại một số hạn chế. Dữ liệu chủ yếu được thu thập từ các bệnh viện đô thị, có thể chưa phản ánh đầy đủ đặc điểm dân số nông thôn. Ngoài ra, mô hình học sâu yêu cầu tài nguyên tính toán lớn và dữ liệu huấn luyện chất lượng cao. Tuy nhiên, với sự phát triển của điện toán đám mây và điện toán biên, các rào cản này đang dần được khắc phục. Việc tích hợp mô hình vào hệ thống hỗ trợ quyết định lâm sàng có thể giúp bác sĩ đưa ra chẩn đoán nhanh và chính xác hơn, đặc biệt trong các tình huống khẩn cấp.
1	Nghiên cứu đã chứng minh hiệu quả rõ rệt của học máy trong chẩn đoán bệnh tim mạch, với độ chính xác vượt trội so với các phương pháp truyền thống. Kết quả cho thấy học máy có tiềm năng lớn trong việc cải thiện chất lượng dịch vụ y tế và giảm chi phí điều trị lâu dài. Trong tương lai, việc mở rộng tập dữ liệu đa dạng hơn và kết hợp các nguồn dữ liệu như hình ảnh y khoa hoặc tín hiệu sinh học sẽ giúp nâng cao hơn nữa hiệu quả của mô hình. Điều này góp phần thúc đẩy quá trình chuyển đổi số trong lĩnh vực y tế.
1	Giao thông đô thị đang đối mặt với nhiều thách thức như ùn tắc, tai nạn và ô nhiễm môi trường. Nghiên cứu này đề xuất một hệ thống giao thông thông minh dựa trên trí tuệ nhân tạo nhằm tối ưu hóa luồng xe và giảm tai nạn giao thông. Hệ thống sử dụng mạng nơ-ron tích chập (CNN) kết hợp học tăng cường để phân tích dữ liệu từ camera giao thông và cảm biến IoT. Thử nghiệm được thực hiện tại một quận trung tâm với hơn 150 nút giao thông trong 6 tháng. Kết quả cho thấy thời gian chờ trung bình giảm 23%, số vụ tai nạn giảm 18% và lượng khí CO₂ phát thải giảm khoảng 12%.
1	Theo thống kê của Ngân hàng Thế giới, ùn tắc giao thông gây thiệt hại khoảng 2–5% GDP mỗi năm tại các thành phố lớn. Các hệ thống điều khiển đèn giao thông truyền thống thường hoạt động theo chu kỳ cố định, thiếu khả năng thích ứng với tình hình thực tế. Trí tuệ nhân tạo, với khả năng học hỏi và tự điều chỉnh, được xem là giải pháp tiềm năng cho bài toán này. Nghiên cứu tập trung vào việc áp dụng AI để phân tích dữ liệu giao thông theo thời gian thực, từ đó đưa ra quyết định điều khiển tối ưu. Mục tiêu là nâng cao hiệu quả lưu thông và đảm bảo an toàn cho người tham gia giao thông.
1	Hệ thống được xây dựng dựa trên dữ liệu video từ 300 camera giao thông và dữ liệu cảm biến từ 1.200 thiết bị IoT. Mạng CNN được sử dụng để nhận dạng phương tiện, phân loại làn đường và ước lượng mật độ giao thông. Sau đó, thuật toán học tăng cường sâu (Deep Reinforcement Learning) điều chỉnh thời gian đèn tín hiệu nhằm tối ưu hóa luồng xe. Mô hình được huấn luyện trên dữ liệu lịch sử 12 tháng và cập nhật liên tục trong quá trình vận hành. Các chỉ số đánh giá bao gồm thời gian chờ trung bình, lưu lượng xe và số vụ tai nạn ghi nhận.
1	Kết quả triển khai thực tế cho thấy hệ thống AI giúp giảm thời gian chờ trung bình tại các nút giao từ 75 giây xuống còn 58 giây. Lưu lượng xe qua các trục chính tăng khoảng 19% trong giờ cao điểm. Đáng chú ý, số vụ va chạm giao thông giảm từ trung bình 11 vụ/tháng xuống còn 9 vụ/tháng. Ngoài ra, việc giảm thời gian xe dừng chờ giúp lượng nhiên liệu tiêu thụ giảm đáng kể, kéo theo mức phát thải CO₂ giảm khoảng 12%. Những con số này chứng minh hiệu quả rõ rệt của AI trong quản lý giao thông đô thị.
1	Mặc dù đạt kết quả khả quan, hệ thống vẫn phụ thuộc vào chất lượng dữ liệu đầu vào. Các điều kiện thời tiết xấu hoặc camera bị che khuất có thể ảnh hưởng đến độ chính xác của mô hình. Bên cạnh đó, chi phí đầu tư ban đầu cho hạ tầng cảm biến và xử lý dữ liệu khá cao. Tuy nhiên, xét về lâu dài, lợi ích kinh tế và xã hội mà hệ thống mang lại vượt trội so với chi phí đầu tư. Việc mở rộng mô hình sang các khu vực khác và tích hợp với phương tiện tự hành là hướng nghiên cứu tiềm năng.
1	Nghiên cứu cho thấy trí tuệ nhân tạo (AI) có thể đóng vai trò then chốt trong việc xây dựng các hệ thống giao thông thông minh, an toàn và bền vững trong bối cảnh đô thị hóa ngày càng nhanh. Thông qua các thuật toán học máy và học sâu, AI có khả năng phân tích dữ liệu giao thông theo thời gian thực, từ đó tối ưu hóa điều khiển đèn tín hiệu, dự báo ùn tắc và hỗ trợ điều hướng thông minh. Việc áp dụng AI không chỉ giúp giảm thiểu tình trạng kẹt xe và tai nạn giao thông mà còn nâng cao hiệu quả sử dụng hạ tầng hiện có, giảm tiêu thụ nhiên liệu và phát thải khí nhà kính, góp phần bảo vệ môi trường đô thị.
1	Sự bùng nổ của giáo dục trực tuyến đặt ra yêu cầu cấp thiết về cá nhân hóa nội dung học tập cho người học. Nghiên cứu này đề xuất một hệ thống học tập thông minh dựa trên học máy nhằm tối ưu hóa lộ trình học cá nhân. Dữ liệu được thu thập từ hơn 25.000 học viên trên một nền tảng e-learning trong 2 năm. Mô hình sử dụng thuật toán Gradient Boosting kết hợp phân tích hành vi người học. Kết quả cho thấy tỷ lệ hoàn thành khóa học tăng từ 62% lên 78%, điểm trung bình của học viên tăng khoảng 15% so với phương pháp truyền thống.
1	Giáo dục trực tuyến mang lại sự linh hoạt và khả năng tiếp cận rộng rãi, tuy nhiên tỷ lệ bỏ học cao vẫn là thách thức lớn. Theo thống kê, hơn 40% người học trực tuyến không hoàn thành khóa học. Nguyên nhân chính là nội dung học chưa phù hợp với trình độ và mục tiêu cá nhân. Học máy, với khả năng phân tích dữ liệu hành vi và dự đoán xu hướng học tập, có thể giúp giải quyết vấn đề này. Nghiên cứu nhằm đánh giá hiệu quả của việc áp dụng học máy trong cá nhân hóa nội dung giáo dục, từ đó nâng cao trải nghiệm và kết quả học tập.
1	Hệ thống thu thập dữ liệu về thời gian học, số lần tương tác, kết quả bài kiểm tra và phản hồi của học viên. Tổng cộng 25.000 hồ sơ học tập với 30 đặc trưng được sử dụng. Mô hình Gradient Boosting được huấn luyện để dự đoán mức độ tiến bộ và nguy cơ bỏ học của từng học viên. Dựa trên kết quả dự đoán, hệ thống tự động đề xuất nội dung, tốc độ học và bài tập phù hợp. Quá trình đánh giá được thực hiện bằng cách so sánh hai nhóm học viên: nhóm sử dụng hệ thống AI và nhóm học theo phương pháp truyền thống.
1	Kết quả thực nghiệm cho thấy nhóm học viên sử dụng hệ thống học máy đạt tỷ lệ hoàn thành khóa học 78%, cao hơn 16% so với nhóm đối chứng học theo phương pháp truyền thống. Sự chênh lệch này phản ánh tác động tích cực của các cơ chế cá nhân hóa trong việc hỗ trợ người học duy trì tiến độ và hạn chế tình trạng bỏ cuộc giữa chừng. Bên cạnh đó, điểm trung bình cuối khóa của nhóm can thiệp cũng ghi nhận mức cải thiện rõ rệt, tăng từ 6,8 lên 7,8 trên thang điểm 10, cho thấy hệ thống không chỉ giúp học viên hoàn thành khóa học mà còn nâng cao chất lượng tiếp thu kiến thức.
1	Mặc dù các kết quả thu được cho thấy hiệu quả rõ rệt của hệ thống học máy trong cá nhân hóa giáo dục, nghiên cứu vẫn đối mặt với một số thách thức quan trọng cần được xem xét kỹ lưỡng. Trước hết, vấn đề quyền riêng tư và bảo mật dữ liệu của học viên là mối quan tâm hàng đầu, đặc biệt khi hệ thống thu thập và phân tích lượng lớn dữ liệu hành vi học tập, kết quả đánh giá và thông tin cá nhân. Do đó, việc tuân thủ các quy định pháp lý về bảo vệ dữ liệu, áp dụng các cơ chế ẩn danh hóa và mã hóa thông tin là yêu cầu bắt buộc.
1	Bên cạnh đó, tính minh bạch và khả năng giải thích của thuật toán cũng cần được cải thiện để giảng viên và người học hiểu rõ cách thức hệ thống đưa ra các khuyến nghị học tập. Ngoài ra, hành vi và nhu cầu của người học có thể thay đổi theo thời gian, đòi hỏi mô hình phải được cập nhật và tái huấn luyện định kỳ nhằm duy trì độ chính xác. Trong tương lai, việc kết hợp học máy với trí tuệ nhân tạo tạo sinh được kỳ vọng sẽ mở ra các hình thức tương tác mới, cho phép xây dựng nội dung học tập linh hoạt, thích ứng theo ngữ cảnh và nâng cao mức độ cá nhân hóa.
1	Nghiên cứu này khẳng định rằng học máy là một công cụ hiệu quả trong việc cá nhân hóa giáo dục trực tuyến, góp phần nâng cao kết quả học tập, cải thiện mức độ hài lòng và giảm đáng kể tỷ lệ bỏ học. Thông qua việc phân tích dữ liệu học tập và hành vi người học, hệ thống có khả năng đưa ra các lộ trình và khuyến nghị phù hợp với từng cá nhân, từ đó tối ưu hóa trải nghiệm học tập. Các kết quả đạt được cho thấy tiềm năng lớn của các hệ thống học tập thông minh trong bối cảnh giáo dục số đang phát triển mạnh mẽ.
1	Trong kỷ nguyên số hóa y tế năm 2026, sự bùng nổ của dữ liệu lớn (Big Data) đã đặt ra yêu cầu cấp bách về một hệ thống hỗ trợ chẩn đoán có khả năng xử lý khối lượng thông tin khổng lồ nhanh hơn con người. Trí tuệ nhân tạo, đặc biệt là các thuật toán học sâu (Deep Learning), đã chuyển mình từ các thử nghiệm trong phòng lab sang ứng dụng thực tiễn tại hơn 3.300 trạm y tế trên toàn quốc. Sự chuyển dịch này không chỉ là xu hướng công nghệ mà còn là giải pháp cứu cánh cho các khu vực vùng sâu vùng xa, nơi đội ngũ bác sĩ chuyên khoa còn thiếu thốn.
1	Việc áp dụng AI giúp thu hẹp khoảng cách tiếp cận dịch vụ y tế chất lượng cao, đồng thời giảm thiểu áp lực quá tải cho các bệnh viện tuyến trung ương vốn luôn trong tình trạng căng thẳng về nguồn lực. Các mô hình mạng thần kinh nhân tạo (Convolutional Neural Networks - CNN) hiện nay đã đạt đến độ chín muồi trong việc phân tích hình ảnh y tế như X-quang, MRI và CT scan. Tại Việt Nam, các thử nghiệm lâm sàng vào đầu năm 2026 cho thấy AI hỗ trợ phát hiện ung thư phổi sớm với độ chính xác lên tới 97%, vượt xa mức trung bình 65-70% của các phương pháp truyền thống khi không có sự hỗ trợ của máy móc.
1	"Trong các mẫu sinh thiết, các thuật toán học sâu có thể xác định tế bào ác tính với sai số chỉ khoảng 2-4%. Không chỉ dừng lại ở chẩn đoán hình ảnh, AI còn đóng vai trò là ""điều dưỡng ảo"" giúp tự động hóa quá trình tiếp nhận bệnh nhân, giảm 20% khối lượng công việc hành chính cho nhân viên y tế và tiết kiệm cho ngành y tế toàn cầu khoảng 20 tỷ USD mỗi năm thông qua việc tối ưu hóa quy trình vận hành. Thị trường AI trong y tế toàn cầu được dự báo sẽ đạt mức 64,8 tỷ USD vào cuối năm 2026, với tốc độ tăng trưởng kép hằng năm (CAGR) đáng kinh ngạc đạt trên 35%."
1	Theo báo cáo mới nhất, khoảng 90% các bệnh viện hiện đại đã tích hợp ít nhất một hệ thống quản lý dữ liệu thông minh dựa trên AI để theo dõi bệnh nhân từ xa. Việc giảm thiểu sai sót trong kê đơn thuốc thông qua AI cũng dự kiến tiết kiệm 16 tỷ USD cho hệ thống y tế Mỹ. Tầm nhìn đến năm 2030, y tế cá nhân hóa sẽ trở thành tiêu chuẩn vàng, nơi mỗi bệnh nhân có một bản đồ số về sức khỏe được cập nhật theo thời gian thực. Mặc dù vẫn còn những thách thức về bảo mật dữ liệu, nhưng sự kết hợp giữa trí tuệ con người và thuật toán máy tính đang kiến tạo nên một kỷ nguyên y học chính xác và nhân văn hơn bao giờ hết.
1	Sự gia tăng nhanh chóng của mật độ dân cư tại các siêu đô thị như Hà Nội và TP. Hồ Chí Minh đã dẫn đến vấn nạn ùn tắc giao thông nghiêm trọng, gây thiệt hại hàng tỷ USD mỗi năm về năng suất lao động và nhiên liệu. Đến năm 2026, việc ứng dụng hệ thống giao thông thông minh (Intelligent Transport Systems - ITS) dựa trên AI không còn là lựa chọn mà là yêu cầu bắt buộc để duy trì sự vận hành của đô thị. Hệ thống này tận dụng mạng lưới camera AI và cảm biến IoT để thu thập dữ liệu dòng chảy phương tiện theo thời gian thực.
1	Thay vì các chu kỳ đèn giao thông cố định, AI thực hiện việc điều chỉnh linh hoạt thời gian đèn xanh, đèn đỏ dựa trên mật độ xe thực tế, giúp tối ưu hóa khả năng lưu thông của các trục đường chính và giảm thiểu thời gian chờ đợi vô ích cho người dân. Tính đến đầu năm 2026, Hà Nội đã lắp đặt hơn 100 cảm biến đo đếm lưu lượng phương tiện tại các tuyến đường cửa ngõ, trong khi TP. Hồ Chí Minh đã tích hợp dữ liệu từ hơn 834 camera giám sát vào trung tâm điều hành chung. Kết quả bước đầu cho thấy, tại các giao lộ ứng dụng AI để điều tiết đèn tín hiệu, thời gian ùn tắc đã giảm từ 20-30% so với trước khi triển khai.
1	Đặc biệt, hệ thống này còn có khả năng dự báo các điểm nóng ùn tắc trước 15-30 phút thông qua việc phân tích dữ liệu lịch sử và các sự kiện đột xuất như tai nạn hay thời tiết xấu. Ngoài ra, việc ứng dụng AI trong quản lý xe buýt điện cũng đóng góp vào mục tiêu chuyển đổi 94% phương tiện công cộng sang năng lượng xanh vào năm 2031, giúp giảm lượng khí thải CO2 đáng kể cho môi trường đô thị. .Việc tối ưu hóa giao thông bằng AI không chỉ giúp cải thiện chất lượng cuộc sống mà còn mang lại lợi ích kinh tế trực tiếp. Ước tính, việc giảm 15% thời gian di chuyển trong đô thị có thể giúp tăng trưởng GDP khu vực thêm 0,5% nhờ việc tiết kiệm nhiên liệu và tăng hiệu suất làm việc.
1	Việc tối ưu hóa giao thông bằng AI không chỉ nâng cao chất lượng cuộc sống mà còn mang lại lợi ích kinh tế rõ rệt. Ước tính, giảm khoảng 15% thời gian di chuyển đô thị có thể giúp GDP khu vực tăng thêm 0,5% nhờ tiết kiệm nhiên liệu và cải thiện hiệu suất lao động. Trong thời gian tới, các hệ thống AI sẽ được mở rộng sang quản lý bãi đỗ xe thông minh và dự báo bảo trì hạ tầng giao thông, góp phần sử dụng hiệu quả nguồn lực đô thị. Về dài hạn, các đô thị thông minh được kỳ vọng vận hành như một hệ thống thống nhất, trong đó AI đóng vai trò trung tâm điều phối các lĩnh vực như giao thông, năng lượng và an ninh.
1	"Ngành thương mại điện tử vào năm 2026 đã chứng kiến một sự thay đổi toàn diện từ mô hình ""bán những gì mình có"" sang ""bán những gì khách hàng cần"" nhờ sự hỗ trợ của các thuật toán gợi ý (Recommendation Engines). Học máy không chỉ đơn thuần phân tích lịch sử mua hàng mà còn học được hành vi, sở thích và thậm chí là tâm trạng của người dùng qua các phiên truy cập. Việc cá nhân hóa sâu (Hyper-personalization) cho phép các nền tảng tạo ra một giao diện độc nhất cho từng cá nhân, nơi các sản phẩm được sắp xếp dựa trên xác suất mua hàng cao nhất."
1	Điều này đã tạo ra một cuộc cạnh tranh khốc liệt về công nghệ, nơi những doanh nghiệp chậm chân trong việc ứng dụng AI sẽ nhanh chóng mất đi thị phần vào tay các đối thủ biết tận dụng sức mạnh của dữ liệu để thấu hiểu khách hàng. Các số liệu thống kê vào tháng 1/2026 chỉ ra rằng, những doanh nghiệp sử dụng AI để cá nhân hóa trải nghiệm có thể tăng doanh thu ít nhất 20% trong khi giảm chi phí vận hành trung bình 8%. Tại các ông lớn như Amazon, hệ thống gợi ý dựa trên AI đóng góp tới 35% tổng doanh thu hàng năm.
1	Đặc biệt, việc ứng dụng Trí tuệ nhân tạo tạo sinh (Generative AI) để tạo nội dung quảng cáo và tư vấn khách hàng đã giúp tăng tỷ lệ chuyển đổi (Conversion Rate) lên mức 15-20%, cao gấp ba lần so với các phương pháp marketing truyền thống. Ngoài ra, AI còn giúp giảm tỷ lệ từ bỏ giỏ hàng (Cart Abandonment) khoảng 25% nhờ việc đưa ra các ưu đãi động (Dynamic Pricing) và thông điệp đúng thời điểm, giúp thúc đẩy quyết định mua sắm của người tiêu dùng một cách tự nhiên và hiệu quả hơn.
1	"Thị trường AI trong thương mại điện tử dự kiến sẽ vượt mốc 11,21 tỷ USD vào cuối năm 2026, phản ánh sự phụ thuộc ngày càng tăng của ngành bán lẻ vào công nghệ. Tuy nhiên, sự phát triển này cũng đi kèm với những lo ngại về quyền riêng tư và bảo mật thông tin cá nhân. Theo các khảo sát mới nhất, mặc dù 81% người tiêu dùng yêu thích sự tiện lợi của việc gợi ý sản phẩm cá nhân hóa, nhưng vẫn có hơn 60% bày tỏ lo ngại về cách dữ liệu của họ được thu thập và sử dụng. Do đó, xu hướng của năm 2026 là xây dựng ""AI đạo đức"", nơi sự minh bạch và bảo mật được đặt lên hàng đầu."
1	Trong bối cảnh y tế hiện đại, học máy đã trở thành công cụ mạnh mẽ để hỗ trợ chẩn đoán bệnh. Nghiên cứu này tập trung vào việc áp dụng các thuật toán học máy, đặc biệt là mạng nơ-ron tích chập (CNN), để phân tích hình ảnh X-quang nhằm phát hiện ung thư phổi ở giai đoạn sớm. Dữ liệu được thu thập từ 5.000 bệnh nhân tại các bệnh viện ở Mỹ và châu Âu, với tỷ lệ chính xác đạt 92%. Kết quả cho thấy mô hình giảm thời gian chẩn đoán từ 15 phút xuống còn 2 phút, đồng thời giảm tỷ lệ chẩn đoán sai từ 20% xuống 5%.
1	Nghiên cứu nhấn mạnh tiềm năng của AI trong việc cải thiện chăm sóc sức khỏe, đặc biệt ở các khu vực thiếu bác sĩ chuyên khoa. Các thử nghiệm trên tập dữ liệu Kaggle Chest X-Ray Images cho thấy độ nhạy (sensitivity) là 94% và độ đặc hiệu (specificity) là 90%. Tuy nhiên, cần thêm dữ liệu đa dạng để tránh thiên kiến. Tổng thể, ứng dụng này có thể cứu sống hàng nghìn bệnh nhân hàng năm bằng cách phát hiện sớm. Ung thư phổi là một trong những nguyên nhân gây tử vong hàng đầu trên thế giới, với hơn 2,2 triệu ca mới được chẩn đoán hàng năm theo Tổ chức Y tế Thế giới (WHO) năm 2020.
1	Việc chẩn đoán sớm có thể tăng tỷ lệ sống sót lên đến 56% nếu phát hiện ở giai đoạn I, so với chỉ 5% ở giai đoạn IV. Tuy nhiên, phương pháp truyền thống dựa vào bác sĩ đọc phim X-quang thường gặp lỗi do yếu tố con người, với tỷ lệ sai sót lên đến 30% theo nghiên cứu của American Journal of Roentgenology. Học máy, đặc biệt là AI, mang lại giải pháp bằng cách tự động hóa quy trình phân tích hình ảnh. Nghiên cứu này khám phá cách các mô hình như ResNet-50 và VGG16 được huấn luyện trên dữ liệu lớn để nhận diện các khối u bất thường. Chúng tôi sử dụng bộ dữ liệu gồm 10.000 hình ảnh, trong đó 60% là trường hợp dương tính.
1	Mục tiêu là đánh giá hiệu suất thực tế và đề xuất tích hợp vào hệ thống y tế. Kết quả ban đầu cho thấy AI có thể hỗ trợ bác sĩ, giảm tải công việc và tăng độ chính xác. Nghiên cứu cũng thảo luận về thách thức như bảo mật dữ liệu và đạo đức sử dụng AI trong y tế. Nghiên cứu sử dụng phương pháp học máy giám sát, với dữ liệu đầu vào là hình ảnh X-quang ngực từ cơ sở dữ liệu công khai như NIH Chest X-ray Dataset, bao gồm 112.120 hình ảnh từ 30.805 bệnh nhân. Chúng tôi áp dụng kỹ thuật tiền xử lý hình ảnh như tăng cường dữ liệu (data augmentation) để tăng số lượng mẫu lên 150.000, bao gồm xoay, phóng to và điều chỉnh độ sáng.
1	Mô hình chính là CNN với kiến trúc InceptionV3, được huấn luyện trên GPU NVIDIA Tesla V100 trong 50 epoch, với learning rate 0.001 và optimizer Adam. Tập dữ liệu được chia thành 70% huấn luyện, 15% xác thực và 15% kiểm tra. Để đánh giá, chúng tôi sử dụng các chỉ số như accuracy (92%), precision (93%), recall (94%) và F1-score (93%). Thử nghiệm so sánh với các mô hình khác như SVM và Random Forest cho thấy CNN vượt trội hơn 15% về độ chính xác. Ngoài ra, chúng tôi tích hợp explainable AI (XAI) bằng Grad-CAM để hiển thị vùng nghi ngờ trên hình ảnh, giúp bác sĩ hiểu quyết định của mô hình. Dữ liệu được anonymized để tuân thủ GDPR.
1	Kết quả từ mô hình cho thấy độ chính xác tổng thể đạt 92% trên tập kiểm tra, với 1.500 hình ảnh dương tính được phát hiện chính xác 94% và 2.000 hình ảnh âm tính chính xác 90%. Trong thử nghiệm thực tế tại bệnh viện Mayo Clinic, AI hỗ trợ chẩn đoán 500 ca, giảm thời gian trung bình từ 20 phút xuống 3 phút, và giảm lỗi chẩn đoán từ 18% xuống 4%. Số liệu thống kê cho thấy mô hình phát hiện khối u kích thước nhỏ (dưới 1cm) với tỷ lệ 85%, cao hơn phương pháp thủ công 30%. Biểu đồ ROC curve hiển thị AUC là 0.95, chứng tỏ khả năng phân biệt tốt.
1	Khi áp dụng trên dữ liệu từ châu Á (1.000 hình ảnh từ Trung Quốc), độ chính xác giảm nhẹ xuống 88% do sự khác biệt về đặc điểm dân số. Tổng cộng, nghiên cứu phân tích 20.000 hình ảnh, với chi phí huấn luyện mô hình khoảng 500 USD trên cloud computing. Kết quả này khẳng định AI có thể tích hợp vào quy trình y tế hàng ngày, đặc biệt ở các nước đang phát triển nơi thiếu chuyên gia. Mặc dù kết quả ấn tượng, nghiên cứu vẫn tồn tại hạn chế như phụ thuộc vào chất lượng dữ liệu đầu vào; hình ảnh mờ có thể làm giảm độ chính xác xuống 80%.
1	So sánh với nghiên cứu của Rajpurkar et al. (2017) trên CheXNet, mô hình của chúng tôi cải thiện 5% nhờ tăng cường dữ liệu. Tuy nhiên, vấn đề thiên kiến (bias) xuất hiện khi dữ liệu chủ yếu từ bệnh nhân da trắng, dẫn đến độ chính xác thấp hơn 10% trên nhóm thiểu số. Để khắc phục, cần đa dạng hóa dữ liệu lên 50.000 hình ảnh từ các châu lục khác. Về mặt đạo đức, AI không thay thế bác sĩ mà chỉ hỗ trợ, tránh trách nhiệm pháp lý. Nghiên cứu cũng chỉ ra tiềm năng mở rộng sang các bệnh khác như COVID-19, nơi AI đã đạt accuracy 96% theo nghiên cứu của Wang et al. (2020).
1	Tương lai, tích hợp AI với IoT có thể theo dõi bệnh nhân thời gian thực, giảm tỷ lệ tử vong 20%. Tổng thể, ứng dụng này chứng minh học máy đang thay đổi y tế, nhưng cần quy định chặt chẽ. Nghiên cứu kết luận rằng học máy và AI có vai trò quan trọng trong chẩn đoán ung thư phổi, với độ chính xác cao và hiệu quả thời gian. Từ dữ liệu 5.000 bệnh nhân, mô hình đạt 92% accuracy, có thể cứu sống hàng nghìn người bằng phát hiện sớm. Tuy nhiên, cần tiếp tục nghiên cứu để cải thiện tính tổng quát hóa và giải quyết vấn đề đạo đức. Các nhà hoạch định chính sách nên đầu tư vào AI y tế, dự kiến thị trường đạt 45 tỷ USD vào 2026 theo McKinsey.
1	Trí tuệ nhân tạo (AI) đang cách mạng hóa giao thông với xe tự lái, giảm tai nạn và ùn tắc. Nghiên cứu này phân tích ứng dụng học sâu trong nhận diện vật cản và điều hướng, sử dụng dữ liệu từ 10.000 giờ lái xe thực tế của Tesla Autopilot. Kết quả cho thấy tỷ lệ tai nạn giảm 40%, từ 5,5 vụ/1 triệu dặm xuống 3,3 vụ. Mô hình đạt độ chính xác 95% trong phát hiện người đi bộ. Nghiên cứu dựa trên bộ dữ liệu KITTI với 15.000 hình ảnh, chứng minh AI cải thiện an toàn giao thông. Tuy nhiên, thách thức về thời tiết xấu làm giảm hiệu suất 15%. Tổng thể, AI có thể tiết kiệm 1,35 triệu mạng sống hàng năm theo WHO.
1	Tai nạn giao thông gây tử vong cho 1,35 triệu người mỗi năm, chủ yếu do lỗi con người (94% theo NHTSA). Xe tự lái sử dụng AI để thay thế, với các công ty như Waymo đã chạy 20 triệu dặm an toàn. Nghiên cứu này khám phá cách học máy, đặc biệt là reinforcement learning, giúp xe học từ dữ liệu thực tế. Chúng tôi sử dụng mô hình DQN (Deep Q-Network) trên dữ liệu từ 5.000 xe ở California, bao gồm tốc độ trung bình 50 mph và mật độ giao thông cao. Mục tiêu là đánh giá hiệu suất trong môi trường đô thị, nơi ùn tắc chiếm 42 giờ/năm mỗi tài xế theo INRIX. Kết quả ban đầu cho thấy AI giảm thời gian di chuyển 20%.
1	Nghiên cứu cũng thảo luận về tích hợp với hệ thống IoT để dự đoán tắc nghẽn, giảm khí thải CO2 15%. Thách thức bao gồm bảo mật dữ liệu và quy định pháp lý. Dữ liệu được thu thập từ cảm biến LIDAR và camera trên 100 xe thử nghiệm, tổng cộng 50.000 km. Chúng tôi áp dụng mô hình YOLOv5 cho object detection, huấn luyện trên GPU RTX 3090 với batch size 32 và 100 epoch. Tập dữ liệu chia 80% huấn luyện, 20% kiểm tra, sử dụng augmentation như thêm nhiễu mưa. Đánh giá bằng mAP (mean Average Precision) đạt 0.92. So sánh với Faster R-CNN, YOLO nhanh hơn 5 lần (30 fps vs 6 fps). Ngoài ra, reinforcement learning với reward function dựa trên an toàn và tốc độ, giảm collision rate từ 10% xuống 2%. Dữ liệu tuân thủ ISO 26262 cho an toàn xe hơi.
1	Mô hình phát hiện vật cản chính xác 95%, với 8.000 trường hợp người đi bộ được nhận diện đúng 97%. Trong thử nghiệm thực tế, xe tự lái giảm tai nạn 40%, từ 4 vụ/1000 km xuống 2,4 vụ. Dữ liệu từ 10.000 giờ lái cho thấy tiết kiệm nhiên liệu 15%, giảm 20% khí thải. AUC của ROC curve là 0.96. Khi thử ở thời tiết mưa, độ chính xác giảm 10%, nhưng vẫn cao hơn lái thủ công 25%. Tổng chi phí triển khai khoảng 100.000 USD/xe, nhưng lợi ích kinh tế đạt 500 tỷ USD toàn cầu theo McKinsey.
1	Kết quả vượt trội, nhưng hạn chế ở môi trường phức tạp như tuyết, giảm accuracy 20%. So với nghiên cứu của NVIDIA (2019), mô hình chúng tôi cải thiện 8% nhờ dữ liệu đa dạng. Vấn đề đạo đức: ai chịu trách nhiệm tai nạn? Cần luật như EU's AI Act. Tiềm năng mở rộng sang xe buýt tự lái, giảm ùn tắc 30% ở thành phố lớn. AI cũng hỗ trợ dự đoán tai nạn, giảm tử vong 50% theo dự báo. AI trong xe tự lái mang lại an toàn cao, giảm tai nạn 40%. Từ dữ liệu 10.000 giờ, nghiên cứu khẳng định ứng dụng thực tế, dự kiến thị trường 7 nghìn tỷ USD vào 2050. Cần đầu tư nghiên cứu thêm.
1	Học máy đang thay đổi giáo dục bằng cách cá nhân hóa nội dung học. Nghiên cứu này đánh giá hệ thống AI như Duolingo và Khan Academy, sử dụng dữ liệu từ 1 triệu học sinh, đạt cải thiện điểm số 25%. Mô hình recommendation system dựa trên collaborative filtering đạt accuracy 90%. Kết quả cho thấy thời gian học giảm 30%, từ 10 giờ/tuần xuống 7 giờ. Nghiên cứu trên bộ dữ liệu MOOC với 50.000 khóa học chứng minh AI tăng tỷ lệ hoàn thành 40%. Tuy nhiên, vấn đề tiếp cận công nghệ làm giảm hiệu quả 15% ở khu vực nông thôn. Tổng thể, AI có thể giảm bất bình đẳng giáo dục toàn cầu.
1	Giáo dục truyền thống thường thiếu cá nhân hóa, dẫn đến tỷ lệ bỏ học 20% ở đại học theo UNESCO. AI giải quyết bằng cách phân tích dữ liệu học tập để đề xuất nội dung phù hợp. Nghiên cứu này khám phá natural language processing (NLP) trong hệ thống tutor ảo, sử dụng dữ liệu từ 500.000 học sinh Mỹ. Mô hình BERT được áp dụng để hiểu câu hỏi, với accuracy 85%. Mục tiêu là tăng hiệu quả học, nơi học sinh yếu cải thiện 30% điểm số. Kết quả ban đầu từ thử nghiệm tại Harvard cho thấy AI hỗ trợ tốt hơn giáo viên 20% ở môn toán. Nghiên cứu thảo luận về tích hợp với VR để học trải nghiệm, giảm chi phí giáo dục 15%. Thách thức bao gồm bảo mật dữ liệu học sinh.
1	Dữ liệu từ nền tảng edX với 100.000 bài kiểm tra, áp dụng mô hình LSTM cho dự đoán tiến độ học. Huấn luyện trên AWS với 200 epoch, learning rate 0.002. Tập dữ liệu chia 75% huấn luyện, 25% kiểm tra, sử dụng metrics như RMSE 0.15. So sánh với K-means clustering, LSTM vượt trội 10%. Ngoài ra, A/B testing trên 10.000 học sinh cho thấy nhóm dùng AI cải thiện 28%. Dữ liệu anonymized theo FERPA. Mô hình dự đoán đúng 90% nội dung cần học, giúp 1 triệu học sinh tăng điểm 25%. Trong thử nghiệm, tỷ lệ hoàn thành khóa học tăng từ 50% lên 70%. Dữ liệu cho thấy học sinh nữ cải thiện 32%, nam 22%. AUC 0.94. Chi phí hệ thống khoảng 50 USD/học sinh/năm, lợi ích kinh tế 100 tỷ USD theo World Bank.
1	Mặc dù đạt được kết quả tích cực, nghiên cứu vẫn tồn tại hạn chế quan trọng về dữ liệu, khi tập huấn luyện chủ yếu thiên về tiếng Anh, dẫn đến việc độ chính xác giảm khoảng 12% khi áp dụng cho các ngôn ngữ khác. Điều này cho thấy nhu cầu cấp thiết trong việc xây dựng các bộ dữ liệu đa ngôn ngữ và đa văn hóa nhằm nâng cao khả năng khái quát của mô hình. Tuy nhiên, khi so sánh với nghiên cứu của Siemens (2018), mô hình đề xuất vẫn cho thấy sự cải thiện đáng kể với mức tăng hiệu năng khoảng 7%, khẳng định tính hiệu quả của phương pháp tiếp cận.
1	"Trong bối cảnh giáo dục toàn cầu năm 2026, mô hình ""một chương trình cho tất cả"" đang dần bộc lộ những hạn chế về khả năng phát huy tiềm năng cá nhân của người học. Sự ra đời của các hệ thống học tập thích ứng dựa trên Trí tuệ nhân tạo đã mở ra một kỷ nguyên mới, nơi lộ trình học tập được thiết kế riêng biệt cho từng cá nhân. Bằng cách sử dụng các thuật toán Học máy để phân tích tốc độ tiếp thu, phong cách học tập và các lỗ hổng kiến thức của sinh viên, AI có thể tự động điều chỉnh độ khó của bài tập và cung cấp các học liệu bổ trợ kịp thời."
1	Điều này không chỉ giúp người học duy trì động lực mà còn hỗ trợ giảng viên trong việc quản lý lớp học hiệu quả hơn, chuyển đổi vai trò từ người truyền đạt kiến thức thuần túy sang người hướng dẫn và điều phối. Các hệ thống học tập tích hợp mô hình ngôn ngữ lớn (LLM) và thuật toán phân loại (Classification Algorithms) đã được triển khai tại hơn 1.500 cơ sở giáo dục đại học trên toàn thế giới tính đến đầu năm 2026. Kết quả thống kê cho thấy, sinh viên sử dụng các nền tảng học tập dựa trên AI có tỷ lệ hoàn thành khóa học tăng 25% và điểm số trung bình cải thiện khoảng 1.5 điểm trên thang điểm 10 so với phương pháp học truyền thống.
1	Đặc biệt, hệ thống đánh giá tự động dựa trên mạng nơ-ron có khả năng chấm điểm các bài luận ngắn với độ tương đồng lên tới 92% so với giám khảo con người, đồng thời cung cấp phản hồi chi tiết chỉ trong vòng 30 giây. Việc này giúp giảm tải 40% khối lượng công việc chấm bài cho giảng viên, cho phép họ tập trung vào các hoạt động nghiên cứu và tương tác chuyên sâu với sinh viên.Thị trường AI trong giáo dục (EdTech AI) dự kiến sẽ đạt giá trị 32 tỷ USD vào cuối năm 2026, với tốc độ tăng trưởng hàng năm đạt mức 45,8%.
1	Theo các báo cáo chuyên ngành, khoảng 70% các ứng dụng học ngoại ngữ hàng đầu hiện nay đã tích hợp gia sư ảo AI có khả năng giao tiếp bằng giọng nói tự nhiên, giúp người học cải thiện kỹ năng phản xạ nhanh hơn 3 lần. Tầm nhìn đến năm 2030, giáo dục sẽ không còn giới hạn trong không gian lớp học mà trở thành một hệ sinh thái học tập suốt đời (Lifelong Learning) được hỗ trợ bởi trợ lý AI cá nhân. Mặc dù vấn đề đạo đức trong học thuật và tính công bằng trong tiếp cận công nghệ vẫn là những thách thức lớn, nhưng AI chắc chắn là chìa khóa để hiện thực hóa một nền giáo dục chất lượng cao, dễ tiếp cận và thực sự cá nhân hóa.
1	Sự biến đổi khí hậu khắc nghiệt và diện tích đất canh tác thu hẹp trong năm 2026 đã đặt ngành nông nghiệp trước những áp lực chưa từng có về năng suất và tính bền vững. Nông nghiệp chính xác (Precision Agriculture) nổi lên như một giải pháp đột phá, tận dụng sức mạnh của Thị giác máy tính (Computer Vision) và học sâu để quản lý trang trại ở cấp độ từng cây trồng đơn lẻ. Thay vì phun thuốc bảo vệ thực vật đại trà gây ô nhiễm môi trường, hệ thống AI tích hợp trên drone và robot tự hành có thể nhận diện chính xác loại sâu bệnh hoặc cỏ dại để xử lý cục bộ.
1	Điều này không chỉ giúp bảo vệ hệ sinh thái mà còn tối ưu hóa chi phí đầu vào, giúp người nông dân thích ứng tốt hơn với những biến động của thị trường và điều kiện thời tiết khắc nghiệt. Tại các vùng canh tác công nghệ cao ở Việt Nam và khu vực Đông Nam Á vào năm 2026, các mô hình học máy như YOLOv10 (You Only Look Once) đã được áp dụng rộng rãi để phát hiện sớm các dấu hiệu thiếu hụt dinh dưỡng trên lá cây với độ chính xác 95%. Hệ thống cảm biến IoT kết hợp với AI phân tích dữ liệu đất đai giúp tiết kiệm đến 35% lượng nước tưới và 20% lượng phân bón hóa học.
1	Các trang trại thông minh báo cáo rằng năng suất cây trồng trung bình tăng từ 15-22% nhờ việc dự báo chính xác thời điểm thu hoạch tối ưu thông qua phân tích hình ảnh vệ tinh và dữ liệu khí tượng. Ngoài ra, việc sử dụng robot thu hoạch tự động giúp giải quyết bài toán thiếu hụt nhân công mùa vụ, giảm 30% chi phí lao động và hạn chế thất thoát sau thu hoạch do thao tác thủ công không chuẩn xác. Quy mô thị trường nông nghiệp thông minh toàn cầu được dự báo sẽ vượt mốc 25 tỷ USD vào cuối năm 2026, trong đó mảng phân tích dữ liệu và AI chiếm tỷ trọng lớn nhất. Việc áp dụng AI trong nông nghiệp không chỉ mang lại lợi ích kinh tế mà còn đóng góp quan trọng vào mục tiêu phát triển bền vững.
1	Các nghiên cứu mới nhất chỉ ra rằng, việc tối ưu hóa quy trình canh tác bằng AI giúp giảm 12% lượng khí thải nhà kính từ hoạt động nông nghiệp mỗi năm. Trong tương lai, sự kết hợp giữa AI và công nghệ sinh học sẽ cho phép lai tạo các giống cây trồng có khả năng chống chịu cao dựa trên các mô hình mô phỏng gene phức tạp. Đây là bước tiến quan trọng để đảm bảo an ninh lương thực cho dân số thế giới dự kiến đạt gần 8.5 tỷ người vào năm 2030.
1	"Trong kỷ nguyên giao dịch số không tiền mặt năm 2026, các hình thức lừa đảo tài chính và tấn công mạng ngày càng trở nên tinh vi, gây thiệt hại hàng trăm tỷ USD cho nền kinh tế toàn cầu. Các hệ thống bảo mật truyền thống dựa trên quy tắc (Rule-based) đã không còn đủ khả năng đối phó với những biến thể tấn công mới. Do đó, việc triển khai các mô hình Học không giám sát (Unsupervised Learning) và Học tăng cường (Reinforcement Learning) đã trở thành ""lá chắn"" cốt lõi cho hệ thống ngân hàng hiện đại."
1	Các thuật toán này có khả năng tự học hỏi từ các hành vi giao dịch bình thường của hàng triệu khách hàng để ngay lập tức phát hiện ra những điểm bất thường (Anomaly Detection) dù là nhỏ nhất, giúp ngăn chặn gian lận ngay từ khi nó mới bắt đầu nhen nhóm. Đến năm 2026, các ngân hàng lớn đã tích hợp hệ thống chấm điểm rủi ro theo thời gian thực (Real-time Fraud Scoring). Khi một giao dịch được thực hiện, AI sẽ phân tích hơn 500 biến số khác nhau bao gồm: vị trí địa lý, thiết bị truy cập, thói quen chi tiêu và tốc độ gõ phím để đưa ra quyết định trong vòng chưa đầy 100 mil giây.
1	Thống kê từ hiệp hội ngân hàng cho thấy, việc ứng dụng AI đã giúp giảm 50% tỷ lệ báo động giả (False Positives), giúp khách hàng không bị gián đoạn giao dịch một cách oan uổng. Đồng thời, tỷ lệ phát hiện các giao dịch gian lận thẻ tín dụng và rửa tiền đã tăng lên mức 98%. Việc tự động hóa quy trình xác minh danh tính (eKYC) bằng sinh trắc học khuôn mặt và giọng nói cũng giúp rút ngắn thời gian mở tài khoản từ 30 phút xuống còn 2 phút, với độ bảo mật gần như tuyệt đối.
1	"Ước tính đến cuối năm 2026, việc ứng dụng AI trong phòng chống gian lận sẽ giúp ngành tài chính toàn cầu tiết kiệm khoảng 42 tỷ USD mỗi năm. Chi phí đầu tư cho AI trong lĩnh vực an ninh mạng dự kiến tăng trưởng với tốc độ CAGR là 24,5% trong giai đoạn 2024-2026. Tuy nhiên, một thách thức mới nảy sinh là cuộc chạy đua giữa ""AI phòng thủ"" và ""AI tấn công"" (Deepfake và mã độc tự học). Do đó, các tổ chức tài chính đang hướng tới việc xây dựng các mô hình AI có khả năng giải thích được (Explainable AI - XAI) để đảm bảo tính minh bạch trong các quyết định từ chối giao dịch."
1	Nông nghiệp thông minh đang trở thành xu hướng quan trọng nhằm đáp ứng nhu cầu lương thực ngày càng tăng và thích ứng với biến đổi khí hậu. Nghiên cứu này đề xuất một mô hình dự báo năng suất cây trồng dựa trên trí tuệ nhân tạo, kết hợp dữ liệu thời tiết, đất đai và hình ảnh vệ tinh. Tập dữ liệu gồm hơn 18.000 mẫu thu thập từ ba vùng trồng lúa trọng điểm trong giai đoạn 2020–2024. Mô hình Random Forest và mạng nơ-ron sâu được huấn luyện để dự đoán năng suất theo từng vụ mùa. Kết quả cho thấy sai số dự báo trung bình giảm từ 14,6% xuống còn 8,2%, giúp nông dân chủ động hơn trong việc lập kế hoạch canh tác và phân bổ tài nguyên.
1	Nông nghiệp truyền thống phụ thuộc nhiều vào kinh nghiệm và điều kiện tự nhiên, dẫn đến rủi ro cao về năng suất. Theo FAO, biến đổi khí hậu có thể làm giảm 10–20% năng suất cây trồng tại các quốc gia đang phát triển nếu không có biện pháp thích ứng. Trí tuệ nhân tạo mang lại khả năng phân tích lượng lớn dữ liệu đa nguồn, từ đó hỗ trợ ra quyết định chính xác hơn trong sản xuất nông nghiệp. Việc áp dụng AI vào dự báo năng suất giúp tối ưu hóa sử dụng phân bón, nước tưới và giảm chi phí sản xuất. Nghiên cứu này tập trung đánh giá hiệu quả của các mô hình học máy trong việc dự báo năng suất lúa ở quy mô thực tế.
1	Dữ liệu đầu vào bao gồm 22 đặc trưng, trong đó có nhiệt độ trung bình, lượng mưa, độ ẩm đất, chỉ số NDVI từ ảnh vệ tinh và lịch sử canh tác. Dữ liệu được chuẩn hóa và chia thành tập huấn luyện 75% và tập kiểm thử 25%. Hai mô hình chính được sử dụng là Random Forest với 300 cây và mạng nơ-ron sâu gồm 3 lớp ẩn. Quá trình huấn luyện được thực hiện trên nền tảng GPU nhằm giảm thời gian tính toán. Hiệu năng mô hình được đánh giá bằng các chỉ số RMSE, MAE và R² để đảm bảo độ tin cậy của kết quả dự báo.
1	Kết quả cho thấy mô hình mạng nơ-ron sâu đạt RMSE trung bình 0,42 tấn/ha, thấp hơn đáng kể so với Random Forest ở mức 0,56 tấn/ha. Hệ số xác định R² của mô hình học sâu đạt 0,91, cho thấy khả năng giải thích cao đối với biến động năng suất. Khi triển khai thử nghiệm tại thực địa, nông dân áp dụng khuyến nghị từ hệ thống AI đã tăng năng suất trung bình 11% so với vụ mùa trước. Ngoài ra, lượng phân bón sử dụng giảm khoảng 9%, góp phần giảm chi phí và tác động môi trường.
1	Mặc dù các kết quả đạt được cho thấy tiềm năng rõ rệt của mô hình đề xuất, hệ thống vẫn phụ thuộc đáng kể vào độ chính xác và tính đầy đủ của dữ liệu đầu vào, đặc biệt là dữ liệu thời tiết và hình ảnh vệ tinh. Ở những khu vực nông thôn, miền núi hoặc các vùng đang phát triển, nơi hạ tầng thu thập dữ liệu còn hạn chế, việc triển khai mô hình có thể gặp nhiều khó khăn, dẫn đến sai lệch trong dự báo. Ngoài ra, các yếu tố bất định như thời tiết cực đoan, sâu bệnh đột phát hoặc thay đổi canh tác cũng có thể ảnh hưởng đến độ tin cậy của mô hình.
1	Tuy nhiên, sự phát triển nhanh chóng của IoT nông nghiệp, cảm biến giá rẻ và các vệ tinh quan sát Trái Đất độ phân giải cao đang từng bước khắc phục những hạn chế này, giúp dữ liệu ngày càng đầy đủ và kịp thời hơn. Việc tích hợp AI vào các hệ thống quản lý nông nghiệp thông minh không chỉ hỗ trợ ra quyết định chính xác mà còn giúp người nông dân tối ưu hóa chi phí, sử dụng tài nguyên hiệu quả và hướng tới lợi ích kinh tế bền vững trong dài hạn.
1	Sự phát triển của thanh toán điện tử kéo theo nguy cơ gia tăng các hành vi gian lận tài chính. Nghiên cứu này trình bày một hệ thống phát hiện gian lận dựa trên học máy nhằm nhận diện giao dịch bất thường theo thời gian thực. Tập dữ liệu gồm hơn 5 triệu giao dịch từ một hệ thống thanh toán trong giai đoạn 2021–2024. Các mô hình như Logistic Regression, XGBoost và mạng nơ-ron sâu được so sánh. Kết quả cho thấy XGBoost đạt độ chính xác 97,1% và tỷ lệ phát hiện gian lận lên tới 94,3%, giúp giảm thiểu thiệt hại tài chính đáng kể cho doanh nghiệp.
1	Gian lận tài chính là vấn đề nghiêm trọng, gây thiệt hại hàng tỷ USD mỗi năm trên toàn cầu. Theo báo cáo của Nilson Report, tổng thiệt hại do gian lận thẻ thanh toán đạt hơn 33 tỷ USD vào năm 2023. Các phương pháp phát hiện truyền thống dựa trên luật cứng nhắc thường không theo kịp các thủ đoạn gian lận ngày càng tinh vi. Học máy cho phép hệ thống học từ dữ liệu lịch sử và tự động phát hiện các mẫu giao dịch bất thường. Nghiên cứu này nhằm đánh giá khả năng ứng dụng học máy trong việc phát hiện gian lận tài chính với độ chính xác cao và thời gian phản hồi nhanh.
1	Dữ liệu đầu vào được tiền xử lý kỹ lưỡng nhằm đảm bảo chất lượng và độ tin cậy cho quá trình huấn luyện mô hình. Trước hết, các giao dịch lỗi, trùng lặp hoặc thiếu thông tin quan trọng được loại bỏ để giảm nhiễu. Sau đó, các đặc trưng số được chuẩn hóa nhằm đưa về cùng thang đo, giúp mô hình hội tụ nhanh và ổn định hơn. Do bài toán tồn tại mất cân bằng lớp nghiêm trọng giữa các giao dịch hợp lệ và gian lận, nghiên cứu áp dụng phương pháp undersampling cho lớp đa số kết hợp với SMOTE để sinh thêm mẫu cho lớp thiểu số, qua đó cải thiện khả năng học của mô hình.
1	Kết quả thực nghiệm cho thấy mô hình XGBoost đạt hiệu năng vượt trội so với các phương pháp còn lại, với F1-score lên tới 0,95 và thời gian xử lý trung bình chỉ 18 ms cho mỗi giao dịch, đáp ứng tốt yêu cầu của các hệ thống thanh toán thời gian thực. Trong khi đó, mạng nơ-ron sâu cũng đạt độ chính xác cao trong việc phát hiện gian lận, nhưng thời gian suy luận dài hơn, khoảng 35 ms, khiến khả năng triển khai ở quy mô lớn bị hạn chế hơn trong các kịch bản có lưu lượng giao dịch cao.
1	Mặc dù đạt hiệu quả cao, mô hình phát hiện gian lận vẫn đối mặt với thách thức về sự thay đổi liên tục của các chiến thuật gian lận, đòi hỏi hệ thống phải được cập nhật và tái huấn luyện định kỳ để duy trì độ chính xác. Bên cạnh đó, việc cân bằng giữa hiệu năng dự đoán và khả năng giải thích mô hình vẫn là vấn đề quan trọng, đặc biệt trong lĩnh vực tài chính, nơi các quyết định tự động cần minh bạch để đáp ứng yêu cầu kiểm toán và tuân thủ pháp lý.
1	Nghiên cứu này khẳng định rằng học máy đóng vai trò then chốt trong phát hiện và phòng chống gian lận tài chính, đặc biệt trong bối cảnh các hệ thống thanh toán số ngày càng phát triển và phức tạp. Việc triển khai các mô hình thông minh cho phép phân tích khối lượng lớn giao dịch theo thời gian thực, từ đó nâng cao độ an toàn, giảm thiểu rủi ro tài chính và hạn chế các tổn thất cho doanh nghiệp cũng như người dùng. Bên cạnh hiệu quả kỹ thuật, các giải pháp dựa trên học máy còn góp phần tăng cường niềm tin của khách hàng đối với các dịch vụ thanh toán điện tử.
1	Nhu cầu tiêu thụ năng lượng ngày càng tăng đặt ra yêu cầu cấp thiết về các giải pháp quản lý và tiết kiệm điện hiệu quả. Nghiên cứu này đề xuất một hệ thống quản lý năng lượng thông minh dựa trên trí tuệ nhân tạo nhằm tối ưu hóa tiêu thụ điện trong các tòa nhà thương mại. Dữ liệu được thu thập từ 50 tòa nhà trong vòng 24 tháng, bao gồm hơn 10 triệu bản ghi tiêu thụ điện. Mô hình LSTM được sử dụng để dự báo nhu cầu năng lượng ngắn hạn. Kết quả cho thấy mức tiêu thụ điện giảm trung bình 13,5% sau khi áp dụng hệ thống AI.
1	Quản lý năng lượng hiệu quả là một trong những yếu tố then chốt để phát triển bền vững. Theo Cơ quan Năng lượng Quốc tế, các tòa nhà chiếm gần 40% tổng mức tiêu thụ điện toàn cầu. Các phương pháp quản lý truyền thống thường thiếu khả năng dự báo chính xác nhu cầu năng lượng. Trí tuệ nhân tạo, đặc biệt là các mô hình chuỗi thời gian, có thể học được các mẫu tiêu thụ phức tạp và hỗ trợ điều chỉnh hệ thống điện theo thời gian thực. Nghiên cứu này tập trung đánh giá hiệu quả của AI trong quản lý năng lượng tại các tòa nhà thương mại.
1	Dữ liệu được thu thập từ hệ thống quản lý năng lượng của các tòa nhà thông minh, bao gồm công suất tiêu thụ điện theo giờ, nhiệt độ môi trường, số lượng người sử dụng, cũng như lịch vận hành của các thiết bị như điều hòa, chiếu sáng và thang máy. Trước khi huấn luyện, dữ liệu được làm sạch, chuẩn hóa và sắp xếp theo chuỗi thời gian nhằm phù hợp với đặc trưng của bài toán dự báo. Mô hình LSTM gồm hai lớp ẩn được thiết kế để học các mối quan hệ phụ thuộc dài hạn trong dữ liệu, từ đó dự báo nhu cầu điện trong 24 giờ tiếp theo. Dữ liệu được chia thành 80% cho huấn luyện và 20% cho kiểm thử để đánh giá khả năng khái quát hóa.
1	Kết quả thực nghiệm cho thấy mô hình LSTM đạt MAPE trung bình 6,8%, chứng tỏ khả năng dự báo nhu cầu điện với độ chính xác cao và ổn định. Khi được tích hợp vào hệ thống quản lý năng lượng, các tòa nhà tham gia thí nghiệm ghi nhận mức giảm tiêu thụ điện trung bình 13,5% và giảm chi phí vận hành khoảng 11%, phản ánh hiệu quả kinh tế rõ rệt. Bên cạnh đó, công suất đỉnh giảm 9%, giúp hạn chế tình trạng quá tải và góp phần giảm áp lực cho lưới điện quốc gia trong các khung giờ cao điểm. Những kết quả này cho thấy việc ứng dụng AI không chỉ mang lại lợi ích về chi phí mà còn đóng góp tích cực vào việc giảm phát thải và sử dụng năng lượng hiệu quả hơn.
1	Mặc dù đạt được các kết quả khả quan, nghiên cứu vẫn đối mặt với một số thách thức đáng kể. Trong đó, chi phí đầu tư ban đầu cho hệ thống cảm biến, thiết bị đo đạc và hạ tầng dữ liệu là rào cản đối với nhiều đơn vị. Ngoài ra, đặc điểm sử dụng năng lượng khác nhau giữa các loại tòa nhà đòi hỏi mô hình phải được hiệu chỉnh và tùy biến để đạt hiệu quả tối ưu. Tuy nhiên, xét về dài hạn, lợi ích từ việc tiết kiệm năng lượng, giảm chi phí vận hành và cắt giảm phát thải khí nhà kính vượt trội so với chi phí ban đầu.
1	Nghiên cứu đã chứng minh rằng trí tuệ nhân tạo, đặc biệt là các mô hình học sâu như LSTM, là giải pháp hiệu quả trong quản lý và tiết kiệm năng lượng. Việc áp dụng các hệ thống AI thông minh cho phép dự báo chính xác nhu cầu điện, tối ưu hóa vận hành và giảm chi phí, đồng thời góp phần bảo vệ môi trường. Trong bối cảnh nhu cầu năng lượng ngày càng gia tăng và yêu cầu phát triển bền vững ngày càng cấp thiết, việc triển khai rộng rãi các giải pháp AI trong quản lý năng lượng sẽ đóng vai trò quan trọng trong việc nâng cao hiệu quả sử dụng tài nguyên và hướng tới một tương lai năng lượng xanh và bền vững hơn.
0	Bệnh động kinh là một trong những rối loạn thần kinh phổ biến nhất, ảnh hưởng đến hàng triệu người trên thế giới. Phương pháp truyền thống trong phát hiện và dự đoán cơn động kinh thường dựa vào phân tích tín hiệu điện não đồ (Electroencephalogram Signals - EEG) qua các thuật toán máy học. Bài báo này đề xuất sử dụng mạng nơ-ron xung (Spiking Neural Networks - SNNs) với kiến trúc mạng truyền thẳng (feedforward) như một giải pháp mới nhằm nâng cao độ chính xác và tốc độ trong nhận dạng các cơn động kinh từ tín hiệu EEG. Kết quả thử nghiệm cho thấy, SNNs có khả năng nhận dạng chính xác cơn động kinh, đồng thời giảm độ phức tạp tính toán mà vẫn đảm bảo yêu cầu về độ chính xác.
0	Tín hiệu điện não sau khi mã hóa thành chuỗi xung được đánh giá trên hai kiến trúc mạng khác nhau. Kết quả tốt nhất là độ trễ phát hiện 97 ms và độ chính xác 96,3% cho mạng feedforward lớn với 150 nơ-ron, với chỉ một số ít xung nằm ngoài sự kiện co giật. Trong những năm gần đây, thông qua việc mô phỏng và tìm hiểu cách thức não bộ con người hoạt động, SNNs (spiking neural networks) đã được ứng dụng rất rộng rãi trong các mô hình và đã trở thành một mô hình tính toán mới với hiệu quả cao cho các ứng dụng của học máy, với các ưu điểm về việc có thể giúp giảm độ phức tạp tính toán mà vẫn đảm bảo yêu cầu về độ chính xác của ứng dụng.
0	Tuy nhiên, thách thức chính của mạng SNNs là việc tìm ra một thuật toán huấn luyện hiệu quả cho SNNs, trong đó yêu cầu chiếm dụng ít bộ nhớ và có khả năng thực thi được trên các nền tảng phần cứng nhúng. Trong nghiên cứu [1] nhóm tác giả phòng Công nghệ mạng và Truyền thông, Viện Công nghệ Thông tin, Đại học quốc gia Hà Nội đã nghiên cứu đề xuất thuật toán huấn luyện ngoại tuyến với SNNs, với các trọng số của mạng được biểu diễn dưới dạng tam phân (được thể hiện với 2-bit). Thuật toán đề xuất giúp giảm yêu cầu bộ nhớ lên đến 16 lần so với việc lưu trữ các trọng số với độ chính xác dấu chấm động.
0	Trong lĩnh vực y sinh, bệnh động kinh được thống kê có thể ảnh hưởng sâu sắc đến hoạt động xã hội, tâm thần và cơ thể của bệnh nhân và sự ảnh hưởng này có thể nặng nề hơn bất cứ một tình trạng mạn tính nào [2,3,4]. Các nghiên cứu này có thể được xem là những công trình đầu tiên đánh giá ảnh hưởng của bệnh động kinh lên chất lượng cuộc sống của bệnh nhân. Các tác giả đã chỉ ra rằng, bệnh động kinh ảnh hưởng sâu sắc đến các khía cạnh xã hội, tâm lý và nghề nghiệp của bệnh nhân.
0	Cụ thể, người bệnh thường phải đối mặt với sự kỳ thị xã hội, các vấn đề về tâm lý như trầm cảm và lo âu, cũng như các khó khăn trong cuộc sống hàng ngày, như mất việc làm hoặc không thể lái xe. Nghiên cứu cũng cho thấy sự kỳ thị xã hội có thể ngăn cản bệnh nhân tìm kiếm sự hỗ trợ y tế, làm trầm trọng thêm tình trạng bệnh. Các nghiên cứu gần đây về SNNs ứng dụng trong lĩnh vực y sinh ngày càng được thu hút bởi chúng cung cấp một mô hình gần gũi hơn với hoạt động của não người khi so với các loại mạng nơ-ron truyền thống như MLP (Perceptron nhiều lớp) hay CNN (Mạng nơ-ron tích chập).
0	Cụ thể, nghiên cứu trong [5] giới thiệu ứng dụng của SNN trong các hệ thống điều khiển cơ thể dựa trên giao diện não-máy tính (Brain-Machine Interface - BMI) nhằm giúp bệnh nhân phục hồi khả năng di chuyển hoặc kiểm soát các thiết bị ngoại vi thông qua các xung điện thần kinh từ não. SNN được sử dụng để giải mã các tín hiệu thần kinh phức tạp từ các cơ quan cảm giác, qua đó cho phép điều khiển chân tay giả một cách chính xác. Trong nghiên cứu [6], SNNs được áp dụng để phát hiện các cơn động kinh thông qua dữ liệu sóng điện não (EEG). Theo đó, mô hình SNNs có khả năng học, tổng hợp và phân tích thông tin từ các mẫu tín hiệu EEG phức tạp để nhận biết các cơn co giật.
0	Nhờ khả năng xử lý theo thời gian thực và độ chính xác cao, SNN được chứng minh là phù hợp cho các ứng dụng phát hiện bệnh động kinh và theo dõi bệnh nhân. Nghiên cứu [7] tập trung vào việc sử dụng SNNs để điều khiển các thiết bị kích thích điện giúp phục hồi khả năng vận động cho bệnh nhân bị tổn thương tủy sống. Ngoài ra, SNNs có thể xử lý tín hiệu thần kinh phức tạp và tạo ra các xung kích thích để kích hoạt cơ bắp, giúp bệnh nhân khôi phục một số khả năng vận động. SNNs còn được sử dụng trong phát hiện các nhịp tim bất thường dựa trên tín hiệu điện tim (ECG) theo kết quả nghiên cứu trong [8].
0	Nhờ khả năng học và phân tích theo thời gian thực, SNNs có thể phát hiện các tình trạng nhịp tim không đều một cách chính xác, giúp hỗ trợ chẩn đoán các bệnh tim mạch và ngăn ngừa nguy cơ đột quỵ. Vì vậy, nhóm tác giả thực hiện nghiên cứu này với mục tiêu sẽ khắc phục những khoảng trống trong nghiên cứu về SNN để phát triển được các mô hình học máy dựa trên kiến trúc SNN để thực hiện được các mục tiêu chẩn đoán và phát hiện bệnh sớm bệnh động kinh thông qua tín hiệu điện não đồ đảm bảo độ chính xác và nhanh chóng.
0	SNNs là một loại mạng nơ-ron chịu ảnh hưởng từ cách hoạt động của các tế bào thần kinh sinh học. Khác với các ANNs truyền thống giao tiếp bằng các số thực liên tục, các nơ-ron trong SNNs truyền đạt thông tin thông qua các chuỗi xung phân tách [9]. Khi một nơ-ron tiền khớp thần kinh (synapse) phát đi một xung, nó ảnh hưởng đến điện thế màng của các nơ-ron sau synapse. Ở cấp độ synapse, các xung này được chuyển đổi thành các điện thế sau synapse (Post-synaptic potentials - PSP), làm thay đổi điện thế màng của nơ-ron nhận.
0	Sự phát triển của điện thế màng của một nơ-ron được xác định bởi các PSP đến, và nếu điện thế này vượt qua ngưỡng cụ thể, nơ-ron sẽ phát ra một xung, sau đó được truyền đến các nơ-ron kết nối của nó [10]. Các hoạt động của một SNNs được minh họa trong Hình 1. Trong hình ảnh này, các chuỗi xung được đưa vào mạng và kết nối với một nơ-ron xung thông qua các synapse. Các trọng số synapse quy định mức điện thế của PSP, từ đó ảnh hưởng đến độ mạnh của kết nối giữa hai nơ-ron. Hình đi kèm cũng mô tả động lực của điện thế màng trong một nơ-ron xung, được thể hiện bởi mô hình nơ-ron đơn giản nhất được gọi là mô hình Tích lũy và Phát xung.
0	Một trong những sự khác biệt chính giữa ANNs truyền thống và SNNs là cơ chế cập nhật. Trong các ANNs thông thường, tất cả các nơ-ron trong mỗi lớp đều được cập nhật trong mỗi chu kỳ tính toán, dẫn đến nhu cầu tính toán cao khi cấu trúc mạng mở rộng. Ngược lại, SNNs chỉ cập nhật giá trị nơ-ron khi xảy ra xung, giúp chúng tiết kiệm tài nguyên tính toán hơn so với ANNs. Tính năng này cho phép SNNs thực hiện xử lý tín hiệu tiêu thụ điện năng thấp cho các tác vụ phức tạp, điều này đặc biệt hữu ích trong các ứng dụng như giám sát và phân loại theo thời gian thực.
0	Các xung đến được truyền qua các synapse, nơi các xung này được chuyển đổi thành các điện thế sau synapse (PSP) với trọng số. Trong hình, điện thế màng được hiển thị, và một xung được phát ra khi điện thế vượt quá ngưỡng Vth. Điện thế màng này được cập nhật dựa trên các PSP nhận được từ các synapse và mô hình nơ-ron (hàm kích hoạt). Quy trình làm việc chung của mạng nơ-ron xung để nhận dạng bệnh động kinh được thể hiện qua Hình 2. Đầu tiên, dữ liệu đầu vào là các tín hiệu điện áp đo được từ các điện cực não, thường là các tín hiệu EEG ghi lại hoạt động điện của não qua các điện cực đặt trên da đầu.
0	Dữ liệu được sử dụng trong bài báo này là các điện thế trường cục bộ (LFP-Local Field Potentials) được ghi lại từ các lát cắt của não, được thu thập trong dự án Châu Âu HERMES [11]. Mục tiêu của dự án này là chữa các rối loạn não như động kinh thông qua việc cấy ghép mô não. Đây là sự tích hợp của mô não được kỹ thuật sinh học, điện tử thần kinh và trí tuệ nhân tạo, có khả năng đọc và kích thích khu vực bị ảnh hưởng. Tiếp theo, sử dụng phương pháp mã hóa tiếp bước (Step-Forward Encoding-SFE) để chuyển đổi tín hiệu liên tục này thành các xung điện [12].
0	Trong quá trình này, sự thay đổi điện áp trong một khoảng thời gian xác định sẽ được chuyển đổi thành các xung bằng cách so sánh mức độ thay đổi. Sau đó, các xung điện này được đưa vào mạng SNNs với kiến trúc mạng là mạng truyền thẳng để nhận diện các mẫu xung liên quan đến động kinh. Kiến trúc mạng truyền thẳng trong SNNs bao gồm các lớp nơ-ron được kết nối theo một hướng từ đầu vào đến đầu ra mà không có vòng lặp. Điều này giúp cải thiện tính đơn giản và hiệu quả trong quá trình học.
0	Trong mạng nơ-ron chuyển tiếp (Feed Forward), có ba thành phần chính: lớp đầu vào (input layer), lớp ẩn (hidden layer) và lớp đầu ra (output layer). Các lớp này được kết nối với nhau thông qua các synapse với trọng số khác nhau, và số lượng kết nối giữa các nơ-ron trong các lớp có thể được điều chỉnh tùy thuộc vào mục đích sử dụng. Trong phạm vi của bài báo này, nhóm tác giả triển khai hai cấu trúc mạng feedforward: mạng feedforward nhỏ và mạng feedforward lớn, nhằm đánh giá hiệu quả của việc lựa chọn cấu trúc mạng với số kết nối phù hợp cho mục tiêu nhận dạng và chẩn đoán bệnh động kinh từ tín hiệu điện não.
0	Mạng Feedforward thường được huấn luyện bằng các quy tắc học Hebbian, cụ thể là quy tắc STDP (Spike-Timing-Dependent Plasticity) [13]. Trong đó, các xung từ nơ-ron tiền synaptic (presynaptic spikes) xảy ra vài ms trước các xung từ nơ-ron hậu synaptic (postsynaptic spikes) sẽ dẫn đến khớp thần kinh sẽ tăng cường (long-term potentiation - LTP) tức tăng trọng số, trong khi các xung từ nơ-ron hậu synaptic xảy ra trước vài ms các xung từ nơ-ron trước synaptic sẽ dẫn đến sự giảm của khớp thần kinh (long-term depression – LTD) tức giảm trọng số. Trong mạng nơ-ron chuyển tiếp (Feed Forward), có ba thành phần chính: lớp đầu vào (input layer), lớp ẩn (hidden layer) và lớp đầu ra (output layer).
0	Các lớp này được kết nối với nhau thông qua các synapse với trọng số khác nhau, và số lượng kết nối giữa các nơ-ron trong các lớp có thể được điều chỉnh tùy thuộc vào mục đích sử dụng. Trong phạm vi của bài báo này, nhóm tác giả triển khai hai cấu trúc mạng feedforward: mạng feedforward nhỏ và mạng feedforward lớn, nhằm đánh giá hiệu quả của việc lựa chọn cấu trúc mạng với số kết nối phù hợp cho mục tiêu nhận dạng và chẩn đoán bệnh động kinh từ tín hiệu điện não. Mạng Feedforward thường được huấn luyện bằng các quy tắc học Hebbian, cụ thể là quy tắc STDP (Spike-Timing-Dependent Plasticity) [13].
0	Trong đó, các xung từ nơ-ron tiền synaptic (presynaptic spikes) xảy ra vài ms trước các xung từ nơ-ron hậu synaptic (postsynaptic spikes) sẽ dẫn đến khớp thần kinh sẽ tăng cường (long-term potentiation - LTP) tức tăng trọng số, trong khi các xung từ nơ-ron hậu synaptic xảy ra trước vài ms các xung từ nơ-ron trước synaptic sẽ dẫn đến sự giảm của khớp thần kinh (long-term depression – LTD) tức giảm trọng số. Cuối cùng, hệ thống sẽ phân loại các xung này thành hai nhóm: xung động kinh và xung không động kinh. Quy trình này giúp nhận diện và phân loại hiệu quả các xung động kinh từ dữ liệu EEG, hỗ trợ trong việc chẩn đoán và theo dõi tình trạng động kinh.
0	Trong nghiên cứu này, tín hiệu EEG từ các bệnh nhân động kinh được thu thập và xử lý để tạo ra các tập dữ liệu. Sau đó, nhóm tác giả xây dựng một SNNs với kiến trúc mạng truyền thẳng để phân tích và nhận dạng các cơn động kinh. Các tham số của mạng như số lượng nơ-ron, kết nối và quy tắc học được tối ưu hóa để đạt hiệu suất tốt nhất. Dữ liệu cho bài báo này là các tín hiệu điện trường cục bộ (Local Field Potentials - LFPs) được ghi lại từ các lát cắt của hồi hải mã bằng một mảng điện cực siêu nhỏ (Micro Electrode Array - MEA) [18].
0	Mỗi bảng ghi bao gồm 28 điện cực, đo LFP tại các điểm gần nhau trong hồi hải mã. Mỗi luồng dữ liệu được gắn nhãn thành ba loại: giai đoạn cơ bản (baseline), giai đoạn giữa các cơn động kinh (interictal), và giai đoạn cơn động kinh (ictal) [16]. Giai đoạn ictal biểu thị khi có cơn động kinh xảy ra trong não, trong khi giai đoạn interictal là các hoạt động bất thường của não xảy ra giữa các giai đoạn ictal. Giai đoạn baseline là khi không có bất thường nào được ghi nhận. Tập dữ liệu bao gồm ba lần ghi lại vào ba ngày khác nhau, mỗi lần ghi kéo dài từ 1200 giây đến 1700 giây, là chuỗi 2,4 triệu giá trị đo điện áp theo thời gian.
0	Một nửa số điện cực trong tập dữ liệu đo được cả các sự kiện giữa cơn co giật (interictal) và các sự kiện co giật (ictal), trong khi nửa còn lại chỉ đo được các sự kiện co giật. Điều này là do vị trí của các điện cực. Cơn co giật sẽ không xảy ra tại cùng một thời điểm ở các vị trí khác nhau trong não, do đó có một độ trễ nhỏ giữa các cơn co giật tại các điện cực trong cùng một não bộ. Phương pháp này tạo ra hai chuỗi xung, một dương và một âm. Chuỗi xung dương chứa các đoạn dốc dương của tín hiệu, còn chuỗi xung âm chứa các đoạn dốc âm. Thường khi đưa vào SNN, hai chuỗi này được gộp lại thành một chuỗi xung, vì các xung âm không thể xử lý trực tiếp.
0	Đối với dữ liệu LFP sử dụng trong nghiên cứu này, tỷ lệ phát xung trung bình (AFR) xấp xỉ 33%, nghĩa là trung bình cứ ba mẫu thì sẽ phát một xung. Trong Hình 5, so sánh giữa tín hiệu gốc và tín hiệu tái tạo được hiển thị. Tín hiệu tái tạo theo sát tín hiệu gốc, chỉ với một sai lệch nhỏ, do ngưỡng được đặt. Ngưỡng để phát xung được xác định bằng cách tìm giá trị có sai số bình phương trung bình (MSE) thấp nhất giữa dữ liệu gốc và dữ liệu tái tạo. Kết quả tìm kiếm tối ưu này có trong Hình 6, quá trình tìm kiếm giá trị lỗi bình phương trung bình thấp nhất (MSE) được thực hiện.
0	Việc tìm kiếm được thực hiện với ngưỡng từ 1 đến 20, tăng dần từng bước 0,1. Kết quả tối ưu tìm được là 4,6, với MSE là 16,3. Như đã mô tả trước đó, mạng chuyển tiếp (feedforward) là cấu trúc mạng đơn giản nhất. Tuy nhiên, khi áp dụng để phân loại, một số tham số có thể được điều chỉnh để đạt kết quả tối ưu. Kích thước và cấu trúc của mạng, cũng như các mô hình nơ-ron, sẽ có ảnh hưởng lớn đến kết quả. Sau đây là nghiên cứu về kết quả xử lý dữ liệu với các mạng chuyển tiếp khi thay đổi kích thước và cấu trúc mạng, số lượng khớp thần kinh hưng phấn/ức chế và các cơ chế học.
0	Mạng đầu tiên được mô phỏng bao gồm 5 nơ-ron LIF (Leaky Integrate-and-Fire) [14], [15], được kết nối như thể hiện trong Hình 7. Các xung dương từ việc mã hóa được truyền vào nơ-ron ở tầng đầu vào phía trên, trong khi các xung âm được đưa vào nơ-ron ở tầng đầu vào phía dưới. Hai nơ-ron này được kết nối với tầng ẩn thông qua một synapse kích thích tĩnh theo phương thức một-một. Các trọng số giữa tầng ẩn và đầu ra được khởi tạo với giá trị ngẫu nhiên trong khoảng từ 100 đến 200, và sau đó được huấn luyện bằng phương pháp STDP trong 400 giây.
0	Kết quả được trình bày trong Hình 8, cho thấy các trọng số đã huấn luyện được giữ cố định trong suốt quá trình mô phỏng. Trong Hình 9, quá trình học của một synapse với STDP được thể hiện. Thời gian học diễn ra nhanh chóng và trọng số hội tụ về giá trị tối đa. Giá trị cuối cùng đạt được là 0,95% của giá trị tối đa, cho thấy rằng trọng số hội tụ về trọng số tối đa cho phép. Cấu trúc huấn luyện này nhằm nâng cao độ chính xác của phản hồi mạng bằng cách tinh chỉnh trọng số của synapse để cải thiện hiệu quả phản ứng và lọc xung.
0	Mạng nơ-ron đã chỉ ra rằng, tần suất xung tăng nhanh khi có sự hiện diện của cơn co giật động kinh, cho thấy khả năng phát hiện cơn co giật. Nguyên nhân cho điều này có thể được tìm thấy trong nội dung tần số của tín hiệu LFP (điện thế trường cục bộ) từ não, vì tần số trong suốt cơn co giật thường tăng lên [16]. Việc mã hóa dữ liệu tương tự thành các chuỗi xung có khả năng bảo tồn nội dung tần số, cho phép mạng feedforward đơn giản lọc bỏ các khoảng thời gian tần số thấp và chỉ phát xung trong các khoảng thời gian tần số cao.
0	Trong thí nghiệm vừa trình bày, trọng số nhanh chóng đạt tới giá trị tối đa, điều này có nghĩa là nó sẽ còn tăng thêm nếu giới hạn trọng số cao hơn. Do đó, có thể cho rằng mạng này hoạt động chủ yếu do giới hạn trọng số, thay vì do quá trình học của synapse. Mặc dù, một mạng rất nhỏ đã chứng tỏ khả năng phát hiện cơn co giật, nhưng không thể kỳ vọng vào việc học tập mạnh mẽ. Do đó, một mạng ba lớp lớn hơn với 150 nơ-ron đã được tạo ra. Tầng đầu vào bao gồm 50 nơ-ron LIF, nơi các xung đầu vào được đưa vào qua một synapse tĩnh với trọng số được rút ra từ một phân phối chuẩn trước khi tạo ra mạng.
0	Các xung đầu vào ở đây là các xung dương và âm từ SFE, được nối thành một vector duy nhất. Mỗi nơ-ron trong tầng đầu vào (50 nơ-ron LIF) được kết nối với 10 nơ-ron ngẫu nhiên trong tầng ẩn (50 nơ-ron). 40 nơ-ron (80%) trong số này có trọng số synapse ban đầu được thiết lập từ một phân phối đồng nhất trong khoảng từ 0 đến 100. 10 nơ-ron còn lại (20%) có synapse được khởi tạo từ một phân phối đồng nhất trong khoảng từ -50 đến 0. STDP được sử dụng để huấn luyện trọng số của các synapse. Các nơ-ron trong tầng ẩn được kết nối với tầng đầu ra (50 nơ-ron LIF) theo phương thức kết nối một-một. Các synapse này được huấn luyện theo cách tương tự như các synapse giữa tầng đầu vào và tầng ẩn.
0	Quá trình học STDP diễn ra trong 400 giây và sau đó trọng số được giữ cố định. Đánh giá trọng số cho thấy, tất cả đều hội tụ về một giá trị sau 400 giây, nghĩa là thời gian học 400 giây là đủ. Mạng này được phác thảo trong Hình 10. Quan sát các xung phát ra từ tất cả các nơ-ron trong tầng đầu ra cho phép phát hiện cơn co giật. Kết quả này được trình bày trong Hình 11. Mạng này giải quyết vấn đề phát xung giữa các cơn co giật, do đó không cần bất kỳ mạch hay quy trình nào để phát hiện tần số xung.
0	Tuy nhiên, điều này đi kèm với việc mạng trở nên lớn hơn nhiều so với trước. Mạng này có các synapse ức chế từ 20% nơ-ron của nó, và điều này đã cho kết quả tốt hơn so với việc giảm hoặc tăng tỷ lệ này. Khi giảm số lượng synapse ức chế, số lượng xung tại đầu ra tăng lên, dẫn đến hiện tượng phát xung giữa các cơn co giật, gây ra sự không rõ ràng trong việc phát hiện. Ngược lại, nếu tăng đáng kể tỷ lệ này (trên 35%), số lượng xung trong mạng sẽ giảm đi rất nhiều. Hệ quả là hiện tượng phát xung trở nên thưa thớt ở đầu ra, do đó không thể phát hiện đúng các sự kiện co giật.
0	Các kết quả cho thấy rằng ngay cả một mạng đơn giản chỉ với 5 nơ-ron cũng có thể lọc được một lượng đáng kể các xung giữa các cơn co giật của bệnh động kinh, đồng thời vẫn duy trì được tần số cao của các xung khi có cơn co giật. Tuy nhiên, cấu hình này chưa đủ để phát hiện một cách đáng tin cậy, vì cần thêm logic bổ sung để nhận diện tần số xung cao một cách chính xác. Ngoài ra, việc học hiệu quả với một mạng nhỏ như vậy là một thách thức. Việc mở rộng mạng lên 150 nơ-ron đã cho thấy hiệu suất được cải thiện, với chỉ một số ít xung xuất hiện ngoài các sự kiện co giật.
0	Trong đó, TP (True Positive): SNN phát hiện đúng các xung co giật khi chúng thực sự xảy ra; TN (True Negative): SNN không phát hiện các xung co giật khi chúng thực sự không xảy ra; FP (False Positive): SNN phát hiện nhầm xung co giật khi thực tế không xảy ra; FN (False Negative): SNN bỏ sót các xung co giật khi chúng thực sự xảy ra. Một yếu tố quan trọng để đạt được kết quả tốt là có một mạng cân bằng, với tỷ lệ tối ưu giữa các synapse kích thích và ức chế được đánh giá cẩn thận. Trong nghiên cứu này, tỷ lệ tốt nhất được tìm thấy là 20% ức chế và 80% kích thích, phù hợp với các nghiên cứu trước đây [17].
0	Vì các kết quả được trình bày ở mục triển khai chỉ được thử nghiệm trên một điện cực dữ liệu, nên các mô phỏng bổ sung đã được thực hiện trên các điện cực khác để xác minh hiệu suất. Điều này được thực hiện trên 3.800 giây dữ liệu bổ sung, trong đó không có đào tạo thêm và các trọng số được cố định ở mức đã tìm thấy trước đó. Kết quả cho ra các đầu ra rất giống nhau, cho thấy phương pháp này có khả năng phát hiện co giật từ các luồng dữ liệu khác nhau. Kết quả từ các thử nghiệm này được trình bày trong Hình 12 và Hình 13, có thể thấy rằng các xung co giật của bệnh động kinh cũng được nhận dạng đúng trong khung nhãn, chỉ vài xung nằm ngoài.
0	Khi được tối ưu hóa dữ liệu từ tín hiệu điện não đồ bằng thuật toán SFE, nghiên cứu tạo ra được các chuỗi xung có khả năng mã hóa tín hiệu tương tự với mức tổn thất tín hiệu thấp. Phương pháp mã hóa này có khả năng bảo tồn nội dung tần số của dữ liệu tương tự, điều này đặc biệt quan trọng khi làm việc với các cơn co giật động kinh, vì tần số thường tăng lên trong thời gian xảy ra cơn co giật. Một lợi ích khác là nó dễ dàng tối ưu hóa thông qua việc tìm kiếm đơn giản cho MSE thấp nhất, và tỷ lệ phát xung trung bình khi được tối ưu hóa thấp hơn 33%.
0	Tỷ lệ phát xung thấp là có lợi, vì các nơ-ron chỉ được kích hoạt khi có xung, tức là năng lượng chỉ được tiêu thụ trong các sự kiện phát xung. Các chuỗi xung sau khi được tạo ra đã được đánh giá dựa trên hai kiến trúc mạng khác nhau, đó là mạng feedforward nhỏ và lớn. Cả hai mạng đều có khả năng học cách phát hiện các cơn co giật động kinh từ các chuỗi xung. Tuy nhiên, kết quả tốt nhất đạt được là độ trễ phát hiện 97 ms và độ chính xác là 96,3% cho mạng feedforward với 150 nơ-ron và với chỉ một số ít xung xuất hiện ngoài các sự kiện co giật.
0	Mạng nơ-ron xung với kiến trúc feedforward đã chứng minh là một giải pháp hiệu quả cho việc nhận dạng bệnh động kinh từ tín hiệu EEG. Nghiên cứu này mở ra hướng đi mới trong việc ứng dụng các công nghệ nơ-ron học vào y học, hứa hẹn mang lại những phương pháp điều trị tiên tiến hơn cho bệnh nhân động kinh. Trong tương lai, nhóm tác giả sẽ tiếp tục tối ưu hóa mô hình và mở rộng nghiên cứu trên nhiều loại dữ liệu hơn để nâng cao độ chính xác và khả năng tổng quát của hệ thống.
0	Công nghệ điện toán đám mây đang là một xu hướng ứng dụng công nghệ thông tin mới và dần trở thành nền tảng để giải quyết các bài toán dữ liệu lớn. Nghiên cứu này ứng dụng khai thác công nghệ điện toán đám mây Google Earth Engine (GEE) để xử lý chiết tách thông tin diện tích ngập lụt từ dữ liệu ảnh vệ tinh quang học Landsat (TM, ETM, OLI) giai đoạn 1996-2016 và vệ tinh radar Sentinel-1 giai đoạn 2015-2017 cho khu vực tỉnh Đồng Tháp, hạ lưu sông Mê Công. Kết quả chuỗi các bản đồ ngập lụt được thành lập chỉ ra lũ ở khu vực Đồng Tháp gây ra diện ngập lớn nhất vào thời điểm năm 2000 chiếm 77,68% diện tích toàn tỉnh, và giảm rõ rệt những năm gần đây, ngập 27,76% năm 2015.
0	Ngoài việc sử dụng để hiệu chỉnh mô hình dự báo ngập lụt, kết quả này cung cấp thêm luận cứ khoa học và thông tin tin cậy cho việc quản lý khai thác và sử dụng nguồn nước ở địa phương. Bên cạnh đó, nghiên cứu cũng cho thấy công cụ GEE có tốc độ truy cập và xử lý ảnh vệ tinh rất nhanh với độ tin cậy cao. Đây là công cụ rất có tiềm năng trong việc khai thác, xử lý, phân tích ảnh vệ tinh và các dữ liệu không gian khác cho nhiều mục tiêu nghiên cứu.
0	Thành lập bản đồ ngập lụt từ tư liệu ảnh viễn thám là bài toán rất phổ biến trong lĩnh vực công nghệ viễn thám (MarionTanguy, 2017). Bản đồ ngập lụt sẽ giải quyết một số các nhu cầu rất bức thiết như xác định khu vực bị ảnh hưởng, ước tính thiệt hại do lũ lụt, xác định vết lũ để hiệu chỉnh mô hình dự báo. Cùng với sự tiến bộ nhanh chóng của khoa học công nghệ, các phương pháp, tư liệu phục vụ thành lập bản đồ ngập lụt cũng có những thay đổi để phù hợp với xu thế hiện nay. khác nhau.
0	Tại Hoa Kỳ, Klemas (2015) đã nghiên cứu vùng ngập lụt khu vực sông Mississippi bằng cặp ảnh quang học Landsat TM trước và trong thời điểm ngập lụt; Kucera (2014) đã dùng ảnh radar Sentinel-1A để thành lập bản đồ ngập lụt khu vực bán đảo Balkan dựa trên ngưỡng giá trị tán xạ ngược của phân cực VV trên ảnh. Trong nghiên cứu nâng cao độ chính xác bản đồ ngập lụt từ ảnh Sentinel, năm 2017 tại Đức, Clement đã sử dụng chuỗi 15 ảnh Sentinel 1 phân cực VV kết hợp với thông tin ngập lụt chiết tách từ dữ liệu viễn thám quang học Landsat, cho phép chiết tách được của từng điểm ngập nhỏ với độ chính xác cao (M.A. Clement, 2017).
0	Tổ chức UNSPIDER năm 2015 cũng đưa ra phương pháp thành lập bản đồ ngập lụt bằng cách sử dụng cặp ảnh Sentinel-1A trước và trong thời điểm ngập (UNSPIDER, 2015). Ở Việt Nam, các nghiên cứu thành lập bản đồ ngập lụt đã được tiến hành khá nhiều và cũng đã xây dựng thành các quy trình, có thể kể tới các nghiên cứu tiêu biểu như: nghiên cứu sử dụng tư liệu ảnh radar ERS-2 SAR PRI trong thành lập bản đồ ngập lụt của Nguyễn Thành Long và Bùi Doãn Trọng (Long N.T., 2001); nghiên cứu sử dụng dữ liệu ảnh radar ENVISAT và RADARSAT-1 theo dõi đánh giá thành lập bản đồ ngập lũ tại tỉnh Long An (Quân N.H., 2013).
0	Hầu hết việc thu thập và xử lý ảnh viễn thám trong các nghiên cứu trên đều thực hiện theo cách truyền thống đó là tải dữ liệu về, sau đó xử lý từng cảnh ảnh sử dụng các phần mềm hoặc công cụ trên máy tính cá nhân. Phương thức này vẫn còn phổ biến hiện nay, tuy nhiên nó có nhiều nhược điểm đó là: tốc độ xử lý phụ thuộc vào độ lớn của dữ liệu cũng như cấu hình máy tính. Những dữ liệu miễn phí hiện nay, dung lượng có thể lên tới vài Gb trên một cảnh ảnh, ví dụ ảnh landsat 8 khoảng 1.8Gb, ảnh Sentinel-2 lên tới trên 6Gb, Sentinel-1 trên 1 Gb, việc xử lý đòi hỏi máy tính có cấu hình phải rất mạnh trên các phần mềm thương mại đắt tiền như ENVI, ERDAS.
0	Bên cạnh đó việc thu thập dữ liệu cũng chiếm nhiều thời gian. Điện toán đám mây đang là một xu thế mới trong xử lý, phân tích và lưu trữ dữ liệu hiện nay. Xuất phát từ những yêu cầu khoa học và công nghệ trong lĩnh vực công nghệ không gian, Google Earth Engine (GEE) đã được nghiên cứu và phát triển. Đây là công nghệ được phát triển trên nền tảng điện toán đám mây, rất mạnh để xử lý ảnh vệ tinh cũng như các nguồn dữ liệu quan trắc khác. Những ứng dụng được khai thác ban đầu trên nền tảng GEE có thể kể tới như sử dụng GEE để phát hiện suy thoái và mất rừng trên phạm vi toàn cầu nhờ nguồn dữ liệu ảnh Landsat đa thời gian,
0	hay sử dụng GEE để phân loại lớp phủ, sử dụng GEE để ước tính sinh khối và trữ lượng carbon rừng (M.C. Hansen, 2013; N. Patel, 2015). Tổ chức SERVIR-MEKONG cũng đã sử dụng công cụ này nhằm hỗ trợ kiểm kê và quản lý nguồn nước mặt. Xuất phát từ thực tiễn trên, nghiên cứu này hướng tới mục tiêu thử nghiệm khả năng phân tích, xử lý và khai thác hệ thống cơ sở dữ liệu ảnh của Google trong việc chiết tách thông tin diện ngập lũ. Công tác thành lập bản đồ ngập lụt được thực hiện trên nền tảng công nghệ điện toán đám mây Google Earth Engine. Mọi công đoạn từ thu thập dữ liệu, xử lý, phân tích ảnh đều tiến hành trực tuyến trên hệ thống máy chủ của Google.
0	Từ đó, đề xuất các phương án sử dụng, khai thác công cụ GEE đầy tiềm năng này. Tỉnh Đồng Tháp là tỉnh nơi sông Mê Công chảy vào Việt Nam từ Campuchia. Đây là nơi có đặc điểm lũ đặc trưng cho khu vực Đồng bằng sông Cửu Long (ĐBSCL). Là tỉnh nằm ở vùng hạ lưu sông Mê Công, Đồng Tháp có hệ thống sông rạch lớn với hai sông chính là sông Tiền và sông Hậu. Sông Tiền chảy vào Việt Nam ở khu vực giáp ranh giữa huyện Hồng Ngự tỉnh Đồng Tháp và Tân Châu, tỉnh An Giang và sông Hậu chảy qua Đồng Tháp qua địa phận huyện Lấp Vò, Lai Vung.
0	Đặc điểm lũ tại tỉnh Đồng Tháp chủ yếu do mưa trên lưu vực sông Mê Công gây ra. Hàng năm, mùa lũ xảy ra đồng thời với mùa mưa, kéo dài liên tục khoảng 4-5 tháng, thường từ tháng 6 đến tháng 11. Lũ ĐBSCL thường có hai đỉnh, đỉnh đầu thường xảy ra cuối tháng 7 đến giữa tháng 8, còn gọi là lũ đầu vụ, lũ tháng 8; đỉnh sau cuối tháng 9 đến đầu tháng 10, thường gọi là lũ chính vụ. Tuy vậy, lũ chính vụ có thể xảy ra muộn hơn, ví dụ như năm 2011, và một số năm không có lũ hay lũ rất nhỏ, chẳng hạn như 1998, 2015. Lũ đồng bằng lên và xuống chậm, theo đúng bản chất của lũ lưu vực lớn.
0	Nói chung, cường suất lũ từ 2-3 cm đến 10-15 cm/ngày. Lũ đầu vụ có cường suất lớn hơn lũ chính vụ. Trong thời gian qua, lũ ở ĐBSCL đang có những biến đổi khác với trước đây, lũ lớn dường như xuất hiện ít hơn trong khí đó lũ vừa và nhỏ nhiều hơn. Lũ trên sông Mê Công vào nước ta theo hai hướng: (1) dòng chính Mê Công; và (2) tràn qua biên giới. Lũ tràn quan biên giới có tác động gây ngập lớn trên đồng bằng. Hiện nay, lũ tràn ở Tứ giác Long Xuyên đã được kiểm soát khá tốt, còn ở Đồng Tháp Mười gần như chưa kiểm soát (SIWRR, 2013).
0	GEE là nền tảng điện toán đám mây để phân tích dữ liệu viễn thám, thông số môi trường, khí tượng cấp độ từ khu vực nhỏ tới quy mô toàn cầu. Trên GEE có tích hợp sẵn một kho lưu trữ hàng chục petabyte dữ liệu ảnh viễn thám miễn phí từ Cơ quan Hàng không Vũ trụ Mỹ (NASA), Cục Địa chất liên bang Hoa Kỳ (USGS), Cơ quan hàng không vũ trụ Châu Âu (ESA) và các dữ liệu khác. Hạ tầng điện toán đám mây của của GEE được tối ưu hoá để xử lý dữ liệu không gian, kể cả xử lý chuỗi dữ liệu viễn thám trong khoảng thời gian dài với dung lượng rất lớn. Điều này cho phép xử lý, chiết tách được thông tin ngập lũ trong lịch sử một cách đồng bộ trên diện rộng.
0	Nghiên cứu này sử dụng ảnh vệ tinh trong khoảng thời gian 20 năm, giai đoạn 1996-2017. Đây cũng là giai đoạn xảy ra các trận lũ lịch sử tại Đồng Tháp. GEE ra đời xuất phát từ những ý tưởng kết hợp tri thức khoa học với nguồn dữ liệu khổng lồ và các nguồn lực công nghệ mới nhất của Google. Sự kết hợp này đem đến những hiệu quả rất lớn như tốc độ xử lý và khả năng tùy biến phát triển ứng dụng. Tốc độ tính toán, xử lý trên GEE nhanh chưa từng có (Noel Gorelick, 2017).
0	Thông thường việc tải và xử lý ảnh viễn thám nhiều thời điểm trong một khu vực nghiên cứu rộng như một tỉnh, một vùng hay toàn quốc mất thời gian tới hàng tuần thậm chí tới vài tháng cho dù sử dụng một máy tính để bàn với cấu hình mạnh và các phần mềm thương mại phổ biến. Tuy nhiên với nền tảng điện toán đám mây của GEE, việc tính toán này chỉ mất khoảng thời gian tính bằng một vài phút nhờ vào việc phân vùng dữ liệu xử lý song song trên hệ thống máy chủ của Google. Google Earrth Engine cho phép xây dựng chương tình tính toán dựa trên một giao diện lập trình ứng dụng (API) sử dụng ngôn ngữ lập trình rất thông dụng là JavaScript và Python.
0	Từ giao diện API này, nhóm nghiên cứu đã xây dựng chương trình chiết tách thông tin diện ngập lũ từ tập dữ liệu ảnh Landsat và Sentinel-1. Nghiên cứu sử dụng dữ liệu ảnh vệ tinh quang học Landsat (cho giai đoạn 1996 trở lại đây) và vệ tinh radar Sentinel-1 (giai đoạn 2015 trở lại đây). Đây là 2 bộ cơ sở dữ liệu ảnh rất lớn của Mỹ và Châu Âu gồm NASA, USGS và ESA đã được tích hợp toàn bộ vào hệ thống cơ sở dữ liệu của GEE. Dữ liệu ảnh Landsat được sử dụng bao gồm ảnh Landsat-5 TM, Landsat-7 ETM+, Landsat8 OLI với các đặc tính chung như độ phân giải không gian các kênh đa phổ là 30 m, thời gian chụp lặp 16 ngày,
0	số kênh phổ đủ lớn để thực hiện các nhiệm vụ quan sát, theo dõi nhiều loại đối tượng, hiện tượng trên bề mặt trái đất, trong đó có lũ lụt. Dữ liệu ảnh vệ tinh radar Sentinel-1A, 1B là dữ liệu vệ tinh thế hệ mới của cơ quan hàng không vũ trụ châu Âu. Các vệ tinh này được phát triển để cung cấp dữ liệu ảnh viễn thám giám sát toàn cầu thuộc Chương trình Europe’s Copernicus. Thông qua việc cung cấp nguồn dữ liệu chất lượng cao và miễn phí trên phạm vi toàn cầu, Chương trình này sẽ tạo bước thay đổi trong cách quản lý, giám sát môi trường, hiểu và giải quyết các ảnh hưởng của biến đổi khí hậu.
0	Vệ tinh này được thiết kế để thực hiện các nhiệm vụ như: giám sát biển, bao gồm giám sát tràn dầu và quản lý an ninh hàng hải; giám sát mặt đất đối với rủi ro sạt lở đất đá, quản lý tài nguyên rừng, nước mặt và đất đai, lập bản đồ ứng phó với các tình huống khẩn cấp, thiên tai phục vụ nhiệm vụ cứu hộ, nhân đạo trong đó đặc biệt là lập bản đồ ngập lụt. Sentinel-1A được phóng vào quỹ đạo ngày 3/4/2014 và Sentinel-1B phóng ngày 25/4/2016. Dữ liệu Sentinel-1A và 1B hiện nay đã được cung cấp miễn phí trên hệ thống điện toán đám mây của GEE, sản phẩm được đưa vào khai thác sử dụng sau khi đã tiến hành tiền xử lý theo quy trình tiêu chuẩn của công cụ tiền xử lý Sentinel.
0	Do vậy dữ liệu Sentinel 1 sử dụng phục vụ chiết tách thông tin vùng ngập lũ tại tỉnh Đồng Tháp được bắt đầu từ năm 2015 tới hiện nay. Trong năm 2015 tới cuối năm 2016 chu kỳ chụp lặp của vệ tinh là 12 ngày do mới chỉ có vệ tinh 1A được phóng lên quỹ đạo. Từ khoảng tháng 10 năm 2016 trở lại đây, sau khi bổ sung thêm vệ tinh 1B, chu kỳ chụp lặp được dày hơn, khoảng 6 ngày một ảnh. Đây là một lợi thế rất lớn trong nghiên cứu, theo dõi và đánh giá diễn biến lũ lụt.
0	Quy trình xử lý ảnh Landsat và ảnh Sentinel-1 chiết tách thông tin vùng ngập lũ được mô tả trong Hình 2 và Hình 3. Về cơ bản những quy trình này đều đã được sử dụng khá phổ biến trên thế giới. Tuy nhiên, trong nghiên cứu này ngoài việc xử lý ảnh vệ tinh thông qua GEE, còn có sự thay đổi trong quy trình chiết tách đó là sử dụng nền mặt nước thường xuyên trong năm được tổ hợp theo giá trị trung vị của khoảng thời gian trước mùa lũ. Nhờ vào phương pháp tổ hợp này, những phần diện tích mặt nước thường xuyên như sông, kênh, rạch, ao nuôi thủy sản được phân tách riêng.
0	Do đó, loại bỏ được phần diện tích mặt nước không phải do nguyên nhân ngập lũ trên ảnh vệ tinh đa thời gian. Nguyên tắc này được áp dụng cho cả 2 loại tư liệu ảnh Landsat và Sentinel-1. Tư liệu ảnh giai đoạn 1996-2017 cũng đã được sử dụng truy vấn lọc kết quả ảnh chụp theo thời điểm xảy ra lũ, đặc biệt là đỉnh lũ các năm. Lũ ở đồng bằng sông Cửu Long nói chung và ở Đồng Tháp nói riêng có đỉnh lũ có thể duy trì liên tục trong vòng 10 ngày, sau đó có thể xuống, nhưng xuống rất chậm. Do vậy, xác xuất chụp được ảnh vệ tinh tại thời điểm lũ là khá cao khi sử kết hợp các nguồn tư liệu ảnh vệ tinh hiện có.
0	Với ảnh vệ tinh quang học Landsat, quy trình xử lý tập trung vào việc loại bỏ các yếu tố nhiễu gây ra bởi mây, tính tổ hợp bù mây, sau đó tính chỉ số nước khác biệt nước - Normalized Difference Water Index (NDWI). NDWI là một phương pháp đã được phát triển để nhận diện đối tượng mặt nước và tăng cường hiển thị đối tượng mặt nước trên tư liệu viễn thám bởi McFeeters (1996). NDWI sử dụng kênh phổ phản xạ ở dải cận hồng ngoại (NIR) và kênh phổ phản xạ ở dải sóng xanh lục (Green) để phát hiện sự có hiện diện của nước mặt, đồng thời loại bỏ sự hiện diện của các đối tượng khác trên bề mặt như đất và thực vật.
0	Trong nghiên cứu này, NDWI cho phép xác định diện tích phân bố nước mặt trên ảnh chụp thời điểm lũ và diện tích nước mặt thường xuyên trên ảnh tổ hợp trước lũ (từ tháng 1 đến tháng 7). Trong đó Green tương ứng với kênh 2 của ảnh Landsat TM, ETM và kênh 3 trên ảnh Landsat 8 OLI. NIR tương ứng với kênh 4 của ảnh TM, ETM và kênh 5 của ảnh OLI. Bản đồ vùng ngập lũ là sản phẩm cuối cùng sau khi đã loại bỏ các đối tượng như mặt nước thường xuyên trong năm, các đối tượng không phải mặt nước (đất, thực vật).
0	Với ảnh radar Sentinel-1, nghiên cứu sử dụng chuỗi ảnh phân cực VV chụp trước và trong thời điểm lũ. Đặc tính sóng radar phân cực VV phản xạ lại từ bề mặt nước tới đầu thu trên vệ tinh là rất nhỏ do tín hiệu tán xạ ngược của các sóng radar từ bề mặt phẳng của nước về đầu thu rất yếu. Vì vậy, các điểm ảnh (pixel) trên ảnh radar thường có giá trị rất thấp, thể hiện bằng các điểm ảnh sẫm màu, đây là cơ sở để để phân tách diện tích nước với các đối tượng bề mặt khác.
0	Ảnh vệ tinh Sentinel-1 phân cực VV chụp trước thời điểm lũ cũng được tổ hợp giá trị trung vị để xác định các diện tích mặt nước thường xuyên, tương tự phương pháp đã tiến hành trên ảnh Landsat. Ảnh Sentinel-1 phân cực VV chụp trong thời điểm lũ sẽ cung cấp thông tin mặt nước phân bố tại thời điểm chụp, tức là bao gồm mặt nước thường xuyên và mặt nước do ngập lũ. Để chiết tách được diện ngập lũ, phương pháp lấy ngưỡng hiệu số giữa ảnh phân cực chụp trong thời điểm ngập lũ và trước khi ngập lũ.
0	Qua phân tích thống kê các điểm mẫu nước thực tế và giá trị các điểm ảnh trên phân cực VV, nghiên cứu đã lấy ngưỡng giá trị hiệu 2 thời điểm với mặt nước là nhỏ hơn -3.5 Decibel. Kết quả các bước xử lý ảnh thực hiện cho một thời điểm lũ sử dụng ảnh Landsat được minh họa trong Hình 4. Theo đó tập ảnh Landsat sau khi truy xuất từ cơ sở dữ liệu của Google sẽ được lọc bỏ mây và tính ảnh tổ hợp cho thời điểm trước và trong lũ. Các pixel được xác định là mây được loại bỏ hết, thay vào đó là các giá trị điểm ảnh được tổ hợp bù vào từ những ảnh không mây có thời gian, mùa tương tự trong năm.
0	Ảnh Landsat tổ hợp trước thời điểm lũ trong năm thể hiện được giá trị trung vị của từng pixel (Hình 4b), những phần diện tích nước thường xuyên trong năm gần như được giữ nguyên đặc tính phổ là nước do vậy dễ dàng được phát hiện (Hình 4d). Phương pháp này cho phép lấy được mép nước ổn định trong năm của các nhánh sông, cũng như ao, hồ nội đồng. Kết quả chiết tách không chỉ ứng dụng trong phạm vi đề tài nghiên cứu mà còn có thể sử dụng để nghiên cứu thay đổi hình thái sông, sạt lở và bồi tụ theo thời gian.
0	Ảnh chụp thời điểm lũ được tính toán chỉ số nước mặt NDWI, qua đó xác định được mức độ phân bố nước mặt tại thời điểm lũ. Để chiết tách riêng diện tích ngập lũ thì phải loại bỏ những pixel không bị ngập và những pixel mặt nước thường xuyên trong năm, do vậy kết hợp giữa ảnh trước và trong lũ sẽ thu được kết quả diện tích vùng ngập lũ (Hình 4e). Vùng ngập lụt này được sử dụng hàm thống kê trực tiếp trên GEE cho phép trích xuất thống kê không gian các vùng ngập lũ. Tiếp cận này được tiến hành tương tự với ảnh Landsat các năm 1996, 2000, 2001, 2004, 2008, 2009, 2011, 2014. Đây là những năm điển hình, có dữ liệu ảnh vệ tinh chụp tại thời điểm lũ.
0	Đặc biệt năm 2000 là năm có trận lũ lịch sử lớn nhất trong hơn 80 năm qua tại đồng bằng sông Cửu Long (Hối T.N., 2009). Kết quả xử lý dữ liệu ảnh radar Sentinel-1 trên Google Earth Engine, được mô tả với thời điểm lũ năm 2017 trên Hình 5. Trong đó Hình 5a thể hiện ảnh Sentinel tổ hợp những tháng trước mùa lũ, từ tháng 1 đến tháng 7. Trong đó, các pixel sẫm màu là diện tích mặt nước thường xuyên trong năm. Hình 5b thể hiện ảnh chụp tại thời điểm lũ, với khoảng diện tích mặt nước là các pixel sẫm màu phân bố khá rộng. Sau khi lấy hiệu giữa hai ảnh chụp trước và trong lũ thu được ảnh như Hình 5c.
0	Từ ảnh hiệu phân cực này, lấy ngưỡng để chiết tách vùng ngập lũ (Hình 5d), một tập hợp các điểm mẫu chiết tách đã được thống kê để phân tích ngưỡng chuẩn, giá trị chuẩn theo đơn vị decibel hiệu hai ảnh trước và trong lũ được xác định ở ngưỡng -3.5. Phương pháp này áp dụng cho ảnh Sentinel-1 năm 2015, 2016 và 2017. Thông qua công cụ GEE, nghiên cứu đã tiến hành phân tích thống kê diện tích ngập lũ các năm. Kết quả được thể hiện trong Bảng 1, biểu đồ trong Hình 6 và Hình 7.
0	Bảng thống kê (Bảng 1) và Biểu đồ (Hình 6) đã chỉ ra, càng về nửa đầu những năm 2000 trở về trước, diện tích ngập lũ càng rộng, nhất là thời điểm năm 2000, năm ghi nhận có đỉnh lũ cao thứ hai trong lịch sử, sau năm 1961. Diện tích ngập lũ lớn nhất vào năm 2000 với 77.68% diện tích tỉnh Đồng Tháp. Diện tích này cao hơn rất nhiều so với những năm còn lại, điều này phần nào cho thấy cường độ của trận lũ lịch sử này. Ngoài ra lũ năm 1996, 2001, 2004 cũng có diện tích ngập lũ khá lớn từ 68-70%,
0	trong đó lũ năm 1996 và 2001 cũng nằm vào nhóm những trận lũ lớn lịch sử tại đồng bằng sông Cửu Long. Với chuỗi số liệu kết quả thu được, diện tích ngập lũ khá tương đồng từ sau năm 2000, và hơi giảm những năm nửa cuối thập niên, chiếm khoảng 62% giai đoạn 2008-2009. Năm 2011 lũ lại tăng trở lại với cường độ rất mạnh, đỉnh lũ đạt 4.87m và diện tích ngập lũ thống kê trên ảnh vào thời điểm tháng 10 (lũ chính vụ) năm 2011 lên tới 67.84%. Số liệu đỉnh lũ cũng như thống kê diện ngập lũ trên ảnh từ sau năm 2011 đã thấy có sự giảm xuống nhanh chóng.
0	Số liệu diện tích ngập lũ thống kê trên ảnh radar Sentinel-1 giai đoạn 2015-2017 cho thấy thời gian đỉnh lũ chỉ ngập khoảng 27.76 %, thấp nhất trong giai đoạn theo dõi. Có nhiều nguyên nhân dẫn tới sự suy giảm này, trong đó phải kể tới hiện tượng El Nino (2014-2016), hoạt động xây dựng đê bao chống lũ để sản xuất lúa vụ 3, tác động tích nước và điều tiết dòng chảy của các đập thủy điện xây dựng trên thượng nguồn sông Mê Công. Đây cũng là khởi đầu cho một năm ĐBSCL gánh chịu một đợt hạn mặn nghiêm trọng nhất vào mùa khô 2015-2016.
0	Số liệu thống kê cho thấy lũ năm 2016 rất nhỏ, chỉ cao hơn năm 2015 một chút với 30.77% ngập. Tuy nhiên đến năm 2017, tình hình lũ tại khu vực Đồng Tháp đã chuyển biến, đỉnh lũ đạt 3.5m diện ngập lũ là 42.7%. Lũ 2017 được đánh giá là vừa phải, không lớn quá và không nhỏ quá đã đem lại nhiều lợi ích cho người dân sinh sống và các hoạt động sản xuất nông nghiệp tại địa phương. Số liệu năm 2011 cho thấy đỉnh lũ rất cao, cao hơn so với năm 1996, 2001 và 2004, tuy nhiên diện ngập lũ lại thấp hơn từ 2-3%.
0	Nhìn trên ảnh ngập lũ có thể thấy một số khu vực sản xuất lúa không bị ngập nhờ những tuyến đê bao ngăn lũ mới được xây dựng tại khu vực này. Biểu đồ Hình 7 mô tả diện ngập lũ so với phần không bị ngập do lũ giữa các năm. Nghiên cứu cho thấy sự đảo chiều của diện ngập lũ giai đoạn trước năm 2011 với giai đoạn sau năm 2011. Số liệu này cũng khắc họa phần nào sự thay đổi điều kiện nước mặt cả về tự nhiên (El Nino) lẫn tác động của con người trong việc xây mới các đê bao ngăn lũ và việc tích trữ, điều tiết nước từ các hồ thủy điện thượng nguồn sông Mê Công.
0	Kết quả nghiên cứu đã xác lập được quy trình xử lý, tính toán chiết tách vùng ngập lụt sử dụng tư liệu ảnh vệ tinh quang học và radar trên nền tảng điện toán đám mây GEE. Nghiên cứu đã thực nghiệm từ khâu khai thác, xử lý và phân tích ảnh vệ tinh trên hệ thống GEE, qua đó chiết tách được diễn biến ngập lụt tại khu vực tỉnh Đồng Tháp. Thông tin diện tích vùng ngập lũ chiết tách từ ảnh vệ tinh qua các năm giai đoạn 1996-2017 đã phần nào cho thấy sự tính chất đặc điểm lũ tại khu vực tỉnh Đồng Tháp và đồng bằng sông Cửu Long. Kết quả này đồng thời có thể được sử dụng để hiệu chỉnh mô hình dự báo lũ sớm cho khu vực nghiên cứu.
0	Với lợi thế phân tích dữ liệu ảnh vệ tinh cho khu vực có phạm vi lớn của GEE, khi mở rộng khu vực nghiên cứu sang vùng Campuchia và hạ Lào có thể chiết tách được thông tin mép nước, vết lũ lịch sử để cung cấp đầu vào cho mô hình dự báo lũ tại Đồng Bằng Sông Cửu Long. Nghiên cứu nhận thấy việc sử dụng nền tảng điện toán đám mây GEE có rất nhiều tiềm năng. Thực tế cho thấy tốc độ xử lý, phân tích ảnh vệ tinh nhanh hơn rất nhiều so với xử lý trên máy tính cá nhân. Do vậy hệ thống phù hợp với các nhiệm vụ có quy mô xử lý dữ liệu ở mọi cấp độ từ lớn tới nhỏ. Có thể phát triển để tích hợp xây dựng các hệ thống theo dõi giám sát gần thời gian thực.
1	Bệnh võng mạc tiểu đường là một trong những nguyên nhân hàng đầu gây mù lòa trên toàn thế giới, ảnh hưởng đến khoảng 1/3 số người mắc bệnh tiểu đường. Việc phát hiện sớm qua hình ảnh đáy mắt là vô cùng quan trọng, tuy nhiên, số lượng bác sĩ nhãn khoa có trình độ chuyên môn cao thường không đủ để đáp ứng nhu cầu sàng lọc quy mô lớn, đặc biệt là tại các khu vực đang phát triển. Nghiên cứu này tập trung vào việc phát triển một hệ thống trí tuệ nhân tạo (AI) dựa trên kiến trúc ResNet-50 để tự động phân loại các giai đoạn của bệnh từ ảnh chụp võng mạc kỹ thuật số.
1	Mục tiêu là tạo ra một công cụ hỗ trợ quyết định giúp giảm tải công việc cho các chuyên gia y tế trong khi vẫn đảm bảo độ chính xác vượt trội so với các phương pháp chẩn đoán thủ công truyền thống vốn dễ sai sót. Chúng tôi sử dụng bộ dữ liệu EyePACS bao gồm hơn 35.000 hình ảnh võng mạc đã được gán nhãn bởi các chuyên gia. Quy trình tiền xử lý bao gồm việc chuẩn hóa kích thước hình ảnh về 224x224 pixels, cân bằng độ tương phản cục bộ và sử dụng các kỹ thuật tăng cường dữ liệu (data augmentation) như xoay, lật và thay đổi độ sáng để tránh hiện tượng quá khớp (overfitting).
0	"Mô hình CNN được huấn luyện bằng hàm mất mát Cross-Entropy và thuật toán tối ưu hóa Adam với tốc độ học (learning rate) ban đầu là 0.0001. Hệ thống không chỉ tập trung vào việc phân loại ""có"" hoặc ""không"" mắc bệnh, mà còn phân tách rõ rệt 5 cấp độ bệnh lý từ bình thường đến tăng sinh nghiêm trọng, giúp định hướng lộ trình điều trị cụ thể cho từng bệnh nhân dựa trên mức độ tổn thương thực tế. Kết quả thử nghiệm trên tập dữ liệu độc lập cho thấy mô hình đạt độ chính xác tổng thể (Accuracy) là 94.5%."
1	Cụ thể hơn, độ nhạy (Sensitivity) đối với việc phát hiện các ca bệnh cần can thiệp y tế ngay lập tức đạt mức 92.3%, trong khi độ đặc hiệu (Specificity) duy trì ở mức 96.8%. Chỉ số diện tích dưới đường cong ROC (AUC) đạt 0.97, một con số ấn tượng chứng minh khả năng phân loại mạnh mẽ của AI. So với việc chẩn đoán bởi các bác sĩ nội trú với kinh nghiệm dưới 5 năm (độ chính xác trung bình khoảng 82-85%), hệ thống AI này cho thấy sự ổn định và tốc độ xử lý nhanh hơn gấp 20 lần. Những số liệu này khẳng định tiềm năng của việc triển khai AI trong các chương trình sàng lọc cộng đồng, giúp tối ưu hóa nguồn lực y tế và cứu vãn thị lực cho hàng triệu người.
1	Trong kỷ nguyên thanh toán số phát triển bùng nổ, các hành vi gian lận thẻ tín dụng ngày càng trở nên tinh vi, gây thiệt hại hàng tỷ USD mỗi năm cho hệ thống ngân hàng toàn cầu. Các phương pháp dựa trên quy luật (rule-based) truyền thống thường thất bại trong việc nhận diện các mẫu gian lận mới phát sinh hoặc tạo ra quá nhiều cảnh báo giả (false positives), gây phiền hà cho khách hàng. Nghiên cứu này đề xuất một kiến trúc kết hợp (Ensemble Learning) sử dụng Random Forest và XGBoost để phân tích hành vi giao dịch trong thời gian thực.
1	Bằng cách khai phá các thuộc tính như vị trí địa lý, tần suất chi tiêu và thói quen mua sắm, hệ thống có khả năng phân biệt chính xác giữa một giao dịch hợp lệ bất thường và một hành vi đánh cắp thông tin thẻ tinh vi. Dữ liệu đầu vào bao gồm hơn 1 triệu bản ghi giao dịch được ẩn danh hóa, với các thuộc tính chuyển đổi PCA để bảo vệ quyền riêng tư. Chúng tôi áp dụng kỹ thuật SMOTE (Synthetic Minority Over-sampling Technique) để giải quyết vấn đề mất cân bằng dữ liệu, do các giao dịch gian lận thường chỉ chiếm dưới 0.2% tổng số dữ liệu. Mô hình được thiết kế để xử lý dữ liệu theo luồng (stream processing), với độ trễ phản hồi dưới 50ms cho mỗi giao dịch.
1	Việc kết hợp nhiều bộ phân loại yếu thành một bộ phân loại mạnh thông qua cơ chế biểu quyết (voting) giúp giảm thiểu đáng kể phương sai và sai số hệ thống, đảm bảo rằng ngay cả những thay đổi nhỏ nhất trong hành vi của tội phạm mạng cũng bị hệ thống ghi nhận và gắn cờ cảnh báo kịp thời. Qua thực nghiệm, mô hình đề xuất đạt chỉ số F1-Score là 0.89, một bước tiến lớn so với mức 0.75 của các hệ thống cũ. Tỷ lệ phát hiện gian lận (Recall) tăng lên mức 91%, đồng thời quan trọng nhất là tỷ lệ báo động giả giảm từ 5.4% xuống còn 1.2%.
1	Điều này có nghĩa là hàng ngàn khách hàng sẽ không còn bị khóa thẻ oan uổng khi đi du lịch hoặc mua sắm các món đồ có giá trị cao. Về mặt kinh tế, ước tính hệ thống này có thể giúp một ngân hàng tầm trung tiết kiệm được khoảng 12 triệu USD thiệt hại trực tiếp từ gian lận mỗi năm. Nghiên cứu chứng minh rằng việc áp dụng học máy không chỉ là vấn đề kỹ thuật, mà còn là yếu tố sống còn để xây dựng lòng tin của khách hàng trong hệ sinh thái tài chính số hiện đại.
1	"Tình trạng tắc nghẽn giao thông tại các đô thị lớn không chỉ gây lãng phí thời gian mà còn làm tăng lượng phát thải CO2, ảnh hưởng nghiêm trọng đến sức khỏe cộng đồng và tăng trưởng kinh tế. Các hệ thống đèn tín hiệu giao thông hiện nay đa phần hoạt động theo chu kỳ cố định, không thể thích ứng với sự thay đổi đột ngột của lưu lượng xe theo thời gian thực. Nghiên cứu này giới thiệu một giải pháp tích hợp Deep Q-Learning (DQN) với camera thông minh sử dụng thuật toán YOLOv8 để đếm phương tiện. Mục tiêu là tạo ra một hệ thống điều khiển đèn giao thông ""tự biết suy nghĩ"", có khả năng tự động điều chỉnh thời gian đèn xanh dựa trên mật độ giao thông thực tế tại các giao lộ phức tạp, từ đó tối ưu hóa luồng di chuyển."
1	"Hệ thống sử dụng các camera được lắp đặt tại các nút giao thông để thu thập hình ảnh và xử lý tại biên (Edge Computing). Thuật toán YOLOv8 được tinh chỉnh để nhận diện đa dạng các loại phương tiện từ xe máy, ô tô đến xe buýt với độ chính xác trên 98% trong điều kiện ánh sáng ban ngày. Sau đó, dữ liệu về số lượng xe và tốc độ trung bình được đưa vào mô hình Học tăng cường (Reinforcement Learning). Tại đây, tác tử (agent) sẽ thực hiện các hành động thay đổi trạng thái đèn và nhận về ""phần thưởng"" là việc giảm thiểu thời gian chờ trung bình của tất cả các phương tiện."
1	Chúng tôi đã xây dựng một môi trường mô phỏng bằng phần mềm SUMO (Simulation of Urban Moverment) để huấn luyện mô hình qua hàng triệu kịch bản giao thông khác nhau, từ giờ cao điểm đến các tình huống tai nạn bất ngờ. Kết quả mô phỏng cho thấy hệ thống giúp giảm thời gian chờ đợi trung bình tại các ngã tư lên đến 28% và tăng lưu lượng xe lưu thông thêm 15% so với hệ thống chu kỳ cố định. Đặc biệt, lượng nhiên liệu tiêu thụ và khí thải carbon tại các điểm nóng giao thông giảm khoảng 12%, góp phần tích cực vào mục tiêu xây dựng thành phố xanh.
1	Số liệu thực tế cho thấy, nếu triển khai trên diện rộng tại một thành phố có quy mô khoảng 5 triệu dân, hệ thống AI này có khả năng giúp tiết kiệm tới 200.000 giờ làm việc mỗi ngày – một con số khổng lồ nếu quy đổi ra giá trị kinh tế trực tiếp dựa trên mức thu nhập bình quân đầu người. Không chỉ dừng lại ở khía cạnh tiền tệ, việc giảm thiểu thời gian đình trệ trong giao thông còn mang lại những lợi ích vô hình nhưng vô giá về sức khỏe tâm thần, giảm bớt căng thẳng (stress) cho người tham gia giao thông và hạn chế các bệnh lý về hô hấp do hít phải khí thải trong thời gian dài tại các điểm ùn tắc.
1	Học máy (Machine Learning) ngày càng được ứng dụng rộng rãi trong lĩnh vực y tế nhằm hỗ trợ chẩn đoán bệnh nhanh chóng và chính xác. Các thuật toán phân loại có thể phân tích hình ảnh y khoa như X-quang, MRI để phát hiện dấu hiệu bất thường. Theo báo cáo của Stanford Medicine năm 2024, hệ thống học máy có thể đạt độ chính xác tới 92% trong việc phát hiện ung thư phổi từ ảnh CT, cao hơn mức trung bình 85% của bác sĩ lâm sàng. Điều này cho thấy tiềm năng lớn trong việc giảm sai sót y khoa và tăng hiệu quả điều trị.
1	Một nghiên cứu thử nghiệm tại Mỹ đã sử dụng mô hình học sâu (Deep Learning) với 50.000 ảnh CT phổi để huấn luyện. Kết quả cho thấy mô hình có khả năng phát hiện khối u nhỏ chỉ 2mm, vốn rất khó nhận diện bằng mắt thường. Ngoài ra, hệ thống còn có thể phân loại mức độ nguy hiểm của khối u dựa trên đặc điểm hình thái. Việc áp dụng học máy giúp giảm thời gian chẩn đoán trung bình từ 20 phút xuống còn 5 phút, tiết kiệm đáng kể chi phí và nhân lực.
1	Trong thử nghiệm lâm sàng quy mô lớn với 1.000 bệnh nhân tham gia, hệ thống học máy đã chứng minh khả năng phân tích hình ảnh vượt xa các phương pháp truyền thống. Cụ thể, mô hình đã phát hiện chính xác 920 ca ung thư phổi giai đoạn sớm, trong khi các bác sĩ chẩn đoán hình ảnh giàu kinh nghiệm chỉ phát hiện được 850 ca thông qua việc đọc phim X-quang và CT cắt lớp. Sự chênh lệch này không chỉ nằm ở con số, mà còn nằm ở khả năng của AI trong việc nhận diện các nốt mờ (nodules) có kích thước dưới 3mm – những dấu hiệu mà mắt thường rất dễ bỏ sót do hiện tượng nhiễu ảnh hoặc mệt mỏi thị giác.
1	Trí tuệ nhân tạo (AI) đang tạo ra một cuộc cách mạng triệt để trong việc quản lý và điều hành giao thông tại các siêu đô thị trên toàn cầu. Trong bối cảnh tốc độ đô thị hóa diễn ra chóng mặt, các hệ thống hạ tầng truyền thống thường xuyên rơi vào tình trạng quá tải, gây ra những tổn thất khổng lồ về kinh tế và môi trường. Các hệ thống AI hiện đại có khả năng xử lý và phân tích đồng thời luồng dữ liệu khổng lồ từ mạng lưới camera giám sát, cảm biến dưới lòng đường và tín hiệu GPS từ các phương tiện cá nhân để dự đoán chính xác tình trạng ùn tắc trước khi nó xảy ra.
1	Theo nghiên cứu thực nghiệm của MIT năm 2023, việc áp dụng thuật toán học sâu trong quản lý giao thông có thể giúp giảm thời gian chờ trung bình tại các nút giao trọng điểm tới 25%, đồng thời cắt giảm lượng khí thải CO2 khoảng 18%, góp phần trực tiếp vào mục tiêu phát triển bền vững của các thành phố thông minh. Một dự án thí điểm quy mô lớn tại Singapore đã minh chứng cho sức mạnh của công nghệ khi triển khai hệ thống AI phân tích dữ liệu tích hợp từ hơn 1.200 camera giao thông độ nét cao và 300.000 thiết bị GPS thời gian thực.
1	Trái tim của hệ thống này là mô hình học tăng cường (Reinforcement Learning), cho phép máy tính tự học hỏi và tối ưu hóa chu kỳ đèn tín hiệu dựa trên sự thay đổi liên tục của mật độ phương tiện. Thay vì các chu kỳ đèn cố định và cứng nhắc, AI có khả năng đưa ra các quyết định điều phối linh hoạt theo từng giây, giúp luồng giao thông luôn được thông suốt. Kết quả ghi nhận tại thực địa cho thấy tốc độ di chuyển trung bình của xe trong giờ cao điểm đã có sự cải thiện đáng kể, tăng từ 18 km/h lên 23 km/h. Hơn nữa, nhờ khả năng dự báo và điều tiết nhịp nhàng, số vụ tai nạn va chạm tại các nút giao cũng giảm tới 12%, tạo ra một môi trường di chuyển an toàn hơn cho cư dân.
1	Chỉ trong vòng 6 tháng vận hành thử nghiệm, hệ thống điều phối AI đã giúp tiết kiệm tổng cộng khoảng 2 triệu giờ di chuyển cho người dân thành phố, con số này khi quy đổi ra giá trị kinh tế tương đương với mức 15 triệu USD. Đây là minh chứng rõ ràng nhất cho việc công nghệ có thể mang lại lợi ích sát sườn cho đời sống xã hội. Tuy nhiên, đi kèm với những thành tựu đó là những thách thức không nhỏ về mặt kỹ thuật và pháp lý, đặc biệt là vấn đề bảo mật dữ liệu định vị và quyền riêng tư của người dùng cuối.
1	"An toàn thông tin (Cybersecurity) đã trở thành yếu tố sống còn, đóng vai trò như ""lá chắn thép"" bảo vệ sự vận hành của toàn bộ xã hội hiện đại trong kỷ nguyên kinh tế số. Khi mọi hoạt động từ giao thương, quản lý nhà nước đến liên lạc cá nhân đều được số hóa, các lỗ hổng bảo mật trở thành mục tiêu béo bở cho tội phạm mạng với những thủ đoạn ngày càng tinh vi. Báo cáo của IBM năm 2024 đã chỉ ra một con số báo động: chi phí trung bình cho mỗi vụ rò rỉ dữ liệu trên phạm vi toàn cầu đã chạm mức 4,45 triệu USD, đánh dấu mức tăng trưởng 15% so với năm 2020."
1	Sự gia tăng này không chỉ phản ánh quy mô của các cuộc tấn công mà còn cho thấy mức độ thiệt hại nặng nề về uy tín và tài chính mà các doanh nghiệp phải gánh chịu. Điều này đặt ra một nhu cầu cấp thiết về việc xây dựng các giải pháp bảo mật thế hệ mới, có khả năng chủ động phòng vệ thay vì chỉ phản ứng thụ động. Để đối phó với những mối đe dọa đa tầng, các công nghệ tiên tiến như AI và học máy (Machine Learning) đang được tích hợp sâu rộng vào các lớp bảo mật.
1	Điển hình là hệ thống phát hiện xâm nhập (IDS) thế hệ mới, có khả năng phân tích và phân loại hơn 1 triệu gói dữ liệu mỗi giây để phát hiện các hành vi bất thường dựa trên dấu hiệu nhận dạng và phân tích hành vi. Tại một ngân hàng lớn ở châu Âu, việc áp dụng trí tuệ nhân tạo để giám sát mạng lưới đã giúp giảm thiểu 40% số vụ tấn công xâm nhập thành công chỉ trong năm 2023. Bên cạnh đó, công nghệ Blockchain cũng được ứng dụng mạnh mẽ như một giải pháp bảo vệ tính toàn vẹn của giao dịch tài chính thông qua cơ chế sổ cái phi tập trung.
1	Với tính chất không thể sửa đổi và minh bạch tuyệt đối, Blockchain giúp loại bỏ nguy cơ giả mạo dữ liệu, tạo ra một nền tảng niềm tin vững chắc cho các giao dịch trong hệ sinh thái ngân hàng số hiện nay. Khảo sát chuyên sâu tại 500 doanh nghiệp hàng đầu cho thấy một xu hướng rõ rệt khi có tới 68% đơn vị đã chính thức áp dụng AI vào quy trình bảo mật cốt lõi của mình. Nhờ đó, tỷ lệ ngăn chặn thành công các cuộc tấn công mạng đã tăng từ mức 70% lên 85%, một bước tiến quan trọng trong việc bảo vệ tài sản số.
1	Tuy nhiên, chúng ta đang đứng trước một cuộc chạy đua vũ trang công nghệ đầy cam go, khi các hacker cũng bắt đầu lợi dụng chính sức mạnh của AI để kiến tạo các phương thức tấn công tinh vi hơn, tiêu biểu là kỹ thuật Deepfake để lừa đảo định danh hoặc các chiến dịch Phishing tự động có khả năng cá nhân hóa cực cao. Do đó, an toàn thông tin không còn là bài toán riêng của công nghệ mà là sự phối hợp chặt chẽ giữa tư duy chiến lược của con người, các chính sách bảo mật nghiêm ngặt và việc nâng cao nhận thức số cho toàn thể cộng đồng để tạo ra một mạng lưới phòng vệ vững chắc nhất.
1	Bệnh tim mạch là một trong những nguyên nhân gây tử vong hàng đầu trên toàn cầu, với Tổ chức Y tế Thế giới (WHO) ước tính khoảng 17,9 triệu ca tử vong mỗi năm, chiếm 31% tổng số ca tử vong toàn cầu. Trong bối cảnh dân số già hóa và lối sống hiện đại, việc phát hiện sớm bệnh tim mạch trở nên cấp thiết. Học máy (Machine Learning - ML) đã nổi lên như một công cụ mạnh mẽ để phân tích dữ liệu y tế, giúp dự đoán rủi ro bệnh với độ chính xác cao hơn so với phương pháp truyền thống.
1	Nghiên cứu này tập trung vào việc áp dụng các thuật toán ML như Random Forest và Neural Networks để phân tích dữ liệu từ hồ sơ bệnh án điện tử, bao gồm các yếu tố như huyết áp, cholesterol, chỉ số BMI và thói quen hút thuốc. Dữ liệu được thu thập từ hơn 10.000 bệnh nhân tại các bệnh viện ở Việt Nam và Mỹ, với tỷ lệ dương tính giả giảm xuống dưới 5% so với mô hình tuyến tính cổ điển. Bằng cách tích hợp ML vào hệ thống chăm sóc sức khỏe, chúng ta có thể giảm chi phí điều trị lên đến 20% và cải thiện tỷ lệ sống sót thêm 15% cho bệnh nhân ở giai đoạn sớm.
1	Nghiên cứu nhằm chứng minh tính khả thi của ML trong đời sống hàng ngày, từ ứng dụng di động theo dõi sức khỏe đến hệ thống cảnh báo tại phòng khám. Các nghiên cứu trước đây đã chứng minh hiệu quả của ML trong y tế, chẳng hạn như mô hình của Framingham Heart Study sử dụng hồi quy logistic để dự đoán rủi ro tim mạch với độ chính xác 75%. Tuy nhiên, với sự phát triển của deep learning, các nghiên cứu gần đây như của Google AI đạt độ chính xác lên đến 92% khi sử dụng Convolutional Neural Networks (CNN) trên dữ liệu hình ảnh tim.
1	Tại Việt Nam, một nghiên cứu từ Đại học Bách Khoa Hà Nội năm 2022 áp dụng Support Vector Machine (SVM) trên 5.000 hồ sơ bệnh nhân, giảm lỗi dự đoán xuống 8%. Những hạn chế bao gồm thiếu dữ liệu đa dạng và vấn đề bảo mật thông tin cá nhân, dẫn đến tỷ lệ sai lệch lên đến 10% ở nhóm dân tộc thiểu số. Nghiên cứu này mở rộng bằng cách kết hợp dữ liệu từ nhiều nguồn, bao gồm wearable devices như Fitbit, thu thập hơn 1 triệu điểm dữ liệu thời gian thực. So sánh với các mô hình truyền thống, ML giảm thời gian xử lý từ 24 giờ xuống còn 5 phút, đồng thời tăng độ nhạy cảm lên 95%.
1	Tài liệu cũng nhấn mạnh vai trò của ML trong đại dịch COVID-19, nơi các thuật toán dự đoán biến chứng tim mạch với tỷ lệ chính xác 85% ở bệnh nhân nhiễm virus. Nghiên cứu sử dụng dữ liệu từ cơ sở dữ liệu Kaggle Heart Disease Dataset, bổ sung bởi 2.500 hồ sơ từ Bệnh viện Tim Tâm Đức TP.HCM, tổng cộng 12.500 mẫu. Các thuật toán ML được triển khai trên Python với thư viện Scikit-learn và TensorFlow, chia dữ liệu thành 70% huấn luyện và 30% kiểm tra. Biến độc lập bao gồm tuổi (trung bình 55 ± 10), giới tính (60% nam), và các chỉ số sinh hóa như LDL cholesterol (130 mg/dL trung bình). Mô hình Random Forest đạt AUC-ROC 0.95, vượt trội so với Logistic Regression (0.82).
1	Để xử lý dữ liệu mất mát, chúng tôi áp dụng kỹ thuật imputation bằng KNN, giảm tỷ lệ dữ liệu thiếu từ 15% xuống 2%. Thử nghiệm thực tế tại 5 phòng khám cộng đồng cho thấy hệ thống dự đoán đúng 88% ca rủi ro cao trong vòng 6 tháng. Chi phí triển khai ước tính 500 triệu VND cho hệ thống phần mềm, nhưng tiết kiệm được 2 tỷ VND/năm nhờ giảm nhập viện không cần thiết. Nghiên cứu tuân thủ GDPR và HIPAA để bảo vệ dữ liệu, với mã hóa AES-256. Kết quả cho thấy mô hình ML dự đoán chính xác 91% ca bệnh tim mạch, cao hơn 15% so với phương pháp lâm sàng truyền thống. Trong nhóm bệnh nhân trên 60 tuổi, độ chính xác đạt 94%, với tỷ lệ dương tính giả chỉ 3%.
1	Dữ liệu thống kê cho thấy giảm 25% số ca nhập viện khẩn cấp sau khi áp dụng hệ thống tại các cộng đồng nông thôn Việt Nam. Tuy nhiên, thách thức lớn là độ chính xác giảm xuống 80% ở nhóm có dữ liệu ít, như phụ nữ mang thai. So sánh với nghiên cứu quốc tế, mô hình của chúng tôi vượt trội hơn 5% so với IBM Watson Health. Thảo luận nhấn mạnh nhu cầu tích hợp ML vào chính sách y tế quốc gia, có thể cứu sống thêm 100.000 người/năm tại Việt Nam. Tương lai, kết hợp với IoT sẽ tăng độ chính xác lên 98%. Ngoài ra, kết quả còn chỉ ra rằng việc sử dụng dữ liệu thời gian thực từ smartwatch giúp phát hiện sớm nhịp tim bất thường với độ nhạy 96%, giảm chi phí y tế cá nhân xuống 30%.
1	Tóm lại, ứng dụng ML trong dự đoán bệnh tim mạch không chỉ nâng cao hiệu quả chẩn đoán mà còn mang lại lợi ích kinh tế xã hội lớn. Với dữ liệu từ hơn 10.000 bệnh nhân, nghiên cứu chứng minh tiềm năng giảm tử vong 20% thông qua phát hiện sớm. Tuy nhiên, cần đầu tư thêm vào dữ liệu đa dạng và đào tạo nhân viên y tế. Khuyến nghị triển khai rộng rãi tại Việt Nam, bắt đầu từ các tỉnh thành lớn như TP.HCM và Hà Nội. (Khoảng 85 từ) – Mở rộng: Nghiên cứu này mở đường cho các ứng dụng tương tự trong các bệnh mãn tính khác, như tiểu đường, với dự báo tiết kiệm 10% ngân sách y tế quốc gia.
1	Trí tuệ nhân tạo (AI) đang cách mạng hóa ngành giao thông với phương tiện tự lái, giảm tai nạn do lỗi con người – nguyên nhân gây ra 94% vụ tai nạn theo báo cáo của NHTSA (Mỹ). Tại Việt Nam, tai nạn giao thông gây tử vong hơn 8.000 người/năm, theo Bộ Giao thông Vận tải. Nghiên cứu này khám phá việc áp dụng AI trong xe tự lái, sử dụng thuật toán deep learning để xử lý dữ liệu từ camera, lidar và radar. Dữ liệu từ Waymo và Tesla cho thấy AI giảm tỷ lệ tai nạn xuống 40% so với xe truyền thống. Chúng tôi phân tích hơn 1 triệu km dữ liệu lái xe thực tế, đạt độ chính xác nhận diện vật cản 99%.
1	Ứng dụng trong đời sống bao gồm xe buýt tự lái tại các thành phố đông đúc, giảm ùn tắc 25%. Nghiên cứu nhằm chứng minh AI không chỉ an toàn mà còn tiết kiệm nhiên liệu lên 15%. Các nghiên cứu tiên phong như của DARPA Grand Challenge năm 2005 đã đặt nền móng cho AI trong xe tự lái, với độ chính xác ban đầu chỉ 70%. Đến nay, Tesla's Autopilot sử dụng Neural Networks để xử lý 1 tỷ km dữ liệu, giảm tai nạn 50%. Tại châu Á, nghiên cứu từ Đại học Quốc gia Singapore năm 2023 áp dụng Reinforcement Learning, đạt tốc độ phản ứng dưới 0.1 giây. Hạn chế bao gồm thời tiết xấu làm giảm độ chính xác xuống 85%.
1	Tài liệu cũng chỉ ra AI giúp giảm khí thải CO2 20% nhờ tối ưu hóa lộ trình. Ở Việt Nam, dự án VinFast tích hợp AI đã thử nghiệm trên 500 xe, giảm lỗi lái 30%. Các nghiên cứu khác từ Uber nhấn mạnh vai trò của AI trong giao thông thông minh, dự báo lưu lượng với độ chính xác 92%. Sử dụng dữ liệu từ KITTI Dataset và 200.000 km lái xe tại TP.HCM, chúng tôi huấn luyện mô hình YOLO cho nhận diện vật thể và LSTM cho dự đoán quỹ đạo. Môi trường mô phỏng Unity Engine được dùng để kiểm tra 10.000 tình huống, với tỷ lệ thành công 98%. Biến số bao gồm tốc độ (trung bình 50 km/h), mật độ giao thông (cao điểm 100 xe/km). AI giảm thời gian phản ứng từ 1 giây xuống 0.05 giây.
1	Chi phí phần cứng khoảng 500 triệu VND/xe, nhưng tiết kiệm 1 tỷ VND/năm cho hãng vận tải. Nghiên cứu tuân thủ tiêu chuẩn ISO 26262 cho an toàn. Thử nghiệm thực tế tại Hà Nội cho thấy giảm tai nạn 35% ở khu vực đô thị. Kết quả cho thấy AI xử lý thành công 97% tình huống phức tạp, giảm tai nạn mô phỏng 45%. Trong thực tế, tỷ lệ lỗi chỉ 2%, thấp hơn 10 lần so với lái xe thủ công. Thảo luận nhấn mạnh nhu cầu luật pháp hỗ trợ, như Luật Giao thông Việt Nam cần cập nhật cho xe tự lái. Tương lai, tích hợp 5G sẽ tăng độ chính xác lên 99.5%. Dữ liệu thống kê từ 1.000 chuyến đi cho thấy tiết kiệm thời gian 20% cho người dùng hàng ngày. Tuy nhiên, thách thức là bảo mật dữ liệu, với nguy cơ hack 5%.
1	Trí tuệ nhân tạo (AI) tích hợp trong công nghệ xe tự lái đang mở ra một kỷ nguyên mới cho an toàn giao thông tại Việt Nam, với hứa hẹn có thể cắt giảm tới 50% tỷ lệ tử vong do tai nạn hàng năm. Bằng cách loại bỏ các yếu tố lỗi từ con người như sự mệt mỏi, mất tập trung hoặc phản xạ chậm, hệ thống tự hành dựa trên mạng thần kinh sâu có khả năng xử lý dữ liệu từ cảm biến LiDAR và Radar để dự đoán các tình huống va chạm tiềm ẩn trước tận 10 giây. Khoảng thời gian này là cực kỳ quý giá, cho phép hệ thống thực hiện các thao tác phòng tránh chủ động hoặc phanh khẩn cấp mà mắt người thường không thể phản ứng kịp trong các tình huống bất ngờ.
1	An toàn thông tin (Cybersecurity) ngày càng quan trọng khi số vụ tấn công mạng tăng 50% hàng năm, theo báo cáo của IBM, gây thiệt hại toàn cầu 6 nghìn tỷ USD/năm. Tại Việt Nam, hơn 10.000 vụ tấn công ransomware năm 2023, theo Cục An toàn Thông tin. Nghiên cứu này áp dụng AI để phát hiện tấn công, sử dụng Machine Learning phân tích lưu lượng mạng. Dữ liệu từ 500 doanh nghiệp Việt Nam cho thấy AI phát hiện 95% mối đe dọa trong vòng 1 phút. Ứng dụng trong đời sống bao gồm bảo vệ ngân hàng trực tuyến và hệ thống chính phủ. Nghiên cứu nhằm giảm thiệt hại kinh tế 30% nhờ AI.
1	"Nghiên cứu mang tính đột phá từ MIT vào năm 2018 đã chứng minh sức mạnh của hệ thống AI2 khi có khả năng phát hiện tới 85% các cuộc tấn công zero-day – những lỗ hổng chưa từng được biết đến và thường xuyên làm bó tay các hệ thống phòng thủ truyền thống. Điểm ưu việt của hệ thống này là sự kết hợp giữa thuật toán học máy và sự thẩm định của con người, giúp mô hình tự học hỏi từ các mẫu hành vi mới. Gần đây hơn, công ty an ninh mạng Darktrace đã áp dụng kỹ thuật Học không giám sát (Unsupervised Learning) để xây dựng một ""hệ thống miễn dịch"" số cho doanh nghiệp."
1	Nghiên cứu này triển khai quy trình thực nghiệm dựa trên bộ dữ liệu tiêu chuẩn NSL-KDD, kết hợp với 100.000 bản ghi lưu lượng thực tế được thu thập từ các doanh nghiệp tại TP.HCM để đảm bảo tính bản địa hóa. Chúng tôi lựa chọn kiến trúc Autoencoder (mạng thần kinh tự mã hóa) huấn luyện trên nền tảng TensorFlow để nhận diện các mẫu hành vi bất thường. Các biến số đầu vào tập trung vào lưu lượng mạng với mức trung bình 1 GB/giờ và tần suất kết nối khoảng 500 lần/giờ. Mô hình đã chứng minh hiệu quả vượt trội khi đạt độ chính xác 96%, đồng thời tối ưu hóa hệ thống bằng cách giảm tỷ lệ báo động giả (false alarm) từ 15% xuống chỉ còn 3%.
1	Kết quả phân tích từ các kịch bản mô phỏng cho thấy hệ thống AI có khả năng phát hiện chính xác 94% các cuộc tấn công mạng, giúp giảm thiểu thiệt hại tài chính trực tiếp tới 25% cho các đơn vị vận hành. Đặc biệt, dữ liệu từ 200 vụ việc thực tế chỉ ra rằng việc ứng dụng trí tuệ nhân tạo giúp tiết kiệm trung bình 500 triệu VND cho mỗi vụ rò rỉ dữ liệu nhờ khả năng phản ứng nhanh. So với phương pháp giám sát thủ công truyền thống, AI cho tốc độ xử lý và phản hồi nhanh hơn gấp 10 lần, đặc biệt hiệu quả trong môi trường IoT khi giúp giảm rủi ro bảo mật tới 30%.
1	Nghiên cứu khẳng định AI là công cụ thiết yếu và không thể tách rời trong chiến lược an toàn thông tin hiện đại, với khả năng giảm thiểu rủi ro tổng thể lên tới 40%. Tiềm năng kinh tế mà công nghệ này mang lại cho khối doanh nghiệp Việt Nam là rất lớn, ước tính có thể tiết kiệm tới 5 nghìn tỷ VND/năm nhờ ngăn ngừa các tổn thất từ tội phạm mạng. Chúng tôi đưa ra khuyến nghị cấp thiết về việc đào tạo chuyên sâu nguồn nhân lực và tăng cường đầu tư hạ tầng công nghệ lõi.
1	Nghiên cứu này trình bày một hệ thống chẩn đoán tự động ung thư phổi sử dụng mạng nơ-ron tích chập sâu (CNN) trên tập dữ liệu hình ảnh CT gồm 15.847 ca bệnh từ 12 bệnh viện tại Việt Nam. Hệ thống đạt độ chính xác 94.7%, độ nhạy 92.3% và độ đặc hiệu 96.1%, vượt trội so với phương pháp chẩn đoán truyền thống. Thời gian phân tích trung bình cho một ca bệnh giảm từ 45 phút xuống còn 3.2 phút, giúp tăng năng suất chẩn đoán lên 14 lần. Kết quả cho thấy tiềm năng ứng dụng trí tuệ nhân tạo trong y tế, đặc biệt tại các vùng thiếu chuyên gia. Nghiên cứu được thực hiện trong 24 tháng với sự tham gia của 45 bác sĩ chuyên khoa hô hấp và 8 kỹ sư AI.
1	Ung  thư phổi là nguyên nhân gây tử vong hàng đầu trong các bệnh ung thư toàn cầu với 2.2 triệu ca mắc mới và 1.8 triệu ca tử vong mỗi năm theo thống kê của Tổ chức Y tế Thế giới năm 2024. Tại Việt Nam, số ca mắc ung thư phổi tăng 7.8% mỗi năm từ 2018 đến 2024, với khoảng 28.300 ca mắc mới được ghi nhận trong năm 2024. Tỷ lệ sống sót sau 5 năm chỉ đạt 18.6% do phần lớn bệnh nhân được phát hiện ở giai đoạn muộn. Việc phát hiện sớm có thể nâng tỷ lệ sống sót lên 56%, nhưng phương pháp chẩn đoán truyền thống phụ thuộc nhiều vào kinh nghiệm bác sĩ và thường mất nhiều thời gian.
1	Chẩn đoán ung thư phổi từ hình ảnh CT scan hiện nay đối mặt với nhiều thách thức. Trung bình một ca chẩn đoán mất 45-60 phút, trong khi số lượng bác sĩ chuyên khoa hình ảnh tại Việt Nam chỉ đạt 0.8 bác sĩ trên 100.000 dân, thấp hơn 4.5 lần so với tiêu chuẩn của WHO. Tỷ lệ chẩn đoán nhầm lẫn giữa các bác sĩ có kinh nghiệm khác nhau dao động từ 8.2% đến 15.7%, gây ảnh hưởng nghiêm trọng đến quyết định điều trị. Hơn nữa, 68% bệnh viện tuyến huyện không có chuyên gia đọc phim CT chuyên sâu, dẫn đến việc bệnh nhân phải chuyển tuyến, kéo dài thời gian chẩn đoán thêm 12-18 ngày.
1	Nghiên cứu này nhằm phát triển một hệ thống AI có khả năng phát hiện và phân loại các tổn thương ung thư phổi với độ chính xác cao hơn 90%, giảm thời gian chẩn đoán xuống dưới 5 phút mỗi ca và hỗ trợ bác sĩ trong việc đưa ra quyết định chính xác hơn. Hệ thống cần đạt độ nhạy trên 90% để giảm thiểu trường hợp bỏ sót bệnh và độ đặc hiệu trên 95% để tránh chẩn đoán dương tính giả. Ngoài ra, nghiên cứu hướng tới xây dựng cơ sở dữ liệu hình ảnh y khoa chuẩn hóa với hơn 15.000 ca bệnh, tạo nền tảng cho các nghiên cứu tiếp theo trong lĩnh vực AI y tế tại Việt Nam.
1	Dữ liệu nghiên cứu gồm 15.847 ca bệnh được thu thập từ 12 bệnh viện lớn tại Hà Nội, Hồ Chí Minh và Đà Nẵng trong khoảng thời gian từ tháng 1/2022 đến tháng 12/2024. Mỗi ca bệnh bao gồm trung bình 248 lát cắt CT với độ dày 1.25mm, tổng cộng 3.93 triệu hình ảnh. Dữ liệu được gán nhãn bởi 45 bác sĩ chuyên khoa với ít nhất 8 năm kinh nghiệm, mỗi ca được 3 bác sĩ độc lập đánh giá để đảm bảo độ tin cậy đạt 96.8%. Hình ảnh được chuẩn hóa về định dạng DICOM, cường độ Hounsfield từ -1000 đến +400 HU, và resize về kích thước 512x512 pixel để phù hợp với mô hình học sâu.
1	Nghiên cứu sử dụng kiến trúc 3D ResNet-101 được cải tiến với cơ chế attention mechanism, gồm 101 lớp tích chập sâu và 7.8 triệu tham số có thể huấn luyện. Mô hình được tiền huấn luyện trên tập ImageNet với 14 triệu hình ảnh tự nhiên, sau đó fine-tuning trên dữ liệu y khoa trong 850 epoch với batch size 16. Tốc độ học được điều chỉnh động từ 0.001 xuống 0.00001 theo phương pháp cosine annealing để tối ưu hóa quá trình hội tụ. Hàm mất mát sử dụng focal loss với gamma=2.5 để xử lý vấn đề mất cân bằng dữ liệu, kết hợp với L2 regularization (lambda=0.0005) để tránh overfitting. Quá trình huấn luyện được thực hiện trên 4 GPU NVIDIA A100 trong 168 giờ.
1	Tập dữ liệu được chia ngẫu nhiên theo tỷ lệ 70% huấn luyện (11.093 ca), 15% validation (2.377 ca) và 15% kiểm tra (2.377 ca), đảm bảo không có sự chồng lấn giữa các bệnh nhân. Hiệu suất được đánh giá qua 7 chỉ số: độ chính xác (accuracy), độ nhạy (sensitivity/recall), độ đặc hiệu (specificity), precision, F1-score, AUC-ROC và AUC-PR. Phương pháp 5-fold cross-validation được áp dụng để đảm bảo tính ổn định của mô hình với độ lệch chuẩn dưới 1.2%. So sánh được thực hiện với 4 phương pháp baseline: VGG-19, Inception-v3, DenseNet-169 và phương pháp chẩn đoán thủ công của bác sĩ. Thử nghiệm lâm sàng được tiến hành tại 5 bệnh viện với 1.240 ca bệnh mới trong 6 tháng.
1	Mô hình 3D ResNet-101 cải tiến đạt độ chính xác 94.7% trên tập kiểm tra, vượt trội so với VGG-19 (87.3%), Inception-v3 (89.6%) và DenseNet-169 (91.2%). Độ nhạy đạt 92.3%, cao hơn 5.8% so với phương pháp chẩn đoán thủ công của bác sĩ (86.5%), giúp phát hiện thêm 137 ca ung thư bị bỏ sót trong 2.377 ca kiểm tra. Độ đặc hiệu đạt 96.1%, giảm tỷ lệ dương tính giả từ 11.4% xuống 3.9%, tương đương giảm 187 ca chẩn đoán nhầm. AUC-ROC đạt 0.972, cho thấy khả năng phân biệt xuất sắc giữa các trường hợp ung thư và không ung thư. F1-score đạt 0.941, cân bằng tốt giữa precision và recall.
1	Đối với ung thư giai đoạn I (kích thước u dưới 3cm), mô hình đạt độ nhạy 88.7% so với 72.3% của bác sĩ, tăng 16.4 điểm phần trăm trong việc phát hiện sớm. Giai đoạn II (u từ 3-5cm) có độ chính xác 93.4%, giai đoạn III (u trên 5cm hoặc di căn hạch) đạt 96.8% và giai đoạn IV (di căn xa) đạt 98.1%. Đặc biệt, với các tổn thương kích thước dưới 5mm (ground-glass opacity), mô hình vẫn đạt độ nhạy 76.3%, cao hơn đáng kể so với 54.7% của phương pháp truyền thống. Thời gian phát hiện trung bình cho tổn thương nhỏ là 2.8 giây, nhanh hơn 512 lần so với con người.
1	Thử nghiệm lâm sàng tại 5 bệnh viện với 1.240 ca bệnh cho thấy hệ thống AI giúp giảm thời gian chẩn đoán trung bình từ 45.3 phút xuống 3.2 phút, tăng năng suất 14.2 lần. Trong 6 tháng triển khai, số ca được chẩn đoán tăng từ 4.800 ca lên 12.100 ca, giúp giảm thời gian chờ đợi của bệnh nhân từ 18 ngày xuống 4.5 ngày. Tỷ lệ đồng thuận giữa AI và bác sĩ chuyên môn cao đạt 91.7%, trong khi 8.3% trường hợp có sự khác biệt được review lại bởi hội đồng chuyên môn và phát hiện 6.2% là AI chính xác hơn. Chi phí chẩn đoán giảm 43% nhờ tối ưu hóa quy trình.
1	Phân tích 126 trường hợp dự đoán sai (5.3% tổng số) cho thấy 47 ca (37.3%) do chất lượng hình ảnh kém với nhiễu cao hoặc artifact, 38 ca (30.2%) do tổn thương có đặc điểm không điển hình như u thần kinh nội tiết, 24 ca (19%) do kích thước tổn thương quá nhỏ dưới 3mm, và 17 ca (13.5%) do sự chồng lấn với bệnh lý khác như viêm phổi hoặc lao. Mô hình gặp khó khăn với các ca bệnh hiếm gặp chiếm dưới 2% tập dữ liệu như ung thư tế bào nhỏ biến thể. Độ chính xác giảm 8.7% khi áp dụng cho dữ liệu từ máy CT các hãng khác chưa có trong tập huấn luyện.
1	Nghiên cứu đã chứng minh hiệu quả vượt trội của học sâu trong chẩn đoán hình ảnh y khoa, đặc biệt với kiến trúc 3D ResNet kết hợp attention mechanism cho phép mô hình học được các đặc trưng không gian ba chiều phức tạp của tổn thương ung thư. Cơ chế attention giúp mô hình tập trung vào vùng quan trọng, tăng độ nhạy lên 7.8% so với ResNet thông thường. Việc sử dụng focal loss và data augmentation với 12 phép biến đổi (rotation, flip, elastic deformation, noise injection) giúp cải thiện hiệu suất trên dữ liệu mất cân bằng và tăng khả năng tổng quát hóa. Nghiên cứu cung cấp bằng chứng thực nghiệm mạnh mẽ về khả năng ứng dụng AI trong y tế với tập dữ liệu lớn nhất Việt Nam (15.847 ca).
1	Hệ thống AI đã được triển khai thử nghiệm tại 5 bệnh viện và cho thấy tác động tích cực: tăng tỷ lệ phát hiện sớm ung thư giai đoạn I từ 34% lên 51%, giúp cải thiện cơ hội điều trị cho 820 bệnh nhân trong 6 tháng. Chi phí vận hành giảm 1.2 tỷ đồng/năm nhờ tối ưu hóa quy trình và giảm sai sót. Đặc biệt, hệ thống hỗ trợ hiệu quả cho các bác sĩ trẻ và bệnh viện tuyến dưới thiếu chuyên gia, thu hẹp khoảng cách về chất lượng chẩn đoán giữa các tuyến. Thời gian đào tạo bác sĩ mới giảm từ 36 tháng xuống 18 tháng nhờ công cụ hỗ trợ AI. 1.240 bệnh nhân được chẩn đoán sớm hơn trung bình 14 ngày.
1	Kết quả nghiên cứu này (94.7% accuracy, 92.3% sensitivity) tương đương với các nghiên cứu hàng đầu thế giới như nghiên cứu của Google Health (94.4%, 92.1%) và Stanford University (93.8%, 91.7%), mặc dù sử dụng tập dữ liệu nhỏ hơn 3.2 lần. Điểm nổi bật là nghiên cứu được thực hiện hoàn toàn trên dữ liệu người Việt Nam với đặc điểm bệnh lý và thiết bị y tế khác biệt, chứng minh tính ứng dụng cao trong bối cảnh địa phương. Thời gian xử lý 3.2 phút/ca nhanh hơn 40% so với hệ thống của Mayo Clinic (5.3 phút), nhờ tối ưu hóa kiến trúc mô hình và pipeline xử lý dữ liệu.
1	Nghiên cứu còn một số hạn chế: tập dữ liệu chưa đại diện đầy đủ các nhóm dân tộc thiểu số (chỉ 7.2%), thiếu dữ liệu từ các loại máy CT mới (Photon Counting CT), và chưa tích hợp thông tin lâm sàng như tiền sử hút thuốc hay xét nghiệm khí máu. Hướng phát triển tương lai bao gồm: mở rộng tập dữ liệu lên 50.000 ca để cải thiện khả năng tổng quát, tích hợp multi-modal learning kết hợp CT với PET-CT và biomarker, phát triển explainable AI để giải thích quyết định của mô hình bằng heatmap và attention visualization, và xây dựng hệ thống dự đoán tiên lượng dựa trên đặc điểm hình ảnh. Nghiên cứu tiếp theo sẽ thử nghiệm federated learning để bảo mật dữ liệu bệnh nhân.
1	Nghiên cứu đã phát triển thành công hệ thống AI chẩn đoán ung thư phổi với độ chính xác 94.7%, vượt trội so với các phương pháp truyền thống và đạt được mục tiêu nghiên cứu ban đầu. Hệ thống giúp giảm thời gian chẩn đoán từ 45.3 phút xuống 3.2 phút, tăng năng suất 14.2 lần, đồng thời cải thiện độ nhạy lên 92.3% giúp phát hiện sớm thêm 16.4% ca ung thư giai đoạn I. Với 15.847 ca bệnh từ 12 bệnh viện, đây là nghiên cứu quy mô lớn nhất tại Việt Nam về ứng dụng AI trong chẩn đoán hình ảnh y khoa, tạo nền tảng vững chắc cho các nghiên cứu tiếp theo và triển khai rộng rãi trong hệ thống y tế quốc gia.
1	Nghiên cứu này phát triển hệ thống an ninh mạng tự động sử dụng Deep Reinforcement Learning (DRL) để phát hiện và phản ứng với các cuộc tấn công mạng theo thời gian thực. Hệ thống được huấn luyện trên 8.4 triệu mẫu tấn công từ 23 loại khác nhau, đạt tỷ lệ phát hiện 97.8% và tỷ lệ báo động nhầm chỉ 1.3%. Thời gian phản ứng trung bình là 0.84 giây, nhanh hơn 156 lần so với hệ thống truyền thống. Triển khai thử nghiệm tại 15 doanh nghiệp trong 9 tháng cho thấy giảm 89.4% số lượng tấn công thành công và tiết kiệm 4.7 tỷ đồng chi phí thiệt hại. Nghiên cứu đóng góp quan trọng vào lĩnh vực an ninh mạng thích ứng và tự động hóa.
1	Tấn công mạng ngày càng tinh vi và gia tăng với tốc độ đáng báo động toàn cầu. Theo báo cáo của Cybersecurity Ventures năm 2024, thiệt hại từ tội phạm mạng ước tính đạt 9.5 nghìn tỷ USD, tăng 73% so với 2020. Tại Việt Nam, Cục An toàn thông tin ghi nhận trung bình 3.200 cuộc tấn công mạng mỗi ngày trong năm 2024, tăng 124% so với năm 2022. Các dạng tấn công phổ biến bao gồm DDoS (34.2%), ransomware (28.7%), phishing (21.3%) và SQL injection (15.8%). Đặc biệt, 67% doanh nghiệp vừa và nhỏ không có hệ thống phòng thủ đầy đủ, dẫn đến thiệt hại trung bình 2.3 tỷ đồng mỗi vụ tấn công.
1	Các hệ thống IDS/IPS (Intrusion Detection/Prevention System) truyền thống dựa trên chữ ký (signature-based) và luật cứng (rule-based) gặp nhiều thách thức. Tỷ lệ báo động nhầm dao động từ 15-35%, gây quá tải cho đội ngũ bảo mật và làm giảm hiệu quả phản ứng. Thời gian phát hiện trung bình là 131 giây, quá chậm với các cuộc tấn công zero-day có thể gây thiệt hại trong vòng 30 giây đầu tiên. Hệ thống không thể tự học và thích ứng với các biến thể tấn công mới, yêu cầu cập nhật thủ công tốn kém với chi phí trung bình 180 triệu đồng/năm cho mỗi tổ chức. Machine learning truyền thống cải thiện được phát hiện nhưng vẫn thiếu khả năng phản ứng tự động và tối ưu.
1	Nghiên cứu nhằm phát triển hệ thống an ninh mạng thông minh có khả năng: (1) phát hiện tấn công với độ chính xác trên 95% và tỷ lệ báo động nhầm dưới 3%, (2) phản ứng tự động trong thời gian dưới 1 giây, (3) tự học và thích ứng với các mẫu tấn công mới không cần can thiệp thủ công, và (4) tối ưu hóa chiến lược phòng thủ dựa trên bối cảnh hệ thống. Đóng góp chính bao gồm kiến trúc DRL mới kết hợp Double DQN và Prioritized Experience Replay, tập dữ liệu tấn công mạng lớn nhất Việt Nam với 8.4 triệu mẫu, và framework triển khai thực tế đã được validation tại 15 doanh nghiệp.
1	Tập dữ liệu gồm 8.4 triệu mẫu lưu lượng mạng được thu thập từ 15 doanh nghiệp trong 14 tháng (01/2023 - 02/2024), bao gồm cả lưu lượng bình thường (5.1 triệu mẫu) và tấn công (3.3 triệu mẫu). Dữ liệu tấn công được phân thành 23 loại: DDoS (1.12M), ransomware (0.87M), phishing (0.71M), SQL injection (0.52M), XSS (0.43M), man-in-the-middle (0.31M), brute force (0.24M) và 16 loại khác. Mỗi mẫu được trích xuất 87 đặc trưng từ header gói tin, payload, và hành vi luồng dữ liệu. Dữ liệu được chuẩn hóa bằng StandardScaler, xử lý missing values bằng KNN imputation, và balanced bằng SMOTE-Tomek với tỷ lệ 1:1.2 giữa normal và attack.
1	Hệ thống sử dụng kiến trúc Double Deep Q-Network (Double DQN) với Prioritized Experience Replay để học chính sách phòng thủ tối ưu. State space gồm 87 đặc trưng mạng và 12 chỉ số hệ thống. Action space bao gồm 9 hành động: allow (cho phép), block IP (chặn IP), rate limit (giới hạn tốc độ), quarantine (cách ly), honeypot redirect (chuyển hướng tới honeypot), deep inspection (kiểm tra sâu), alert admin (báo động quản trị), backup data (sao lưu dữ liệu), và shutdown service (tắt dịch vụ). Reward function được thiết kế với 4 thành phần: +100 khi ngăn chặn thành công tấn công, -50 khi để lọt tấn công, -10 cho mỗi false positive, và +5 cho việc tiết kiệm tài nguyên hệ thống.
1	Mạng nơ-ron gồm 5 lớp fully-connected: 87-512-256-128-64-9 với hàm kích hoạt ReLU và dropout 0.3 để tránh overfitting. Tổng số tham số là 234.217. Huấn luyện được thực hiện trong 500.000 episodes với epsilon-greedy exploration (epsilon giảm từ 1.0 xuống 0.01). Experience replay buffer chứa 100.000 transitions, với prioritized sampling theo TD-error. Target network được cập nhật mỗi 1000 steps để ổn định học. Optimizer sử dụng Adam với learning rate 0.00025, discount factor gamma=0.99. Training được thực hiện trên 8 GPU NVIDIA A100 trong 96 giờ. Validation được thực hiện trên môi trường mô phỏng mạng với 50.000 kịch bản tấn công khác nhau.
1	ghiên cứu này trình bày một mô hình học sâu dựa trên kiến trúc CNN (Convolutional Neural Network) để phát hiện các nốt ung thư phổi từ hình ảnh X-quang ngực. Sử dụng tập dữ liệu gồm 15.247 hình ảnh X-quang từ bệnh viện Chợ Rẫy và bệnh viện Bạch Mai trong giai đoạn 2020-2023, mô hình đạt độ chính xác 94.3% trong việc phân loại hình ảnh bình thường và bất thường. Kết quả cho thấy thời gian chẩn đoán giảm từ trung bình 18 phút xuống còn 2.4 phút, giúp bác sĩ xử lý được nhiều ca bệnh hơn trong cùng một khoảng thời gian làm việc.
1	Ung thư phổi là một trong những nguyên nhân gây tử vong hàng đầu tại Việt Nam với khoảng 23.667 ca mới mắc mỗi năm theo số liệu của Bộ Y tế năm 2022. Việc phát hiện sớm ung thư phổi có thể nâng tỷ lệ sống sót 5 năm từ 15% lên 56%. Tuy nhiên, Việt Nam hiện chỉ có khoảng 3.200 bác sĩ chuyên khoa hình ảnh y khoa, trong khi số lượng ca chụp X-quang hàng năm lên đến 8,5 triệu ca, tạo ra áp lực lớn cho hệ thống y tế. Công nghệ trí tuệ nhân tạo, đặc biệt là học sâu, đã chứng minh hiệu quả vượt trội trong phân tích hình ảnh y khoa với độ chính xác cao và tốc độ xử lý nhanh, mở ra hướng giải pháp hỗ trợ đắc lực cho các bác sĩ.
1	Nghiên cứu sử dụng kiến trúc ResNet-50 được tinh chỉnh với 23,5 triệu tham số, huấn luyện trên tập dữ liệu 15.247 hình ảnh X-quang ngực độ phân giải 512x512 pixels. Dữ liệu được chia thành tập huấn luyện (70%, 10.673 ảnh), tập xác thực (15%, 2.287 ảnh) và tập kiểm tra (15%, 2.287 ảnh). Quá trình tiền xử lý bao gồm chuẩn hóa histogram, tăng cường dữ liệu bằng phép xoay ngẫu nhiên từ -15° đến +15°, và điều chỉnh độ sáng trong khoảng 0.8-1.2. Mô hình được huấn luyện trong 150 epoch với learning rate ban đầu 0.001, sử dụng optimizer Adam và hàm mất mát cross-entropy. Để tránh overfitting, chúng tôi áp dụng kỹ thuật dropout với tỷ lệ 0.5 và early stopping với patience là 15 epoch.
1	Mô hình đạt độ chính xác 94.3% trên tập kiểm tra, với độ nhạy (sensitivity) 92.7% và độ đặc hiệu (specificity) 95.8%. Diện tích dưới đường cong ROC (AUC-ROC) đạt 0.967, cho thấy khả năng phân biệt tốt giữa các trường hợp bình thường và bệnh lý. Thời gian xử lý trung bình cho một hình ảnh là 2.4 giây trên GPU NVIDIA RTX 3090, nhanh hơn 7.5 lần so với thời gian đọc phim trung bình của bác sĩ (18 phút). Trong thử nghiệm thực tế tại 3 bệnh viện ở TP.HCM, Hà Nội và Đà Nẵng với 1.842 ca bệnh, mô hình phát hiện được 287 ca ung thư phổi giai đoạn sớm mà ban đầu bị bỏ sót do khối lượng công việc quá tải.
1	Kết quả nghiên cứu cho thấy công nghệ học sâu có tiềm năng lớn trong việc hỗ trợ chẩn đoán y khoa tại Việt Nam. Tuy nhiên, mô hình vẫn còn một số hạn chế cần khắc phục. Tỷ lệ dương tính giả (false positive) 4.2% có thể gây lo lắng không cần thiết cho bệnh nhân và tăng chi phí xét nghiệm tiếp theo. Ngoài ra, mô hình gặp khó khăn với các trường hợp u nhỏ hơn 5mm (chỉ đạt 78.3% độ chính xác) hoặc hình ảnh chất lượng thấp do máy móc cũ. Nghiên cứu tiếp theo sẽ tập trung vào việc tích hợp thêm dữ liệu CT scan, kết hợp với hồ sơ bệnh án điện tử và các yếu tố nguy cơ như tuổi tác, tiền sử hút thuốc để nâng cao độ chính xác lên 97-98%.
1	Nghiên cứu đã phát triển thành công một hệ thống AI hỗ trợ chẩn đoán ung thư phổi với độ chính xác 94.3%, giảm thời gian đọc phim từ 18 phút xuống 2.4 giây, và phát hiện thêm 287 ca bệnh giai đoạn sớm trong thử nghiệm thực tế. Hệ thống đã được triển khai thí điểm tại 5 bệnh viện lớn và đang trong quá trình mở rộng ra 20 bệnh viện khác trên toàn quốc. Với chi phí triển khai khoảng 150 triệu đồng cho mỗi bệnh viện (bao gồm phần cứng, phần mềm và đào tạo), giải pháp này có tính khả thi cao cho các cơ sở y tế tuyến tỉnh và tuyến huyện, góp phần giảm tải cho tuyến trung ương và nâng cao chất lượng chăm sóc sức khỏe người dân.
1	Bài báo này giới thiệu một hệ thống phát hiện xâm nhập mạng (Intrusion Detection System - IDS) sử dụng thuật toán Random Forest và LSTM kết hợp để bảo vệ mạng doanh nghiệp. Hệ thống được thử nghiệm trên môi trường mạng thực tế của 47 doanh nghiệp vừa và nhỏ tại Việt Nam trong 6 tháng, xử lý trung bình 2.3 triệu gói tin mỗi giờ. Kết quả cho thấy khả năng phát hiện 96.8% các cuộc tấn công với tỷ lệ cảnh báo sai chỉ 2.1%, giúp giảm 78% thời gian phản ứng sự cố so với phương pháp truyền thống.
1	Theo báo cáo của Cục An toàn thông tin (Bộ Thông tin và Truyền thông), Việt Nam ghi nhận trung bình 8.500 cuộc tấn công mạng mỗi ngày trong năm 2023, tăng 34% so với năm 2022. Thiệt hại kinh tế ước tính lên đến 2,4 tỷ USD, trong đó doanh nghiệp vừa và nhỏ (SME) chiếm 62% số vụ tấn công nhưng chỉ 18% có hệ thống phòng thủ đầy đủ do hạn chế về ngân sách và nhân lực. Các giải pháp bảo mật truyền thống dựa trên chữ ký (signature-based) không hiệu quả với các cuộc tấn công zero-day và biến thể mới. Machine learning cung cấp khả năng học từ mẫu hành vi và phát hiện các mối đe dọa chưa từng biết, phù hợp với điều kiện SME cần giải pháp tự động hóa cao.
1	Hệ thống được thiết kế với kiến trúc hai tầng: tầng đầu sử dụng Random Forest với 200 cây quyết định để phân loại lưu lượng mạng thành 5 nhóm (Normal, DDoS, Port Scan, Malware, Intrusion), tầng hai sử dụng mạng LSTM với 128 hidden units để phát hiện các mẫu tấn công phức tạp theo thời gian. Dữ liệu huấn luyện bao gồm 18,4 triệu mẫu lưu lượng mạng được thu thập từ 47 doanh nghiệp trong 3 tháng, với 89 đặc trưng được trích xuất như số lượng packet, kích thước payload, thời gian kết nối, tỷ lệ TCP flags, v.v. Hệ thống sử dụng kỹ thuật SMOTE để cân bằng dữ liệu cho các lớp thiểu số và áp dụng feature selection dựa trên Information Gain để giảm chiều dữ liệu xuống 43 đặc trưng quan trọng nhất.
1	Trong giai đoạn triển khai thực tế 6 tháng, hệ thống đã xử lý 33,1 tỷ gói tin và phát hiện 12.847 cuộc tấn công thực sự với độ chính xác tổng thể 96.8%. Cụ thể, tỷ lệ phát hiện DDoS đạt 98.2% (127/129 cuộc tấn công), Port Scan đạt 94.7% (856/904 cuộc tấn công), Malware đạt 96.1% (1.243/1.294 cuộc tấn công), và Intrusion đạt 95.4% (10.621/11.134 cuộc tấn công). Thời gian phản ứng trung bình giảm từ 4.7 giờ xuống còn 1.0 giờ, trong đó 78% sự cố được phát hiện trong vòng 15 phút đầu tiên. Chi phí thiệt hại trung bình của các doanh nghiệp tham gia giảm 67%, từ 145 triệu đồng/năm xuống còn 48 triệu đồng/năm, tiết kiệm đáng kể so với chi phí triển khai hệ thống là 85 triệu đồng.
1	Hệ thống đã chứng minh hiệu quả cao trong môi trường thực tế, tuy nhiên vẫn tồn tại một số thách thức. Tỷ lệ cảnh báo sai 2.1% (khoảng 48 cảnh báo giả mỗi ngày cho một doanh nghiệp có 100 máy tính) có thể gây mệt mỏi cho đội ngũ IT và giảm độ tin cậy của hệ thống. Các cuộc tấn công APT (Advanced Persistent Threat) phức tạp và kéo dài vẫn khó phát hiện do chúng mô phỏng hành vi người dùng bình thường. Hệ thống cũng yêu cầu phần cứng có cấu hình khá cao (16GB RAM, CPU 8 cores) để xử lý real-time, điều này có thể là rào cản cho các doanh nghiệp rất nhỏ. Phiên bản tiếp theo sẽ tích hợp thêm Threat Intelligence feeds quốc tế và tối ưu hóa thuật toán để chạy trên phần cứng khiêm tốn hơn.
1	"Nghiên cứu đã phát triển và triển khai thành công một hệ thống IDS dựa trên machine learning với độ chính xác 96.8%, giảm 78% thời gian phản ứng sự cố và tiết kiệm 67% chi phí thiệt hại cho doanh nghiệp. Hệ thống đã được thương mại hóa dưới dạng sản phẩm ""VN-SecureNet"" với mức giá 6 triệu đồng/năm cho gói cơ bản, phù hợp với ngân sách của SME Việt Nam. Hiện tại có 127 doanh nghiệp đang sử dụng và mục tiêu là mở rộng lên 500 doanh nghiệp trong năm 2026. Sản phẩm cũng được tích hợp vào chương trình hỗ trợ chuyển đổi số của Bộ Thông tin và Truyền thông, hướng tới mục tiêu nâng cao năng lực an ninh mạng quốc gia."
1	Nghiên cứu này phát triển một chatbot thông minh sử dụng mô hình Transformer và PhoBERT (BERT cho tiếng Việt) để hỗ trợ khách hàng ngân hàng 24/7. Hệ thống được huấn luyện trên 487.000 cuộc hội thoại thực tế từ 3 ngân hàng thương mại lớn, có khả năng xử lý 23 loại yêu cầu khác nhau bằng tiếng Việt. Sau 9 tháng triển khai, chatbot đã xử lý 1,8 triệu cuộc hội thoại với tỷ lệ giải quyết thành công 87.4%, giảm 43% khối lượng công việc cho tổng đài viên và tiết kiệm 12,3 tỷ đồng chi phí vận hành hàng năm.
1	Ngành ngân hàng Việt Nam đang chịu áp lực lớn trong việc nâng cao trải nghiệm khách hàng khi số lượng tài khoản giao dịch điện tử tăng từ 45 triệu (2020) lên 112 triệu (2023), tăng 149% chỉ trong 3 năm. Trung bình mỗi ngân hàng lớn tiếp nhận 35.000-50.000 cuộc gọi/ngày với thời gian chờ trung bình 8,5 phút, gây bức xúc cho khách hàng. Chi phí nhân sự cho một tổng đài viên là 12-18 triệu đồng/tháng, và cần tối thiểu 200-300 nhân viên để vận hành tổng đài 24/7. Công nghệ xử lý ngôn ngữ tự nhiên (NLP) cho phép tự động hóa các câu hỏi thường gặp, giải phóng nhân viên để tập trung vào các vấn đề phức tạp hơn, đồng thời cải thiện chất lượng phục vụ với khả năng phản hồi tức thì.
1	Hệ thống sử dụng kiến trúc Transformer với PhoBERT-base (135 triệu tham số) đã được pre-train trên 20GB văn bản tiếng Việt, sau đó fine-tune trên 487.000 cặp câu hỏi-trả lời trong lĩnh vực ngân hàng. Dữ liệu bao gồm 23 chủ đề như tra cứu số dư (18,3%), chuyển khoản (15,7%), mở thẻ tín dụng (12,4%), khóa thẻ (9,8%), lãi suất (8,2%), v.v. Mô hình intent classification sử dụng softmax layer với 23 lớp output, đạt độ chính xác 94.7% trên tập validation. Mô hình entity extraction sử dụng CRF layer để trích xuất thông tin như số tài khoản, số tiền, ngày tháng với F1-score 91.3%. Hệ thống tích hợp với API core banking để tra cứu thông tin real-time và thực hiện một số giao dịch cơ bản sau khi xác thực OTP.
1	Trong 9 tháng vận hành thực tế tại 3 ngân hàng (Vietcombank, Techcombank, MB Bank), chatbot đã xử lý 1.847.392 cuộc hội thoại với 1.614.458 cuộc thành công (87.4%). Thời gian phản hồi trung bình là 1.2 giây, nhanh hơn 710% so với thời gian chờ tổng đài viên (8.5 phút). Điểm hài lòng của khách hàng (CSAT) tăng từ 73% lên 86%, và Net Promoter Score (NPS) cải thiện từ 42 lên 61. Phân tích chi tiết cho thấy các yêu cầu đơn giản như tra cứu số dư (98.2% success rate), lịch sử giao dịch (96.7%), và thông tin lãi suất (95.3%) được xử lý rất tốt. Các yêu cầu phức tạp hơn như khiếu nại (68.4%), tư vấn sản phẩm đầu tư (71.2%) vẫn cần chuyển cho nhân viên. Nhờ đó, số lượng cuộc gọi đến tổng đài giảm 43%, từ trung bình 42.000 xuống 24.000 cuộc/ngày.
1	"Kết quả triển khai cho thấy chatbot hiệu quả cao với các yêu cầu giao dịch chuẩn hóa nhưng gặp khó khăn với ngôn ngữ tự nhiên đa dạng của người Việt Nam. Chatbot chỉ đạt 76.3% độ chính xác với các câu hỏi sử dụng tiếng lóng, từ địa phương (ví dụ: ""mở cái app lên mà nó cứ treo treo ý""), hoặc câu hỏi không rõ ràng. Vấn đề bảo mật cũng là thách thức lớn - mặc dù có xác thực OTP, vẫn có 3 trường hợp khách hàng bị lừa đảo cung cấp thông tin cho tin tặc mạo danh chatbot qua các kênh không chính thức. Giải pháp là tích hợp xác thực sinh trắc học (vân tay, khuôn mặt) và giáo dục khách hàng về an toàn thông tin."
1	Nghiên cứu đã phát triển thành công một chatbot banking với tỷ lệ thành công 87.4%, xử lý 1,8 triệu cuộc hội thoại trong 9 tháng, tiết kiệm 12,3 tỷ đồng chi phí vận hành và nâng điểm hài lòng khách hàng từ 73% lên 86%. Giải pháp đã được đăng ký bản quyền phần mềm và đang trong quá trình mở rộng sang 7 ngân hàng khác. Kế hoạch phát triển tiếp theo bao gồm tích hợp voice assistant để hỗ trợ khách hàng cao tuổi, hỗ trợ đa ngôn ngữ (Anh, Trung, Nhật) cho khách hàng quốc tế, và ứng dụng sentiment analysis để phát hiện khách hàng không hài lòng và chuyển ngay cho nhân viên cấp cao xử lý, nhằm giảm thiểu khả năng mất khách hàng.
0	Trước sự gia tăng các tác động của biến đổi khí hậu, dự đoán lưu lượng dòng chảy là công cụ thiết yếu trong quản lí tài nguyên nước và ứng phó với thiên tai. Việc dự báo chính xác dòng chảy là một vấn đề rất phức tạp thu hút sự quan tâm của nhiều nhà nghiên cứu trong và ngoài nước. Trong nghiên cứu này, mạng nơ ron tích chập (Convolutional Neural Network - CNN) được kết hợp mạng nơ-ron trí nhớ dài ngắn hạn (Long Short Term Memory - LSTM) tạo một mô hình mới là CNN-LSTM dùng để dự đoán lưu lượng dòng chảy. Với CNN trích dẫn đặc điểm thời gian và LSTM dự đoán lưu lượng.
0	Mục tiêu chính của bài báo này là so sánh hiệu suất dự đoán của ba mô hình: CNN, LSTM và CNN-LSTM nhằm xác định mô hình nào có khả năng dự đoán lưu lượng dòng chảy tốt nhất. Kết quả thử nghiệm mô hình, CNN-LSTM có giá trị R2 (R2 CNN = 0,950, R2 LSTM = 0,956, R2 CNN-LSTM = 0,960) và NSE (NSECNN = 0,948, NSELSTM = 0,953, NSECNN-LSTM = 0,958) cao nhất cho thấy mô hình này dự đoán dòng chảy với độ chính xác cao hơn hai mô hình còn lại. Với sai số RMSE thấp nhất (RMSECNN = 422,375, RMSELSTM = 402,139, RMSECNN-LSTM = 379,384) mô hình CNN-LSTM vượt trội hơn tất cả mô hình AI thông thường. Do đó CNNLSTM có giá trị thực tế lớn trong dự báo lưu lượng dòng chảy.
0	Dự báo dòng chảy đóng vai trò quan trọng trong việc quản lý tối ưu và vận hành hiệu quả tài nguyên nước. Do đó, chủ đề này đã nhận được sự chú ý từ nhiều nhà nghiên cứu, thúc đẩy sự phát triển của hàng loạt mô hình dự báo trong những thập kỷ qua. Trong số các mô hình dự báo, các kỹ thuật dựa trên mô hình thống kê và mô hình dựa trên dữ liệu đang trở nên phổ biến do tính đơn giản và độ tin cậy cao của chúng [1]. Các mô hình dựa trên dữ liệu này có thể được phân loại thành mô hình chuỗi thời gian và mô hình trí tuệ nhân tạo (AI).
0	Nhiều nhà nghiên cứu đã triển khai các mô hình chuỗi thời gian trong việc dự báo lưu lượng dòng chảy, bao gồm mô hình tự hồi quy (AR), trung bình trượt (MA), mô hình trung bình trượt tự hồi quy (ARMA), và mô hình trung bình trượt tích hợp tự hồi quy (ARIMA) [2]. Tuy nhiên, do các giả thuyết tuyến tính của các mô hình này, chúng không phù hợp để dự báo dòng chảy có đặc tính phi tuyến tính và không cố định. Do đó, các mô hình trí tuệ nhân tạo (AI) với khả năng lập bản đồ phi tuyến tính đã được ứng dụng vào dự báo dòng chảy, bao gồm máy vectơ hỗ trợ hồi quy (SVR) [3], hệ thống suy luận mờ (FIS) [4], hồi quy Bayesian (BR) [5] và mạng lưới thần kinh nhân tạo (ANN) [6].
0	Tuy nhiên, phần lớn các mô hình này không thể biểu diễn đầy đủ thông tin theo bản chất phi tuyến tính và không cố định của dòng chảy [7]. Các mô hình học sâu, như mạng niềm tin sâu (DBN) và mạng thần kinh tái phát (RNN), có thể khắc phục nhược điểm này nhờ khả năng học sâu hơn. Tuy nhiên, các mô hình học sâu này hoàn toàn dựa vào dữ liệu được quan sát trong quá khứ. Do đó có thể xảy ra trường hợp thông tin dòng chảy từ thời điểm trong quá khứ có thể không còn phù hợp so với thời điểm hiện tại.
0	Do đó, việc sử dụng mô hình học sâu có khả năng tự động “ghi nhớ” hoặc “quên” thông tin trước đó sẽ có thể nâng cao độ chính xác trong dự báo dòng chảy [8]. Long Short Term Memory (LSTM) một trong những mô hình học sâu, có khả năng giải quyết nhiệm vụ này. LSTM đã được triển khai hiệu quả trong một số lĩnh vực, chẳng hạn như chẩn đoán tai nạn [9], dự đoán giá điện [10], dự báo độ sâu mực nước ngầm [11] và nhiều lĩnh vực khác. Trong nghiên cứu này, các tác giả đã sử mô hình học sâu dựa trên sự tích hợp của mạng nơ-ron tích chập (Convolutional Neural Network - CNN) và mạng nơ-ron trí nhớ dài ngắn hạn (Long Short Term Memory - LSTM) để dự đoán lưu lượng dòng chảy.
0	Trong đó, mô hình CNN được áp dụng để trích xuất các đặc điểm nội tại của chuỗi thời gian dòng chảy, mô hình LSTM sử dụng các đặc điểm được trích xuất bởi CNN để dự đoán dòng chảy. Mục đích áp dụng mô hình CNN-LSTM để dự đoán dòng chảy là nhằm tận dụng khả năng xử lý phi tuyến tính của CNN, từ đó đạt được độ chính xác cao trong dự báo dòng chảy ngắn hạn. Hơn nữa, trong mô hình CNN-LSTM, CNN được sử dụng để loại bỏ nhiễu và xem xét mối tương quan giữa các biến có độ trễ của dòng chảy. Mô hình LSTM sau đó xử lý thông tin thời gian và ánh xạ chuỗi thời gian vào các không gian có thể tách rời để đưa ra dự đoán chính xác hơn.
0	Bên cạnh mô hình CNN-LSTM, nghiên cứu này còn tiến hành so sánh hiệu suất của mô hình với các mô hình trí tuệ nhân tạo khác để kiểm tra khả năng dự đoán lưu lượng dòng chảy của các mô hình học sâu và AI truyền thống. Trong nghiên cứu này nhóm nghiên cứu đã sử dụng dữ liệu thủy văn và khí tượng tại trạm Sơn Tây, Hà Nội để thử nghiệm mô hình dự đoán lưu lượng dòng chảy. Trong đó, dữ liệu lưu lượng được thu thập liên tục hàng ngày từ năm 1961 đến năm 2022.
0	Ngoài ra, nghiên cứu này còn khai thác dữ liệu về nhiệt độ, lượng mưa được thu thập tại cùng trạm khí tượng nhằm mục đích cải thiện độ chính xác kết quả dự báo. Nội dung chi tiết dữ liệu nghiên cứu được sử dụng được thể hiện Bảng 1. Lý do chọn trạm thủy văn Sơn Tây để thử nghiệm mô hình vì trạm thủy văn Sơn Tây là trạm nằm ở vị trí giao nhau các nhánh của sông Hồng. Dự báo được lưu lượng dòng chảy tại đây sẽ giúp cảnh báo lũ và quản lý tài nguyên nước cho khu vực hạ lưu. Sơ đồ nghiên cứu và vị trí trạm thủy văn được thể hiện ở Hình 1.
0	Đầu tiên, Bước 1 dữ liệu cho nghiên cứu thu thập trạm thủy văn Sơn Tây, Hà Nội. Dữ liệu thực nghiệm bao gồm dữ liệu về lưu lượng dòng chảy và các yếu tố khí tượng thủy văn như nhiệt độ, độ ẩm, lượng mưa, bốc hơi trong quá khứ. Có tổng cộng 22524 mẫu dữ liệu được thu thập từ 01/01/1961 -31/12/2022. Cần chuyển đổi dữ liệu dạng thô sang dạng dữ liệu phù hợp cho mô hình, để làm được điều đó cần một chuỗi chức năng được sử dụng. Sau đó, chia tập dữ liệu thành tập con để huấn luyện, xác thực và kiểm tra mô hình.
0	Đánh giá, xác định các tham số đầu vào ảnh hưởng đến kết quả dự báo Bước 2. Mô hình sẽ liên tục học hỏi từ bộ dữ liệu huấn luyện trước đó, từ đó dần dần cải thiện khả năng mô hình trong việc dự đoán. Khi hoàn tất việc huấn luyện, mô hình sẽ được xác thực và kiểm tra. Để đánh giá hiệu quả hoạt động của mô hình CNN-LSTM, các mô hình CNN và LSTM độc lập được chọn để so sánh hiệu quả Bước 3. Cuối cùng Bước 4 phân tích kết quả các mô hình trong việc dự đoán lưu lượng dòng chảy. Ba sai số được dùng làm tiêu chí đánh giá: Hệ số xác định (𝑅²), hệ số Nash (NSE) và sai số bình phương trung bình gốc (RMSE). Và cuối cùng đưa ra kết luận về hiệu quả mô hình.
0	CNN là một mô hình mạng được đề xuất bởi Lecun và các cộng sự vào năm 1998 [12]. Đây là một loại ngôn ngữ mạng nơ-ron chuyển tiếp, có khả năng cao trong việc xử lí hình ảnh và ngôn ngữ tự nhiên. Nó có thể được sử dụng một cách hiệu quả để dự đoán chuỗi thời gian [13]. CNN chủ yếu gồm 2 phần: lớp tích chập và lớp gộp. Trong nghiên cứu này, loại mạng CNN một chiều (Conv1D) được sử dụng. Trong Conv1D các hạt nhân tích chập đi theo một hướng. Dữ liệu đầu ra của Conv1D là hai chiều [14]. Conv1D có khả năng trích xuất tính năng mạnh mẽ, đặc biệt là trong các ứng dụng liên quan đến dữ liệu tuần tự hoặc chuỗi thời gian [15].
0	Lớp chập của CNN gồm nhiều hạt nhân tích chập và công thức tính toán [16]. Đa số các hạt nhân tích chập và công thức tính toán thể hiện trong công thức. Shu và cộng sự (2021) đã khám phá tính ứng dụng mạng nơ-ron tích chập - CNN để dự báo lưu lượng dòng chảy hằng tháng của một con sông, so sánh hiệu suất của CNN với mạng nơ-ron nhân tạo và học máy cực đại. Kết quả cho thấy CNN có hiệu suất vượt trội hơn [17]. Nghiên cứu [17] đã đề xuất mạng nơ-ron tích chập (CNN) để dự báo lũ và hạn hán ở cả vùng khô cằn và nhiệt đới. Kết quả cho thấy CNN nổi bật hơn so với các mô hình khác trong dự báo lũ và có thể xử lý tốt hơn nhiều đặc điểm trong đầu vào.
0	Tuy nhiên, CNN yêu cầu khối lượng dữ liệu lớn để thực hiện trong quá trình huấn luyện và gặp khó khăn trong việc xử lí các mối quan hệ dài hạn trong dữ liệu. Mạng nơ ron tái phát - RNN là loại mạng thần kinh nhân tạo mạnh mẽ và sử dụng dữ liệu chuỗi thời gian quá khứ và hiện tại để dự đoán dữ liệu trong tương lai ở một khoảng thời gian xác định. Tuy nhiên, một nhược điểm của RNN là chỉ ghi nhớ những thông tin gần đây mà không thể nhớ lại thông tin từ xa hơn [15], khắc phục những nhược điểm đó mô hình LSTM ra đời.
0	Đây là một biến thể của mạng nơ-ron tái phát RNN, được áp dụng để giải quyết các bài toán có sự phụ thuộc dài hạn (Long-term dependency). Mô hình LSTM lần đầu tiên được giới thiệu vào năm 1997, sau đó mô hình này đã trải qua nhiều lần cải tiến và phổ biến rộng rãi [13]. Mạng nơ-ron LSTM có cấu trúc dạng chuỗi, gồm nhiều mô đun được lặp lại, có 4 lớp tương tác với nhau theo một cách đặc biệt. Cách thức hoạt động của LSTM là ghi nhớ những thông tin quan trọng liên quan đến việc dự đoán và loại bỏ những dữ liệu không cần thiết [16]. Mỗi mô-đun LSTM bao gồm có trạng thái tế bào (cell state) và các cổng (gate).
0	Các cổng đều có nhiệm vụ sàng lọc thông tin với những mục đích khác nhau (Cổng quên, cổng đầu vào và cổng đầu ra) [13]. Cổng quên (Foget gate) có chức năng loại bỏ những thông tin không cần thiết khỏi trạng thái tế bào bên trong. Cổng đầu vào (Input gate) giúp sàng lọc những thông tin cần thiết để thêm vào trạng thái tế bào. Cổng đầu ra (Output gate) xác định những thông tin nào từ các trạng thái tế bào bên trong sẽ được sử dụng làm đầu ra [18]. Các cổng giúp cập nhập và kiểm soát luồng thông tin qua các khối bộ nhớ [15]. Sơ đồ cấu trúc LSTM chi tiết được thể hiện trong Hình 3.
0	Nghiên cứu [19] đã áp dụng thành công mô hình LSTM để mô tả đặc tính lượng mưa và dòng chảy trong lưu vực lớn và phức tạp. LSTM thể hiện hiệu suất dự đoán vượt trội hơn các mô hình truyền thống, tuy nhiên, bên cạnh đó một trong những hạn chế của mô hình này là nó cần một lượng dữ liệu lớn để có thể hiệu chỉnh chính xác. Năm 2018, nghiên cứu [20] đã sử dụng mô hình ANN và LSTM để mô phỏng lượng mưa và dòng chảy, mô hình LSTM được chứng minh có hiệu suất tốt hơn trong dự đoán chuỗi thời gian thủy văn. So với các nghiên cứu trước đây trong các mô hình dự đoán lượng mưa-dòng chảy, kết quả mô hình LSTM có giá trị R² và NSE cao hơn.
0	Tuy nhiên, cần nhiều nghiên cứu hơn trong việc mô hình hóa dự đoán thủy văn bằng cách sử dụng học máy sâu. CNN có khả năng tập trung vào các đặc trưng nổi bật được áp dụng trong xử lí hình ảnh như nhận dạng đối tượng, phân loại hình ảnh và phát hiện gương mặt, vì vậy nó thường được sử dụng trong các kỹ thuật nhận diện và trích xuất đặc trưng [13]. LSTM có đặc điểm mở rộng theo chuỗi thời gian nên được sử dụng rộng rãi trong các bài toán dự đoán chuỗi thời gian. Dựa trên các đặc điểm trên của CNN và LSTM, một mô hình dự đoán lưu lượng dòng chảy được ra đời là mô hình CNN-LSTM. Sơ đồ cấu trúc mô hình được trình bày ở Hình 4.
0	Cấu trúc chính bao gồm các lớp CNN và LSTM: lớp đầu vào, lớp tích chập, lớp gộp, lớp LSTM và lớp kết nối [22]. CNN là một trong những cấu trúc mạng tiêu biểu nhất trong Deep Learning. So với các mô hình Deep Learning khác, CNN có khả năng khai thác thông tin và tính năng mạnh mẽ hơn [23]. Kết hợp CNN và LSTM có thể trích xuất hiệu quả đặc điểm hơn đặc điểm không gian thời gian từ dữ liệu đầu vào [23]. Mô hình CNN-LSTM kết hợp cho ra kết quả dự đoán có độ chính xác cao hơn so với việc sử dụng riêng biệt mô hình CNN và LSTM độc lập.
0	Nghiên cứu này đã sử dụng bộ dữ liệu khí tượng thủy văn tại trạm thủy văn Sơn Tây để chứng minh rằng mô hình kết hợp CNN-LSTM là hệ thống dự báo dòng chảy hiệu quả và có khả năng áp dụng thực tiễn trong việc dự đoán lưu lượng dòng chảy. Hiện nay nhiều nghiên cứu sử dụng ba trị số chính để đánh giá độ chính xác của mô hình là sai số bình phương trung bình gốc (Root Mean Square Error - RMSE) và hệ số Nash (Nash Sutcliffe Efficiency - NSE) và hệ số xác định (R²) [25, 26].
0	Các trị số này thường được áp dụng để so sánh giữa các giá trị thực tế đo được và các giá trị dự báo trong các mô hình dự báo thủy văn. Hệ số xác định R² cho phép đo lường mức độ chặc chẽ của mối quan hệ giữa biến phản ứng và biến dự báo trong mô hình [27]. Nó khác với chỉ số Nash và RMSE ở đặc điểm thang đo kết quả không phụ thuộc vào thang đo đầu vào [28]. Hệ số xác định R² mô tả lượng phương sai quan sát được giải thích bởi mô hình [20]. Công thức tính được trình bày tại (1). Trong hầu hết các trường hợp, giá trị R² nằm trong khoảng từ [0,1].
0	Giá trị bằng 0 biểu thị hiệu suất thấp nhất chỉ ra không có sự tương quan và giá trị càng gần 1 có nghĩa là dự đoán càng chính xác [29]. Hệ số Nash (Nash Sutcliffe Efficiency - NSE) là một thước đo phổ biến và đáng tin cậy được sử dụng trong đánh giá hiệu suất chất lượng mô hình thủy văn [29]. NSE đo lường khả năng của mô hình trong việc dự đoán các giá trị thực tế so với giá trị trung bình được quan sát đồng thời cho biết tỷ lệ phương sai ban đầu được mô hình giải thích [20]. Mô hình dự báo có độ chính xác cao khi có hệ số Nash gần bằng 1 [30].
0	RMSE được sử dụng như một chỉ số thống kê tiêu chuẩn để đánh giá hiệu suất mô hình trong các nghiên cứu liên quan đến khí tượng, chất lượng không khí và khí hậu [31]. Giá trị RMSE thường được áp dụng để đo lường độ lệch chuẩn giữa các giá trị dự đoán và giá trị quan sát thực tế. Các trị số được tính toán theo công thức dưới đây (3). Trong đó 𝑦𝑖 là giá trị thực tế biến mục tiêu, 𝑦̂𝑖 là giá trị dự đoán biến mục tiêu, 𝑦̅𝑖 là giá trị trung bình của tất cả các biến thực tế, 𝑦𝑖′ giá trị tính toán tương ứng thứ i; n là số lần phát báo. Mẫu số trong công thức là biến thiên của các giá trị thực tế.
0	Nghiên cứu đã sử dụng 3 mô hình CNN, LSTM, CNN-LSTM huấn luyện với các bộ dữ liệu khí tượng thủy văn thu tập tại trạm thủy văn Sơn Tây trong hơn 60 năm. Tập dữ liệu phục vụ cho mô hình được chia thành 3 phần chính với các mục đích khác nhau: 80% cho huấn luyện mô hình (chuỗi dữ liệu này được đo đạc từ 01/01/1961-20/08/2010), 10% cho xác thực (từ ngày 21/08/2010-30/10/2016) và 10% dùng để đánh giá kiểm tra hiệu suất mô hình sau khi huấn luyện xong (từ ngày 31/10/2016-31/12/2022). Quá trình huấn luyện thử nghiệm nhiều lần đã giúp tạo ra mô hình tốt nhất dự báo lưu lượng dòng chảy.
0	Các thông số của mô hình và cấu trúc mô hình được điều chỉnh để tối ưu hiệu suất, và kết quả tóm tắt các mô hình được trình bày ở Bảng 2. Mỗi mô hình có cấu trúc khác nhau về số lớp ẩn, số lượng tham số đã được đào tạo. CNN có lớp tích chập Conv1D để trích xuất đặc trưng, trong khi LSTM có các lớp LSTM để xử lí chuỗi dữ liệu. CNN-LSTM kết hợp cả 2 loại nên số lớp ẩn nhiều hơn. Các tham số đã được đào tạo thể hiện khả năng học hỏi và độ phức tạp của mô hình.
0	Kết quả hiệu chỉnh và kiểm định các mô hình CNN, LSTM và CNN-LSTM sử dụng số liệu thực đo tại trạm thủy văn Sơn Tây, Hà Nội. Hình 5 minh họa hệ số quan trọng của các đặc trưng đầu vào trong các mô hình dự báo dòng chảy. Các mô hình đều chỉ ra rằng đặc trưng lưu lượng tại thời điểm t-1 (Q(t-1)) là yếu tố quan trọng nhất, cho thấy tính nhất quán trong các kết quả. Tuy nhiên, sự khác biệt về tỉ lệ đóng góp giữa các mô hình cho thấy rằng cách tiếp cận mô hình hóa khác nhau có thể dẫn đến sự thay đổi trong mức độ quan trọng của các yếu tố khác.
0	Các yếu tố được sắp xếp theo thứ tự giảm dần dự trên mức độ quan trọng của chúng. Đặc trưng lưu lượng tại thời điểm t-1 (Q(t-1)) có đóng góp lớn nhất, chiếm hơn 80% trong cả ba mô hình được thử nghiệm. Đặc trưng lưu lượng tại thời điểm t-2 (Q(t-2)) có ảnh hưởng tương đối nhưng nhỏ hơn nhiều so với Q(t-1). Các đặc trưng còn lại có đóng góp nhưng rất nhỏ không ảnh hưởng đáng kể đến hiệu suất của mô hình. Hình 6 hiển thị các đường cong của hàm mất mát các mô hình trong giai đoạn huấn luyện và xác thực thu được trong 100 lần lặp. Đường cong mất mát có xu hướng giảm dần theo số lần lặp.
0	Hình 6a mô hình CNN-LSTM thể hiện hiệu suất tốt nhất trong dự đoán dòng chảy, với hàm tổn thất giảm nhanh và dao động ở mức thấp, cho thấy khả năng học và dự đoán chính xác trên cả tập dữ liệu huấn luyện và kiểm định. Trong khi đó, Hình 6b mô hình LSTM và Hình 6c mô hình CNN cũng đạt kết quả khá tốt, nhưng vẫn gặp khó khăn trong việc xử lý dữ liệu phức tạp. Hình 7 hiển thị kết quả dự đoán lưu lượng dòng chảy được quan sát và mô phỏng từ mô hình CNN-LSTM (Hình 7a), LSTM (Hình 7b) và CNN (Hình 7c) trong khoảng thời gian từ 31/10/2016 - 31/12/2022 và các tiêu chí đánh giá hiệu suất các mô hình trong Bảng 3.
0	Các dòng mô phỏng Hình 7 cho thấy mô hình CNN-LSTM có hiệu suất tốt nhất trong số 3 mô hình được xem xét. Kết quả dự đoán dữ liệu dòng chảy mô hình CNN-LSTM (Hình 7a) với đường dự đoán gần như trùng khớp với dữ liệu thực tế, đặc biệt là ở các đỉnh lưu lượng. Quan sát trên biểu đồ tương quan Hình 7a các điểm trên biểu đồ tương quan của mô hình CNN-LSTM chủ yếu phân bố rất gần đường chéo (đường y = x), cho thấy sự dự báo của mô hình này gần với giá trị thực tế nhất. Độ phân tán của các điểm xung quanh đường y = x là rất nhỏ thể hiện sự sai lệch của mô hình này là ít.
0	Điều này đặc biệt rõ ràng ở các giá trị cao (các đỉnh lưu lượng lớn), khi mô hình dự báo khá chính xác. Điều này chứng minh được rằng CNN-LSTM có khả năng dự đoán lưu lượng dòng chảy với độ chính xác cao với dữ liệu đầu vào phức tạp, có khả năng dự báo bất thường về lũ. CNN-LSTM thể hiện vượt trội trong việc dự báo các đỉnh dòng chảy, một yếu tố then chốt trong quản lí tài nguyên nước và cảnh báo lũ lụt. Mô hình LSTM với khả năng ghi nhớ các mối quan hệ dài hạn giúp ích rất nhiều cho mô hình dự đoán phi tuyến tính tuy nhiên LSTM vẫn có sai số trong dự đoán, hầu hết các dự đoán đỉnh chính lưu lượng dòng chảy đều có sự chênh lệch, nhất là khi dữ liệu có biến động lớn (Hình 7b).
0	Các điểm trên biểu đồ tương quan của LSTM cũng phân bố khá gần với đường chéo, tuy nhiên có sự phân tán nhiều hơn so với mô hình CNN-LSTM, đặc biệt là ở các giá trị lưu lượng cực đoan (cao hoặc thấp). Độ phân tán của các điểm xung quanh đường y = x lớn hơn so với CNN-LSTM. Mô hình CNN có thể dự đoán xu hướng chính lưu lượng nhưng có một số điểm giá trị dự đoán có sự chênh lệch lớn so với giá trị thực tế (Hình 7c). Biểu đồ tương quan của CNN cho thấy các điểm phân tán nhiều hơn so với cả hai mô hình trên, đặc biệt là ở các giá trị cực trị.
0	Các điểm dữ liệu không tập trung sát đường y = x, độ phân tán của các điểm rất lớn, và điều này rõ ràng nhất ở các đỉnh và đáy của lưu lượng. Mô hình CNN dường như không thể học các chuỗi thời gian phức tạp mà chỉ dựa trên đặc trưng không gian, nên việc dự báo chuỗi thời gian của dòng chảy sẽ kém chính xác hơn. Mô hình CNN còn gặp khó khăn trong việc nắm bắt các mối quan hệ dài hạn trong dự đoán chuỗi thời gian. Nhằm kiểm tra hiệu suất của các mô hình CNN, LSTM và CNN-LSTM trong dự đoán lưu lượng dòng chảy. Sau khi huấn luyện các mô hình trên, dữ liệu thử nghiệm sẽ được đưa vào mô hình để dự đoán các biến mục tiêu.
0	Các mô hình được đánh giá và so sánh bằng các chỉ số R2, NSE, RMSE. Kết quả được tóm tắt Bảng 3. Mô hình LSTM (R2 = 0,956, NSE = 0,953, RMSE = 402,139) hoạt động tốt hơn mô hình CNN (R2 = 0,950, NSE = 0,948, RMSE = 422,375). Mô hình LSTM hoạt động tốt hơn mô hình CNN trong dự đoán chuỗi thời gian nhưng mô hình kết hợp CNN-LSTM hoạt động tốt hơn các mô hình độc lập CNN và LSTM với giá trị chỉ số kiểm định đều cao hơn thể hiện: R2 = 0,960, NSE = 0,958, RMSE = 379,384.
0	Trong ba mô hình, giá trị RMSE của CNN-LSTM là thấp nhất cho thấy sự kết hợp CNN-LSTM đã cải thiện độ chính xác tổng thể, hoạt động tốt hơn các mô hình CNN và LSTM độc lập. Điều này là do LSTM có khả năng học sự phụ thuộc lâu dài và CNN có thể trích xuất các tính năng bất biến theo thời gian. Nghiên cứu này đã chứng minh rằng mô hình CNN-LSTM cho độ chính xác cao hơn các mô hình AI thông thường khi dự đoán lưu lượng dòng chảy. Với các chỉ số kiểm định vượt trội thể hiện khả năng mô hình dự đoán gần với thực tế.
0	Có thể thấy được sự kết hợp giữa CNN và LSTM đã khai thác tối ưu điểm mạnh của từng mô hình riêng biệt. Kết quả này phù hợp với nghiên cứu [15] trong đó mô hình CNN-LSTM cũng được sử dụng để dự đoán dòng chảy và cho thấy hiệu suất vượt trội so với các mô hình AI thông thường như LSTM và các mạng nơ-ron sâu (DNN). Sự đồng nhất trong các kết quả này cho thấy rằng việc sử dụng mô hình CNN-LSTM là một công cụ mạnh mẽ và đáng tin cậy trong các ứng dụng dự báo chuỗi thời gian phức tạp.
0	Trong bài báo này, tác giả đề xuất mô hình kết hợp CNN-LSTM và ứng dụng trong dự báo lưu lượng dòng chảy với dữ liệu đầu vào là số liệu khí tượng thủy văn tại trạm thủy văn Sơn Tây, Hà Nội. Để chứng minh tính hiệu quả của mô hình CNN-LSTM, tác giả đã so sánh với các mô hình CNN và LSTM độc lập. Hiệu suất mô hình được đánh giá qua ba chỉ số R2, NSE, RMSE. Kết quả dự đoán các mô hình có thể được xếp hạng từ cao đến thấp như sau: CNN-LSTM > LSTM > CNN.
0	Mô hình CNN-LSTM có khả năng dự đoán lưu lượng dòng chảy vượt trội về tính khái quát hóa và độ chính xác so với các mô hình khác, được thể hiện thông qua các tiêu chí đánh giá mô hình, CNN-LSTM có giá trị RMSE thấp nhất và giá trị R2, NSE cao nhất. CNN-LSTM đã chứng minh sự vượt trội trong việc dự báo các đỉnh dòng chảy, một yếu tố then chốt trong quản lí tài nguyên nước và cảnh báo lũ lụt. Thông qua các việc đào tạo và kiểm định mô hình, các tác giả nhận thấy rằng CNN-LSTM có thể phân tích và kết hợp hiệu quả các dữ liệu đầu vào phức tạp bao gồm lưu lượng dòng chảy trong quá khứ và các yếu tố khí tượng như lượng mưa, nhiệt độ, độ ẩm.
0	Nhờ vậy, mô hình CNN-LSTM không chỉ cải thiện độ chính xác quá trình dự báo mà vượt trội hơn hẳn các mô hình khác về mức độ phù hợp và sai số. Tuy nhiên, để đảm bảo độ chính xác mô hình học sâu vào dự báo dòng chảy thì việc nghiên cứu bổ sung thêm các vấn đề như mô hình thiếu tính cập nhập trong các tình huống khẩn cấp; dữ liệu có thể bị ảnh hưởng bởi quá trình điều tiết dòng chảy hồ chứa và sông nhánh tại trạm thủy văn Sơn Tây là điều rất cần thiết.
0	Bài toán nhận dạng tuổi và giới tính đang thu hút được nhiều sự chú ý từ các nhà nghiên cứu đặc biệt là khi mạng xã hội và mạng truyền thông ngày càng phổ biến. Các phương pháp được công bố gần đây cho kết quả khá tốt về độ chính xác nhưng còn tỏ ra kém hiệu quả trong vấn đề nhận diện thời gian thực bởi vì các mô hình này được thiết kế quá phức tạp. Trong bài báo này, chúng tôi đề xuất một mô hình nhẹ mang tên lightweight CNN thực hiện song song 2 nhiệm vụ là phân lớp tuổi và giới tính.
0	Về độ chính xác trong nhận diện tuổi thì lightweight CNN tốt hơn 5.1% so với mô hình tốt nhất đã được công bố gần đây. Về thời gian chạy và số lượng tham số được sử dụng thì lightweight CNN sử dụng ít hơn nhiều so với các mô hình khác trên bộ dữ liệu Adience, đáp ứng được yêu cầu về nhận dạng trong thời gian thực. Xử lý ảnh và thị giác máy tính đang là những lĩnh vực được quan tâm nhiều nhất trong trí tuệ nhân tạo với nhiều bài toán thực tế. Bên cạnh đó, sự phát triển vượt bậc của các thuật toán học sâu đặc biệt là mạng lưới thần kinh tích chập (covolutional neural network – CNN) đã cho những kết quả vượt bậc trong các bài toán điển hình.
0	Ví dụ Alex cùng các cộng sự [1] đã đề xuất một mô hình sử dụng mạng CNN và giành chiến thắng trong cuộc thi ImageNet với tỷ lệ lỗi đạt 15.3% vào năm 2012. Đây là cuộc thi có quy mô lớn nhất thế giới về bài toán nhận diện đối tượng trong ảnh. Năm 2013, Zeiler và Fergus [2] đã đề xuất một mô hình có tên ZFNet và giảm lỗi từ 15,3% xuống còn 14,8%. GoogleNet (Inception) và VGGNet đã được đề xuất năm 2014 [3] với tỷ lệ lỗi lần lượt là 6,67% và 7,32%. Năm 2015, Kaiming He [4] đã đề xuất kiến trúc mạng ResNet và đạt tỷ lệ lỗi 3,57%, tỷ lệ lỗi này còn tốt hơn cả hiệu suất của con người.
0	Ngoại trừ bài toán nhận diện đối tượng trong ảnh, CNN thường được áp dụng cho nhiều bài toán khác như: Phát hiện đa đối tượng trong ảnh, đặt tiêu đề cho ảnh, phân đoạn ảnh,… Thậm chí, Yoo Kim [5] đã áp dụng mạng CNN cho bài toán phân lớp câu và đạt hiệu quả cao trong nhiều bộ cơ sở dữ liệu về văn bản khác nhau. Khuôn mặt là một đối tượng trong cơ thể con người và hình ảnh khuôn mặt mang rất nhiều thông tin quan trọng như: tuổi tác, giới tính, trạng thái cảm xúc, dân tộc,… Trong đó, việc xác định tuổi tác và giới tính là hết sức quan trọng, đặc biệt trong giao tiếp, chúng ta cần sử dụng những từ ngữ phù hợp với giới tính của người nghe ví dụ trong tiếng Việt chúng ta có: anh/chị, chú/cô...
0	Hay với nhiều ngôn ngữ khác nhau trên thế giới, chẳng hạn như tiếng Việt thì lời chào hỏi dành cho người lớn tuổi khác với người trẻ tuổi. Do đó, việc xác định tuổi và giới tính dựa trên khuôn mặt là một bài toán hết sức quan trọng, có ý nghĩa thực tế to lớn. Bài toán ước lượng tuổi và giới tính đã được quan tâm nhiều trong suốt 20 năm gần đây, đã có rất nhiều các công trình được công bố với nhiều kỹ thuật khác nhau chẳng hạn như: AGing pattErn Subspace (AGES), Gaussian Mixture Models (GMM), Hidden-Markov Model (HMM), Support Vector Machines (SVM), ...
0	Từ khi các mô hình học sâu được áp dụng cho bài toán này đã cải thiện đáng kể kết quả về mặt hiệu suất cũng như tốc độ. Độ chính xác của mô hình khi ước lượng tuổi đạt 62,8% và đối với giới tính đạt 92,6% [6]. Tuy nhiên, để đạt được hiệu suất cao thì các mô hình thường được xây dựng càng phức tạp với số lượng tham số lớn (từ 10 triệu đến hơn 100 triệu tham số), do đó gây khó khăn trong vấn đề nhận dạng trong thời gian thực. Trong bài báo này, chúng tôi đề xuất một mô hình nhẹ sử dụng CNN với khoảng 1 triệu tham số nhưng đạt kết quả nhận diện tuổi lên đến 67,9% và nhận diện giới tính lên đến 88,8%.
0	Với số lượng tham số nhỏ này thì mô hình của chúng tôi hoàn toàn có thể chạy được trên các thiết bị nhúng và thiết bị di động một cách dễ dàng đảm bảo vấn đề thời gian thực. Sự đóng góp của chúng tôi trong bài báo này là: (1) Xây dựng một mô hình nhẹ để giải quyết bài toán đa nhiệm vụ (dự đoán tuổi và giới tính từ ảnh chụp khuôn mặt). (2) Từ kết quả của mô hình cho thấy rằng thuật toán không chỉ tốt về mặt hiệu suất mà còn giảm thiểu số lượng tham số được sử dụng từ đó giúp cải thiện tốc độ của mô hình và đáp ứng được yêu cầu về nhận diện trong thời gian thực.
0	Như đã được đề cập ở phần Giới thiệu, bài toán ước lượng tuổi và giới tính đã được nghiên cứu từ rất lâu. Nhưng hầu như chúng chỉ được nghiên cứu tách rời nhau. Các mô hình được xây dựng riêng biệt cho từng nhiệm vụ. Cho đến năm 2016, Linnan Zhu cùng các cộng sự [7] đề xuất một mô hình đa nhiệm vụ giải quyết cả hai bài toán cùng một lúc. Trước tiên chúng ta xét lần lượt các bài toán để có cái nhìn tổng quan. Bài toán phân lớp tuổi: Nhiệm vụ của bài toán này là đưa ra ước lượng tuổi của một người từ bức ảnh chụp khuôn mặt của họ.
0	Bài toán này được giới thiệu lần đầu tiên bởi Kwon và Lobo [8] trong đó, họ sử dụng phương pháp phát hiện và tính toán tỷ lệ của các nếp nhăn trên khuôn mặt để có thể dự đoán độ tuổi và sau đó nó được cải tiến bởi Ramanathan và Chellappa [9]. Tuy nhiên, phương pháp này có thể phân biệt được độ tuổi giữa người lớn và trẻ em, nhưng rất khó có thể phân biệt được độ tuổi giữa những người lớn với nhau. Một cách tiếp cận khác do Geng cùng các cộng sự [10] trình bày là sử dụng AGES cho hiệu quả cao hơn nhưng thuật toán này cần một lượng lớn hình ảnh khuôn mặt của từng người và đặc biệt hình ảnh đầu vào này cần phải ở chính giữa, mặt hướng thẳng và được căn chỉnh đúng kích thước.
0	Tuy nhiên, trên thực tế thì các bức ảnh chụp lại rất ít khi thỏa mãn điều kiện như vậy do đó cách tiếp cận này không được phù hợp với nhiều ứng dụng thực tế. Một cách tiếp cận khác dựa trên các thuật toán thống kê đã được sử dụng như GMM [11] và HMM, super-vectors [12] được sử dụng để làm đại diện cho từng phần của khuôn mặt. Trong thập kỷ qua, khi các thuật toán học máy dần được cải tiến và đạt được thành tựu to lớn đặc biệt là học sâu, thì một loạt các công trình nghiên cứu về phân lớp tuổi được công bố cho kết quả khả quan, có thể kể đến như: Eidinger cùng các cộng sự [13] đã sử dụng SVM kết hợp với dropout cho bài toán nhận diện tuổi và nhận diện giới tính.
0	Năm 2015, Gil Levi và Tal Hassner [14] đã đưa ra mô hình Deep Neural Network đầu tiên cho bài toán phân lớp tuổi và giới tính. Sau đó, Zhu cùng các cộng sự [7] đã xây dựng một mô hình đa nhiệm vụ cho phép chia sẻ và tìm hiểu các tính năng tối ưu để cải thiện hiệu suất nhận dạng cho cả hai nhiệm vụ. Đây là bài báo đầu tiên áp dụng mô hình tối ưu hóa bài toán nhận diện tuổi và giới tính cùng nhau để thấy được mối quan hệ giữa 2 bài toán.
0	Bài toán phân lớp giới tính: Cùng với sự phát triển của bài toán nhận dạng tuổi, bài toán nhận biết giới tính đã được đề xuất và giải quyết từ những năm 1990. Tổng quan về các phương pháp phân lớp giới tính bạn đọc có thể được tìm thấy trong [15]. Sau đây, chúng tôi sẽ tóm tắt một số phương pháp liên quan. Cottrell [16] là người đầu tiên đề xuất mô hình mạng nơron giải quyết bài toán nhận dạng giới tính, tuy nhiên các khuôn mặt đầu vào phải đảm bảo nhiều yêu cầu nhất định, gây ra nhiều hạn chế cho mô hình.
0	Sau đó, Lyons cùng các cộng sự [17] đã sử dụng thuật toán PCA (Principal Component Analysis) và LDA (Linear Discriminant Analysis) để nhận diện ra giới tính. SVM và AdaBoost được sử dụng trong [18] và [19]. Trong [20], Ullah đã sử dụng Bộ mô tả kết cấu cục bộ Webers để nhận dạng giới tính. Hầu hết các phương pháp được thảo luận ở trên đã sử dụng bộ cơ sở dữ liệu FERET để đánh giá hiệu suất của mô hình. Tuy nhiên, các hình ảnh trong bộ dữ liệu FERET được chụp trong điều kiện tốt, hình ảnh các khuôn mặt không bị che phủ, và hướng thẳng.
0	Hơn nữa, kết quả thu được trên bộ dữ liệu này cho thấy nó đã bão hòa và không thách thức đối với các phương pháp hiện đại. Do đó, những năm gần đây bộ cơ sở dữ liệu Adience thường được sử dụng để so sánh kết quả giữa các mô hình. Bởi vì bộ dữ liệu này chứa hình ảnh thách thức hơn so với bộ dữ liệu FERET và được thiết kế để khai thác tốt hơn các thông tin từ các ảnh dữ liệu đào tạo [14]. Cũng tương tự như bài toán phân lớp tuổi, các mô hình như SVM, Deep Neural Network bao gồm AdienceNet [14], CaffeNet, VGG-16, và GoogleNet [6] cũng được áp dụng cho bài toán nhận diện giới tính.
0	Tuy nhiên, các phương pháp được nêu ở trên đều tồn tại những hạn chế nhất định. Với các phương pháp gần đây sử dụng mạng neural network thì đã khắc phục được những hạn chế đó nhưng số lượng tham số được sử dụng còn rất lớn, gây khó khăn cho vấn đề nhận diện trong thời gian thực và cho các thiết bị nhúng. Trong phần này, chúng tôi sẽ trình bày một mô hình đa tác vụ nhẹ mang tên lightweight CNN để giải quyết bài toán phân lớp tuổi và giới tính. Mô hình của chúng tôi được trình bày thành 3 phần bao gồm: Mạng tích chập nhẹ, kiến trúc mô hình và cuối cùng là huấn luyện và thử nghiệm. Sau đây, chúng tôi sẽ giới thiệu về mạng tích chập nhẹ.
0	Mạng tích chập nhẹ: là sử dụng mạng CNN để xây dựng ra mô hình với số lượng tham số ít, nhưng vẫn đảm bảo hiệu quả về mặt hiệu suất. Hay nói cách khác là làm thế nào để xây dựng một mô hình CNN với số lượng tham số ít nhất có thể nhưng lại đạt hiệu quả tốt nhất có thể, đây cũng là thách thức khó khăn nhất được đặt ra đối với các mô hình nhẹ nói chung. Khác với các mô hình như VGG Net hay ResNet sử dụng hơn 40 triệu hoặc thậm chí hơn 100 tham số, các mô hình nhẹ chỉ sử dụng vài triệu hoặc thậm chí chỉ hơn 1 triệu tham số.
0	Ví dụ: với phân loại độ tuổi, mô hình AdienceNet từ [14] đã sử dụng hơn 10 triệu tham số và độ chính xác là 50,7%, mô hình VGG-16 từ [6] đã sử dụng hơn 100 triệu tham số và độ chính xác là 62,8%, nhưng mô hình nhẹ từ [7] chỉ sử dụng 10 triệu tham số và độ chính xác lên tới 46,0%. Kiến trúc mô hình: mô hình lightweight CNN được mô tả như trong hình 1. Phần đầu tiên của mô hình, chúng tôi sử dụng mạng CNN để trích chọn ra các đặc trưng từ dữ liệu ảnh đầu vào.
0	Các hoạt động trong tầng CNN này bao gồm: Convolution (Conv) + Batch Normalization (BN) + Rectified Linear Unit (ReLU) + Max Pooling (MaxPool) với kích thước cửa sổ trượt là 2x2, bước nhảy bằng 2 + Drop out (Dropout) với tỷ lệ drop là 0,25. Ở phần sau của mô hình, chúng tôi sử dụng mạng Fully Connected (FC) với tỷ lệ dropout là 0,25. Huấn luyện và thử nghiệm: Đầu vào của mô hình là các hình ảnh RGB được thay đổi kích thước xuống còn 64x64, đầu ra của mô hình là vectơ y bao gồm 2 giá trị tương ứng với ước lượng tuổi và ước lượng giới tính của người trong ảnh đầu vào.
0	Hàm mất mát của mô hình L được thiết kế như trong công thức (1). Trong đó, N là số mẫu đưa vào mô hình huấn luyện, T là số lượng nhiệm vụ (với bài toán này T = 2). Chúng ta có là kết quả đầu ra của mô hình và y là kết quả thực tế của dữ liệu. Hàm mất mát được xây dựng dựa trên công thức MSE và áp dụng cho bài toán đa nhiệm vụ. Bộ cơ sở dữ liệu Adience: Như đã được đề cập ở mục trước, chúng tôi sử dụng bộ cơ sở dữ liệu Adience từ [21] để tiến hành huấn luyện và đánh giá mô hình.
0	Bộ cơ sở dữ liệu Adience chủ yếu được xây dựng để nhận biết độ tuổi và giới tính dựa vào ảnh chụp khuôn mặt. Adience chứa hơn 26 nghìn hình ảnh với độ phân giải 816 × 816 của hơn 2 nghìn người khác nhau. Hầu hết các hình ảnh từ bộ dữ liệu được tự động tải xuống từ Flickr và chúng được thu thập trực tiếp từ các thiết bị di động mà không qua lọc thủ công trước đó. Có 8 nhóm đại diện cho độ tuổi của các đối tượng bao gồm 0-2, 4-6, 8-13, 15-20, 25-32, 38-43, 48-53, 60-. Hình 2 là một ví dụ về các hình ảnh với chất lượng điều kiện ánh sáng kém, bị che một phần khuôn mặt, các tư thế đầu khác nhau, ... cho thấy sự thách thức từ bộ cơ sở dữ liệu này.
0	Chúng tôi không sử dụng bất kỳ dữ liệu bên ngoài nào trong giai đoạn huấn luyện. Mô hình lightweight CNN được đào tạo từ đầu với hàm tối ưu hóa là Adam. Các hình ảnh huấn luyện được chia thành nhiều phần với kích thước là 32 hình ảnh trên mỗi batch và tỷ lệ học tập chúng tôi sử dụng là 0,001. Để đánh giá chính xác hiệu suất của mô hình, chúng tôi sử dụng five-fold cross validation và so sánh kết quả của mô hình với các phương pháp đã được đề xuất gần đây trong [14], [6], [7], [22] về cả độ chính xác, lượng tham số sử dụng cũng như thời gian thực hiện.
0	Từ bảng 1, có thể thấy rằng mô hình của chúng tôi cho kết quả cao nhất trong việc ước lượng độ tuổi (đạt 67.9% cao hơn 5.1% so với kết quả tốt nhất hiện tại là VGG-16), về mặt dự đoán giới tính, mô hình của chúng tôi kém hơn 5% so với các mô hình học sâu khác như VGG-16. Bảng 2, cho thấy số lượng tham số được sử dụng của các mô hình. Mô hình light weight CNN chỉ sử dụng khoảng 1 triệu tham số, trong khi các mô hình khác sử dụng vài triệu thậm chí hơn 100 triệu tham số chẳng hạn như VGG-16 sử dụng tới 138 triệu tham số.
0	Trong bài báo này, chúng tôi đã đề xuất một mô hình học sâu nhẹ sử dụng mạng CNN để nhận diện tuổi và giới tính dựa vào hình ảnh khuôn mặt. Mô hình mới này cho phép sử dụng một số lượng nhỏ các tham số nhưng đạt hiệu suất tốt hơn các mô hình đã được công bố gần đây, đồng thời góp phần giải quyết vấn đế nhận diện trong thời gian thực. Trong tương lai gần, chúng tôi đang có kế hoạch cải thiện độ chính xác của mô hình, đặc biệt là đối với ước lượng giới tính. Mặt khác, chúng tôi sẽ áp dụng mô hình của chúng tôi cho các bài toán khác trong lĩnh vực thị giác máy tính và xử lý hình ảnh.
1	Trong những năm gần đây, mạng nơ-ron tích chập (Convolutional Neural Network – CNN) đã trở thành một trong những mô hình học sâu quan trọng nhất trong lĩnh vực xử lý ảnh và thị giác máy tính. Đặc biệt, trong y tế, CNN được ứng dụng rộng rãi để hỗ trợ chẩn đoán bệnh thông qua hình ảnh như X-quang, MRI và CT-scan. Việc tự động phân tích hình ảnh y khoa giúp giảm tải cho bác sĩ, đồng thời nâng cao độ chính xác và tốc độ phát hiện bệnh. Theo báo cáo của WHO, tỷ lệ sai sót trong chẩn đoán hình ảnh thủ công có thể lên tới 15%, trong khi các hệ thống dựa trên CNN có thể giảm tỷ lệ này xuống dưới 7%. Do đó, nghiên cứu và triển khai CNN trong y tế có ý nghĩa thực tiễn và khoa học rất lớn.
1	Nghiên cứu này sử dụng tập dữ liệu gồm 12.000 ảnh X-quang phổi, trong đó có 6.500 ảnh dương tính với viêm phổi và 5.500 ảnh âm tính. Mô hình CNN được thiết kế gồm 5 lớp tích chập, 3 lớp pooling và 2 lớp fully connected, sử dụng hàm kích hoạt ReLU và hàm mất mát Cross-Entropy. Dữ liệu được chia theo tỷ lệ 80% cho huấn luyện và 20% cho kiểm thử. Quá trình huấn luyện diễn ra trong 50 epoch với learning rate 0,001. Để tăng tính tổng quát, kỹ thuật data augmentation như xoay ảnh, lật ảnh và thay đổi độ sáng được áp dụng. Các chỉ số đánh giá chính bao gồm Accuracy, Precision, Recall và F1-score.
1	Kết quả thực nghiệm cho thấy mô hình CNN đạt độ chính xác trung bình 94,2% trên tập kiểm thử, trong đó Precision đạt 93,5% và Recall đạt 95,1%. So với phương pháp phân loại truyền thống sử dụng SVM chỉ đạt độ chính xác 86,7%, CNN cho thấy hiệu quả vượt trội. Thời gian xử lý trung bình cho mỗi ảnh là 0,18 giây, đáp ứng tốt yêu cầu ứng dụng thực tế. Tuy nhiên, nghiên cứu cũng chỉ ra rằng mô hình có xu hướng nhầm lẫn trong các trường hợp ảnh nhiễu hoặc độ phân giải thấp. Trong tương lai, việc kết hợp CNN với Transformer hoặc mô hình học đa phương thức có thể giúp cải thiện hơn nữa hiệu suất chẩn đoán.
1	Thuật toán K-Nearest Neighbors (KNN) là một trong những phương pháp học máy đơn giản nhưng hiệu quả, đặc biệt trong các bài toán phân loại và gợi ý. Trong đời sống, KNN được ứng dụng rộng rãi trong các hệ thống đề xuất sản phẩm, phim ảnh và nội dung cá nhân hóa. Bằng cách so sánh hành vi của người dùng hiện tại với các người dùng tương tự trong quá khứ, hệ thống có thể đưa ra gợi ý phù hợp. Theo thống kê của McKinsey, các hệ thống gợi ý hiệu quả có thể giúp doanh nghiệp tăng doanh thu từ 10% đến 30%. Vì vậy, việc nghiên cứu ứng dụng KNN trong phân tích hành vi người dùng mang lại giá trị kinh tế và xã hội rõ rệt.
1	Trong nghiên cứu này, dữ liệu được thu thập từ một nền tảng thương mại điện tử với 50.000 người dùng và hơn 200.000 lượt đánh giá sản phẩm. Mỗi người dùng được biểu diễn bằng vector đặc trưng gồm tần suất mua hàng, giá trị đơn hàng trung bình và loại sản phẩm quan tâm. Thuật toán KNN sử dụng khoảng cách Euclidean để xác định mức độ tương đồng giữa các người dùng, với giá trị k được lựa chọn là 5 thông qua quá trình thử nghiệm. Dữ liệu được chia thành 75% cho huấn luyện và 25% cho đánh giá. Các chỉ số đánh giá bao gồm độ chính xác gợi ý và tỷ lệ click vào sản phẩm được đề xuất.
1	Kết quả cho thấy hệ thống gợi ý dựa trên KNN đạt độ chính xác trung bình 88%, cao hơn 12% so với phương pháp gợi ý ngẫu nhiên. Tỷ lệ click vào sản phẩm tăng từ 18% lên 27% sau khi triển khai hệ thống. Tuy nhiên, thời gian tính toán của KNN tăng đáng kể khi số lượng người dùng lớn, gây khó khăn cho các hệ thống thời gian thực. Điều này cho thấy KNN phù hợp hơn với các hệ thống quy mô vừa hoặc khi được kết hợp với kỹ thuật giảm chiều dữ liệu như PCA. Trong tương lai, việc tích hợp KNN với học sâu có thể giúp cải thiện cả độ chính xác và tốc độ xử lý.
1	Nhận dạng khuôn mặt là một trong những ứng dụng quan trọng của trí tuệ nhân tạo trong đời sống hiện đại, đặc biệt trong các hệ thống an ninh và giám sát. Việc kết hợp CNN để trích xuất đặc trưng và KNN để phân loại đã được chứng minh là một hướng tiếp cận hiệu quả. CNN giúp tự động học các đặc trưng khuôn mặt phức tạp, trong khi KNN cho phép so sánh nhanh với các mẫu đã lưu. Theo nghiên cứu của NIST, các hệ thống nhận dạng khuôn mặt hiện đại có thể đạt độ chính xác trên 95% trong điều kiện ánh sáng chuẩn. Do đó, mô hình kết hợp CNN–KNN được xem là giải pháp khả thi cho các hệ thống an ninh thông minh.
1	Nghiên cứu sử dụng bộ dữ liệu gồm 8.000 ảnh khuôn mặt của 200 người khác nhau, được chụp trong nhiều điều kiện ánh sáng và góc nhìn. Mạng CNN được huấn luyện để trích xuất vector đặc trưng 128 chiều từ mỗi ảnh. Sau đó, thuật toán KNN với k = 3 được sử dụng để phân loại danh tính dựa trên khoảng cách cosine. Dữ liệu được chia thành 70% cho huấn luyện và 30% cho kiểm thử. Các chỉ số đánh giá bao gồm độ chính xác nhận dạng, tỷ lệ nhận dạng sai và thời gian phản hồi của hệ thống trong môi trường mô phỏng thực tế.
1	Kết quả thực nghiệm cho thấy hệ thống CNN–KNN đạt độ chính xác nhận dạng 96,1%, với tỷ lệ nhận dạng sai chỉ 3,9%. Thời gian phản hồi trung bình là 0,25 giây cho mỗi khuôn mặt, phù hợp với yêu cầu giám sát thời gian thực. So với phương pháp chỉ sử dụng CNN kết hợp Softmax, mô hình đề xuất cho kết quả ổn định hơn khi số lượng người trong cơ sở dữ liệu tăng. Tuy nhiên, hệ thống vẫn gặp khó khăn trong điều kiện ánh sáng yếu hoặc khuôn mặt bị che khuất. Trong tương lai, việc bổ sung dữ liệu và cải tiến kiến trúc CNN có thể giúp nâng cao độ bền vững của mô hình.
1	Trong kỷ nguyên y tế số, việc tự động hóa quá trình chẩn đoán hình ảnh đóng vai trò then chốt trong việc giảm tải cho hệ thống y tế. Bệnh võng mạc tiểu đường là một trong những nguyên nhân hàng đầu gây mù lòa, nhưng nếu được phát hiện sớm, tỷ lệ điều trị thành công có thể lên tới 90%. Tuy nhiên, việc soi đáy mắt thủ công đòi hỏi bác sĩ có chuyên môn cao và tốn nhiều thời gian. Mạng nơ-ron tích chập (CNN) với khả năng trích xuất đặc trưng không gian tự động đã chứng minh ưu thế vượt trội so với các phương pháp xử lý ảnh truyền thống.
1	Nghiên cứu này tập trung vào việc xây dựng mô hình CNN dựa trên kiến trúc ResNet-50 để phân loại 5 cấp độ tổn thương võng mạc, từ mức không bệnh đến mức tăng sinh nghiêm trọng, giúp hỗ trợ các cơ sở y tế tuyến dưới trong việc sàng lọc ban đầu. Nghiên cứu sử dụng tập dữ liệu EyePACS từ Kaggle bao gồm hơn 35.000 hình ảnh đáy mắt đã được dán nhãn bởi các chuyên gia. Trước khi đưa vào huấn luyện, chúng tôi thực hiện các bước tiền xử lý như cân bằng độ tương phản (CLAHE) và chuẩn hóa kích thước ảnh về 224x224 pixels.
1	Mô hình ResNet-50 được tinh chỉnh (fine-tuning) thông qua kỹ thuật Transfer Learning để tận dụng các đặc trưng đã học từ tập ImageNet. Quá trình huấn luyện sử dụng hàm mất mát Categorical Cross-entropy và bộ tối ưu hóa Adam với tốc độ học 10^{-4}. Chúng tôi chia dữ liệu theo tỷ lệ 8:1:1 cho huấn luyện, kiểm định và kiểm tra. Các kỹ thuật tăng cường dữ liệu như xoay ảnh, lật ngang và thay đổi độ sáng cũng được áp dụng để hạn chế tình trạng quá khớp (overfitting), đảm bảo tính tổng quát hóa của mô hình trên các thiết bị chụp ảnh khác nhau.
1	Kết quả thực nghiệm trên tập kiểm tra cho thấy mô hình đạt độ chính xác tổng thể (Accuracy) là 94,5%, một con số ấn tượng so với mức trung bình 88% của các bác sĩ nội trú. Cụ thể, độ nhạy (Sensitivity) đối với các trường hợp bệnh nghiêm trọng đạt 96,2%, trong khi độ đặc hiệu (Specificity) đạt 93,8%. Điểm F1-score trung bình cho cả 5 lớp đạt 0,92. Phân tích ma trận nhầm lẫn cho thấy sai số chủ yếu xảy ra giữa cấp độ 1 và cấp độ 2 do sự tương đồng lớn về các vi mạch máu. Tuy nhiên, thời gian xử lý trung bình cho mỗi bức ảnh chỉ mất 0,45 giây, nhanh gấp 200 lần so với quy trình chẩn đoán thủ công.
1	"Sự bùng nổ của thương mại điện tử đã tạo ra một lượng dữ liệu khổng lồ về hành vi người dùng, đặt ra thách thức trong việc cá nhân hóa trải nghiệm mua sắm. Thuật toán K-Nearest Neighbors (KNN) là một phương pháp tiếp cận hiệu quả trong các hệ thống lọc cộng tác (Collaborative Filtering) nhờ tính đơn giản và khả năng nắm bắt sự tương đồng giữa các khách hàng. Mục tiêu của nghiên cứu này là xây dựng một công cụ gợi ý sản phẩm cho các doanh nghiệp vừa và nhỏ, nơi nguồn lực tính toán còn hạn chế. Bằng cách phân tích lịch sử mua hàng và đánh giá (rating), hệ thống sẽ tìm kiếm ""những người hàng xóm"" có thị hiếu tương đồng để đưa ra các đề xuất sản phẩm mới."
1	Nghiên cứu tập trung vào việc tối ưu hóa tham số K và khoảng cách đo lường để đạt được sự cân bằng giữa độ chính xác và tốc độ phản hồi. Chúng tôi thực hiện thử nghiệm trên tập dữ liệu gồm 50.000 giao dịch của một cửa hàng thời trang trực tuyến. Dữ liệu thô được chuyển đổi thành ma trận người dùng - sản phẩm (User-Item Matrix) với các giá trị thưa thớt. Để xử lý tính thưa thớt, chúng tôi áp dụng kỹ thuật lọc các người dùng có ít hơn 5 tương tác. Khoảng cách Cosine được lựa chọn thay vì khoảng cách Euclidean vì nó hiệu quả hơn trong việc đo lường xu hướng mua hàng thay vì giá trị tuyệt đối của số lượng.
1	Chúng tôi tiến hành thử nghiệm với các giá trị K từ 3 đến 25 để tìm ra điểm tối ưu. Hệ thống được triển khai trên nền tảng Python với thư viện Scikit-learn, sử dụng cấu trúc dữ liệu BallTree để tăng tốc độ tìm kiếm hàng xóm gần nhất trong không gian đa chiều, giúp giảm độ phức tạp tính toán từ O(N) xuống O(log N). Kết quả cho thấy tại giá trị K=15, hệ thống đạt độ chính xác dự đoán (Precision@K) cao nhất là 78,4% và sai số tuyệt đối trung bình (MAE) ở mức 0,65 trên thang điểm 5.
1	So với việc không sử dụng hệ thống gợi ý, tỷ lệ chuyển đổi đơn hàng (Conversion Rate) của nhóm người dùng được gợi ý tăng thêm 15,6% trong vòng 3 tháng thực nghiệm. Đặc biệt, thời gian phản hồi của hệ thống duy trì ở mức dưới 150ms ngay cả khi số lượng sản phẩm tăng lên 10.000 mục. Nghiên cứu cũng chỉ ra rằng việc tích hợp thêm trọng số thời gian (decay factor) vào KNN giúp cải thiện độ chính xác thêm 3,2% do nắm bắt được sự thay đổi sở thích theo mùa của khách hàng. Đây là giải pháp chi phí thấp nhưng mang lại hiệu quả kinh tế cao cho các doanh nghiệp đang chuyển đổi số.
1	Sản xuất nông nghiệp hiện đại đang chuyển mình mạnh mẽ sang mô hình nông nghiệp thông minh, nơi các quyết định canh tác dựa trên dữ liệu thực tế thay vì kinh nghiệm cảm tính. Việc dự báo chính xác năng suất cây trồng trước khi thu hoạch giúp nông dân tối ưu hóa quy trình bón phân, tưới tiêu và chủ động trong việc tìm kiếm thị trường đầu ra. Thuật toán Random Forest, với bản chất là một mô hình học máy dạng tập hợp (Ensemble Learning), cực kỳ phù hợp cho các bài toán dữ liệu nông nghiệp vốn có nhiều biến số không tuyến tính và nhiễu.
1	Nghiên cứu này nhằm mục đích xây dựng mô hình dự báo năng suất lúa dựa trên các chỉ số hóa lý của đất và điều kiện khí tượng, từ đó đưa ra các khuyến nghị canh tác cụ thể cho từng vùng canh tác đặc thù. Nghiên cứu thu thập dữ liệu từ 1.200 mẫu đất tại khu vực Đồng bằng sông Cửu Long trong giai đoạn 2023-2025. Các biến đầu vào bao gồm 8 chỉ số quan trọng: nồng độ Đạm (N), Lân (P), Kali (K), độ pH, độ ẩm đất, nhiệt độ trung bình, lượng mưa và mật độ gieo sạ.
1	Thuật toán Random Forest được cấu hình với 500 cây quyết định (n_estimators) và độ sâu tối đa (max_depth) là 15 để tránh hiện tượng quá khớp. Chúng tôi sử dụng phương pháp Bootstrap để tạo ra các tập dữ liệu con khác nhau cho mỗi cây, giúp mô hình có độ ổn định cao trước những biến động bất thường của thời tiết. Tầm quan trọng của các đặc trưng (Feature Importance) cũng được trích xuất để xác định xem yếu tố nào ảnh hưởng lớn nhất đến năng suất cuối cùng, phục vụ cho công tác quản lý rủi ro.
1	Mô hình thực nghiệm đạt hệ số xác định (R^2) là 0,89, cho thấy khả năng giải thích được 89% sự biến thiên của năng suất thực tế. Sai số căn bậc hai trung bình (RMSE) chỉ ở mức 0,42 tấn/ha, thấp hơn đáng kể so với mô hình hồi quy tuyến tính (0,85 tấn/ha). Phân tích cho thấy nồng độ Kali và lượng mưa vào giai đoạn làm đòng là hai yếu tố có ảnh hưởng lớn nhất, chiếm 45% trọng số quyết định. Khi áp dụng mô hình để điều chỉnh lượng phân bón, các nông hộ tham gia thử nghiệm đã giảm được 22% chi phí vật tư nông nghiệp trong khi năng suất vẫn duy trì ổn định, tăng lợi nhuận trung bình thêm 12 triệu đồng/ha mỗi vụ.
1	Nghiên cứu này trình bày một hệ thống giám sát giao thông thông minh dựa trên mạng nơ-ron tích chập (CNN) nhằm phát hiện và phân loại các hành vi vi phạm giao thông trong môi trường đô thị. Hệ thống sử dụng dữ liệu video thời gian thực từ camera giao thông để nhận diện phương tiện, xác định làn đường và phát hiện các hành vi như vượt đèn đỏ, đi sai làn và dừng xe trái phép. Tập dữ liệu gồm hơn 25.000 khung hình được gán nhãn thủ công, đại diện cho nhiều điều kiện thời tiết và ánh sáng khác nhau. Kết quả thực nghiệm cho thấy mô hình CNN đạt độ chính xác trung bình 93,6% trong việc phát hiện vi phạm, góp phần nâng cao hiệu quả quản lý giao thông và giảm phụ thuộc vào giám sát thủ công.
1	Sự gia tăng nhanh chóng của số lượng phương tiện giao thông tại các đô thị lớn đã tạo ra nhiều thách thức trong công tác quản lý và đảm bảo an toàn giao thông. Các phương pháp giám sát truyền thống chủ yếu dựa vào con người thường tốn nhiều nhân lực, chi phí cao và dễ xảy ra sai sót. Trong bối cảnh đó, trí tuệ nhân tạo, đặc biệt là mạng CNN, đã mở ra hướng tiếp cận mới cho việc tự động phân tích hình ảnh và video giao thông. CNN có khả năng trích xuất đặc trưng không gian hiệu quả, giúp nhận diện phương tiện và hành vi phức tạp trong môi trường thực tế.
1	Theo thống kê của Bộ Giao thông Vận tải, việc ứng dụng hệ thống giám sát thông minh có thể giúp giảm tới 20% số vụ vi phạm giao thông tại các nút giao trọng điểm. Nhiều nghiên cứu trước đây đã tập trung vào việc sử dụng các phương pháp học máy truyền thống như SVM hoặc Haar Cascade để phát hiện phương tiện giao thông. Tuy nhiên, các phương pháp này thường gặp hạn chế khi điều kiện ánh sáng thay đổi hoặc khi mật độ phương tiện cao. Gần đây, các mô hình CNN như YOLO và Faster R-CNN đã được áp dụng rộng rãi trong phát hiện đối tượng thời gian thực và cho thấy hiệu suất vượt trội. Một số nghiên cứu báo cáo độ chính xác trên 90% trong nhận diện xe cộ, nhưng chưa tập trung sâu vào phân loại hành vi vi phạm.
1	Do đó, vẫn tồn tại khoảng trống nghiên cứu trong việc xây dựng hệ thống CNN chuyên biệt cho phát hiện vi phạm giao thông trong môi trường đô thị phức tạp. Hệ thống đề xuất sử dụng kiến trúc CNN gồm 6 lớp tích chập, 3 lớp pooling và 2 lớp fully connected. Dữ liệu đầu vào là các khung hình video được chuẩn hóa về kích thước 224×224 pixel. Tập dữ liệu gồm 25.000 ảnh, trong đó 18.000 ảnh dùng cho huấn luyện và 7.000 ảnh cho kiểm thử. Quá trình huấn luyện được thực hiện trong 60 epoch với batch size 32 và learning rate 0,0005. Các kỹ thuật tăng cường dữ liệu như xoay ảnh, làm mờ và thay đổi độ tương phản được áp dụng nhằm cải thiện khả năng tổng quát hóa của mô hình. Các chỉ số đánh giá bao gồm Accuracy, Precision và Recall.
1	Kết quả thực nghiệm cho thấy mô hình CNN đạt độ chính xác trung bình 93,6% trong phát hiện vi phạm giao thông. Precision và Recall lần lượt đạt 92,8% và 94,1%, cho thấy khả năng cân bằng tốt giữa phát hiện đúng và giảm báo động giả. Đặc biệt, hệ thống đạt hiệu suất cao nhất trong phát hiện hành vi vượt đèn đỏ với độ chính xác 95,2%. Thời gian xử lý trung bình cho mỗi khung hình là 0,21 giây, đáp ứng yêu cầu vận hành gần thời gian thực. So với phương pháp truyền thống dựa trên xử lý ảnh thủ công, mô hình CNN cải thiện độ chính xác khoảng 18%.
1	Mặc dù đạt được kết quả khả quan, hệ thống vẫn gặp một số hạn chế trong điều kiện thời tiết xấu như mưa lớn hoặc sương mù dày. Ngoài ra, khi mật độ phương tiện quá cao, hiện tượng che khuất làm giảm hiệu quả nhận diện. Tuy nhiên, kết quả nghiên cứu cho thấy tiềm năng lớn của CNN trong giám sát giao thông thông minh. Việc kết hợp thêm dữ liệu radar hoặc cảm biến IoT có thể giúp cải thiện độ ổn định của hệ thống. Nghiên cứu này đóng góp cơ sở thực nghiệm cho việc triển khai các giải pháp giao thông thông minh tại các đô thị đang phát triển.
1	Nghiên cứu này đã đề xuất và đánh giá thành công một hệ thống giám sát giao thông thông minh dựa trên mạng nơ-ron tích chập (CNN) với kiến trúc tiên tiến YOLOv8, đạt độ chính xác trung bình (mAP) lên tới 92,6% trên tập dữ liệu thực tế tại đô thị. Kết quả thực nghiệm cho thấy mô hình không chỉ có khả năng nhận diện chính xác các loại phương tiện trong các điều kiện ánh sáng phức tạp hay thời tiết xấu mà còn duy trì tốc độ xử lý ổn định ở mức 55 FPS, đáp ứng hoàn hảo các yêu cầu khắt khe về giám sát và phản ứng thời gian thực.
1	Bài báo này trình bày việc ứng dụng thuật toán K-Nearest Neighbors (KNN) trong phân loại khách hàng nhằm hỗ trợ cá nhân hóa dịch vụ tài chính. Dữ liệu nghiên cứu được thu thập từ hơn 30.000 khách hàng của một ngân hàng thương mại, bao gồm thông tin thu nhập, lịch sử giao dịch và hành vi sử dụng dịch vụ. Thuật toán KNN được sử dụng để phân nhóm khách hàng dựa trên mức độ tương đồng, từ đó đề xuất các sản phẩm tài chính phù hợp. Kết quả cho thấy mô hình đạt độ chính xác phân loại 89,4% và giúp tăng tỷ lệ sử dụng dịch vụ thêm 17% sau khi triển khai thử nghiệm.
1	Trong lĩnh vực tài chính – ngân hàng, việc hiểu rõ hành vi và nhu cầu của khách hàng đóng vai trò then chốt trong việc nâng cao chất lượng dịch vụ và khả năng cạnh tranh. Các phương pháp phân loại khách hàng truyền thống thường dựa vào quy tắc cố định, thiếu tính linh hoạt và khó thích nghi với dữ liệu lớn. Thuật toán KNN, với ưu điểm đơn giản và dễ triển khai, cho phép phân loại khách hàng dựa trên sự tương đồng trong không gian đặc trưng. Theo báo cáo của Deloitte, các ngân hàng áp dụng phân tích dữ liệu thông minh có thể tăng doanh thu bán chéo lên đến 25%. Do đó, việc nghiên cứu ứng dụng KNN trong cá nhân hóa dịch vụ tài chính mang ý nghĩa thực tiễn cao.
1	Nhiều nghiên cứu trước đây đã áp dụng các thuật toán phân cụm như K-means hoặc mô hình cây quyết định trong phân loại khách hàng tài chính. Tuy nhiên, các phương pháp này thường yêu cầu giả định trước về số lượng nhóm hoặc cấu trúc dữ liệu. KNN không cần giai đoạn huấn luyện phức tạp và có khả năng thích nghi tốt với dữ liệu mới. Một số nghiên cứu quốc tế cho thấy KNN đạt hiệu suất tương đương hoặc cao hơn các mô hình phức tạp trong các bài toán phân loại quy mô vừa. Tuy nhiên, vấn đề chi phí tính toán vẫn là thách thức lớn khi dữ liệu tăng nhanh.
1	Tập dữ liệu nghiên cứu gồm 30.000 khách hàng với 12 đặc trưng chính như thu nhập trung bình, số lần giao dịch hàng tháng và mức độ sử dụng dịch vụ tín dụng. Dữ liệu được chuẩn hóa trước khi áp dụng KNN nhằm giảm ảnh hưởng của các thang đo khác nhau. Giá trị k được lựa chọn là 7 thông qua phương pháp cross-validation. Dữ liệu được chia thành 70% cho huấn luyện và 30% cho kiểm thử. Khoảng cách Euclidean được sử dụng để đo độ tương đồng giữa các khách hàng. Các chỉ số đánh giá bao gồm độ chính xác phân loại và tỷ lệ phản hồi của khách hàng.
1	Kết quả thực nghiệm cho thấy mô hình KNN đạt độ chính xác phân loại trung bình 89,4% trên tập dữ liệu gồm 100.000 hồ sơ khách hàng. Trong quá trình thử nghiệm thực tế kéo dài ba tháng, tỷ lệ khách hàng chấp nhận các gợi ý dịch vụ cá nhân hóa đã tăng mạnh từ 22% lên 39%, cho thấy khả năng bắt kịp xu hướng tiêu dùng của thuật toán. Về mặt hiệu suất, so với phương pháp phân loại thủ công dựa trên bộ quy tắc cứng (rule-based), KNN giúp giảm thời gian xử lý dữ liệu khoảng 35%, cho phép hệ thống đưa ra phản hồi gần như tức thì với các nhóm dữ liệu vừa và nhỏ.
1	"Kết quả nghiên cứu khẳng định KNN là một công cụ hiệu quả trong việc phân loại phân khúc khách hàng và cá nhân hóa dịch vụ tài chính nhờ tính đơn giản và khả năng thích ứng cao với dữ liệu phi tuyến tính. Tuy nhiên, thách thức lớn nhất nằm ở ""lời nguyền đa chiều"" (Curse of Dimensionality) khi số lượng thuộc tính tài chính (thu nhập, nợ cũ, thói quen chi tiêu,...) gia tăng, làm loãng giá trị của các phép đo khoảng cách. Để triển khai trên quy mô lớn, nghiên cứu đề xuất cần kết hợp các kỹ thuật tối ưu hóa như giảm chiều dữ liệu (PCA) hoặc sử dụng các cấu trúc dữ liệu tiên tiến như KD-tree và Ball-tree để tăng tốc độ tìm kiếm hàng xóm."
1	"Bài báo đã chứng minh một cách thuyết phục tiềm năng ứng dụng của thuật toán KNN trong việc chuyển đổi số ngành tài chính – ngân hàng. Mô hình đề xuất không chỉ dừng lại ở lý thuyết mà đã thực sự nâng cao hiệu quả phân loại khách hàng, giúp các tổ chức tín dụng tối ưu hóa chiến dịch marketing và quản trị rủi ro cá nhân. Mặc dù vẫn còn tồn tại hạn chế về tốc độ xử lý khi đối mặt với dữ liệu khổng lồ (Big Data), nhưng đây là bước đệm quan trọng để các ngân hàng tiến tới mô hình ""Personalized Banking"" toàn diện."
1	Nghiên cứu này đề xuất một hệ thống giám sát an toàn lao động thông minh dựa trên việc kết hợp CNN và KNN nhằm phát hiện các hành vi nguy hiểm tại công trường xây dựng. CNN được sử dụng để trích xuất đặc trưng hình ảnh, trong khi KNN thực hiện phân loại hành vi. Tập dữ liệu gồm 15.000 ảnh được thu thập từ các công trường thực tế. Kết quả cho thấy hệ thống đạt độ chính xác 95,3% trong phát hiện vi phạm an toàn, góp phần giảm thiểu tai nạn lao động và nâng cao hiệu quả quản lý.
1	Tai nạn lao động là một trong những vấn đề nghiêm trọng tại các công trường xây dựng, gây thiệt hại lớn về người và tài sản. Các biện pháp giám sát truyền thống phụ thuộc nhiều vào con người và khó đảm bảo giám sát liên tục. Việc ứng dụng trí tuệ nhân tạo, đặc biệt là CNN và KNN, mang lại giải pháp tự động và hiệu quả hơn. CNN có khả năng học các đặc trưng hình ảnh phức tạp, trong khi KNN cho phép phân loại hành vi dựa trên mức độ tương đồng. Theo thống kê của ILO, việc áp dụng công nghệ giám sát thông minh có thể giúp giảm tới 30% số vụ tai nạn lao động.
1	Quy trình thực nghiệm được thiết kế dựa trên sự kết hợp giữa sức mạnh trích xuất đặc trưng của mạng nơ-ron tích chập (CNN) và khả năng phân loại linh hoạt của thuật toán K-Nearest Neighbors (KNN). Cụ thể, chúng tôi sử dụng một kiến trúc CNN đã qua huấn luyện (Pre-trained) để đóng vai trò như một bộ trích xuất đặc trưng, chuyển đổi mỗi khung hình từ camera giám sát thành một vector đặc trưng cô đọng gồm 256 chiều. Vector này chứa đựng các thông tin quan trọng về cấu trúc không gian, tư thế người lao động và sự hiện diện của các trang bị bảo hộ cá nhân (PPE).
1	Kết quả thực nghiệm trên tập dữ liệu kiểm thử độc lập khẳng định hiệu quả vượt trội của mô hình lai CNN–KNN khi đạt độ chính xác tổng thể lên tới 95,3%. Đáng chú ý nhất là chỉ số Recall (độ gợi nhớ) đối với các hành vi nguy hiểm (như không đeo mũ bảo hiểm hoặc xâm nhập vùng cấm) đạt mức 96,7%, một con số cực kỳ quan trọng trong các bài toán an toàn lao động để giảm thiểu tối đa rủi ro bỏ sót lỗi. Về mặt hiệu suất tính toán, thời gian phản hồi trung bình đo được là 0,23 giây cho mỗi khung hình (tương đương khoảng 4,3 FPS), cho phép hệ thống vận hành mượt mà trên các máy tính biên (Edge Computing) có cấu hình tầm trung.
1	Hệ thống đề xuất trong nghiên cứu này mở ra một hướng đi mới đầy triển vọng trong việc tự động hóa quy trình quản lý an toàn tại các nhà máy và công trường xây dựng hiện đại. Khả năng phát hiện sớm và cảnh báo tức thì các hành vi vi phạm an toàn giúp doanh nghiệp giảm thiểu nguy cơ tai nạn lao động và các chi phí phát sinh liên quan đến bồi thường hoặc đình chỉ thi công. Tuy nhiên, qua phân tích sâu các trường hợp dự đoán sai, nghiên cứu nhận thấy độ chính xác vẫn bị ảnh hưởng nhất định bởi các yếu tố ngoại cảnh khắc nghiệt như tình trạng thiếu sáng vào ban đêm, bụi bẩn hoặc hiện tượng che khuất (occlusion) khi nhiều công nhân đứng quá gần nhau.
1	"Nghiên cứu này đã chứng minh một cách thuyết phục tính khả thi và hiệu quả của việc kết hợp kiến trúc CNN và KNN trong bài toán giám sát an toàn lao động tự động. Mô hình đề xuất không chỉ đạt được các thông số kỹ thuật ấn tượng về độ chính xác và tốc độ xử lý mà còn mang lại giá trị nhân văn to lớn thông qua việc bảo vệ sức khỏe và tính mạng người lao động. Hệ thống hoàn toàn có khả năng triển khai thực tế như một thành phần cốt lõi của mô hình ""Nhà máy thông minh"" (Smart Factory), giúp nâng cao năng lực quản trị rủi ro và giảm bớt gánh nặng cho đội ngũ giám sát trực tiếp."
1	Nghiên cứu này trình bày việc ứng dụng mạng nơ-ron tích chập (Convolutional Neural Network - CNN) để phát hiện và phân loại các tổn thương da ác tính, đặc biệt là ung thư hắc tố (melanoma). Chúng tôi đã phát triển một mô hình CNN sâu gồm 18 lớp tích chập, huấn luyện trên tập dữ liệu ISIC 2019 với 25,331 hình ảnh da được gán nhãn bởi các bác sĩ da liễu chuyên khoa. Kết quả cho thấy mô hình đạt độ chính xác 94.7%, độ nhạy 92.3% và độ đặc hiệu 96.1% trong việc phân biệt các tổn thương lành tính và ác tính. Thời gian xử lý trung bình cho mỗi hình ảnh là 0.23 giây, nhanh hơn 15-20 lần so với phương pháp đọc phim truyền thống.
1	Nghiên cứu được thực hiện tại Bệnh viện Đa khoa Trung ương Quân đội 108 và Trường Đại học Bách khoa Hà Nội từ tháng 3/2023 đến tháng 11/2024. Ung thư da là một trong những loại ung thư phổ biến nhất toàn cầu với hơn 5.4 triệu ca mới được chẩn đoán mỗi năm theo thống kê của Tổ chức Y tế Thế giới năm 2024. Tại Việt Nam, số ca mắc ung thư da tăng trung bình 8.3% mỗi năm trong giai đoạn 2018-2023, với khoảng 12,500 ca mới được ghi nhận năm 2023. Việc phát hiện sớm ung thư da, đặc biệt là melanoma, có thể nâng cao tỷ lệ sống sót 5 năm lên tới 99% nếu được phát hiện ở giai đoạn I, so với chỉ 27% khi phát hiện ở giai đoạn IV.
1	Tuy nhiên, Việt Nam hiện chỉ có khoảng 850 bác sĩ da liễu chuyên khoa cho 98 triệu dân, tạo ra khoảng cách lớn về khả năng tiếp cận dịch vụ chẩn đoán chuyên sâu. Công nghệ học sâu, đặc biệt là CNN, đã chứng minh hiệu quả vượt trội trong phân tích hình ảnh y khoa và có tiềm năng lớn trong việc hỗ trợ chẩn đoán. CNN là một loại mạng nơ-ron nhân tạo được thiết kế đặc biệt để xử lý dữ liệu dạng lưới như hình ảnh. Kiến trúc cơ bản của CNN bao gồm các lớp tích chập (convolutional layers) với các bộ lọc (filters) kích thước 3×3 hoặc 5×5 pixel, lớp gộp (pooling layers) thường sử dụng max-pooling với kích thước 2×2, và các lớp kết nối đầy đủ (fully-connected layers) ở cuối mạng.
1	Trong nghiên cứu này, chúng tôi sử dụng kiến trúc ResNet-50 đã được tinh chỉnh với 50 lớp sâu, bao gồm 48 lớp tích chập và 2 lớp fully-connected. Mỗi lớp tích chập sử dụng hàm kích hoạt ReLU (Rectified Linear Unit) với công thức f(x) = max(0,x) để giảm vấn đề vanishing gradient. Mô hình có tổng cộng 23.5 triệu tham số cần học, được khởi tạo ngẫu nhiên theo phân phối Xavier để đảm bảo hội tụ nhanh hơn trong quá trình huấn luyện. Quá trình trích xuất đặc trưng trong CNN diễn ra qua nhiều tầng với độ phức tạp tăng dần.
1	Các lớp tích chập đầu tiên (lớp 1-5) học các đặc trưng cơ bản như cạnh, góc và kết cấu với kích thước receptive field từ 3×3 đến 11×11 pixel. Lớp trung gian (lớp 6-15) nhận diện các pattern phức tạp hơn như hình dạng, màu sắc và cấu trúc bề mặt da với receptive field mở rộng đến 43×43 pixel. Các lớp sâu hơn (lớp 16-48) học các đặc trưng trừu tượng cao như hình thái tổn thương, biên giới bất thường và sự không đồng nhất màu sắc đặc trưng của melanoma. Chúng tôi áp dụng kỹ thuật batch normalization sau mỗi lớp tích chập với momentum 0.9 và epsilon 1e-5 để ổn định quá trình huấn luyện và giảm 18% thời gian hội tụ so với không sử dụng batch normalization.
1	Tập dữ liệu nghiên cứu bao gồm 25,331 hình ảnh da được thu thập từ tập ISIC 2019 (International Skin Imaging Collaboration) và 3,450 hình ảnh bổ sung từ Bệnh viện Đa khoa Trung ương Quân đội 108. Tổng cộng 28,781 hình ảnh được chia thành 8 loại tổn thương: melanoma (4,522 ảnh), nevus lành tính (12,875 ảnh), carcinoma tế bào đáy (3,323 ảnh), keratosis quang hóa (2,624 ảnh), carcinoma tế bào vảy (628 ảnh), tổn thương mạch máu (1,099 ảnh), dermatofibroma (1,458 ảnh) và các tổn thương khác (2,252 ảnh). Các hình ảnh được chuẩn hóa về kích thước 224×224 pixel, sau đó được tăng cường dữ liệu (data augmentation) bằng các kỹ thuật xoay ngẫu nhiên (±30 độ), lật ngang/dọc với xác suất 0.5, điều chỉnh độ sáng (±15%), độ tương phản (±20%) và zoom (0.9-1.1 lần) để tạo ra 142,905 mẫu huấn luyện,
1	giúp mô hình robust hơn với các điều kiện chụp ảnh khác nhau. Mô hình CNN được xây dựng dựa trên kiến trúc ResNet-50 với các block residual để giải quyết vấn đề degradation trong mạng sâu. Chúng tôi sử dụng transfer learning từ mô hình pre-trained trên ImageNet với 1.28 triệu hình ảnh tự nhiên, sau đó fine-tuning toàn bộ mạng trên dữ liệu y khoa. Hàm mất mát sử dụng là categorical cross-entropy có trọng số (weighted) để xử lý vấn đề mất cân bằng dữ liệu giữa các lớp, với trọng số tỷ lệ nghịch với số lượng mẫu của mỗi lớp. Quá trình huấn luyện sử dụng optimizer Adam với learning rate ban đầu 0.0001, giảm xuống 0.00001 sau epoch 30 và 0.000001 sau epoch 50, batch size 32 mẫu và tổng 80 epoch.
1	Dropout với tỷ lệ 0.5 được áp dụng ở các lớp fully-connected để tránh overfitting. Thời gian huấn luyện tổng cộng là 37.5 giờ trên GPU NVIDIA Tesla V100 với 32GB VRAM. Để đánh giá toàn diện hiệu suất mô hình, chúng tôi sử dụng tập validation gồm 5,756 hình ảnh (20% tổng dữ liệu) và tập test độc lập gồm 2,878 hình ảnh (10% tổng dữ liệu) chưa từng được mô hình nhìn thấy. Các metrics đánh giá bao gồm: độ chính xác tổng thể (overall accuracy), độ nhạy (sensitivity/recall) đo khả năng phát hiện đúng các ca bệnh, độ đặc hiệu (specificity) đo khả năng loại trừ đúng các ca không bệnh,
1	giá trị dự đoán dương (PPV - Positive Predictive Value), giá trị dự đoán âm (NPV - Negative Predictive Value), F1-score là trung bình điều hòa của precision và recall, và AUC-ROC (Area Under the Curve - Receiver Operating Characteristic) với giá trị từ 0 đến 1. Chúng tôi cũng so sánh hiệu suất của mô hình với 5 bác sĩ da liễu có kinh nghiệm từ 8-15 năm trên cùng 500 ca test ngẫu nhiên để đánh giá tính ứng dụng thực tế. Mô hình CNN đạt độ chính xác tổng thể 94.7% trên tập test với 2,725/2,878 ca được phân loại đúng. Cụ thể, độ nhạy trung bình là 92.3% (dao động từ 87.8% đến 96.4% tùy loại tổn thương), độ đặc hiệu trung bình 96.1% (dao động từ 93.2% đến 98.7%), và F1-score trung bình 0.931.
1	Đối với nhiệm vụ quan trọng nhất là phát hiện melanoma, mô hình đạt độ nhạy 96.4%, độ đặc hiệu 95.8%, PPV 89.3%, NPV 98.6% và AUC-ROC 0.982. Ma trận nhầm lẫn (confusion matrix) cho thấy có 17 ca melanoma bị bỏ sót (false negative - 3.6%) và 52 ca lành tính bị chẩn đoán nhầm là ác tính (false positive - 4.2%). Tỷ lệ false negative thấp là rất quan trọng trong y học vì việc bỏ sót ung thư nguy hiểm hơn nhiều so với chẩn đoán dương tính giả. Thời gian suy luận trung bình cho mỗi hình ảnh là 0.23 giây trên GPU và 1.47 giây trên CPU Intel Core i7-11800H.
1	Trên tập 500 ca test ngẫu nhiên, 5 bác sĩ da liễu đạt độ chính xác trung bình 87.6% (dao động từ 84.2% đến 91.3%), độ nhạy trung bình 84.8% và độ đặc hiệu 89.2%, đều thấp hơn đáng kể so với mô hình CNN. Đặc biệt, mô hình vượt trội ở các ca khó với tổn thương nhỏ (< 6mm), biên không rõ ràng hoặc nằm ở vị trí khó quan sát như da đầu, lòng bàn tay. Tuy nhiên, trong 17 ca melanoma bị mô hình bỏ sót, các bác sĩ đã phát hiện đúng 12 ca (70.6%), cho thấy sự kết hợp giữa AI và chuyên gia y tế có thể đạt kết quả tối ưu nhất. Thời gian đọc của bác sĩ trung bình 3.8 phút/ca bao gồm xem ảnh, phân tích và ghi chép, trong khi mô hình AI chỉ mất 0.23 giây, tiết kiệm 98.9% thời gian.
1	Khi kết hợp cả hai (bác sĩ xem xét lại các ca AI đánh dấu nghi ngờ), độ chính xác đạt 97.2% với thời gian trung bình 1.2 phút/ca. Sử  dụng kỹ thuật Gradient-weighted Class Activation Mapping (Grad-CAM), chúng tôi trực quan hóa các vùng hình ảnh mà mô hình tập trung vào khi đưa ra quyết định. Kết quả cho thấy mô hình học được các đặc trưng y khoa quan trọng theo quy tắc ABCDE của melanoma: Asymmetry (độ bất đối xứng - 89.3% ca melanoma được phát hiện có điểm asymmetry cao), Border irregularity (biên giới không đều - 92.7% ca), Color variegation (màu sắc không đồng nhất với 3+ màu - 87.4% ca), Diameter (đường kính > 6mm - 76.8% ca), và Evolving (thay đổi theo thời gian - không áp dụng cho ảnh tĩnh).
1	Các lớp sâu của mạng học được các pattern phức tạp như: mạng lưới sắc tố (pigment network) bất thường xuất hiện ở 78.3% ca melanoma, các vùng màu xanh-trắng (blue-white veil) ở 45.2% ca, và cấu trúc mạch máu bất thường (polymorphous vessels) ở 62.7% ca. Điều thú vị là mô hình cũng học được cả các yếu tố gây nhiễu như dấu hiệu bong bóng khí, lông, và phản chiếu ánh sáng để loại trừ chúng khỏi phân tích. Kết quả nghiên cứu cho thấy CNN có tiềm năng lớn trong việc hỗ trợ chẩn đoán sớm ung thư da, đặc biệt tại các vùng thiếu bác sĩ chuyên khoa.
1	Với độ nhạy 96.4% trong phát hiện melanoma, hệ thống có thể được triển khai như công cụ screening tầm soát ban đầu tại phòng khám tuyến cơ sở, trung tâm y tế xã/phường hoặc thậm chí qua ứng dụng di động để người dân tự kiểm tra. Chi phí triển khai ước tính chỉ 2,500 USD cho mỗi điểm khám (bao gồm máy tính, camera dermoscopy và phần mềm), thấp hơn nhiều so với việc đào tạo và duy trì một bác sĩ da liễu (chi phí 15,000-25,000 USD/năm). Nếu triển khai rộng rãi tại 11,000 trạm y tế xã trên toàn quốc, hệ thống có thể phục vụ khoảng 45-50 triệu lượt khám/năm, tăng 180% khả năng tiếp cận dịch vụ chẩn đoán so với hiện tại.
0	Tuy nhiên, kết quả AI luôn cần được xác nhận bởi bác sĩ chuyên khoa trước khi đưa ra quyết định điều trị cuối cùng. Nghiên cứu còn một số hạn chế cần khắc phục: (1) Dữ liệu huấn luyện chủ yếu từ người da trắng (78.3% mẫu), trong khi hiệu suất trên da màu đậm thấp hơn 7.2% do đặc trưng sắc tố khác biệt - cần bổ sung thêm 5,000-8,000 ảnh da người châu Á và châu Phi; (2) Mô hình chỉ phân tích ảnh tĩnh mà chưa tích hợp thông tin lâm sàng như tuổi, giới tính, tiền sử gia đình, vị trí tổn thương - việc kết hợp các yếu tố này có thể tăng độ chính xác thêm 3-5%;
1	"(3) Chưa xử lý được video dermoscopy theo thời gian để theo dõi sự thay đổi của tổn thương - đây là tiêu chí ""Evolving"" quan trọng trong chẩn đoán melanoma. Hướng phát triển tiếp theo bao gồm: xây dựng mô hình multimodal kết hợp hình ảnh và văn bản bệnh án, áp dụng kỹ thuật federated learning để huấn luyện trên dữ liệu y tế phân tán mà vẫn bảo mật, và phát triển explainable AI để tạo báo cáo chi tiết giúp bác sĩ hiểu rõ cơ sở quyết định của mô hình. Nghiên cứu đã chứng minh hiệu quả vượt trội của CNN trong chẩn đoán ung thư da với độ chính xác 94.7%, vượt qua cả bác sĩ chuyên khoa trong nhiều trường hợp."
1	Mô hình đạt độ nhạy 96.4% trong phát hiện melanoma, có khả năng giảm 30-40% ca bệnh bị bỏ sót so với phương pháp truyền thống tại tuyến cơ sở. Với thời gian xử lý chỉ 0.23 giây/hình ảnh và chi phí triển khai thấp, công nghệ này có tiềm năng lớn để mở rộng khả năng tiếp cận dịch vụ chẩn đoán chất lượng cao đến vùng sâu, vùng xa. Tuy nhiên, AI không thay thế mà bổ trợ cho bác sĩ, đặc biệt trong các quyết định điều trị phức tạp. Việc kết hợp giữa sức mạnh phân tích của AI và kinh nghiệm lâm sàng của bác sĩ sẽ mang lại kết quả tối ưu nhất cho bệnh nhân, với độ chính xác tổng hợp 97.2% trong nghiên cứu này.
1	Tiểu đường type 2 là bệnh mãn tính ngày càng phổ biến với gánh nặng y tế và kinh tế lớn, đặc biệt tại các nước đang phát triển. Nghiên cứu này áp dụng thuật toán K-Nearest Neighbors (KNN) để dự đoán nguy cơ tiểu đường dựa trên các chỉ số sinh học và lối sống dễ thu thập tại cộng đồng. Chúng tôi thu thập dữ liệu từ 8,742 người tham gia khám sức khỏe định kỳ tại 6 quận/huyện thuộc TP.HCM trong giai đoạn 2022-2024, bao gồm 23 đặc trưng như tuổi, BMI, huyết áp, glucose lúc đói, lipid máu, hoạt động thể chất và tiền sử gia đình. Mô hình KNN với k=15 đạt độ chính xác 89.3%, độ nhạy 86.7%, độ đặc hiệu 91.2% và AUC 0.924 trên tập test.
1	Hệ thống được tích hợp vào ứng dụng di động cho phép người dân tự đánh giá nguy cơ trong vòng 30 giây, đã phát hiện 1,247 ca nguy cơ cao trong 3 tháng triển khai pilot, trong đó 423 ca (33.9%) được xác nhận tiểu đường khi xét nghiệm chuyên sâu. Tiểu đường type 2 là một trong những thách thức y tế công cộng lớn nhất thế kỷ 21 với 537 triệu người mắc bệnh toàn cầu năm 2024 theo Liên đoàn Tiểu đường Quốc tế (IDF), dự kiến tăng lên 783 triệu vào năm 2045. Tại Việt Nam, tỷ lệ mắc tiểu đường ở người trưởng thành tăng từ 2.7% năm 2002 lên 6.8% năm 2024, tương đương khoảng 5.4 triệu bệnh nhân, trong đó 45-50% chưa được chẩn đoán do thiếu triệu chứng rõ ràng ở giai đoạn sớm.
1	Phát hiện sớm tiểu đường và tiền tiểu đường có thể giúp can thiệp kịp thời thông qua thay đổi lối sống (chế độ ăn, vận động) và thuốc, giảm 58% nguy cơ tiến triển thành tiểu đường trong 3 năm theo nghiên cứu Diabetes Prevention Program. Tuy nhiên, các phương pháp screening truyền thống yêu cầu xét nghiệm máu tốn kém (150,000-300,000 VNĐ/lần) và khó tiếp cận tại vùng xa. Thuật toán KNN với khả năng học từ dữ liệu lịch sử và dự đoán nhanh chóng, có tiềm năng lớn trong việc tầm soát tiểu đường quy mô cộng đồng với chi phí thấp.
1	"Nguyên lý Thuật toán K-Nearest Neighbors K-Nearest Neighbors (KNN) là thuật toán supervised learning thuộc nhóm instance-based learning, hoạt động dựa trên nguyên lý ""những điểm dữ liệu gần nhau có xu hướng thuộc cùng một lớp"". Khi cần phân loại một điểm dữ liệu mới x, KNN tìm k điểm gần nhất trong tập huấn luyện dựa trên khoảng cách (thường là Euclidean distance), sau đó gán nhãn cho x bằng cách vote đa số từ k láng giềng đó. Khoảng cách Euclidean giữa hai điểm p=(p₁,p₂,...,pₙ) và q=(q₁,q₂,...,qₙ) được tính theo công thức: d(p,q) = √Σ(pᵢ-qᵢ)². Tham số k có ảnh hưởng lớn đến hiệu suất: k nhỏ (1-5) làm mô hình nhạy cảm với nhiễu, k lớn (> 30) có thể làm mờ ranh giới giữa các lớp."
1	KNN có ưu điểm là đơn giản, dễ hiểu, không cần huấn luyện phức tạp, hoạt động tốt với dữ liệu phi tuyến và không giả định phân phối dữ liệu. Tuy nhiên, KNN yêu cầu lưu trữ toàn bộ tập huấn luyện, tốc độ dự đoán chậm với dữ liệu lớn (O(n) complexity) và nhạy cảm với scaling của các đặc trưng. Khoảng cách và Chuẩn hóa Dữ liệu Việc đo khoảng cách chính xác là yếu tố quyết định hiệu suất của KNN. Ngoài Euclidean distance, nghiên cứu cũng thử nghiệm Manhattan distance (d = Σ|pᵢ-qᵢ|) phù hợp với dữ liệu có nhiều chiều và Minkowski distance tổng quát hóa cả hai.
1	Chuẩn hóa dữ liệu là bước quan trọng vì các đặc trưng có đơn vị và phạm vi khác nhau: glucose lúc đói (70-200 mg/dL), BMI (15-45 kg/m²), tuổi (18-80 năm). Chúng tôi áp dụng Min-Max Scaling để đưa tất cả đặc trưng về khoảng [0,1] theo công thức: x_scaled = (x - x_min)/(x_max - x_min). Kỹ thuật feature weighting cũng được sử dụng để tăng trọng số cho các đặc trưng quan trọng hơn: glucose lúc đói có trọng số 2.5, HbA1c (2.3), BMI (1.8), tuổi (1.5), trong khi các đặc trưng ít liên quan như nhóm máu chỉ có trọng số 0.3.
1	Việc tối ưu trọng số này được thực hiện bằng grid search trên tập validation, giúp tăng độ phân biệt giữa các lớp nguy cơ và cải thiện hiệu suất tổng thể của mô hình. Kết quả cho thấy việc chuẩn hóa và gán trọng số đặc trưng phù hợp giúp AUC tăng từ 0.887 lên 0.924, đồng thời giảm tỷ lệ dự đoán sai ở nhóm tiền tiểu đường – nhóm có ranh giới đặc trưng chồng lấn mạnh với nhóm bình thường. Điều này khẳng định vai trò quan trọng của tiền xử lý dữ liệu trong các mô hình KNN ứng dụng cho y tế cộng đồng, nơi dữ liệu thu thập thường không đồng nhất và có nhiễu cao.
1	Nghiên cứu sử dụng dữ liệu từ 8.742 người trưởng thành (tuổi ≥ 18) tham gia khám sức khỏe định kỳ tại 6 quận/huyện thuộc TP.HCM trong giai đoạn 2022–2024. Dữ liệu được thu thập theo chuẩn thống nhất, bao gồm thông tin nhân khẩu học, chỉ số sinh học và yếu tố lối sống. Tổng cộng 23 đặc trưng được sử dụng, trong đó có 14 đặc trưng định lượng (tuổi, BMI, huyết áp tâm thu, glucose lúc đói, HbA1c, cholesterol toàn phần, HDL, LDL, triglyceride) và 9 đặc trưng định tính được mã hóa nhị phân hoặc one-hot (giới tính, hút thuốc, mức độ vận động, tiền sử gia đình).
1	Trong tập dữ liệu, 2.316 người (26.5%) được xác định có tiểu đường hoặc tiền tiểu đường theo tiêu chuẩn ADA, phản ánh đúng phân bố dịch tễ tại cộng đồng đô thị Việt Nam. Dữ liệu bị thiếu chiếm 3.2% và được xử lý bằng phương pháp nội suy trung vị theo nhóm tuổi và giới. Mô hình KNN được triển khai với nhiều giá trị k khác nhau từ 3 đến 35 nhằm xác định tham số tối ưu. Quá trình lựa chọn k được thực hiện thông qua 5-fold cross-validation trên tập huấn luyện. Kết quả cho thấy k = 15 mang lại sự cân bằng tốt nhất giữa độ nhạy và độ đặc hiệu.
1	Khoảng cách Euclidean có trọng số được sử dụng do cho kết quả ổn định hơn so với Manhattan trong không gian đặc trưng đã chuẩn hóa. Tập dữ liệu được chia theo tỷ lệ 75% huấn luyện và 25% kiểm thử, đảm bảo phân bố nhãn đồng đều giữa hai tập. Để giảm độ phức tạp tính toán khi triển khai thực tế, cấu trúc KD-tree được sử dụng để tăng tốc quá trình tìm kiếm láng giềng gần nhất, giúp giảm thời gian dự đoán trung bình từ 1.8 giây xuống còn 0.4 giây cho mỗi người dùng.
1	Trên tập kiểm thử gồm 2.186 mẫu, mô hình KNN đạt độ chính xác (Accuracy) 89.3%, độ nhạy (Sensitivity) 86.7%, độ đặc hiệu (Specificity) 91.2% và giá trị AUC đạt 0.924. Đặc biệt, mô hình thể hiện khả năng phát hiện tốt nhóm tiền tiểu đường với tỷ lệ nhận diện đúng 83.4%, cao hơn đáng kể so với các mô hình logistic regression (74.6%) và decision tree (78.1%) được huấn luyện trên cùng tập dữ liệu. Ma trận nhầm lẫn cho thấy tỷ lệ false negative thấp (6.1%), điều này rất quan trọng trong sàng lọc y tế cộng đồng, nơi bỏ sót ca nguy cơ cao có thể dẫn đến hậu quả nghiêm trọng về sức khỏe lâu dài.
1	Mô hình KNN được so sánh với Logistic Regression, Random Forest và SVM. Random Forest đạt Accuracy cao nhất (90.1%) nhưng độ nhạy thấp hơn (82.9%) và chi phí tính toán cao gấp 3 lần khi triển khai trên thiết bị di động. Logistic Regression có ưu điểm dễ diễn giải nhưng AUC chỉ đạt 0.861. SVM cho kết quả tương đối tốt (AUC 0.901) nhưng yêu cầu tuning phức tạp và khó mở rộng. Trong khi đó, KNN đạt hiệu năng cân bằng, dễ triển khai và phù hợp với dữ liệu cập nhật liên tục từ cộng đồng, đặc biệt khi kết hợp các kỹ thuật tối ưu tìm kiếm láng giềng.
1	Mô hình KNN được tích hợp vào một ứng dụng di động hỗ trợ Android và iOS, cho phép người dùng nhập dữ liệu sức khỏe cơ bản và nhận kết quả đánh giá nguy cơ trong vòng dưới 30 giây. Trong giai đoạn pilot kéo dài 3 tháng, ứng dụng được sử dụng bởi 3.518 người dân, trong đó hệ thống phát hiện 1.247 trường hợp nguy cơ cao. Trong số này, 423 người (33.9%) được xác nhận mắc tiểu đường hoặc tiền tiểu đường thông qua xét nghiệm chuyên sâu tại cơ sở y tế. Kết quả cho thấy công cụ có giá trị thực tiễn cao trong việc sàng lọc ban đầu, giúp giảm tải cho hệ thống y tế và nâng cao nhận thức cộng đồng về bệnh tiểu đường.
1	Mặc dù đạt kết quả khả quan, nghiên cứu vẫn tồn tại một số hạn chế. Thứ nhất, dữ liệu được thu thập chủ yếu tại khu vực đô thị, có thể chưa phản ánh đầy đủ đặc điểm dân cư nông thôn. Thứ hai, KNN nhạy cảm với dữ liệu nhiễu và phân bố không cân bằng, đòi hỏi quy trình tiền xử lý chặt chẽ. Tuy nhiên, ưu điểm lớn của mô hình là tính đơn giản, dễ mở rộng và khả năng cập nhật linh hoạt khi có dữ liệu mới. Việc kết hợp KNN với các mô hình học sâu để trích xuất đặc trưng tự động là hướng nghiên cứu tiềm năng trong tương lai.
1	Nghiên cứu đã chứng minh hiệu quả của thuật toán KNN trong dự đoán nguy cơ tiểu đường type 2 tại cộng đồng với độ chính xác và độ tin cậy cao. Mô hình đề xuất không chỉ phù hợp cho nghiên cứu học thuật mà còn có khả năng triển khai thực tế trên quy mô lớn với chi phí thấp. Trong tương lai, việc mở rộng dữ liệu đa vùng và tích hợp thêm các chỉ số sinh học mới sẽ giúp nâng cao hơn nữa hiệu quả của hệ thống, góp phần hỗ trợ chiến lược phòng chống tiểu đường tại Việt Nam.
1	Trong bối cảnh an ninh kỹ thuật số ngày càng được chú trọng, các phương pháp nhận diện truyền thống như thẻ từ hay mã PIN đang dần bộc lộ nhiều kẽ hở về bảo mật và tính tiện dụng. Sự ra đời của các hệ thống sinh trắc học, đặc biệt là nhận diện khuôn mặt không tiếp xúc, đã mở ra một cuộc cách mạng trong việc quản lý nhân sự và kiểm soát ra vào tại các văn phòng hiện đại. Nghiên cứu này tập trung vào việc xây dựng một hệ thống nhận diện khuôn mặt thời gian thực sử dụng mạng nơ-ron tích chập (CNN) với kiến trúc FaceNet.
1	Mục tiêu trọng tâm là tối ưu hóa mô hình để đạt được độ chính xác tuyệt đối trong việc phân biệt các cá nhân trong cơ sở dữ liệu nhân viên, đồng thời phải đảm bảo tốc độ xử lý cực nhanh để không gây ra tình trạng ùn tắc tại các cửa kiểm soát vào giờ cao điểm. Hệ thống được thiết kế dựa trên kiến trúc Inception-ResNet v1, một dạng biến thể của CNN tối ưu cho việc học các đặc trưng tinh vi trên khuôn mặt người. Thay vì phân loại trực tiếp qua lớp Softmax, chúng tôi sử dụng lớp cuối cùng để trích xuất ra một vector đặc trưng 128 chiều (Face Embeddings).
1	Điểm cốt lõi của phương pháp này là việc áp dụng hàm mất mát Triplet Loss, giúp tối ưu hóa không gian vector sao cho khoảng cách giữa các ảnh của cùng một người là nhỏ nhất và khoảng cách giữa ảnh của những người khác nhau là lớn nhất. Tập dữ liệu huấn luyện bao gồm 10.000 hình ảnh khuôn mặt được thu thập từ nhiều góc độ, điều kiện ánh sáng và biểu cảm khác nhau. Chúng tôi áp dụng kỹ thuật MTCNN để tự động phát hiện và căn chỉnh (alignment) khuôn mặt trước khi đưa vào mạng CNN, giúp loại bỏ các nhiễu từ môi trường xung quanh và nâng cao tính ổn định cho toàn bộ quy trình trích xuất.
1	Sau quá trình huấn luyện 100 epoch với bộ tối ưu hóa Adam, hệ thống đã đạt được những con số ấn tượng trong môi trường thử nghiệm thực tế. Độ chính xác tổng thể (Accuracy) ghi nhận được là 98,2% trên tập kiểm tra độc lập. Đặc biệt, trong lĩnh vực an ninh, hai chỉ số FAR (False Acceptance Rate - Tỷ lệ chấp nhận sai) và FRR (False Rejection Rate - Tỷ lệ từ chối sai) là quan trọng nhất; kết quả cho thấy FAR chỉ ở mức 0,01% và FRR là 0,5%. Thời gian xử lý trung bình từ lúc camera bắt được hình ảnh đến khi đưa ra kết quả nhận diện là 0,15 giây, đáp ứng hoàn hảo yêu cầu thực tế.
1	Hệ thống cũng được kiểm tra trong điều kiện ánh sáng yếu (dưới 50 lux) và vẫn duy trì được độ chính xác trên 90%, chứng minh khả năng thích ứng linh hoạt của mô hình CNN đối với các biến động vật lý của môi trường lắp đặt camera. Kết quả nghiên cứu khẳng định rằng việc sử dụng Face Embeddings từ mạng CNN vượt trội hơn hẳn các phương pháp so khớp ảnh truyền thống nhờ khả năng nắm bắt các đặc điểm nhân dạng không đổi (như khoảng cách giữa hai hốc mắt, cấu trúc xương hàm).
1	Tuy nhiên, nghiên cứu cũng phát hiện ra một thách thức lớn là tình trạng giả mạo bằng ảnh chụp hoặc video (Presentation Attacks). Để giải quyết vấn đề này, việc tích hợp thêm module Liveness Detection (phát hiện thực thể sống) thông qua phân tích cử động mắt hoặc sử dụng camera cảm biến chiều sâu (Depth camera) là hướng đi bắt buộc để hệ thống đạt được độ bảo mật cấp độ doanh nghiệp. Ngoài ra, việc triển khai mô hình lên các thiết bị Edge AI như Jetson Nano đã cho thấy tiềm năng giảm tải cho máy chủ trung tâm, giúp hệ thống có khả năng mở rộng lên tới hàng trăm camera giám sát mà không làm tăng độ trễ xử lý.
1	"Nghiên cứu đã chứng minh tính khả thi và độ tin cậy cao của việc ứng dụng Deep Learning trong kiểm soát an ninh ra vào. Hệ thống không chỉ mang lại trải nghiệm tiện lợi không chạm"" cho người dùng mà còn cung cấp một công cụ quản lý nhân sự minh bạch, chính xác cho các tổ chức. Trong tương lai, chúng tôi hướng tới việc tích hợp hệ thống nhận diện khuôn mặt với các mô hình quản lý chấm công tự động và hệ thống quản trị tòa nhà thông minh (BMS). Bên cạnh đó, việc áp dụng các kỹ thuật mã hóa vector đặc trưng để bảo vệ quyền riêng tư của dữ liệu khuôn mặt cũng sẽ được nghiên cứu sâu hơn, nhằm đảm bảo tuân thủ các quy định quốc tế về bảo mật dữ liệu cá nhân."""
0	Vấn đề lưu trữ thông tin số trong các thư viện sách hiện nay rất được quan tâm. Do số lượng đầu sách nhiều nên khi cần tra cứu cùng lúc sẽ gặp khó khăn. Do vậy, nếu có một chương trình tự động nhận dạng thông tin sách sẽ giúp người quản lý rất nhiều trong việc sắp xếp, quản lý sách thích hợp, cũng như giúp người đọc có thể tìm đến quyển sách một cách nhanh chóng. Nghiên cứu này đã đề xuất một phương pháp để trích xuất văn bản tự động từ bìa màu dựa trên các thuật toán tiền xử lý và thuật toán CNN. Kết quả cho thấy phương pháp đề xuất có thể phát hiện chính xác 97% văn bản đối với ảnh bìa có nền phức tạp hoặc màu kí tự gần trùng với màu nền.
0	Phương pháp trên có tiềm năng cao trong việc ứng dụng lưu trữ thông tin sách tự động ở các thư viện sách hiện nay. Ngày nay, sự tăng trưởng nhanh chóng về số lượng tài liệu trong các thư viện sách làm cho việc tìm kiếm ngày càng khó khăn hơn. Sự phức tạp của bìa tài liệu như màu chữ gần trùng với màu nền, các họa tiết màu xen lẫn hoặc ảnh nền phức tạp làm cho việc tự động truy xuất thông tin liên quan từ các bìa tài liệu như tên tài liệu, tác giả, số chỉ mục, v.v. là một nhiệm vụ đầy thách thức. Ngoài ra, các yếu tố văn bản cũng gây thêm khó khăn không chỉ về màu sắc, phông chữ và kích thước, mà còn ở sự liên kết và định hướng.
0	Vì thế việc hướng tới phát triển một ứng dụng tự động nhận dạng tiêu đề sách để xây dựng kho dữ liệu thông tin số về nguồn tài liệu ở các thư viện sách được nhanh chóng và tiện lợi là một việc rất cần thiết. Có rất nhiều phương pháp nhận dạng bìa sách đã được công bố. Các phương pháp nhị phân ảnh khác nhau đã được áp dụng để có được hình ảnh nhị phân đưa trực tiếp vào bộ nhận dạng ký tự quang học (OCR) có sẵn [1-3]. Tuy nhiên, do sự khác biệt nhiều về độ phân giải, điều kiện chiếu sáng, kích thước và kiểu phông chữ giữa văn bản trong hình ảnh tự nhiên và văn bản trong tài liệu được scan thông thường nên kết quả nhị phân ảnh thường khó nhận dạng được ký tự.
0	Ngoài ra, việc mất thông tin trong quá trình nhị phân ảnh là gần như không thể phục hồi. Do đó, nếu kết quả nhị phân ảnh quá kém thì khả năng phát hiện chính xác văn bản là rất nhỏ. Bên cạnh đó, thuật toán SIFT được đề xuất để trích xuất các đặc trưng quan trọng từ ảnh chụp biển số xe hay thuật toán Maximally Stable Extremal Region (MSER) được sử dụng với ảnh chụp cảnh tự nhiên [4, 5]. Hai giải thuật này đều sử dụng ảnh đầu vào chụp từ điện thoại và đạt được độ chính xác cao với ảnh có nền đơn hoặc màu kí tự khác biệt với màu nền. Tuy nhiên với ảnh có nền phức tạp hoặc có màu kí tự gần trùng với màu nền thì kết quả bị giảm độ chính xác rất nhiều.
0	Một giải thuật nhận dạng kí tự trong hình ảnh cảnh tự nhiên như ảnh bìa sách, biển báo đường, biển quảng cáo và hộp đóng gói đã được đề xuất [6]. Với giải thuật này, các kí tự được trích xuất tốt bằng phương pháp trích xuất và chọn thành phần được kết nối. Tuy nhiên, còn tồn tại một số vùng mà trong đó văn bản và nền có màu tương tự hoặc văn bản nằm trong một nền phức tạp khó chuyển đổi ảnh hưởng đến hiệu quả của giải thuật. Ngoài ra, một hệ thống sử dụng Modest AdaBoost không đối xứng để phát hiện văn bản trong cảnh tự nhiên cũng được đề xuất [7].
0	Trong đó, 59 đặc trưng trên 16 thang không gian đã được trích xuất để tạo CART như một bộ phân loại yếu của Modest AdaBoost, nhờ đó nâng cao kết quả nhận dạng văn bản. Bên cạnh đó, một phương pháp phân tách cảnh tự nhiên thành các thành phần nền và văn bản bằng cách sử dụng phân tích thành phần hình thái (MCA) được đề xuất [9]. Điều này sẽ làm giảm các tác động bất lợi của nền phức tạp lên kết quả phát hiện. Ngoài ra, một mạng thần kinh đệ quy với mô hình chú ý (R2AM) được đề xuất để nhận dạng văn bản trong cảnh tự nhiên [10]. Tuy nhiên, những giải thuật này đều đòi hỏi một quá trình huấn luyện với lượng lớn bộ nhớ [8-10].
0	Trong bài báo này, các thuật toán tiền xử lý ảnh được đề xuất để làm tăng độ chính xác khi tách các phần ảnh chứa kí tự, loại bỏ ảnh hưởng của màu nền bìa sách phức tạp hoặc màu nền bìa trùng với màu kí tự. Sau đó kết hợp thuật toán CNN phân loại và nhận dạng các kí tự quang học để trích xuất thông tin cần thiết trên ảnh chụp bìa sách. Ngoài ra, giải thuật này cũng sẽ không bị hạn chế nhiều về tốc độ xử lý, hay đòi hỏi bộ nhớ lớn khi huấn luyện.
0	Lưu đồ giải thuật nhận dạng bìa sách được thể hiện trong Hình 1. Đầu tiên, bìa sách được scan hoặc chụp lại từ camera để làm ảnh đầu vào. Tiếp theo, ảnh bìa sách được hiệu chỉnh độ nghiêng để đảm bảo các hàng chữ không bị lệch so với lề chuẩn một góc bất kì. Sau đó, giải thuật tiền xử lý ảnh và MSER được áp dụng để phân đoạn vùng chứa thông tin sách như tựa sách, tên tác giả… Cuối cùng, vùng kí tự này được tách thành những kí tự riêng lẻ và đem đi phân loại với thuật toán CNN để thu được thông tin bìa sách.
0	Giải thuật đề xuất được thực hiện trên Python 3.7.4. Ảnh bìa sách đầu vào là ảnh được tải lên từ cơ sở dữ liệu trong máy, hoặc là ảnh được chụp trực tiếp từ camera trong môi trường tự nhiên. Trong cả hai trường hợp, ảnh đầu vào đều là ảnh màu RGB và được resize lại với chiều cao là 1000 pixel để tăng tốc độ xử lý. Do bìa sách khi scan không cẩn thận hoặc do góc nghiêng khi chụp ảnh, các hàng chữ có thể bị lệch so với lề chuẩn một góc bất kỳ. Điều này gây khó khăn trong việc tách chữ và nhận dạng chữ, đôi khi không thể tách được hoặc không nhận dạng được.
0	Vì vậy, bước đầu tiên sau khi chọn ảnh đầu vào là hiệu chỉnh góc nghiêng. Đối với bìa sách, chữ in trên đó thường có phương thẳng đứng, tức là cùng phương với gáy sách. Do đó, để hiệu chỉnh góc nghiêng của chữ, chúng tôi đề xuất hiệu chỉnh góc nghiêng của đoạn thẳng dài nhất được tìm thấy trong ảnh, thường là gáy sách hoặc một phần cạnh gáy sách, cạnh của chữ hoặc cạnh của họa tiết trên bìa sách. Giải thuật được đề xuất ở Hình 2 có thể giải quyết vấn đề về hiệu chỉnh sai lệch góc nghiêng với độ chính xác 90%.
0	Thuật toán MSER trong [5] được đề xuất để phân đoạn vùng chứa văn bản trên ảnh bìa sách đã hiệu chỉnh góc nghiêng. Về cơ bản, MSER sẽ trích xuất các vùng có màu sắc nhất quán và độ tương phản cao. Tuy nhiên, thuật toán này chỉ chính xác khi màu kí tự khác biệt với màu nền hoặc màu họa tiết gần quanh kí tự. Do đó, để cải thiện kết quả, chúng tôi đề xuất sử dụng các thuật toán tiền xử lý ảnh để loại bỏ ảnh hưởng của màu sắc trên ảnh, với mục tiêu hướng tới là biến ảnh bìa sách thành ảnh trắng đen gồm các vùng trắng là vùng chứa chữ hoặc họa tiết kín, còn vùng đen là vùng chứa nền và các họa tiết hở. Quá trình phân đoạn ảnh được mô tả trong lưu đồ giải thuật ở Hình 3.
0	Trong thực tế, ngoài tựa sách và tên tác giả, bìa sách còn chứa thông tin của nhà xuất bản, số tái bản, các giải thưởng và nhận xét. v.v. Những thông tin này không cần thiết đối với mục tiêu trích xuất thông tin bìa sách để lập mục lục sách. Do đó, một bộ lọc dựa trên vị trí và kích thước của kí tự được chúng tôi đề xuất để loại bỏ các vùng chứa thông tin không cần thiết trên. Để tăng cường độ chính xác khi nhận dạng kí tự, một thuật toán tách các kí tự nằm trên cùng một dòng trong khung giới hạn ở bước trên thành các kí tự riêng lẻ được đề xuất [8].
0	Sau đó, mỗi kí tự này được đặt ở trung tâm của khung vuông giới hạn nhỏ nhất, được chuyển thành ảnh xám và thay đổi kích thước về 28x28 pixel. Ảnh này được sử dụng làm đầu vào cho bộ phân loại CNN đã được huấn luyện nhận dạng để tìm ra kết quả nhận dạng kí tự cuối cùng. Bước cuối cùng là nhận dạng kí tự. Trong bài báo này, thuật toán CNN được đề xuất sử dụng vì nó là một trong những mô hình Deep Learning tiên tiến giúp cho chúng ta xây dựng được những hệ thống thông minh với độ chính xác cao như hiện nay.
0	Mạng neural tích chập (CNN) điển hình bao gồm một hoặc nhiều khối lớp lấy mẫu (sampling layer), kèm theo sau đó là một hoặc nhiều lớp kết nối đầy đủ (fully connected layer) và một lớp đầu ra (output layer) như trong Hình 4. Lớp tích chập (convolutional layer) là phần trung tâm của mạng CNN. Đối với ảnh tĩnh, sự tổ chức ở một phần bất kì của ảnh cũng giống như ở bất kì phần nào còn lại trên ảnh. Do đó, đặc trưng học được ở một khu vực ảnh có thể phù hợp với đặc trưng ở những khu vực còn lại.
0	Trong một ảnh lớn, chúng tôi lấy một phần nhỏ và dịch chuyển nó qua tất cả các điểm trong ảnh lớn (đầu vào). Khi vượt qua bất kỳ một điểm nào, việc kết hợp chúng vào một vị trí duy nhất (đầu ra) đã được thực hiện. Mỗi phần nhỏ của hình ảnh đi qua hình ảnh lớn được gọi là bộ lọc (kernel). Các bộ lọc sau đó được cấu hình dựa trên kỹ thuật lan truyền ngược. Lớp tổng hợp (pooling layer) làm giảm số lượng tham số được tính. Có nhiều kỹ thuật gộp khác nhau như gộp tối đa (max pooling), gộp chung (mean pooling), gộp trung bình (average pooling), v.v. Gộp tối đa (max pooling) chiếm giá trị pixel lớn nhất của một vùng như trong Hình 6.
0	Phần cuối của mạng CNN về cơ bản là các lớp kết nối đầy đủ (fully connected layer) như mô tả trong Hình 7. Lớp này lấy đầu vào từ tất cả các neural ở lớp trước và thực hiện hoạt động với từng neural riêng lẻ trong lớp hiện tại để tạo đầu ra. Mô hình mạng CNN được chúng tôi đề xuất như trong Hình 8, bao gồm hai lớp tích chập (convolutional layer), hai lớp tổng hợp (pooling layer), và hai lớp kết nối đầy đủ (fully connected layer). Các lớp lớp tích chập (convolutional layer) có 32 bộ lọc (kernel) với kích thước 3x3, sử dụng hàm ReLU để kích hoạt các trọng số trong các node.
0	Các lớp tổng hợp (pooling layer) có kích thước cửa sổ là 2x2. Lớp flatten chuyển từ tensor sang vector, cho phép đầu ra được xử lý bởi các lớp kết nối đầy đủ (fully connected layer) tiêu chuẩn. Sau đó, mạng có một lớp kết nối đầy đủ (fully connected layer) với 128 node và dùng hàm ReLU để kích hoạt. Cuối cùng là lớp đầu ra (output layer) với 62 node sử dụng hàm softmax để chuyển sang xác suất. Trong Bảng 1, một tập dữ liệu bao gồm 100 ảnh bìa sách với các góc quay khác nhau (từ -450 đến +450) được tạo ra. Với thuật toán được đề xuất, kết quả hiệu chỉnh góc nghiêng chính xác là 97/100 (97%) hình ảnh.
0	Lý do chính của các trường hợp không thành công là do màu nền hoặc chất lượng ảnh và độ tương phản của ảnh kém. Do đó, thuật toán không thể phát hiện chính xác đường dài nhất, thường là mép bìa sách. So sánh với các nghiên cứu trước đó, phương pháp được đề xuất có thể phân đoạn vùng chứa văn bản chính xác hơn [4], [5] và [6] trong trường hợp nền phức tạp hoặc màu nền gần trùng với màu văn bản. Kết quả phân đoạn vùng chứa thông tin sách trong trường hợp nền đơn sắc, nền phức tạp và màu chữ gần trùng với màu nền lần lượt được thể hiện trong Hình 9, Hình 10 và Hình 11.
0	Ở Bảng 2, một tập dữ liệu 100 ảnh scan bìa sách bao gồm 50 bìa có nền đơn sắc, 50 bìa có nền phức tạp và bìa có màu nền gần trùng với màu văn bản để tính toán độ chính xác của thuật toán đã được tạo ra. Kết quả được chia thành 5 mức độ:  Mức 1: các vùng chứa tựa sách và tên tác giả được phát hiện. Mức 2: vùng chứa tựa sách được phát hiện. Mức 3: vùng chứa tên tác giả được phát hiện. Mức 4: không thể phát hiện vùng chứa tựa sách và tên tác giả. Các trường hợp không thành công rơi vào các ảnh scan bìa sách có nền phức tạp, trong đó vùng nền xung quanh vùng văn bản chứa các họa tiết phức tạp xen lẫn vào chữ như Hình 12.
0	Do không có cơ sở dữ liệu chữ đánh máy có sẵn, chúng tôi thực hiện thu thập một tập hợp các kí tự gồm 44640 mẫu từ 720 font thuộc 5 nhóm font chữ thường gặp và được sử dụng nhiều trong in ấn, đó là nhóm Geometric Sans, nhóm Humanist San, nhóm Old Style, nhóm Transitional Modern và nhóm Slab Serifs. Mỗi bộ kí tự của một font chữ có 62 mẫu, bao gồm 10 chữ số, 26 chữ viết hoa và 26 chữ viết thường như trong Hình 13. Để kiểm tra mô hình đề xuất, cơ sở dữ liệu kí tự được chia thành hai phần. 37200 mẫu được sử dụng cho giai đoạn huấn luyện và 7440 mẫu được sử dụng cho giai đoạn thử nghiệm.
0	Quá trình huấn luyện mô hình CNN đề xuất như trong Bảng 3 tốn khoảng 4 tiếng trên máy tính xách tay cấu hình CPU i7-4600U, Ram 8GB, ko có GPU. Kết quả huấn luyện đạt độ chính xác 97.69%. Bài báo này đề xuất một phương pháp phân loại nhận dạng kí tự dựa trên cơ sở kết hợp thuật toán tiền xử lý ảnh và thuật toán CNN trong trường hợp nền phức tạp hoặc màu kí tự gần trùng với màu nền. Tính khả thi của phương pháp đề xuất được xác minh bằng kết quả mà có thể phát hiện chính xác 97% văn bản của những ảnh bìa có nền phức tạp.
0	Trong bài báo này, một mô hình tìm kiếm ảnh sử dụng cấu trúc RS-Tree và mạng học sâu Faster R-CNN được đề xuất nhằm nâng cao hiệu suất truy vấn ảnh. Trong mô hình này, các công việc sau được thực hiện: (1) cấu trúc RS-Tree được cải tiến thuật toán tách nút để nâng cao hiệu quả gom cụm các véc-tơ đặc trưng của tập ảnh đa đối tượng; (2) mạng học sâu Faster R-CNN được sử dụng để phát hiện và phân loại các đối tượng trên hình ảnh; (3) các hộp giới hạn chứa đối tượng trên ảnh được trích xuất đặc trưng cấp thấp và lưu trữ trên cấu trúc RS-Tree;
0	Với mỗi ảnh đầu vào, hệ thống phát hiện và phân loại từng đối tượng bằng mạng học sâu Faster R-CNN; trích xuất véc-tơ đặc trưng cấp thấp; thực hiện truy vấn ảnh tương tự dựa trên cấu trúc RS-Tree. Thực nghiệm được thực hiện trên bộ ảnh đa đối tượng MS-COCO gồm 5000 ảnh với độ chính xác là 77.39%. Kết quả thực nghiệm được so sánh với các công trình khác trên cùng bộ ảnh nhằm đánh giá tính đúng đắn của mô hình đề xuất. Tra cứu ảnh tương tự và phân lớp ngữ nghĩa hình ảnh là một trong những bài toán quan trọng và phù hợp với xu thế của xã hội hiện đại (Chou et al., 2016; Liu et al., 2015).
0	Vì vậy, các hệ thống tìm kiếm ảnh tương tự được các nhà nghiên cứu quan tâm trong nhiều thập niên gần đây và có nhiều phương pháp khác nhau được đề xuất nhằm nâng hiệu quả tìm kiếm. Mô tả nội dung thị giác của hình ảnh và lập chỉ mục cho nội dung thị giác là hai vấn đề cần thiết khi thực hiện bài toán truy vấn ảnh theo nội dung (Begum & Supreethi, 2018; Sivakumar et al., 2021). Hiện nay, có nhiều phương pháp lập chỉ mục cho dữ liệu đa chiều như KD-Tree, QuarTree, M-Tree, R-Tree...
0	Trong đó, R-Tree là một trong những cấu trúc được sử dụng phổ biến để lưu trữ chỉ mục dựa trên phân vùng dữ liệu (Manolopoulos et al., 2006). Trong những thập niên gần đây, các phương pháp học máy được áp dụng rộng rãi cho bài toán tìm kiếm ảnh nhằm nâng cao chất lượng truy vấn. Dữ liệu đa phương tiện ngày càng gia tăng nhanh theo thời gian là thách thức cho việc lưu trữ và tìm kiếm hiệu quả. Do đó, việc kết hợp các phương pháp khác nhau cho bài toán truy vấn ảnh cần được thực hiện nhằm nâng cao hiệu suất, giảm thời gian tìm kiếm cũng như tối ưu hóa không gian lưu trữ là cần thiết (Zhou et al., 2022).
0	Phát hiện đối tượng là một chủ đề được nhiều nhà khoa học quan tâm trong lĩnh vực thị giác máy tính. Mục đích chính của phát hiện đối tượng là tìm đối tượng quan tâm trong hình ảnh hoặc video, phát hiện vị trí và kích thước của chúng. Trong những năm gần đây, phát hiện đối tượng đã được sử dụng rộng rãi trong trí tuệ nhân tạo, nhận dạng khuôn mặt, lái xe không người lái và các lĩnh vực khác. Các thuật toán phát hiện đối tượng hiện có bao gồm thuật toán phát hiện truyền thống và thuật toán phát hiện dựa trên học sâu.
0	Các thuật toán phát hiện đối tượng truyền thống chủ yếu dựa trên khung cửa sổ trượt hoặc đối sánh dựa trên các điểm đặc trưng. Với sự phát triển của công nghệ học sâu, các thuật toán phát hiện đối tượng đã chuyển từ phương pháp truyền thống dựa trên các đặc trưng được lựa chọn thủ công sang phương pháp phát hiện dựa trên mạng nơ-ron sâu. Các phương pháp phát hiện dựa trên mạng nơ-ron sâu có thể chủ yếu được chia thành hai loại: (1) thuật toán phát hiện đối tượng hai giai đoạn kết hợp mạng đề xuất vùng RPN (Region Proposal Network) và mạng nơ-ron tích chập (CNN), chẳng hạn như R-CNN; (2) thuật toán phát hiện đối tượng một giai đoạn chuyển đổi phát hiện đối tượng thành một bài toán hồi quy (ví dụ: YOLO).
0	Đối với ảnh đa đối tượng, việc phát hiện đối tượng và phân lớp đối tượng trên ảnh là cần thiết để áp dụng cho các bài toán tìm kiếm ảnh nhằm nâng cao độ chính xác. Nhiều phương pháp học hiện đại được sử dụng để phát hiện và phân lớp ảnh đa đối tượng bao gồm: Mạng học sâu Fast R-CNN (Girshick, 2015), Faster R-CNN (Amitha & Narayanan, 2021; Ren et al., 2015) Yolo (Pestana et al., 2021) (Singh et al., 2021)... Trong bài báo này, cấu trúc chỉ mục Improved-RST, một cải tiến của cấu trúc RS-Tree (Le et al., 2022) được đề xuất.
0	Trong cấu trúc Improved-RST, các véc-tơ đặc trưng hình ảnh được biểu diễn dưới dạng các khối cầu và được lưu trữ tại các nút lá của cây tương tự như RS-Tree. Nhằm nâng cao hiệu quả lưu trữ và truy vấn trên cấu trúc Improved-RST, một phương pháp thêm phần tử vào cây được đề xuất để cải thiện thời gian tạo cây và giúp cân bằng dữ liệu tại các nút lá. Việc cải tiến bao gồm các nội dung sau: (1) Khi một nút đầy, một nút tràn cho nút đó được tạo ra và tất cả các nút tràn được lưu trong một bảng băm; (2) Nếu nút đó tiếp tục được thêm dữ liệu, dữ liệu sẽ được thêm vào nút tràn của nó; (3) Khi một nút tràn bị đầy, quá trình tách nút được thực hiện.
0	Trên cơ sở đó, một mô hình truy vấn ảnh sử dụng cấu trúc Improved-RST và mạng học sâu Faster R-CNN được đề xuất để nâng cao hiệu quả tìm kiếm ảnh. Thực nghiệm truy vấn ảnh tương tự được thực hiện trên bộ ảnh MS-COCO gồm 5000 ảnh. Gần đây, mạng nơ-ron tích chập (Convolution Neural Network - CNN) đã được chứng minh là đạt được hiệu suất cao trong nhiều nhiệm vụ thị giác máy tính như phân loại hình ảnh (Simonyan & Zisserman, 2014), phát hiện đối tượng (Ren et al., 2015) hoặc phân đoạn ngữ nghĩa (Long et al., 2015). Các mạng CNN được huấn luyện với lượng lớn dữ liệu đã được chứng minh là học được các biểu diễn đặc trưng tổng quát để sử dụng khi giải quyết các nhiệm vụ mà chúng chưa được huấn luyện (Wang et al., 2019).
0	Đặc biệt đối với việc truy xuất hình ảnh, nhiều công trình đã áp dụng các giải pháp dựa trên các đặc trưng vượt trội được trích xuất từ một CNN được huấn luyện trước cho nhiệm vụ phân loại hình ảnh (Babenko & Lempitsky, 2015; Tolias et al., 2015). Wenze Li (2021) và (Li, 2021) đã thực hiện phân tích hiệu suất của các mô hình Faster R-CNN dựa trên các mô hình tiền huấn luyện khác nhau và tiến hành đánh giá toàn diện về hiệu suất của Faster R-CNN. Kết quả thử nghiệm cho thấy độ chính xác và tốc độ phát hiện của R-CNN, Fast R-CNN và Faster R-CNN nhanh hơn dựa trên ba tập dữ liệu khác nhau.
0	Thuật toán Faster R-CNN được thực hiện dựa trên Pytorch, VGG16 và ResNet101 được sử dụng làm mô hình tiền huấn luyện để huấn luyện và ghi lại thời gian trên hai tập dữ liệu Pascal VOC và COCO tương ứng. Hiệu suất của Faster R-CNN được phân tích theo các mô hình tiền huấn luyện và các tập dữ liệu khác nhau. Kết quả thử nghiệm chứng minh rằng độ chính xác và tốc độ phát hiện của Faster R-CNN được cải thiện rất nhiều so với R-CNN và Fast RCNN vì nó sử dụng RPN để thay thế bộ tìm kiếm chọn lọc.
0	Wenmei Wang và cộng sự (2019) (Wang et al., 2019) đã đề xuất một phương pháp dựa trên học sâu để truy xuất ảnh nhãn hiệu. Faster R-CNN được áp dụng trong việc truy xuất hình ảnh nhãn hiệu để trích xuất các đặc điểm ngữ nghĩa cấp cao của hình ảnh nhãn hiệu. Bộ mô tả đặc điểm toàn cục của hình ảnh được Faster R-CNN trích xuất và đặc điểm cục bộ của hình ảnh được trích xuất thông qua các vùng đề xuất đối tượng bởi mạng đề xuất khu vực (RPN). Để có được hiệu quả truy xuất tốt hơn, nhóm tác giả áp dụng kết hợp chiến lược truy xuất xếp hạng và sắp xếp lại theo không gian.
0	Amaia Salvador và cộng sự (2016) (Salvador et al., 2016) đã đề xuất sử dụng CNN tiền huấn luyện phát hiện đối tượng để trích xuất các đặc trưng toàn cục và cục bộ của hình ảnh. Nhóm tác giả sử dụng phương pháp xếp hạng lại và vận dụng các vị trí mà mạng đề xuất khu vực (RPN) học được để cung cấp định vị đối tượng cho các hình ảnh được truy xuất. Bên cạnh đó, nhiều công trình đã áp dụng các kĩ thuật lập chỉ mục hình ảnh nhằm nâng cao tốc độ tìm kiếm ảnh. Haldurai và cộng sự (2015) đã đề xuất một hệ truy vấn ảnh tương tự theo nội dung sử dụng cấu trúc cây R-Tree (Haldurai & Vinodhini, 2015).
0	Vanitha và cộng sự (2017) đã đề xuất một cấu trúc chỉ mục SR-Tree ứng dụng cho hệ thống tìm kiếm ảnh tương tự theo nội dung. Hệ thống thực hiện trích xuất đặc trưng màu sắc, đặc trưng không gian và lưu trữ véc-tơ đặc trưng trên cây SR-Tree (Vanitha & SenthilMurugan, 2017). Shama và cộng sự (2015) đã đề xuất một hệ thống truy vấn ảnh tương tự sử dụng cấu trúc R*-Tree cho bộ ảnh thực vật. Nhóm tác giả sử dụng phương pháp ma trận đồng xuất hiện và phép lọc Gabour để trích xuất đặc trưng ảnh (Shama et al., 2015). Alfarrarjeh và cộng sự (2020) đã đề xuất một lớp chỉ mục R*-Tree ứng dụng cho bài toán tìm kiếm ảnh tương tự với dữ liệu ảnh đường phố (Alfarrarjeh et al., 2020).
0	Từ việc phân tích các nghiên cứu liên quan ở trên cho thấy mô hình truy xuất ảnh dựa trên cấu trúc cây được đánh giá là đáng tin cậy và hiệu quả. Bên cạnh đó, những cách tiếp cận gần đây đã tập trung vào các phương pháp học máy cho bài toán truy vấn ảnh nhằm nâng cao độ chính xác. Kết quả của những công trình đó cho thấy việc áp dụng mạng học sâu Faster R-CNN cho bài toán truy xuất hình ảnh là khả thi. Trên cơ sở kế thừa và khắc phục những hạn chế của các công trình liên quan, một mô hình truy xuất hình ảnh sử dụng cấu trúc cây kết hợp mạng học sâu Faster R-CNN được giới thiệu để cải thiện hiệu suất truy xuất hình ảnh.
0	Thử nghiệm được thực hiện xây dựng trên bộ dữ liệu ảnh MS-COCO (bao gồm 5000 hình ảnh, 80 lớp), để chứng minh hiệu quả truy vấn ảnh của phương pháp đề xuất. Một ảnh cần truy vấn được phát hiện đối tượng và trích xuất véc-tơ đặc trưng bằng mạng học sâu Faster R-CNN và thực hiện truy vấn trên cấu trúc cây Improved-RST. Quá trình truy vấn trên cây cho đến khi gặp được nút lá phù hợp thì tập hợp tất cả các phần tử dữ liệu trong nút lá đó được gọi là một tập ảnh tương tự của ảnh truy vấn. Sau đó, tập ảnh này được sắp xếp theo độ đo tương tự để tìm ra các ảnh tương tự gần nhất.
0	Mô hình truy vấn ảnh tương tự theo nội dung với một ảnh truy vấn đầu vào cho trước dựa trên cây Improved-RST được minh họa như Hình 1. Quá trình tìm kiếm ảnh được thực hiện gồm hai pha, pha thứ nhất thực hiện gom cụm và lưu trữ dữ liệu hình ảnh trên cây Improved-RST, pha thứ hai thực hiện tìm kiếm các hình ảnh tương tự và phân lớp ngữ nghĩa cho ảnh đầu vào. Quá trình thực hiện được mô tả như sau: Xây dựng cây phân cụm. Quá trình xây dựng cây gom cụm dữ liệu không gian Improved-RST dựa trên véc-tơ đặc trưng của tập dữ liệu ảnh gồm 3 bước như sau:
0	Bước 1. Trích xuất véc-tơ đặc trưng fi của tập dữ liệu ảnh sử dụng mạng Faster R-CNN. Bước 2. Biểu diễn véc-tơ đặc trưng của tập dữ liệu ảnh thành khối cầu không gian. Bước 3. Dựa trên độ đo tương tự đề xuất và phương pháp phân cụm K-mean, cây gom cụm chỉ mục không gian Improved-RST được tạo ra với mỗi nút lá của cây là tập các thực thể khối cầu chứa các véc-tơ fi mô tả đặc trưng thị giác của hình ảnh. Tìm kiếm ảnh. Việc tìm kiếm ảnh tương tự được thực hiện với đầu vào là một hình ảnh truy vấn và đầu ra là tập ảnh tương tự và phân lớp ngữ nghĩa hình ảnh dựa trên cây gom cụm chỉ mục Improved-RST.
0	Quá trình tìm kiếm ảnh tương tự được thực hiện theo các bước như sau: Bước 1. Trích xuất véc-tơ đặc trưng của ảnh cần truy vấn dựa trên mạng Faster R-CNN và chuyển đổi thành dạng khối cầu không gian. Bước 2. Thực hiện truy vấn ảnh tương tự dựa trên cấu trúc không gian Improved-RST. Bước 3. Tra cứu tập ảnh tương tự dựa trên tập chỉ mục đã được truy vấn. Faster R-CNN là phiên bản hiện đại được sử dụng rộng rãi nhất của họ R-CNN. Các nhiệm vụ của mạng này bao gồm: (1) Thuật toán đề xuất vùng để tạo ra các “hộp giới hạn” hoặc vị trí của các đối tượng có thể có trong hình ảnh; (2) Giai đoạn tạo đặc trưng để có được các đặc trưng của các đối tượng này, thường sử dụng CNN;
0	(3) Lớp phân loại để dự đoán đối tượng này thuộc lớp nào; (4) Lớp hồi quy để làm cho tọa độ của hộp giới hạn đối tượng chính xác hơn. Fast R-CNN đã sử dụng thuật toán tìm kiếm chọn lọc dựa trên CPU cho đề xuất vùng, mất khoảng 2 giây cho mỗi hình ảnh và chạy trên tính toán của CPU. Faster R-CNN khắc phục điều này bằng cách sử dụng mạng đề xuất khu vực (RPN) để tạo các đề xuất khu vực. Điều này làm giảm thời gian đề xuất khu vực từ 2 giây xuống 10 mili giây cho mỗi hình ảnh. Kiến trúc của Faster R-CNN được thể hiện trong Hình 2.
0	Trong bài báo này, mạng Faster R-CNN như trong công trình (Ren et al., 2015) được sử dụng để phát hiện và phân lớp đối tượng trong hình ảnh. Kết quả của quá trình này là một bộ các véc-tơ đặc trưng của các đối tượng trên hình ảnh và các phân lớp của chúng. Tập các thông tin này được sử dụng để gom cụm trên cấu trúc Improved-RST. Một ví dụ của việc phân lớp ảnh đầu vào dựa trên mạng Faster R-CNN được minh họa như Hình 3. Mỗi đối tượng sau khi được trích xuất bằng mạng Faster R-CNN, các đặc trưng cấp thấp được trích xuất riêng biệt dựa trên khối chữ nhật chứa đối tượng đó. Các đặc trưng này được đưa vào lưu trữ trên cấu trúc RS-Tree cải tiến.
0	Việc tách nút thường xuyên dẫn đến làm tăng chiều cao của cây và gia tăng vùng chồng lắp không gian, ảnh hưởng đến hiệu quả truy vấn. Để cải thiện hai vấn đề này, bài báo đề xuất chiến lược thuật toán tách nút có độ trễ để tối ưu hóa quy trình tạo cây RS-Tree. Thuật toán được chia thành hai phần: thêm dữ liệu và tách nút. Trong quá trình thêm, việc phân tách có độ trễ được áp dụng. Trong quá trình phân tách nút, một thuật toán tách tốt hơn được áp dụng để thực hiện phân tách có độ trễ. Thuật toán hoàn chỉnh làm giảm số lần phân tách xảy ra trong quá trình xây dựng cây RS-Tree và tăng sự phân chia không gian của cây RS-Tree, do đó cải thiện hiệu quả tìm kiếm.
0	Trong quá trình thêm dữ liệu vào cây, để chọn một nút lá phù hợp để thêm vào, tiêu chuẩn là giảm thiểu diện tích của các khối cầu của các nút khi dữ liệu được thêm vào. Khi một nút đã đầy, tức là số lượng dữ liệu mà nút chứa đã đạt đến M, thuật toán tách nút không thực hiện ngay mà thay vào đó sẽ thêm dữ liệu vào một nút tràn. Khi quá trình thêm kết thúc, nút tràn được xem xét đã đầy hay không; nếu đầy quá trình hợp nhất và tách nút được thực hiện: nút hiện hành và nút tràn của nó được hợp nhất dữ liệu gồm có 2M phần tử.
0	Sau đó, thuật toán tách nút được thực hiện tương tự như trong thuật toán tách nút của cây RS-Tree. Để thực hiện thao tác thêm, một bảng băm được sử dụng, kí hiệu HM, chứa các nút tràn để ghi lại các dữ liệu. Khi thêm dữ liệu vào nút chứa dữ liệu, thuật toán đầu tiên xác định xem nút có đầy hay không, nếu không, nó sẽ thêm dữ liệu trực tiếp và cập nhật khối cầu của nút đó. Nếu nút đầy, thuật toán sẽ kiểm tra xem nút đó có nút tràn trong bảng băm hay không. Nếu không có nút tràn nào tồn tại trong bảng băm, một nút tràn sẽ được tạo cho nút đó và dữ liệu cần thêm sẽ được thêm vào nút tràn.
0	Sau đó, nó cập nhật khối cầu của nút hiện hành và khối cầu của nút tràn tương ứng. Khi nút đã có nút tràn, thuật toán sẽ thêm dữ liệu trực tiếp vào nút tràn, cập nhật khối cầu của nút và khối cầu của nút tràn và khi quá trình thêm hoàn tất, sẽ kiểm tra xem nút tràn có đầy hay không. Khi nút tràn của nút hiện tại đã đầy, bước tiếp theo là thực hiện thao tác tách-gộp bằng cách chia nhỏ dữ liệu của nút và nút tràn thành hai nút, mỗi nút có khối lượng dữ liệu M, ghi chúng trở lại cây Improved-RST và loại bỏ nút khỏi các nút tràn trong bảng băm HM. Hình 4 minh họa cấu trúc của nút tràn và bảng băm.
0	Trong bài báo này, chúng tôi tiến hành thực nghiệm trên bộ ảnh MS-COCO gồm 5000 ảnh được chia thành 80 phân lớp. Ứng dụng thực nghiệm được minh họa trong Hình 5. Để đánh giá hiệu quả của phương pháp tìm kiếm ảnh, phần thực nghiệm được đánh giá các giá trị gồm: độ chính xác (precision), độ phủ (recall) và độ đo dung hòa F-measure. Kết quả thực nghiệm được thể hiện như trong Hình 6. Mỗi đường cong trên đồ thị mô tả kết quả truy vấn từ một chủ đề ảnh trong bộ dữ MS-COCO, mỗi điểm trên đường cong là một hình ảnh theo từng chủ đề. Đồng thời, đường cong tương ứng trong đồ thị ROC cho biết tỷ lệ kết quả truy vấn đúng và sai, nghĩa là diện tích dưới đường cong này đánh giá tính đúng đắn của các kết quả truy vấn.
0	Diện tích AUC dưới đường cong của đồ thị ROC nằm trên đường baseline, cho thấy kết quả phân loại trong bài báo của chúng tôi là đúng. Trong bài báo này, chúng tôi đã xây dựng một cấu trúc Improved-RST, một cải tiến của cấu trúc RS-Tree, áp dụng cho bài toán tìm kiếm ảnh. Trong cấu trúc này, thuật toán thêm phần tử được cải tiến để cân bằng dữ liệu trong cây, đồng thời hạn chế thời gian tách nút trên cây giúp tăng thời gian tạo cây; thuật toán thêm phần tử vào cây được cải tiến bằng cách sử dụng bảng băm để lưu trữ các phần tử thuộc nút tràn nhằm tạo độ trễ cho quá trình tách nút.
0	Từ đó, một mô hình truy vấn ảnh tương tự được đề xuất kết hợp mạng học sâu Faster R-CNN và cấu trúc Improved-RST để nâng cao hiệu suất truy vấn. Trong mô hình này, mạng học sâu Faster R-CNN được sử dụng để phân lớp cho tập dữ liệu ảnh và ảnh đầu vào. Các đối tượng được trích xuất đặc trưng và được lưu trữ trên cấu trúc cây Improved-RST nhằm nâng cao hiệu quả cho việc tìm kiếm tập ảnh tương tự. Kết quả thực nghiệm trên bộ dữ liệu ảnh MS-COCO có độ chính xác là 77.39%. Theo kết quả thực nghiệm cho thấy tính hiệu quả so với các công trình khác trên cùng một tập dữ liệu ảnh.
0	Việc giải bài toán thủy lực một chiều đã được nhiều người nghiên cứu và thực hiện giải trên máy PC, tuy nhiên tốc độ tính toán trên máy PC là tuần tự nên thời gian tính toán lớn. Công nghệ Mạng nơ ron tế bào là một kiến trúc tính toán song song vật lý nên có tốc độ tính toán cực kỳ nhanh. Báo cáo trình bày quá trình phân tích, thiết kế chip CNN thực thi giải bài toán thủy lực một chiều. Kết quả thực nghiệm cho thấy ưu thế tốc độ tính toán của công nghệ và độ chính xác đảm bảo yêu cầu thực tế. Báo cáo có 04 phần:
0	Phần mở đầu giới thiệu mô hình toán học của bài toán thủy lực một chiều; phần hai phân tích mô hình bài toán theo thuật toán CNN; phần 3 thiết kế chip CNN thực thi tính toán; phần 4 kết quả thực nghiệm và đánh giá thuật toán, giải pháp công nghệ. Kênh thủy lực một chiều về mặt cơ học là một hệ thống động lực rất phức tạp có nhiều tham số, trong đó hai tham số quan trọng mà ta cần biết là độ cao mực nước và lưu lượng dòng chảy [2,3]. Để nghiên cứu, ta đặt kênh vào hệ tọa độ hai chiều: chiều ngang dọc theo dòng kênh, chiều dọc là độ cao dòng kênh. trong đó Q là lưu lượng dòng chảy, cũng là hàm tùy thuộc mỗi vị trí trên dòng kênh.
0	Các tham số khác trong hai phương trình trên được làm rõ trong [4]. Để giải hệ phương trình trên bằng công nghệ CNN chúng ta phải phân tích bài toán, tìm ra các mẫu thích hợp, từ đó thiết kế kiến trúc mạng thực hiện tính toán [4]. Trong phương trình (4) ta chú ý chỉ số (i) nói lên phương trình này viết cho điểm sai phân thứ (i), và tương ứng với tế bào tại vị trí i trong mạng CNN. (Tuy nhiên các tế bào có cùng phương trình toán học mô tả, có cùng mẫu, nên ta chỉ cần xác định mẫu cho một tế bào).
0	Để giải bài toán này ta thấy có 2 ẩn hàm Q(x,t) và h(x,t), mỗi hàm là một hàm số hai biến không gian x và thời gian t, do vậy để xử lý bài toán chúng ta cần hệ CNN có 2 lớp CNN 1D. Như ta đã thấy máy tính CNN-UM là một kiến trúc đa năng, để giải mỗi bài toán phải chế tạo một phần cứng có kiến trúc riêng [1]. Dựa trên các mẫu tìm được, ta phải thiết kế, chế tạo kiến trúc chip CNN để thực thi tính toán. Tùy theo loại chip FPGA sử dụng, ta căn cứ vào tài nguyên, đặc tính của mỗi loại chip để thiết kế cho phù hợp với yêu cầu tính toán (số lượng tế bào, số lần tính, độ chính xác...).
0	Trên thị trường hiện nay phổ biến có hai hãng sản xuất chip FPGA là Xinlinx và Altera. Nhóm nghiên cứu đã sử dụng loại chip Vertex 6 (XC6VLX240T1FFG1156) của Xilinx. Tài nguyên của chip XC6VLX240T-1FFG1156 gồm có 37,680 Slice (tương đương với 241,152 logic element -LE); 416 khối RAM36K tốc độ truy xuất 350 MHz; 768 khối nhân 25x18 bit (DSP481) tốc độ 275 MHz; 12 bộ nhân/chia tần MMCM; số chân vào ra 600. Tốc độ xung đồng hồ tối đa có thể cấp cho chip là 700 MHz. Một ưu điểm của việc sử dụng chip của Xilinx là Xilinx cung cấp nhiều khối chuyên dụng IP core sử dụng cho các phép toán số học với tốc độ tính toán có thể tới 1CLK/1 phép toán.
0	Chip Virtex 6 có tốc độ nhanh và cấu trúc phần tử LE cải tiến giúp tiết kiệm được nhiều các phần tử LE khi cài đặt một phần cứng trên chip. Theo thiết kế lô gic ở mục (2) ta có mạng hai lớp CNN 1D. Khi thiết kế mạch phần cứng, để đảm bảo kiến trúc tính toán song song nhóm đã thiết kế theo từng phần tử mỗi phần tử là một cặp tính giá trị của h, Q. Như trong Hình 3 giả sử không gian tính toán cho n điểm ta có n phần tử cho n cặp (h,Q). Với kiến trúc này ta cần rất nhiều tài nguyên nhưng tốc độ tính toán cực nhanh 07 xung CLK cho một lần tính (với chip XC6VLX240T-1FFG1156 khoảng 70 ns).
0	Trong thực tế, khi cấu hình chế tạo phần cứng điều quan trọng là việc sử dụng tài nguyên trên chip. Dù công nghệ FPGA hiện nay có tài nguyên rất lớn nhưng cũng không phải là vô hạn, và giá thành của các chip kích thước lớn cũng còn khá cao. Với tình hình cụ thể nhóm nghiên cứu đã thiết kế một số mô hình kiến trúc để lựa chọn. Mô hình này chia không gian tính toán n điểm thành k khối tính toán mỗi khối có (n/k) phần tử, các khối tính có kiến trúc giống hệt nhau và tính toán song song với nhau. Kiến trúc trong một khối được mô tả như trong Hình 4, (các khối tính toán cộng, trừ, nhân, chia số học được cấu hình từ các phần tử LE và DSP của chip FPGA).
0	Trong mô hình này ta thấy sau khi đưa dữ liệu vào thiết lập trạng thái ban đầu cho tế bào (ứng với các điều kiện ban đầu của phương trình đạo hàm riêng), khối điều khiển (CU) gọi các khối tính toán (Add, Mult, Div) thực hiện các phép toán số học để thực hiện. Các khối xử lý số học có thể dùng chung cho các phần tử trong khối và được điều khiển bởi khối CU. Giải pháp này sử dụng ít tài nguyên hơn, ta chia được nhiều khối song song. Với chip XC6VLX240T-1FFG1156 chia được khoảng 95 khối. Trong mỗi khối các phần tử thực hiện tính toán tuần tự nên thời gian tính toán chậm (nếu có 10 phần tử thời gian tính toán là 10 x 9 CLK = 90 CLK, khoảng 900 ns cho một lần tính).
0	Giải pháp này cũng chia không gian tính toán thành các khối, tuy nhiên trong một khối sử dụng nhiều khối tính toán số học hơn nên giảm việc dùng chung tài nguyên, (sơ đồ như trong Hình 5). Do vậy giải pháp này tốn tài nguyên tính toán (cũng chip XC6VLX240T-1FFG1156 chia được 24 khối), tuy nhiên tốc độ tính toán nhanh hơn nhiều, phần tử đầu tiên cần 7 CLK cho một lần tính toán các phần tử sau chỉ cần 1 CLK, (như vậy, nếu khối có 10 phần tử tốc độ tính toán là 7 + 9 = 16 CLK).
0	Ngoài các khối tính toán, ta còn phải sử dụng tài nguyên để lưu trữ dữ liệu. Với chip XC6VLX240T1FFG1156 ta sử dụng bộ nhớ RAM 36K có sẵn. Mạch tính toán được thiết kế như trong Hình 6, dữ liệu đưa vào RAM qua máy PC (dạng file data) hoặc nạp trực tiếp từ cảm biến đo. Trường hợp thực nghiệm ở đây là nạp trực tiếp dữ liệu vào RAM. Có 6 đầu vào song song và cần 6 bộ nhớ RAM cho mỗi cặp tế bào h, Q (ứng với các giá trị biến trong phương trình sai phân hi-1; hi; hi+1; Qi-1; Qi; Qi+1) cho phép đưa dữ liệu vào khối tính toán để tính toán được một đầu ra cho đại lượng h, Q.
0	Để giải quyết vấn đề trên nhóm đã sử dụng thêm thanh ghi dịch cho 3 giá trị liên tục đọc dữ liệu vào khối tính toán (Hình 7). Giải pháp này tiết kiệm bộ nhớ RAM chỉ thêm có 02 xung ban đầu sau đó mỗi lần đọc chỉ cần 1 xung). Tài nguyên của chip sử dụng cho cấu hình mạng gồm: 150,726 LUT; 301,440 REG; 768 DSP. Ta có hai giải pháp như đã phân tích ở trên, nhóm đã chọn giải pháp Pipelines để làm thực nghiệm. Như tính toán trên ta có thể cấu hình được 24 khối, mỗi khối khoảng 10 000 phần tử (h, Q), tính được cho không gian 24 x 10 000 là 24 000 điểm. Tốc độ tính toán hết (7 + 49) CLK = 56 CLK (560 ns) cho một lần tính.
0	Thực tế, nhóm đã thực nghiệm tính toán cho 500 điểm, 50 lần tính; không gian tính toán là 500 x 50 tương ứng với dòng sông dài 250 km, tính toán cho 500 điểm dọc theo chiều dài, khoảng cách giữa hai điểm tính là 500 m. Nhóm nghiên cứu đã cấu hình và tính toán trên chip XC6VLX240T-1FFG1156, hệ thống kết nối giữa máy PC và chip như trong Hình 8. Hình 9 cho thấy kết quả tính toán trên không gian tính toán cho 500x50 của độ cao mực nước h. Hình 10 là kết quả tính toán lưu lượng mực nước Q. Bài báo giới thiệu giải pháp phân tích thiết kế và cầu hình chip CNN giải bài toán thủy lực chiều.
0	Hiện nay, trí tuệ nhân tạo và học máy đang ngày càng phát triển và được ứng dụng rộng rãi trong nhiều lĩnh vực trong đời sống. Trí tuệ nhân tạo có khả năng xử lý dữ liệu khổng lồ, học hỏi từ dữ liệu và đưa ra quyết định một cách tự động. Trong quá trình nghiên cứu, nhóm đã ứng dụng nhận diện cà chua bằng thị giác máy tính thông qua thuật toán YOLOv7. Từ đó đánh giá, phân tích dựa trên dữ liệu cà chua thu thập được. Mô hình nhận diện quả cà chua đã đạt được độ chính xác 93,3 % đối với quả bình thường và đạt 89,1 % với quả hỏng trên tập dữ liệu thử nghiệm.
0	Phát hiện đối tượng theo thời gian thực là một chủ đề rất quan trọng trong thị giác máy tính, vì nó thường là một thành phần cần thiết trong hệ thống thị giác máy tính. Ví dụ: Theo dõi đa đối tượng, lái xe tự động, robot, phân tích hình ảnh y tế,... Thuật toán phát hiện đối tượng được chia thành hai loại chính như phát hiện một giai đoạn (Single-shot object detection) và phát hiện hai giai đoạn (Two-shot object detection). YOLO là mô hình phát hiện, phân loại đối tượng tiên tiến và hiện đại được biết đến với độ chính xác cao và tốc độ nhanh. Việc ứng dụng YOLOv7 trong nhận dạng cà chua là một ý tưởng mới mẻ trong thời đại phát triển của trí tuệ nhân tạo (AI).
0	Có nhiều giải thuật ứng dụng trong nhận dạng như HOG (Histogram of Oriented Gradients), SIFT (Scale Invariant Feature Transform), CNN (Convolutional Neural Networks), R-CNN (Region based Convolutional Neural Networks), SVM (Support Vector Machine). Sự ra đời của YOLO với phiên bản mới của mình là YOLOv7, có nhiều cải tiến so với các phiên bản ra đời trước đó. Một trong những cải tiến quan trọng trong YOLOv7 là việc áp dụng hàm loss mới gọi là “focal loss”. Các phiên bản trước của YOLO sử dụng cross-entropy và loss function được biết là không chính xác trong các đối tượng nhỏ và quá nhỏ [1]. Để giải quyết vấn đề này “focal loss” đã giảm trọng số mất mát và tập trung vào các đối tượng khó phát hiện.
0	YOLOv7 sử dụng cụ thể là 9 anchor box, cho phép YOLO phát hiện hình dạng và kích thước, phạm vi đối tượng rộng so với các phiên bản đi trước của mình, từ đó tăng tính chính xác và hiệu xuất của mô hình [2]. Nghiên cứu này nhằm tìm hiểu, tối ưu và cải tiến thuật toán YOLOv7 bằng cách sử dụng tập dữ liệu cà chua, cải tiến kiến trúc mạng nơ-ron và tối ưu hóa thuật toán huấn luyện để nâng cao khả năng nhận dạng và định vị cà chua trong ảnh. Nhóm tác giả tiến hành phân tích chi tiết các thành phần quan trọng của thuật toán, xây dựng và sử dụng một tập dữ liệu đa dạng và đại diện chứa ảnh cà chua từ nhiều nguồn khác nhau để đảm bảo tính đáng tin cậy và hiệu quả của thuật toán.
0	Kết quả của nghiên cứu này sẽ đóng góp vào việc cải thiện quá trình nhận dạng cà chua, giúp tăng cường năng suất và chất lượng trong ngành nông nghiệp, đồng thời mang lại lợi ích thiết thực cho người nông dân và ngành công, nông nghiệp. You Only Look Once hay còn gọi tắt là YOLO. YOLO thuộc về Object Detection trong lĩnh vực Computer Vision và là mô hình có những ưu điểm khiến nó trở thành một phương pháp rất hiệu quả trong những bài toán nhận diện đối tượng (object detection). Thuật toán Object Detecttion được chia làm 2 nhóm chính: Các mô hình RCNN, mô hình về YOLO. Mô hình được thiết kế để nhận diện các vật thể real-time.
0	YOLO là mô hình mạng CNN sử dụng để phát hiện, phân loại đối tượng. YOLO là sự kết hợp giữa các convolutional layers và connected layers. Các convolutional layers sử dụng để lấy ra feature của ảnh, full-connected layers dự đoán xác suất và tọa độ của các đối tượng. Mô hình đầu vào là ảnh và nhận dạng ảnh đó có những đối tượng nào, xác định tọa độ của các đối tượng đó. Ảnh đầu vào có thể là 3 × 3, 7 × 7 việc chia ô như này ảnh hưởng tới việc nhận dạng đối tượng của mô hình YOLO như sau: Input là 1 ảnh truyền vào, Output của mô hình là một ma trận 3 chiều với kích thước như sau: S × S × (5 × N + M) và tham số mỗi ô có số lượng (5 × N + M).
0	Trong đó, N là số lượng Box và M Class ô cần phải dự đoán. Với ví dụ trên hình ảnh chia thành ô 7 × 7, mỗi ô cần mô hình dự đoán gồm 2 bounding box, 3 objects: Dog, car, bike. Vậy đầu ra là 7 × 7 × 13, có nghĩa là có 13 tham số cho mỗi ô, chúng ta có kết quả là: (7 × 7 × 2 = 98) bounding box. Bounding box gồm 5 thành phần như sau: (x, y, w, h, prediction). Trong đó, x và y là tọa độ giá trị âm của bounding box, còn w và h là chiều rộng và chiều cao bounding box, prediction là Pr (Object).
0	Bag-of-freebies là lần đầu tiên xuất hiện và phát triển trong YOLOv4. BoF bao gồm các kỹ thuật Augmentations, hàm loss, label smoothing, ... được thêm vào trong đào tạo có thể tăng độ chính xác mà không tăng thời gian xử lý. Re-parameterization lần đầu được sử dụng ở phiên bản YOLOv5, nó thực hiện việc hợp nhất lớp Convolution (Conv) và lớp BatchNorm (BN) vào làm một lớp, khiến việc inference diễn ra nhanh hơn (từ 2 layers là Conv và BN sinh ra Conv). Quá trình hợp nhất diễn ra trong inference, còn training model thì vẫn hoạt động bình thường, có 2 lớp riêng biệt đó là Conv và BN.
0	Khuếch đại độ model là một phương pháp quan trọng để tăng hiệu năng của model. Bằng cách sử dụng kỹ thuật tăng chiều tổng hợp ba chiều của mạng nơron (chiều sâu, chiều rộng và chiều độ phân giải của ảnh đầu vào), model tăng chiều đã được phân tích lần đầu tiên trong EfficientNet. Để khuếch đại độ lớn của model, đạt hiệu năng tốt hơn ta sử dụng Model scaling. Model scaling được phân tích kĩ lần đầu tiên trong EfficientNet sử dụng kỹ thuật scale cả 3 chiều của mạng nơ-ron là chiều rộng và chiều sâu, chiều độ phân giải của ảnh đầu vào. Trong YOLO, Implicit knowledge (kiến thức tiềm ẩn) được giới thiệu và áp dụng.
0	Ví dụ, con người có thể hiểu biết một sự vật hay hiện tượng bằng cách trải nghiệm trực tiếp và rút kinh nghiệm từ những trải nghiệm đó. YOLO muốn đưa kiến thức gián tiếp vào mạng nơ-ron. Trong mạng nơ-ron, kiến thức gián tiếp là kiến thức mà model học được thông qua sự tiếp xúc với các input, trong khi kiến thức gián tiếp là kiến thức mà model sẽ tự rút ra trong quá trình đào tạo, độc lập với các input. YOLO đã đưa ra 3 phương pháp để trình bày kiến thức implicit. Tuy nhiên, phương pháp đơn giản nhất, dưới dạng vector, được YOLO sử dụng và đạt được hiệu quả ổn định.
0	Cũng như các phiên bản tiền nhiệm của YOLO, YOLOv7 có kiến trúc 3 phần: Backbone, Neck và Head. YOLOv7 sử dụng backbone là ELAN (E¿cient Layer Aggregation Network). Một ELAN block gồm 3 phần chính: Cross Stage Partial, Computation Block và phép PointWise Conv. Thiết kế của ELAN Block có sự kế thừa từ hai nghiên cứu trước đó là CSPNet và VoVNet [3, 4]. Ý tưởng về CSP hóa một block là việc tạo thêm một nhánh “cross stage partial” đã xuất hiện từ phiên bản YOLOv4. Các lớp Conv nằm trong block Computation Block tính toán và thông qua các 3 × 3 Conv sinh ra tính năng mới.
0	Sau đó, tổng hợp các feature map lại ở cuối và sử dụng toán tử concatenate như VoVNet trên chiều channel, tiếp theo đưa qua PointWise Conv (1 × 1 Conv). Các ELAN Block được kết nối với nhau thông qua các Transition Block là một lần giảm 2 kích cỡ của feature maps. Input ảnh sẽ đi qua Stem Block trước khi tiến vào ELAN Block đầu tiên trong backbone. Vì vậy, một backbone hoàn chỉnh của YOLOv7 sẽ là tập hợp của các ELAN Block và các Transition block. Spatial Pyramid Pooling (SPP) là một lớp trong mạng nơ-ron tích chập (CNN) giúp loại bỏ yêu cầu về ảnh đầu vào có kích thước cố định. SPP được thêm vào phía trên lớp chập cuối cùng, thực hiện tích chập các đặc trưng và tạo ra các đầu ra có độ dài cố định.
0	SPP giải quyết vấn đề về kích thước đầu vào cố định mà không làm giảm hiệu suất tổng thể của mô hình. SPP duy trì thông tin không gian trong các vùng không gian cục bộ, với một số lượng và kích thước vùng cố định. Trong mỗi vùng không gian, các phản hồi của từng bộ lọc được pooling. SPP cho phép ảnh đầu vào có kích thước bất kỳ, điều này cho phép tỷ lệ khung hình và tỷ lệ ảnh tùy ý. SPP được sử dụng trong việc nhận dạng đối tượng. CSPSPP (Cross Stage Partial Spatial Pyramid Pooling) là phiên bản SPP được sử dụng trong YOLOv7.
0	Việc sử dụng CSPSPP nhằm xử lý ảnh có kích thước khác nhau mà không cần thay đổi kích thước trước, điều này rất quan trọng cho các ứng dụng thực tế, nơi ảnh đầu vào có thể đến từ nhiều nguồn khác nhau. Bên cạnh đó, SPP còn có thể nắm bắt các đặc trưng ở nhiều tỷ lệ khác nhau đồng thời bảo toàn vị trí của chúng trong ảnh, việc này hỗ trợ rất lớn cho việc phát hiện đối tượng. So với SPP tiêu chuẩn, CSPSPP giảm số lượng phép tính cần thiết, từ đó giúp mô hình hoạt động hiệu quả hơn. Mặt khác việc sử dụng CSPSPP làm tăng độ phức tạp của mạng so với các phương pháp pooling đơn giản hơn.
0	Ở phiên bản này, head của YOLOv7 không có nâng cấp gì mới, vẫn giữ nguyên việc sử dụng YOLO và auxiliary head như các phiên bản tiền nhiệm của dòng họ YOLO. Cấu trúc của head trong YOLOv7 gồm 3 lớp mạng nơ-ron hoàn toàn kết nối (fully-connected layers). Lớp thứ nhất đó là dự đoán các ô lưới (grid cells) chứa đối tượng. YOLOv7 chia ảnh đầu vào thành một lưới các ô vuông và lớp này dự đoán xem có đối tượng nào nằm trong mỗi ô lưới hay không. Lớp thứ hai là dự đoán các bounding box cho các đối tượng.
0	Nếu lớp thứ nhất dự đoán một ô lưới chứa đối tượng, lớp thứ hai sẽ dự đoán kích thước và tỷ lệ khung hình (bounding box) của đối tượng đó. Lớp thứ ba là dự đoán lớp (class) cho mỗi đối tượng được phát hiện. Lớp này sẽ dự đoán đối tượng thuộc lớp nào trong số các lớp đã được xác định trước. Điểm khác biệt so với các phiên bản YOLO trước là YOLOv7 có thể sử dụng một hoặc nhiều “head” tùy thuộc vào phiên bản cụ thể. Một số cải tiến trong Head của YOLOv7 đó là GIOU (Generalized Intersection over Union) để huấn luyện “head”, giúp cải thiện khả năng dự đoán bounding box chính xác hơn.
0	Thêm vào đó, năng lực học tập nhiều kích thước (Multi-scale training), trong quá trình huấn luyện, YOLOv7 sử dụng ảnh đầu vào với nhiều kích thước khác nhau, giúp “head” học cách phát hiện đối tượng ở các kích thước đa dạng. Trong kiến trúc của YOLOv7, “head” chịu trách nhiệm cho đầu ra cuối được gọi là “lead head” và “head” được sử dụng để hỗ trợ việc huấn luyện được gọi là “auxiliary head”. YOLOv7 sử dụng “lead head” làm hướng dẫn để tạo ra các nhãn phân cấp từ thô đến mịn. Các nhãn này lần lượt được sử dụng để huấn luyện “auxiliary head” và “lead head” [7].
0	Để xây dựng chương trình phân loại cà chua dựa trên thuật toán YOLOv7 cần phải trải qua các bước trong quy trình sau: Tổng cộng 1.029 ảnh trong đó có 887 ảnh được huấn luyện (training) và 142 ảnh được kiểm tra (test), đã được thu thập từ bộ dữ liệu của RoboÀow và được chụp bổ sung thêm bằng điện thoại (Redmi Note 8), mỗi ảnh có kích thước là 1.024 × 631 pixels, một bit màu sâu 24 và độ phân giải 96 dpi. Để duy trì tính nhất quán và độ tin cậy, một giao thức camera tỉ mỉ đã được tuân thủ.
0	Hình ảnh thu được từ khoảng cách tiêu chuẩn khoảng 30 cm, đảm bảo độ biến dạng tối thiểu và duy trì tỷ lệ vật thể nhất quán. Máy ảnh được đặt vuông góc với mặt phẳng của mặt đất để giảm thiểu mọi sai lệch về góc độ. Để tăng độ mạnh mẽ của mô hình đào tạo, hình ảnh được thu thập từ các góc độ khác nhau về điều kiện chiếu sáng và mức độ tương phản khác nhau giữa mục tiêu (quả cà chua) và nền (sàn gạch). Các hình ảnh được chuẩn hóa thành kích thước 640 × 640 pixel để phù hợp với mô hình YOLOv7 và tăng tính nhất quán của các mẫu huấn luyện.
0	Trong ứng dụng thực tế, một camera chuyên nghiệp có thể được sử dụng để chụp ảnh thay vì điện thoại thủ công. Do đó, khoảng cách và góc quay của hình ảnh có thể bị thay đổi. Các mô hình dựa trên YOLO của CNN, chẳng hạn như nhiều thuật toán phát hiện đối tượng khác, rất nhạy cảm với những thay đổi về khoảng cách và góc quay của hình ảnh. Các mô hình YOLO được thiết kế để phát hiện các đối tượng trong hình ảnh bằng cách chia hình ảnh thành một lưới và dự đoán các hộp giới hạn cũng như xác suất lớp cho mỗi ô lưới. Thiết kế này cho phép YOLO xác định các đối tượng ở các vị trí khác nhau trong một hình ảnh.
0	Ảnh sau khi được thu thập sẽ được tiến hành gán nhãn (Data labeling and Data annotations). Ở đây, nhóm tác giả sử dụng công cụ Labeling để thực hiện việc gán nhãn. Kết quả thu được sau khi xử lý thu được YOLO annotations là <object-class> <x> <y> <width> <height>. Trong đó object-class gồm 2 giá trị là “0” và “1”, tương ứng với tình trạng hỏng và không hỏng của quả cà chua. “x” và “y” lần lượt là tọa độ trục hoành và trục tung cho center box của bounding box. “Width” và “Height” là chiều dài và rộng của bouding box. Tất cả các chỉ số đều được chuẩn hóa về khoảng giá trị [0,1].
0	Để thực hiện việc huấn luyện model, ta cần cài đặt một số phần mềm sau: CUDA 11.3, cuDNN 8.2.1, Python 3.7.3 trở lên. Sau đó chúng ta tiến hành cài đặt những gói (package) cần thiết cho quá trình huấn luyện từ tập tin “requirements.txt”. Sử dụng le pretrain có sẵn trong tài liệu của YOLOv7 và nhân ản clone) ể huấn luyện model theo các ước. Quá rình thực hiện sẽ được thực iện rên oogle Colab (Colaboratory), một công ụ miễn phí của Google cho hép ượn ấu hình (CPU à GPU) để thực iện iệc hực thi, huấn luyện dữ iệu ằng gôn gữ Python.
0	Sau khi đã hoàn thiện việc huấn luyện và thử nghiệm các mô hình của các bài toán con, nhóm tác giả sẽ thực hiện chạy hoàn chỉnh trên hệ điều hành Windows. Cấu hình phần cứng cơ bản là một bộ CPU (4 nhân trở lên), GPU có khả năng xử lý tính toán tốt nhiều VRAM (từ 2GB trở lên) và một camera HD. Ảnh sẽ được đưa từ thư mục để nhận dạng hoặc có thể sử dụng camera để nhận dạng trực tiếp. Kết quả được hiển thị dưới dạng thông số trong Bouding box như phần 3.2 ở trên.
0	Để thực hiện việc test mô hình, ta sử dụng câu lệnh “detect.py” kết hợp cùng model đã “train” và đầu vào là một ảnh sau: Số lượng quả cà chua thực tế trong Hình 12 là 14 quả cà chua bao gồm 7 quả tốt và 7 quả hỏng. Việc áp dụng model sau khi “train” để nhận diện và phân loại ta thu được: 6 quả tốt (OK), 8 quả không tốt (NG). Vậy hiệu suất model đạt độ chính xác khá tốt trong trường hợp trên. Hình 13 gồm các chỉ số sau: Precision Curve (P_curve.png): Biểu diễn các giá trị chính xác ở các ngưỡng khác nhau. Biểu đồ thể hiện độ chính xác thay đổi khi ngưỡng thay đổi. Recall Curve (R_curve.png): Tương ứng, biểu đồ này biểu diễn cách thay đổi qua các ngưỡng khác nhau của các giá trị recall [5].
0	Box/val Box: Mất hộp giới hạn của tập dữ liệu huấn luyện hoặc tập dữ liệu xác thực, hộp càng nhỏ thì càng chính xác. Objectness/val Objectness: Là tổn thất trung bình của việc phát hiện mục tiêu và việc phát hiện mục tiêu càng nhỏ thì việc phát hiện càng chính xác. Classication/val classication: Đào tạo hoặc xác nhận được suy đoán là giá trị trung bình của mất phân loại và phân loại càng nhỏ thì càng chính xác. mAP50: Độ chính xác trung bình được xác định với giá trị 0,50 ở ngưỡng giao nhau trên giao nhau (IoU). Đó là thước đo độ chính xác của mô hình chỉ tính đến các kết quả “dễ dàng”. mAP50-95: Trung bình của độ chính được tính ở các ngưỡng IoU khác nhau, dao động từ 0,50 đến 0,95 [6].
0	Nó cung cấp cái nhìn tổng thể về hiệu suất mô hình qua các mức độ và trường hợp khó phát hiện khác nhau. Việc huấn luyện mô hình trong vài lần đầu có độ chính xác chưa cao. Tuy nhiên, sau khi thực hiện “training” nhiều lần bằng cách thêm dữ liệu và lặp lại việc huấn luyện lại các bức ảnh mà model chưa học kỹ, mô hình nhận diện quả cà chua đã đạt được độ chính xác 93,3% đối với quả bình thường và đạt 89,1% với quả hỏng trên tập dữ liệu thử nghiệm. Mô hình thử nghiệm này vẫn cần cải thiện.
0	Độ chính xác có thể tăng thêm nếu có thêm dữ liệu hơn. Trong nghiên cứu này, nhóm tác giả đã phân tích, huấn luyện model dựa trên thuật toán YOLOv7 trong việc nhận diện cà chua hỏng và cà chua không hỏng. Kết quả cho thấy, model sau khi train và test đạt được hiệu suất cao và đáng tin cậy trong việc phân loại cà chua. Đề tài đã vượt qua những hạn chế của các phương pháp truyền thống trong việc nhận diện và phân loại cà chua. Nó cho phép chúng ta xử lý nhanh chóng các hình ảnh có kích thước lớn và đồng thời đảm bảo độ chính xác cao.
0	Kết quả của nghiên cứu này có thể được áp dụng vào các quy trình sản xuất và kiểm tra chất lượng cà chua trong ngành công nghiệp thực phẩm. Các nhà sản xuất và nhà nghiên cứu có thể sử dụng để tự động hóa quá trình phân loại cà chua theo tình trạng hỏng và tốt, từ đó giảm thiểu sự lãng phí và tăng cường hiệu suất sản xuất. Tuy nhiên, để nâng cao hiệu quả và ứng dụng thực tế, cần tiếp tục nghiên cứu và phát triển. Các nỗ lực có thể tập trung vào việc tăng cường tốc độ xử lý và độ chính xác của model, cũng như mở rộng phạm vi ứng dụng đến các loại trái cây và thực phẩm khác.
1	Trong những năm gần đây, sự phát triển mạnh mẽ của học sâu đã mở ra nhiều hướng tiếp cận mới trong lĩnh vực y tế, đặc biệt là chẩn đoán hình ảnh. Mạng nơ-ron tích chập (Convolutional Neural Network – CNN) được xem là một trong những mô hình hiệu quả nhất cho các bài toán xử lý ảnh y sinh như X-quang, CT và MRI. Nghiên cứu này trình bày việc xây dựng và đánh giá một mô hình CNN nhằm hỗ trợ phát hiện sớm bệnh viêm phổi từ ảnh X-quang ngực. Bộ dữ liệu sử dụng gồm 5.863 ảnh, trong đó 3.875 ảnh viêm phổi và 1.988 ảnh bình thường. Kết quả thực nghiệm cho thấy mô hình đạt độ chính xác cao, góp phần hỗ trợ bác sĩ trong quá trình ra quyết định lâm sàng.
1	Mô hình CNN được thiết kế gồm 4 lớp tích chập kết hợp với các lớp pooling và hai lớp fully connected. Dữ liệu đầu vào được chuẩn hóa về kích thước 224x224 pixel và tăng cường dữ liệu bằng các kỹ thuật xoay, lật và thay đổi độ sáng nhằm giảm hiện tượng overfitting. Tập dữ liệu được chia theo tỷ lệ 80% cho huấn luyện và 20% cho kiểm thử. Quá trình huấn luyện được thực hiện trong 50 epoch với batch size bằng 32, sử dụng hàm mất mát binary cross-entropy và bộ tối ưu Adam với learning rate 0,0001. Các chỉ số đánh giá bao gồm Accuracy, Precision, Recall và F1-score nhằm phản ánh toàn diện hiệu quả mô hình.
1	Kết quả cho thấy mô hình CNN đạt độ chính xác 92,4%, Precision 91,8%, Recall 93,1% và F1-score 92,4% trên tập kiểm thử. So với phương pháp phân loại truyền thống dựa trên đặc trưng thủ công, hiệu suất của CNN cao hơn trung bình 15–18%. Điều này chứng minh khả năng tự động trích xuất đặc trưng hiệu quả của CNN từ dữ liệu hình ảnh phức tạp. Tuy nhiên, nghiên cứu vẫn tồn tại hạn chế về chi phí tính toán và yêu cầu dữ liệu lớn. Trong tương lai, việc kết hợp CNN với học chuyển giao (Transfer Learning) có thể giúp giảm thời gian huấn luyện và nâng cao khả năng ứng dụng thực tế trong các bệnh viện tuyến dưới.
1	Bệnh tiểu đường type 2 là một trong những bệnh mãn tính phổ biến nhất hiện nay, gây ra nhiều biến chứng nguy hiểm nếu không được phát hiện sớm. Nghiên cứu này đề xuất sử dụng thuật toán K-Nearest Neighbors (KNN) để dự đoán nguy cơ mắc bệnh tiểu đường type 2 dựa trên các chỉ số sức khỏe cơ bản. Dữ liệu được thu thập từ 768 bệnh nhân, bao gồm các thuộc tính như độ tuổi, chỉ số BMI, huyết áp, nồng độ glucose và tiền sử gia đình. Mục tiêu của nghiên cứu là xây dựng một mô hình dự đoán đơn giản, dễ triển khai tại cộng đồng và các cơ sở y tế tuyến cơ sở.
1	Thuật toán KNN được áp dụng với khoảng cách Euclid để đo độ tương đồng giữa các mẫu dữ liệu. Trước khi huấn luyện, dữ liệu được chuẩn hóa bằng phương pháp Min-Max Scaling nhằm đảm bảo các thuộc tính có cùng thang đo. Tập dữ liệu được chia thành 75% cho huấn luyện và 25% cho kiểm thử. Giá trị k được lựa chọn thông qua phương pháp cross-validation, trong đó k tối ưu được xác định là 7. Các chỉ số đánh giá mô hình bao gồm Accuracy, Sensitivity, Specificity và ROC-AUC để đánh giá khả năng phân biệt giữa nhóm bệnh và không bệnh.
1	Kết quả thực nghiệm cho thấy mô hình KNN đạt độ chính xác 78,6%, Sensitivity 80,2% và Specificity 76,4%. Diện tích dưới đường cong ROC đạt 0,82, cho thấy khả năng phân loại khá tốt. Mặc dù độ chính xác không cao bằng các mô hình học sâu phức tạp, KNN có ưu điểm là dễ triển khai, không yêu cầu huấn luyện phức tạp và phù hợp với dữ liệu quy mô vừa và nhỏ. Nghiên cứu chỉ ra rằng KNN có tiềm năng trở thành công cụ hỗ trợ sàng lọc ban đầu bệnh tiểu đường type 2 tại cộng đồng, đặc biệt ở những khu vực hạn chế về hạ tầng công nghệ.
1	Nhận diện khuôn mặt là một trong những ứng dụng tiêu biểu của trí tuệ nhân tạo trong đời sống hiện đại, được sử dụng rộng rãi trong an ninh, giám sát và kiểm soát ra vào. Nghiên cứu này đề xuất một hệ thống kết hợp CNN và KNN nhằm nâng cao độ chính xác nhận diện khuôn mặt trong môi trường thực tế. CNN được sử dụng để trích xuất đặc trưng khuôn mặt, trong khi KNN đảm nhiệm vai trò phân loại. Bộ dữ liệu thực nghiệm gồm 10.000 ảnh khuôn mặt của 500 người khác nhau, được thu thập trong nhiều điều kiện ánh sáng và góc nhìn khác nhau.
1	Mô hình CNN được huấn luyện để tạo vector đặc trưng 128 chiều cho mỗi khuôn mặt. Sau khi trích xuất đặc trưng, thuật toán KNN được áp dụng để so sánh và nhận diện danh tính dựa trên khoảng cách cosine. Dữ liệu được chia thành 70% cho huấn luyện và 30% cho kiểm thử. Giá trị k được lựa chọn là 5 nhằm cân bằng giữa độ chính xác và thời gian xử lý. Hệ thống được triển khai thử nghiệm trên máy tính có GPU NVIDIA GTX 1660, với thời gian nhận diện trung bình khoảng 0,15 giây mỗi khuôn mặt.
1	Kết quả cho thấy hệ thống đạt độ chính xác nhận diện 95,1% trong điều kiện ánh sáng chuẩn và 91,3% trong điều kiện ánh sáng yếu. So với việc sử dụng CNN kết hợp Softmax, phương pháp CNN-KNN cho khả năng mở rộng tốt hơn khi số lượng người dùng tăng lên mà không cần huấn luyện lại toàn bộ mô hình. Tuy nhiên, nhược điểm chính là yêu cầu bộ nhớ lớn khi dữ liệu tăng nhanh. Trong tương lai, việc kết hợp thêm kỹ thuật giảm chiều dữ liệu như PCA có thể giúp tối ưu hiệu suất và khả năng ứng dụng trong các hệ thống an ninh thông minh quy mô lớn.
1	Trong kỷ nguyên y học hiện đại, việc phát hiện sớm các bệnh lý về da, đặc biệt là ung thư biểu mô, đóng vai trò sống còn trong việc tăng tỷ lệ sống sót cho bệnh nhân. Phương pháp soi da truyền thống thường phụ thuộc lớn vào kinh nghiệm chủ quan của bác sĩ, dẫn đến tỷ lệ sai sót nhất định trong các trường hợp bệnh lý phức tạp. Mạng nơ-ron tích chập (CNN) nổi lên như một công cụ đột phá nhờ khả năng tự động trích xuất các đặc trưng không gian từ hình ảnh y khoa.
1	Nghiên cứu này tập trung vào việc xây dựng một mô hình CNN dựa trên kiến trúc ResNet-50 để phân loại các tổn thương sắc tố da, giúp hỗ trợ các chuyên gia y tế trong việc đưa ra quyết định lâm sàng nhanh chóng và chính xác hơn, đặc biệt là tại các khu vực thiếu hụt nhân lực chuyên môn cao. Nghiên cứu sử dụng bộ dữ liệu HAM10000 bao gồm 10.015 hình ảnh nội soi da được dán nhãn cụ thể. Chúng tôi thực hiện các bước tiền xử lý nghiêm ngặt bao gồm chuẩn hóa kích thước hình ảnh về 224x224 pixel và áp dụng kỹ thuật tăng cường dữ liệu (data augmentation) như xoay, lật và điều chỉnh độ sáng để tránh hiện tượng quá khớp (overfitting).
1	Kiến trúc mô hình được xây dựng dựa trên ResNet-50 với việc tinh chỉnh (fine-tuning) các lớp cuối cùng để phù hợp với 7 loại tổn thương da khác nhau. Quá trình huấn luyện sử dụng hàm tối ưu Adam với tốc độ học 10^{-4} trong suốt 50 chu kỳ (epochs). Việc sử dụng kỹ thuật Transfer Learning cho phép mô hình tận dụng các đặc trưng đã học từ tập dữ liệu ImageNet, từ đó rút ngắn thời gian hội tụ và cải thiện độ chính xác trên dữ liệu y tế chuyên biệt. Sau quá trình thực nghiệm, mô hình CNN đạt được độ chính xác tổng thể là 94,2% trên tập dữ liệu kiểm thử độc lập.
1	Phân tích ma trận nhầm lẫn (Confusion Matrix) cho thấy độ nhạy đối với ung thư biểu mô tế bào đáy đạt 91,5%, trong khi độ đặc hiệu duy trì ở mức cao là 96,8%. Chỉ số F1-score trung bình cho tất cả các lớp là 0,93, chứng minh sự ổn định của mô hình trên các nhóm dữ liệu không cân bằng. So với các phương pháp phân loại thủ công của nhóm bác sĩ nội trú tham gia đối chứng (đạt độ chính xác trung bình 82,4%), mô hình đề xuất đã cải thiện hiệu suất lên thêm gần 12%. Kết quả này khẳng định tiềm năng to lớn của Deep Learning trong việc tự động hóa chẩn đoán hình ảnh y tế, giảm thiểu đáng kể khối lượng công việc cho đội ngũ bác sĩ.
1	Sự phát triển mạnh mẽ của lĩnh vực tài chính tiêu dùng đòi hỏi các ngân hàng phải có hệ thống đánh giá rủi ro tín dụng cực kỳ nhanh chóng và chính xác. Các phương pháp thống kê truyền thống thường gặp khó khăn khi xử lý các mối quan hệ phi tuyến tính giữa các biến số khách hàng như thu nhập, thói quen chi tiêu và lịch sử trả nợ. Thuật toán K-Láng giềng gần nhất (KNN) là một phương pháp học máy giám sát hiệu quả dựa trên nguyên lý tương đồng về đặc điểm giữa các thực thể dữ liệu.
1	Mục tiêu của nghiên cứu này là ứng dụng KNN để xây dựng mô hình dự báo khả năng nợ xấu của khách hàng cá nhân, từ đó giúp các tổ chức tài chính tối ưu hóa quy trình xét duyệt khoản vay và giảm thiểu tổn thất rủi ro tín dụng trong ngắn hạn và dài hạn. Dữ liệu nghiên cứu được thu thập từ một tổ chức tín dụng với hơn 12.500 hồ sơ khách hàng, bao gồm 25 thuộc tính định lượng và định tính. Bước đầu tiên và quan trọng nhất là chuẩn hóa dữ liệu (Min-Max Scaling) để đưa các giá trị về khoảng [0, 1], đảm bảo rằng các thuộc tính có đơn vị lớn không lấn át các thuộc tính khác trong tính toán khoảng cách.
1	Chúng tôi sử dụng khoảng cách Euclidean để đo lường sự tương đồng và áp dụng kỹ thuật kiểm chéo 10 lớp (10-fold cross-validation) để tìm giá trị K tối ưu. Thông qua quá trình lặp, giá trị K=7 được xác định là điểm cân bằng tốt nhất giữa độ chệch (bias) và phương sai (variance). Ngoài ra, kỹ thuật lấy mẫu lại SMOTE cũng được áp dụng để xử lý tình trạng mất cân bằng dữ liệu giữa nhóm khách hàng tốt và nhóm khách hàng nợ xấu. Kết quả thực nghiệm cho thấy hệ thống đạt độ chính xác 89,7% trong việc phân loại hồ sơ tín dụng.
1	Đặc biệt, chỉ số AUC-ROC của mô hình đạt 0,91, cho thấy khả năng phân biệt cực tốt giữa khách hàng có rủi ro cao và rủi ro thấp. Khi so sánh với thuật toán Hồi quy Logistic truyền thống (chỉ đạt độ chính xác 84,1%), KNN cho thấy sự vượt trội trong việc bắt kịp các mẫu hành vi khách hàng phức tạp. Tuy nhiên, nghiên cứu cũng ghi nhận rằng thời gian dự báo tăng tỷ lệ thuận với kích thước tập dữ liệu huấn luyện, điều này đòi hỏi việc triển khai các cấu trúc dữ liệu tối ưu như KD-Tree để cải thiện tốc độ truy vấn. Tổng kết lại, mô hình KNN đề xuất cung cấp một giải pháp đáng tin cậy cho các ngân hàng trong việc tự động hóa quy trình chấm điểm tín dụng với độ tin cậy cao.
1	Ô nhiễm không khí, đặc biệt là bụi mịn PM2.5, đã trở thành một thách thức nghiêm trọng đối với sức khỏe cộng đồng tại các đô thị lớn như Hà Nội và TP. Hồ Chí Minh. Việc dự báo chính xác nồng độ ô nhiễm trong 24 giờ tới có ý nghĩa quan trọng để các cơ quan chức năng đưa ra cảnh báo kịp thời cho người dân. Mô hình Rừng ngẫu nhiên (Random Forest) là một kỹ thuật học máy mạnh mẽ dựa trên sự kết hợp của nhiều cây quyết định, có khả năng xử lý tốt các biến số khí tượng phức tạp và tương quan đa chiều.
1	Nghiên cứu này được thực hiện nhằm xây dựng một hệ thống dự báo chỉ số chất lượng không khí (AQI) dựa trên dữ liệu lịch sử và các yếu tố thời tiết như nhiệt độ, độ ẩm, tốc độ gió và mật độ giao thông. Tập dữ liệu đầu vào bao gồm các thông số đo đạc hàng giờ từ các trạm quan trắc tự động trong giai đoạn từ năm 2023 đến 2025. Mô hình được cấu tạo từ 200 cây quyết định độc lập, sử dụng kỹ thuật lấy mẫu Bootstrap để tạo sự đa dạng cho các cây thành phần.
1	Tại mỗi nút phân tách, thuật toán chỉ chọn ngẫu nhiên một nhóm nhỏ các đặc trưng để đảm bảo tính không tương quan giữa các cây, từ đó giảm thiểu đáng kể sai số dự báo tổng thể. Chúng tôi cũng tích hợp thêm các biến trễ (lag variables) như nồng độ PM2.5 của 3 giờ trước đó để mô hình hóa tính tuần tự của dữ liệu thời gian. Việc sử dụng độ đo Gini giúp xác định tầm quan trọng của các yếu tố, qua đó nhận thấy hướng gió và mật độ phương tiện là hai yếu tố có ảnh hưởng lớn nhất đến sự biến động của nồng độ ô nhiễm.
1	Mô hình dự báo đạt sai số tuyệt đối trung bình (MAE) là 12,4 mg/m^3 đối với bụi PM2.5, một con số rất ấn tượng so với các mô hình dự báo thời tiết truyền thống. Hệ số xác định R^2 đạt 0,88, chứng tỏ mô hình giải thích được phần lớn sự biến động của dữ liệu thực tế. Trong các kịch bản ô nhiễm cực đoan, Random Forest thể hiện khả năng chống nhiễu tốt và không bị ảnh hưởng bởi các giá trị ngoại lai (outliers) so với mô hình Hồi quy tuyến tính. Bảng so sánh cho thấy độ chính xác dự báo trong 6 giờ đầu đạt mức 95,2% và giảm nhẹ xuống 87,6% cho dự báo 24 giờ.
1	Trong bối cảnh y học hiện đại, nhu cầu chẩn đoán nhanh và chính xác các bệnh lý thông qua hình ảnh y tế ngày càng trở nên cấp thiết. Các phương pháp chẩn đoán truyền thống phụ thuộc nhiều vào kinh nghiệm của bác sĩ, dễ dẫn đến sai sót chủ quan khi khối lượng bệnh nhân tăng cao. Sự phát triển của trí tuệ nhân tạo, đặc biệt là học sâu, đã mở ra hướng tiếp cận mới trong việc hỗ trợ ra quyết định lâm sàng. Mạng nơ-ron tích chập (Convolutional Neural Network – CNN) được đánh giá là mô hình hiệu quả nhất trong xử lý và phân tích hình ảnh.
1	Nhờ khả năng tự động trích xuất đặc trưng không gian, CNN đã được ứng dụng rộng rãi trong phát hiện ung thư, viêm phổi, bệnh tim mạch và nhiều bệnh lý khác, góp phần nâng cao chất lượng chăm sóc sức khỏe cộng đồng. Nhiều nghiên cứu trước đây đã chứng minh hiệu quả vượt trội của CNN so với các phương pháp học máy truyền thống trong lĩnh vực y tế. Theo một số báo cáo, CNN có thể đạt độ chính xác trên 90% trong bài toán phân loại ảnh X-quang và MRI. Các mô hình tiêu biểu như VGG16, ResNet và DenseNet đã được áp dụng thành công để phát hiện ung thư vú và tổn thương phổi.
1	Tuy nhiên, phần lớn các nghiên cứu này sử dụng mô hình có số lượng tham số lớn, đòi hỏi tài nguyên tính toán cao. Do đó, việc xây dựng các mô hình CNN gọn nhẹ nhưng vẫn đảm bảo độ chính xác là một hướng nghiên cứu quan trọng, đặc biệt phù hợp với các bệnh viện tuyến dưới và hệ thống y tế có hạ tầng hạn chế. Nghiên cứu này đề xuất một mô hình CNN gồm 4 lớp tích chập, 4 lớp pooling và 2 lớp fully connected nhằm phân loại ảnh X-quang ngực. Bộ dữ liệu sử dụng gồm 5.863 ảnh, trong đó có 3.875 ảnh bệnh viêm phổi và 1.988 ảnh bình thường. Dữ liệu được chuẩn hóa về kích thước 224x224 pixel và tăng cường bằng các kỹ thuật xoay ảnh, lật ngang và điều chỉnh độ sáng.
1	Tập dữ liệu được chia theo tỷ lệ 80% cho huấn luyện và 20% cho kiểm thử. Mô hình được huấn luyện trong 50 epoch với batch size 32, sử dụng hàm mất mát Binary Cross-Entropy và bộ tối ưu Adam với learning rate 0,0001. Kết quả thực nghiệm cho thấy mô hình CNN đạt độ chính xác trung bình 92,4% trên tập kiểm thử. Các chỉ số Precision, Recall và F1-score lần lượt đạt 91,8%, 93,1% và 92,4%. So sánh với phương pháp SVM sử dụng đặc trưng HOG, mô hình CNN cho hiệu suất cao hơn khoảng 17%. Thời gian xử lý trung bình cho mỗi ảnh là 0,12 giây trên GPU NVIDIA GTX 1660.
1	Các kết quả này cho thấy mô hình đề xuất có khả năng hỗ trợ bác sĩ trong việc phát hiện sớm bệnh viêm phổi, đồng thời giảm tải áp lực cho hệ thống y tế trong bối cảnh số lượng bệnh nhân ngày càng gia tăng. Mặc dù đạt kết quả khả quan, nghiên cứu vẫn tồn tại một số hạn chế như phụ thuộc vào chất lượng dữ liệu đầu vào và khả năng tổng quát hóa khi áp dụng cho dữ liệu từ bệnh viện khác. Trong tương lai, việc kết hợp học chuyển giao và dữ liệu đa trung tâm có thể giúp cải thiện độ tin cậy của mô hình.
1	Bệnh tiểu đường type 2 là một trong những bệnh mãn tính phổ biến nhất trên thế giới, ảnh hưởng nghiêm trọng đến chất lượng cuộc sống của người bệnh. Theo thống kê của Tổ chức Y tế Thế giới, tỷ lệ mắc bệnh tiểu đường đang tăng nhanh, đặc biệt tại các quốc gia đang phát triển. Việc phát hiện sớm nguy cơ mắc bệnh đóng vai trò quan trọng trong công tác phòng ngừa và điều trị. Trong bối cảnh đó, các thuật toán học máy được xem là công cụ hỗ trợ hiệu quả cho việc phân tích dữ liệu y tế. Thuật toán K-Nearest Neighbors (KNN) với ưu điểm đơn giản, dễ triển khai được lựa chọn trong nghiên cứu này nhằm xây dựng mô hình dự đoán nguy cơ tiểu đường type 2 tại cộng đồng.
1	KNN là thuật toán học máy không tham số, hoạt động dựa trên độ tương đồng giữa các mẫu dữ liệu. Nhiều nghiên cứu trước đây đã áp dụng KNN trong phân loại bệnh lý với độ chính xác dao động từ 70% đến 85%. So với các mô hình phức tạp như Neural Network hay Random Forest, KNN không yêu cầu quá trình huấn luyện phức tạp nhưng lại nhạy cảm với nhiễu và thang đo dữ liệu. Do đó, việc tiền xử lý và lựa chọn tham số k đóng vai trò quan trọng trong hiệu suất của mô hình. Các nghiên cứu gần đây cho thấy việc chuẩn hóa dữ liệu có thể cải thiện độ chính xác của KNN lên khoảng 5–10%.
1	Bộ dữ liệu sử dụng trong nghiên cứu gồm 768 mẫu bệnh nhân, bao gồm 8 thuộc tính chính như tuổi, giới tính, chỉ số BMI, huyết áp, nồng độ glucose, insulin và tiền sử gia đình. Dữ liệu được chuẩn hóa bằng phương pháp Min-Max Scaling nhằm đảm bảo các thuộc tính có cùng thang đo. Tập dữ liệu được chia thành 75% cho huấn luyện và 25% cho kiểm thử. Giá trị k được xác định thông qua cross-validation, trong đó k = 7 cho kết quả tối ưu. Khoảng cách Euclid được sử dụng để tính độ tương đồng giữa các mẫu dữ liệu trong không gian đặc trưng.
1	Kết quả thực nghiệm cho thấy mô hình KNN đạt độ chính xác 78,6% trên tập kiểm thử. Độ nhạy (Sensitivity) đạt 80,2%, trong khi độ đặc hiệu (Specificity) đạt 76,4%. Diện tích dưới đường cong ROC đạt 0,82, phản ánh khả năng phân biệt tốt giữa nhóm bệnh và không bệnh. So sánh với Logistic Regression, mô hình KNN cho độ chính xác cao hơn khoảng 6%. Mặc dù chưa đạt hiệu suất tối ưu như các mô hình học sâu, KNN vẫn cho thấy tiềm năng ứng dụng trong sàng lọc ban đầu nhờ tính đơn giản và chi phí triển khai thấp.
1	"Nghiên cứu đã chứng minh một cách thuyết phục khả năng ứng dụng của thuật toán KNN trong dự đoán nguy cơ bệnh tiểu đường Type 2 dựa trên dữ liệu y tế cơ bản. Mô hình không chỉ là một công cụ phân loại đơn thuần mà còn là ""trợ lý ảo"" đắc lực, giúp các bác sĩ cộng đồng thu hẹp phạm vi nghi vấn và ưu tiên nguồn lực cho những bệnh nhân có nguy cơ cao nhất. Tuy nhiên, để đạt được độ chính xác tiệm cận mức tuyệt đối, hướng phát triển tương lai sẽ tập trung vào việc kết hợp KNN với các kỹ thuật chọn lọc đặc trưng (Feature Selection) như Thuật toán Di truyền (Genetic Algorithms) hoặc Phân tích Thành phần Chính (PCA)."
1	Nhận diện khuôn mặt là một trong những ứng dụng nổi bật của trí tuệ nhân tạo trong đời sống hiện đại, đặc biệt trong lĩnh vực an ninh và quản lý truy cập. Các hệ thống truyền thống thường gặp khó khăn khi điều kiện ánh sáng và góc chụp thay đổi. Với sự phát triển của học sâu, CNN đã chứng minh khả năng trích xuất đặc trưng khuôn mặt hiệu quả. Tuy nhiên, việc phân loại bằng Softmax gặp hạn chế khi số lượng người dùng tăng nhanh. Do đó, nghiên cứu này đề xuất kết hợp CNN và KNN nhằm xây dựng hệ thống nhận diện khuôn mặt có độ chính xác cao và khả năng mở rộng tốt trong môi trường thực tế.
1	Mô hình CNN được sử dụng để trích xuất vector đặc trưng 128 chiều từ ảnh khuôn mặt đầu vào. Sau đó, thuật toán KNN được áp dụng để thực hiện nhận diện dựa trên khoảng cách cosine. Bộ dữ liệu sử dụng gồm 10.000 ảnh của 500 người, mỗi người có 20 ảnh trong các điều kiện ánh sáng khác nhau. Dữ liệu được chia thành 70% cho huấn luyện và 30% cho kiểm thử. Giá trị k được lựa chọn là 5 nhằm đảm bảo cân bằng giữa độ chính xác và thời gian xử lý. Kết quả cho thấy hệ thống đạt độ chính xác 95,1% trong điều kiện ánh sáng chuẩn và 91,3% trong điều kiện ánh sáng yếu.
1	Thời gian nhận diện trung bình cho mỗi khuôn mặt là 0,15 giây trên GPU NVIDIA GTX 1660. So với mô hình CNN-Softmax, phương pháp đề xuất cải thiện độ chính xác khoảng 4% khi số lượng người dùng tăng lên. Điều này chứng minh tính linh hoạt và khả năng mở rộng của mô hình trong các hệ thống an ninh thông minh. Mặc dù đạt hiệu suất cao, mô hình CNN-KNN vẫn tồn tại hạn chế về bộ nhớ lưu trữ khi số lượng dữ liệu tăng lớn. Trong tương lai, việc tích hợp thêm các kỹ thuật giảm chiều như PCA hoặc FAISS có thể giúp tối ưu hiệu năng.
1	"Trong bối cảnh nền kinh tế số phát triển vượt bậc, các giao dịch tài chính trực tuyến qua thẻ tín dụng đã trở thành mục tiêu hàng đầu của tội phạm công nghệ cao. Việc phát hiện kịp thời các giao dịch bất thường không chỉ bảo vệ tài sản của khách hàng mà còn duy trì uy tín của các tổ chức tài chính. Nghiên cứu này tập trung vào việc ứng dụng thuật toán Máy vectơ hỗ trợ (SVM) – một phương pháp học máy giám sát mạnh mẽ trong việc phân loại nhị phân trên không gian đa chiều. Mục tiêu cốt lõi là xây dựng một ""siêu mặt phẳng"" tối ưu để phân tách rạch ròi giữa giao dịch hợp lệ và giao dịch gian lận."
1	Bằng cách sử dụng các hàm nhân (kernel) phức tạp, mô hình có khả năng xử lý các dữ liệu phi tuyến tính, vốn là đặc trưng của các hành vi gian lận ngày càng tinh vi và biến hóa liên tục trong thực tế. Chúng tôi sử dụng tập dữ liệu giao dịch thẻ tín dụng từ châu Âu với hơn 284.807 bản ghi, trong đó chỉ có 492 giao dịch gian lận (tỷ lệ cực thấp gây mất cân bằng dữ liệu nghiêm trọng). Để giải quyết vấn đề này, nghiên cứu áp dụng kỹ thuật kết hợp giữa lấy mẫu quá mức SMOTE và thuật toán SVM với hàm nhân RBF (Radial Basis Function).
1	Quá trình tối ưu hóa các siêu tham số C và gamma được thực hiện thông qua phương pháp tìm kiếm lưới (Grid Search) cùng với kiểm tra chéo 5 lớp. Việc chuẩn hóa dữ liệu thông qua RobustScaler là bắt buộc để giảm thiểu tác động của các giá trị ngoại lai thường xuất hiện trong các thuộc tính giao dịch. Mô hình được huấn luyện trên hệ thống máy chủ hiệu năng cao để đảm bảo thời gian xử lý các ma trận khoảng cách lớn trong không gian đặc trưng của SVM. Kết quả thực nghiệm cho thấy mô hình SVM đạt độ chính xác (Accuracy) lên tới 99,94%.
1	Tuy nhiên, do tính chất mất cân bằng của dữ liệu, chúng tôi tập trung phân tích chỉ số Recall (độ nhạy) đối với lớp gian lận, đạt mức 92,1%. Chỉ số Precision (độ chính xác dự báo) đạt 88,5%, giúp giảm thiểu tối đa các trường hợp báo động giả gây phiền hà cho khách hàng. So sánh với thuật toán Cây quyết định (Decision Tree) có độ chính xác chỉ đạt 98,2% và Recall 85,6%, mô hình SVM thể hiện sự vượt trội rõ rệt trong việc xác định các mẫu gian lận ẩn sâu. Phân tích chi phí cho thấy việc áp dụng mô hình này có thể giúp ngân hàng tiết kiệm được khoảng 1,2 triệu USD thiệt hại do gian lận mỗi quý. Điều này khẳng định SVM là công cụ phòng thủ lớp đầu hiệu quả cho các hệ thống thanh toán điện tử hiện đại.
1	Việc dự báo chính xác nhu cầu tiêu thụ điện năng (phụ tải điện) là yếu tố sống còn để vận hành hệ thống điện an toàn và kinh tế, đặc biệt là trong các lưới điện thông minh (Smart Grid). Sự biến động của phụ tải chịu ảnh hưởng bởi nhiều yếu tố phức tạp như thời tiết, chu kỳ kinh tế và hành vi sinh hoạt của cư dân. Các mô hình thống kê truyền thống thường thất bại trong việc ghi nhớ các phụ thuộc dài hạn trong chuỗi thời gian. Nghiên cứu này đề xuất sử dụng mạng Long Short-Term Memory (LSTM) – một dạng cải tiến của mạng nơ-ron hồi quy (RNN) có khả năng giải quyết vấn đề mất mát đạo hàm.
1	Mục tiêu là dự báo phụ tải điện theo từng giờ cho khu vực đô thị, giúp các nhà quản lý tối ưu hóa kế hoạch huy động nguồn điện và giảm thiểu lãng phí năng lượng không cần thiết. Dữ liệu đầu vào được thu thập từ lưới điện thành phố trong giai đoạn 2020-2024, bao gồm nồng độ tiêu thụ (MW), nhiệt độ, độ ẩm và các biến chỉ thị ngày lễ. Chúng tôi thực hiện bước tiền xử lý bằng cách loại bỏ nhiễu qua bộ lọc Kalman và chuẩn hóa dữ liệu về khoảng [0, 1]. Kiến trúc LSTM đề xuất bao gồm 2 lớp ẩn với mỗi lớp có 128 đơn vị nhớ, kết hợp với các lớp Dropout (tỷ lệ 0.2) để chống quá khớp.
1	Cửa sổ thời gian (look-back window) được thiết lập là 24 giờ để mô hình có thể học được đặc tính chu kỳ ngày đêm của phụ tải. Thuật toán tối ưu hóa RMSprop được lựa chọn để điều chỉnh trọng số với hàm mất mát là Sai số bình quân bình phương (MSE). Quá trình huấn luyện diễn ra trong 100 chu kỳ với kích thước lô (batch size) là 32 để đảm bảo sự ổn định của gradient. Mô hình LSTM đạt sai số phần trăm tuyệt đối trung bình (MAPE) là 2,15% trên tập dữ liệu kiểm thử, thấp hơn đáng kể so với mô hình ARIMA truyền thống (đạt 5,84%).
1	Trong các giai đoạn cao điểm như mùa hè, khi nhiệt độ vượt ngưỡng 38°C, mô hình vẫn duy trì được độ chính xác dự báo trên 96%, cho thấy khả năng thích ứng cực tốt với các biến động thời tiết cực đoan. Phân tích sai số theo giờ cho thấy độ lệch lớn nhất chỉ xảy ra vào các khung giờ chuyển giao đột ngột (như 6h sáng hoặc 18h tối) với mức sai số tối đa không quá 4,5%. Kết quả nghiên cứu này cung cấp một công cụ dự báo có độ tin cậy cao, hỗ trợ đắc lực cho việc tích hợp các nguồn năng lượng tái tạo như điện mặt trời và điện gió vào lưới điện quốc gia, góp phần thúc đẩy lộ trình phát triển năng lượng bền vững.
1	Trong thị trường thương mại điện tử cạnh tranh khốc liệt, việc hiểu rõ hành vi khách hàng để đưa ra các chương trình khuyến mãi phù hợp là chìa khóa để giữ chân người dùng. Thay vì áp dụng một chiến dịch đại trà cho tất cả mọi người, các doanh nghiệp cần phân loại khách hàng thành các nhóm có đặc điểm tương đồng. Nghiên cứu này ứng dụng thuật toán K-Means – một phương pháp học máy không giám sát phổ biến nhất để phân cụm dữ liệu. Bằng cách phân tích các thuộc tính như tần suất mua hàng, giá trị đơn hàng trung bình và thời gian truy cập, doanh nghiệp có thể xác định được nhóm khách hàng trung thành, nhóm khách hàng tiềm năng và nhóm có nguy cơ rời bỏ,
1	"từ đó tối ưu hóa ngân sách marketing và nâng cao trải nghiệm mua sắm cá nhân cho từng đối tượng cụ thể. Nghiên cứu sử dụng mô hình RFM (Recency, Frequency, Monetary) làm nền tảng đặc trưng cho thuật toán K-Means trên tập dữ liệu gồm 50.000 khách hàng của một nền tảng bán lẻ trực tuyến. Bước đầu tiên là xử lý các giá trị khuyết thiếu và áp dụng thang đo chuẩn hóa Z-score để đưa các biến RFM về cùng một đơn vị đo lường. Để xác định số lượng cụm K tối ưu, chúng tôi sử dụng phương pháp ""khuỷu tay"" (Elbow Method) kết hợp với phân tích hệ số Silhouette. Kết quả phân tích cho thấy K=5 là số lượng nhóm khách hàng phản ánh rõ nét nhất cấu trúc của thị trường."
1	"Thuật toán sau đó được khởi chạy với 50 lần lặp lại (n-init) để tránh việc rơi vào các cực trị địa phương, đảm bảo các tâm cụm (centroids) được xác định ở vị trí ổn định và có ý nghĩa thống kê cao nhất. Phân tích sau phân cụm cho thấy 5 nhóm khách hàng rõ rệt: Nhóm ""Kim cương"" (chiếm 12%) có chi tiêu cao nhất, nhóm ""Tiềm năng"" (25%), nhóm ""Mới"" (18%), nhóm ""Cần chú ý"" (30%) và nhóm ""Nguy cơ"" (15%). Hệ số Silhouette trung bình đạt 0,68, xác nhận sự phân tách tốt giữa các cụm."
1	"Số liệu cho thấy giá trị vòng đời khách hàng (CLV) của nhóm ""Kim cương"" cao gấp 8,5 lần so với trung bình, nhưng chi phí giữ chân lại thấp hơn 30%. Dựa trên kết quả này, chúng tôi đề xuất chiến lược marketing tập trung: tặng mã giảm giá sâu cho nhóm ""Nguy cơ"" để kích cầu và áp dụng đặc quyền VIP cho nhóm ""Kim cương"". Thực nghiệm triển khai thực tế trên một nhóm mẫu nhỏ cho thấy tỷ lệ chuyển đổi đơn hàng tăng thêm 14,2% và mức độ hài lòng của khách hàng cải thiện đáng kể sau 3 tháng áp dụng mô hình phân cụm này."
1	Nghiên cứu này trình bày một hệ thống chẩn đoán ung thư phổi tự động sử dụng mạng nơ-ron tích chập (Convolutional Neural Network - CNN) trên tập dữ liệu 15.247 hình ảnh X-quang ngực được thu thập từ bệnh viện Bạch Mai và bệnh viện K trong giai đoạn 2021-2024. Mô hình CNN được thiết kế với kiến trúc 8 lớp tích chập đạt độ chính xác 94.7% trong việc phân loại các tổn thương ác tính, vượt trội so với phương pháp chẩn đoán truyền thống. Kết quả cho thấy độ nhạy (sensitivity) đạt 96.2%, độ đặc hiệu (specificity) đạt 93.5% và giá trị AUC-ROC là 0.968. Thời gian xử lý trung bình cho mỗi hình ảnh chỉ 1.3 giây, giảm 87% so với thời gian đọc phim của bác sĩ chuyên khoa.
1	Nghiên cứu mở ra tiềm năng ứng dụng trí tuệ nhân tạo trong sàng lọc ung thư phổi quy mô lớn tại Việt Nam. Ung thư phổi là một trong những nguyên nhân hàng đầu gây tử vong do ung thư trên toàn cầu với khoảng 1.8 triệu ca tử vong mỗi năm theo thống kê của Tổ chức Y tế Thế giới năm 2023. Tại Việt Nam, theo số liệu từ Bộ Y tế, mỗi năm có khoảng 23.667 ca mắc mới và 20.710 ca tử vong do ung thư phổi. Tỷ lệ sống sót sau 5 năm chỉ đạt 18.6% do phần lớn bệnh nhân được phát hiện ở giai đoạn muộn.
1	Chẩn đoán sớm là yếu tố then chốt giúp tăng tỷ lệ sống sót lên 56% khi phát hiện ở giai đoạn I. Tuy nhiên, việc đọc và phân tích hình ảnh X-quang ngực đòi hỏi chuyên môn cao và tốn thời gian, đặc biệt tại các vùng thiếu bác sĩ chuyên khoa. Với sự phát triển của deep learning, CNN đã chứng minh hiệu quả vượt trội trong phân tích hình ảnh y khoa, đạt độ chính xác tương đương hoặc vượt bác sĩ chuyên khoa trong nhiều nghiên cứu quốc tế. Nghiên cứu sử dụng tập dữ liệu gồm 15.247 hình ảnh X-quang ngực kỹ thuật số được thu thập từ 8.923 bệnh nhân tại bệnh viện Bạch Mai (7.845 ảnh) và bệnh viện K (7.402 ảnh) từ tháng 01/2021 đến tháng 12/2024.
1	Dữ liệu được phân loại thành 4 nhóm: bình thường (5.127 ảnh), ung thư phổi giai đoạn sớm (3.456 ảnh), ung thư phổi giai đoạn muộn (4.231 ảnh), và các bệnh lý phổi khác (2.433 ảnh). Tất cả hình ảnh được chuyên gia có trên 15 năm kinh nghiệm gán nhãn và xác thực bằng kết quả sinh thiết hoặc CT scan. Hình ảnh có độ phân giải từ 1024x1024 đến 2048x2048 pixels, được chuẩn hóa về 512x512 pixels để đưa vào mô hình. Tập dữ liệu được chia theo tỷ lệ 70% training (10.673 ảnh), 15% validation (2.287 ảnh) và 15% testing (2.287 ảnh).
1	Mô hình CNN được thiết kế với kiến trúc gồm 8 lớp tích chập (convolutional layers), mỗi lớp sử dụng bộ lọc kích thước 3x3 với số lượng filter tăng dần từ 32, 64, 128, 256, đến 512. Sau mỗi lớp tích chập là lớp batch normalization để chuẩn hóa dữ liệu và lớp max pooling 2x2 để giảm kích thước. Hàm kích hoạt ReLU được áp dụng để tăng tính phi tuyến. Phần fully connected gồm 3 lớp dense với 1024, 512 và 256 neurons, sử dụng dropout 0.5 để tránh overfitting. Lớp output có 4 neurons với hàm softmax cho bài toán phân loại đa lớp. Tổng số tham số của mô hình là 47.3 triệu. Mô hình được huấn luyện trên GPU NVIDIA RTX 4090 với bộ nhớ 24GB trong 150 epochs, mỗi epoch mất khoảng 23 phút.
1	Optimizer Adam được sử dụng với learning rate ban đầu 0.0001 và giảm dần theo cosine annealing. Để tăng khả năng tổng quát hóa của mô hình và giảm overfitting, chúng tôi áp dụng các kỹ thuật data augmentation bao gồm: xoay ngẫu nhiên từ -15° đến +15° (probability 0.6), lật ngang (probability 0.5), điều chỉnh độ sáng từ 0.8 đến 1.2 (probability 0.4), điều chỉnh độ tương phản từ 0.8 đến 1.2 (probability 0.4), zoom từ 0.9 đến 1.1 (probability 0.3), và thêm nhiễu Gaussian với standard deviation 0.01 (probability 0.2). Các phép biến đổi này được áp dụng ngẫu nhiên trong quá trình training. Kỹ thuật mixup với alpha 0.2 cũng được sử dụng để tạo ra các mẫu kết hợp giữa hai ảnh khác nhau. Điều này giúp mô hình học được các đặc trưng bền vững hơn và giảm 23.4% validation loss so với không sử dụng augmentation.
1	Mô hình CNN đạt độ chính xác tổng thể 94.7% trên tập test, trong đó độ chính xác cho từng lớp như sau: bình thường 96.3%, ung thư giai đoạn sớm 91.8%, ung thư giai đoạn muộn 97.2%, và bệnh lý khác 93.4%. Độ nhạy (sensitivity/recall) đạt 96.2% cho nhóm ung thư, cho thấy khả năng phát hiện tốt các trường hợp dương tính. Độ đặc hiệu (specificity) đạt 93.5%, chứng tỏ mô hình ít dương tính giả. Precision đạt 94.1%, F1-score đạt 95.1% và Matthews Correlation Coefficient (MCC) đạt 0.923. Diện tích dưới đường cong ROC (AUC-ROC) đạt 0.968, thể hiện khả năng phân biệt tuyệt vời giữa các lớp. So sánh với 5 bác sĩ chuyên khoa phổi, mô hình có độ chính xác cao hơn 2.3% và thời gian xử lý nhanh hơn 87%, từ 10.2 phút xuống còn 1.3 giây mỗi ảnh.
1	Ma  trận nhầm lẫn cho thấy trong 2.287 mẫu test, mô hình phân loại đúng 2.166 mẫu và nhầm lẫn 121 mẫu. Cụ thể, trong 769 ảnh bình thường, 741 được phân loại đúng, 15 bị nhầm với ung thư giai đoạn sớm, 3 với ung thư muộn, và 10 với bệnh lý khác. Trong 519 ảnh ung thư giai đoạn sớm, 476 đúng, 23 bị nhầm với bình thường, 11 với ung thư muộn, và 9 với bệnh lý khác. Đây là nhóm có tỷ lệ sai lệch cao nhất do đặc điểm tổn thương chưa rõ ràng. Trong 634 ảnh ung thư muộn, 616 đúng, chỉ 7 bị nhầm với giai đoạn sớm, 4 với bình thường và 7 với bệnh lý khác.
1	Cuối cùng, trong 365 ảnh bệnh lý khác, 341 đúng, còn lại phân bố đều cho các nhóm khác. Tỷ lệ false negative nguy hiểm (bỏ sót ung thư) chỉ 3.8%. Sử dụng kỹ thuật Grad-CAM (Gradient-weighted Class Activation Mapping), chúng tôi trực quan hóa các vùng mà mô hình tập trung khi đưa ra quyết định. Phân tích 500 ảnh ngẫu nhiên cho thấy mô hình tập trung vào các đặc trưng y khoa quan trọng: khối u hoặc nốt bất thường (87.3% trường hợp ung thư), vùng đậm mật độ không đồng nhất (76.5%), đường viền không đều (68.2%), và vị trí gần rốn phổi (54.7%). Đối với ung thư giai đoạn muộn, mô hình chú ý đến tràn dịch màng phổi (91.2%), xẹp phổi (73.4%), và di căn hạch trung thất (62.8%).
1	Heatmap cho thấy mô hình không chỉ nhìn vào một điểm mà phân tích toàn bộ ngữ cảnh xung quanh, tương tự cách bác sĩ chẩn đoán. Điều này chứng minh mô hình học được các pattern y khoa có ý nghĩa thay vì chỉ ghi nhớ dữ liệu training. Chúng tôi so sánh mô hình CNN tùy chỉnh với các kiến trúc tiên tiến khác trên cùng tập dữ liệu. ResNet-50 đạt 92.3% accuracy, VGG-16 đạt 89.7%, DenseNet-121 đạt 93.1%, EfficientNet-B3 đạt 93.8%, và Vision Transformer (ViT) đạt 91.5%. Mô hình của chúng tôi đạt 94.7%, cao nhất trong tất cả. Về tốc độ inference, mô hình chúng tôi xử lý 1 ảnh trong 1.3 giây, nhanh hơn ResNet-50 (1.8s), DenseNet-121 (2.1s) và ViT (3.4s), chỉ chậm hơn VGG-16 (1.1s) nhưng chính xác hơn nhiều.
1	Về kích thước, mô hình chúng tôi có 47.3M parameters, nhỏ hơn ResNet-50 (25.6M) nhưng lớn hơn EfficientNet-B3 (12.2M). Trade-off giữa accuracy, speed và size cho thấy mô hình phù hợp cho triển khai thực tế tại bệnh viện. Kết quả nghiên cứu cho thấy hệ thống CNN có thể hỗ trợ đắc lực cho bác sĩ trong việc sàng lọc và chẩn đoán ung thư phổi. Với độ nhạy 96.2%, hệ thống có khả năng phát hiện 962 trong 1000 ca ung thư thực sự, giảm thiểu nguy cơ bỏ sót chỉ còn 3.8%. Điều này đặc biệt quan trọng vì ung thư phổi giai đoạn sớm thường không có triệu chứng rõ ràng và dễ bị bỏ qua trên phim X-quang.
1	"Tại các bệnh viện tuyến huyện thiếu bác sĩ chuyên khoa, hệ thống có thể đóng vai trò ""second opinion"" để gắn cờ các ca nghi ngờ cần chuyển tuyến. Thời gian xử lý chỉ 1.3 giây cho phép sàng lọc hàng trăm phim mỗi ngày, tăng 87% năng suất so với đọc thủ công. Chi phí triển khai ước tính 45 triệu đồng cho phần cứng và 15 triệu/năm bảo trì, thấp hơn nhiều so với tuyển thêm bác sĩ chuyên khoa. Nghiên cứu còn một số hạn chế cần khắc phục trong tương lai. Thứ nhất, tập dữ liệu chủ yếu từ hai bệnh viện lớn tại Hà Nội, chưa đại diện cho đa dạng dân số và thiết bị X-quang khác nhau trên toàn quốc."
1	Thứ hai, mô hình chưa được đánh giá trên các subtype ung thư phổi cụ thể như adenocarcinoma, squamous cell carcinoma hay small cell lung cancer, mỗi loại có đặc điểm hình ảnh riêng. Thứ ba, độ chính xác 91.8% cho ung thư giai đoạn sớm vẫn thấp hơn giai đoạn muộn (97.2%), cho thấy mô hình gặp khó khăn với các tổn thương nhỏ và mờ nhạt. Thứ tư, chúng tôi chưa kiểm tra khả năng tổng quát hóa trên dữ liệu từ các máy X-quang khác nhau, có thể ảnh hưởng hiệu suất. Cuối cùng, nghiên cứu thiếu đánh giá prospective trong môi trường lâm sàng thực tế để xác định impact lên quyết định điều trị.
1	Để nâng cao hiệu suất và khả năng ứng dụng, chúng tôi đề xuất các hướng nghiên cứu tiếp theo. Thứ nhất, mở rộng tập dữ liệu lên 50.000+ ảnh từ nhiều tỉnh thành và thiết bị khác nhau, đặc biệt bổ sung thêm ca ung thư giai đoạn rất sớm (stage IA). Thứ hai, phát triển mô hình multi-task learning để đồng thời phân loại loại ung thư, giai đoạn, và vị trí tổn thương. Thứ ba, tích hợp thêm dữ liệu lâm sàng như tuổi, giới tính, tiền sử hút thuốc, triệu chứng để tăng độ chính xác dự đoán nguy cơ. Thứ tư, nghiên cứu federated learning để huấn luyện mô hình trên dữ liệu từ nhiều bệnh viện mà không cần chia sẻ dữ liệu gốc, bảo vệ quyền riêng tư bệnh nhân.
1	Thứ năm, phát triển ứng dụng mobile giúp bác sĩ có thể sử dụng mọi lúc mọi nơi. Cuối cùng, tiến hành thử nghiệm lâm sàng đa trung tâm để đánh giá tác động thực tế. Nghiên cứu đã phát triển thành công hệ thống chẩn đoán ung thư phổi sử dụng CNN với độ chính xác 94.7%, độ nhạy 96.2% và thời gian xử lý chỉ 1.3 giây trên tập dữ liệu 15.247 hình ảnh X-quang từ bệnh viện Việt Nam. Kết quả vượt trội so với các mô hình deep learning khác và có độ chính xác cao hơn 2.3% so với bác sĩ chuyên khoa trong điều kiện thử nghiệm.
1	Hệ thống có tiềm năng lớn trong việc hỗ trợ sàng lọc ung thư phổi quy mô lớn, đặc biệt tại các vùng thiếu chuyên gia, góp phần phát hiện sớm và cải thiện tỷ lệ sống sót cho bệnh nhân. Tuy nhiên, cần nghiên cứu thêm để nâng cao độ chính xác cho ung thư giai đoạn sớm và đánh giá hiệu quả trong môi trường lâm sàng thực tế. Với sự phát triển không ngừng của AI, chúng tôi tin rằng trong 3-5 năm tới, các hệ thống hỗ trợ chẩn đoán tự động sẽ trở thành công cụ không thể thiếu trong y học hiện đại.
1	Nghiên cứu này áp dụng thuật toán K-Nearest Neighbors (KNN) để xây dựng mô hình dự đoán nguy cơ mắc bệnh tiểu đường typ 2 dựa trên 11 chỉ số sinh hóa và nhân trắc học từ 8.642 người tham gia khám sức khỏe định kỳ tại 12 bệnh viện và trung tâm y tế ở TP.HCM trong giai đoạn 2022-2024. Mô hình KNN với k=7 đạt độ chính xác 89.3%, độ nhạy 87.6%, độ đặc hiệu 90.2%, và AUC 0.912 trên tập kiểm tra. Thời gian dự đoán cho mỗi cá nhân chỉ 0.08 giây, cho phép ứng dụng thực tế trong khám sàng lọc. Phân tích feature importance cho thấy HbA1c (24.7%), glucose lúc đói (21.3%), và BMI (18.9%) là ba yếu tố quan trọng nhất.
1	Hệ thống đã được thử nghiệm tại 3 trạm y tế, giúp phát hiện sớm 347 ca tiền tiểu đường chưa được chẩn đoán, tăng 156% so với phương pháp sàng lọc truyền thống chỉ dựa vào glucose. Bệnh tiểu đường typ 2 là một đại dịch toàn cầu với 537 triệu người mắc bệnh năm 2023 theo Liên đoàn Tiểu đường Quốc tế (IDF), dự kiến tăng lên 783 triệu vào năm 2045. Tại Việt Nam, theo khảo sát quốc gia năm 2023, tỷ lệ mắc tiểu đường ở người trưởng thành là 6.7% (khoảng 5.2 triệu người), trong đó 52.3% chưa được chẩn đoán.
1	Bệnh tiểu đường gây ra nhiều biến chứng nghiêm trọng như bệnh tim mạch (tăng nguy cơ 2-4 lần), suy thận (chiếm 44% ca lọc máu), mù loà (nguyên nhân hàng đầu ở người trưởng thành), và cắt cụt chi (tăng nguy cơ 15-40 lần). Chi phí điều trị trung bình 23.7 triệu đồng/người/năm, gây gánh nặng lớn cho hệ thống y tế. Phát hiện sớm giai đoạn tiền tiểu đường có thể ngăn chặn hoặc trì hoãn tiến triển thành tiểu đường qua can thiệp lối sống, giảm 58% nguy cơ theo nghiên cứu DPP (Diabetes Prevention Program). Tuy nhiên, sàng lọc truyền thống chủ yếu dựa vào glucose máu đói, bỏ sót nhiều ca nguy cơ cao.
1	"Machine learning, đặc biệt là các thuật toán như KNN, mang lại nhiều lợi thế trong dự đoán nguy cơ bệnh so với các phương pháp thống kê truyền thống. KNN là thuật toán supervised learning đơn giản nhưng hiệu quả, dựa trên nguyên tắc ""những người giống nhau có nguy cơ bệnh giống nhau"". Thuật toán phân loại một cá nhân mới bằng cách tìm k người gần nhất trong không gian đa chiều của các chỉ số sinh học, sau đó dự đoán dựa trên đa số nhãn của k láng giềng này. Ưu điểm của KNN là không cần giả định về phân phối dữ liệu, dễ hiểu và giải thích, có thể xử lý biên quyết định phức tạp và phi tuyến."
1	KNN đặc biệt phù hợp với bài toán y tế vì có thể tận dụng tri thức rằng bệnh nhân có profile tương tự thường có kết cục tương tự. Nhiều nghiên cứu quốc tế đã chứng minh KNN đạt accuracy 82-91% trong dự đoán tiểu đường, cạnh tranh với các thuật toán phức tạp hơn như SVM hay Random Forest. Nghiên cứu hồi cứu-tiến cứu được thực hiện trên 8.642 người tham gia khám sức khỏe định kỳ tại 12 bệnh viện và trung tâm y tế ở TP.HCM từ tháng 01/2022 đến tháng 12/2024. Tiêu chí chọn mẫu bao gồm: tuổi từ 30-70 tuổi, không mắc bệnh tiểu đường typ 1, không có bệnh mạn tính nặng (ung thư, suy tim nặng), không mang thai, và đồng ý tham gia.
1	Trong tổng số 8.642 người, có 2.147 người được chẩn đoán tiểu đường typ 2 (24.8%) dựa trên tiêu chuẩn ADA 2024: glucose máu đói ≥126 mg/dL hoặc HbA1c ≥6.5% hoặc glucose sau test dung nạp ≥200 mg/dL hoặc glucose bất kỳ ≥200 mg/dL kèm triệu chứng. Nhóm còn lại 6.495 người khỏe mạnh được theo dõi 24 tháng, trong đó 783 người (12.1%) chuyển sang tiểu đường, được gộp vào nhóm dương tính. Tổng cộng 2.930 ca dương tính (33.9%) và 5.712 ca âm tính (66.1%). Mỗi người tham gia được thu thập 11 biến số bao gồm: tuổi (trung bình 48.3±12.7 tuổi, min 30, max 70), giới tính (45.2% nam), BMI (24.6±4.2 kg/m²,
1	tính từ cân nặng và chiều cao), vòng eo (86.3±11.4 cm), huyết áp tâm thu (124.7±16.3 mmHg), huyết áp tâm trương (78.4±10.2 mmHg), glucose máu đói (98.7±24.3 mg/dL), HbA1c (5.8±1.1%), cholesterol toàn phần (201.4±42.7 mg/dL), triglyceride (152.3±89.6 mg/dL), và HDL-C (47.8±11.2 mg/dL). Tất cả các xét nghiệm được thực hiện tại các phòng xét nghiệm chuẩn ISO 15189 theo quy trình chuẩn. BMI và vòng eo được đo bởi điều dưỡng được đào tạo. Huyết áp được đo 2 lần cách nhau 5 phút và lấy trung bình. Dữ liệu được nhập vào hệ thống điện tử và kiểm tra chất lượng, loại bỏ 237 mẫu có giá trị thiếu hoặc bất thường.
1	Dữ liệu thô được làm sạch và chuẩn hóa qua nhiều bước. Đầu tiên, xử lý outliers bằng phương pháp IQR (Interquartile Range), loại bỏ 143 mẫu có giá trị nằm ngoài khoảng Q1-1.5×IQR và Q3+1.5×IQR. Thứ hai, xử lý missing values bằng phương pháp KNN imputation với k=5, điền giá trị trung bình của 5 mẫu gần nhất cho 94 mẫu còn thiếu. Thứ ba, chuẩn hóa dữ liệu bằng StandardScaler để đưa tất cả biến về cùng thang đo với mean=0 và std=1, điều này rất quan trọng vì KNN nhạy cảm với scale của dữ liệu. Ví dụ, glucose (50-300 mg/dL) và tuổi (30-70) có đơn vị khác nhau sẽ ảnh hưởng khoảng cách. Thứ tư, kiểm tra multicollinearity bằng VIF (Variance Inflation Factor), phát hiện cholesterol và triglyceride có VIF=7.8, nhưng vẫn giữ lại vì dưới ngưỡng 10.
1	Cuối cùng, chia dữ liệu theo tỷ lệ 70% training (6.050 mẫu), 15% validation (1.296 mẫu), và 15% testing (1.296 mẫu), đảm bảo tỷ lệ dương/âm tính cân bằng ở mỗi tập. Việc chọn giá trị k phù hợp là then chốt cho hiệu suất của KNN. Chúng tôi thử nghiệm các giá trị k từ 1 đến 50 và đánh giá trên tập validation. Kết quả cho thấy k=1 cho accuracy 82.4% nhưng bị overfitting nghiêm trọng với độ chênh lệch training-validation accuracy là 15.3%. Khi k tăng, overfitting giảm nhưng accuracy cũng giảm dần. K=3 đạt 86.7%, k=5 đạt 88.2%, k=7 đạt 89.3% (cao nhất), k=9 đạt 89.1%, k=11 đạt 88.6%, và từ k=15 trở đi accuracy giảm xuống dưới 87%.
1	Chúng tôi cũng đánh giá các độ đo khác: AUC-ROC cao nhất tại k=7 (0.912), F1-score cao nhất tại k=7 (0.876), và MCC cao nhất tại k=7 (0.763). Thời gian inference tăng tuyến tính với k: k=3 mất 0.06s, k=7 mất 0.08s, k=15 mất 0.13s. Dựa trên phân tích đa tiêu chí, k=7 được chọn làm giá trị tối ưu, cân bằng giữa accuracy, generalization và tốc độ. KNN sử dụng metric khoảng cách để xác định các láng giềng gần nhất. Chúng tôi so sánh 3 metric phổ biến: Euclidean distance, Manhattan distance, và Minkowski distance. Euclidean distance (L2 norm) tính khoảng cách đường thẳng giữa hai điểm, được định nghĩa là sqrt(Σ(xi-yi)²), phù hợp khi các feature có ý nghĩa liên tục và có thể cộng trừ.
1	Manhattan distance (L1 norm) tính tổng khoảng cách theo từng chiều |xi-yi|, ít nhạy cảm với outliers hơn. Minkowski distance với p=3 là tổng quát hóa của hai metric trên. Kết quả thử nghiệm cho thấy Euclidean đạt accuracy 89.3%, Manhattan đạt 88.7%, và Minkowski(p=3) đạt 88.9%. Euclidean cũng cho AUC cao nhất (0.912 vs 0.905 và 0.908). Phân tích sâu hơn cho thấy Euclidean phù hợp vì các biến sinh hóa như glucose, HbA1c có quan hệ liên tục với nguy cơ tiểu đường. Do đó, Euclidean distance được chọn làm metric chính. Trong 8.642 người tham gia, tuổi trung bình là 48.3±12.7, trong đó nhóm tiểu đường có tuổi cao hơn có ý nghĩa (53.7±11.4 vs 45.6±12.5, p<0.001).
1	Tỷ lệ nam giới trong nhóm tiểu đường cao hơn (52.3% vs 41.7%, p<0.001). BMI trung bình nhóm tiểu đường là 27.8±4.6 so với 23.1±3.4 ở nhóm khỏe (p<0.001), với 67.2% nhóm tiểu đường thừa cân/béo phì (BMI≥23) theo tiêu chuẩn châu Á. Vòng eo trung bình 94.7±10.2 cm ở nhóm tiểu đường so với 82.4±9.8 cm ở nhóm khỏe. Glucose máu đói trung bình 127.3±31.2 vs 86.4±8.7 mg/dL (p<0.001). HbA1c trung bình 7.2±1.3% vs 5.2±0.4% (p<0.001). Huyết áp tâm thu 135.2±18.4 vs 119.3±12.7 mmHg. Triglyceride 198.4±102.3 vs 131.2±68.4 mg/dL. HDL-C thấp hơn: 42.3±9.7 vs 50.4±11.1 mg/dL. Tất cả sự khác biệt đều có ý nghĩa thống kê p<0.001, xác nhận các yếu tố nguy cơ đã biết.
1	Mô hình KNN với k=7 và Euclidean distance đạt các chỉ số sau trên tập test 1.296 mẫu: Accuracy 89.3% (1.158/1.296 mẫu phân loại đúng), Sensitivity/Recall 87.6% (phát hiện đúng 384/438 ca tiểu đường), Specificity 90.2% (phân loại đúng 774/858 ca khỏe mạnh), Precision 82.1% (384/468 ca dự đoán dương tính là đúng), F1-score 84.7%, NPV (Negative Predictive Value) 93.5% (774/828 ca dự đoán âm tính là đúng), AUC-ROC 0.912, và MCC 0.763. Tỷ lệ false positive 9.8% (84 ca khỏe bị chẩn đoán nhầm), tỷ lệ false negative 12.4% (54 ca tiểu đường bị bỏ sót). Thời gian inference trung bình 0.08 giây/mẫu trên máy Intel Core i7-12700. Kết quả này vượt trội so với ngưỡng glucose đơn thuần (accuracy 76.4%), chứng tỏ lợi ích của phương pháp đa biến.
1	So với các nghiên cứu quốc tế trên dataset PIMA (accuracy 76-82%), mô hình chúng tôi đạt hiệu suất cao hơn nhờ tập dữ liệu lớn hơn và đa dạng hơn. Sử dụng phương pháp permutation importance, chúng tôi đánh giá đóng góp của từng biến vào hiệu suất mô hình. Khi loại bỏ hoặc hoán vị ngẫu nhiên một biến, độ giảm accuracy phản ánh tầm quan trọng của biến đó. Kết quả xếp hạng như sau: HbA1c quan trọng nhất với importance score 24.7% (khi loại bỏ, accuracy giảm từ 89.3% xuống 67.4%), glucose máu đói đứng thứ 2 với 21.3% (accuracy giảm xuống 70.2%), BMI đứng thứ 3 với 18.9% (accuracy giảm xuống 72.8%), vòng eo 12.4%, tuổi 8.7%, huyết áp tâm thu 5.3%, triglyceride 4.1%, HDL-C 2.8%, huyết áp tâm trương 1.2%, cholesterol 0.4%, và giới tính 0.2%.
1	Kết quả này phù hợp với sinh lý bệnh tiểu đường: HbA1c phản ánh glucose trung bình 3 tháng qua, là chỉ số vàng trong chẩn đoán. BMI và vòng eo liên quan đến kháng insulin. Các biến lipid có vai trò nhỏ hơn nhưng vẫn đóng góp vào dự đoán. Chúng tôi so sánh KNN với 6 thuật toán machine learning khác trên cùng tập dữ liệu. Logistic Regression đạt accuracy 84.7%, sensitivity 81.3%, specificity 86.5%, AUC 0.878, thời gian training 2.3s và inference 0.02s. Decision Tree đạt 82.1%, 78.9%, 83.7%, 0.843, training 5.7s, inference 0.01s. Random Forest đạt 91.2%, 89.4%, 92.1%, 0.937, training 127s, inference 0.34s. SVM (RBF kernel) đạt 88.6%, 86.2%, 89.8%, 0.903, training 213s, inference 0.15s. Naive Bayes đạt 79.3%, 84.7%, 76.4%, 0.861, training 1.1s, inference 0.01s. XGBoost đạt 92.4% (cao nhất), 90.8%, 93.2%, 0.945, training 89s, inference 0.11s.
1	Kết quả cho thấy KNN (89.3%) xếp thứ 3 về accuracy, sau XGBoost và Random Forest, nhưng có thời gian inference nhanh thứ 3 (0.08s) và đơn giản hơn nhiều để triển khai và giải thích. Trade-off này làm KNN phù hợp cho ứng dụng thực tế tại trạm y tế. Chúng tôi phân tích hiệu suất mô hình trên các nhóm con khác nhau. Theo giới tính: nam đạt accuracy 90.1%, nữ đạt 88.7%, sự khác biệt không có ý nghĩa thống kê (p=0.24). Theo tuổi: <40 tuổi đạt 86.2%, 40-50 tuổi đạt 89.7%, 50-60 tuổi đạt 91.3%, >60 tuổi đạt 90.8%, cho thấy mô hình hoạt động tốt hơn ở người trung niên và cao tuổi có nhiều yếu tố nguy cơ rõ ràng hơn.
1	"Theo BMI: gầy (<18.5) đạt 82.4%, bình thường (18.5-22.9) đạt 87.3%, thừa cân (23-24.9) đạt 90.2%, béo phì (≥25) đạt 92.1%, xác nhận BMI cao làm tăng độ chính xác dự đoán. Theo HbA1c: <5.7% đạt 85.4%, 5.7-6.4% (tiền tiểu đường) đạt 93.7%, ≥6.5% đạt 96.8%. Mô hình hoạt động tốt nhất ở nhóm có nhiều yếu tố nguy cơ, và kém hơn ở nhóm khỏe mạnh hoàn toàn do ít tín hiệu để phân biệt. KNN mang lại nhiều lợi thế trong bài toán dự đoán tiểu đường. Thứ nhất, tính đơn giản và dễ hiểu: thuật toán dựa trên nguyên tắc trực quan ""người giống nhau có nguy cơ giống nhau"", dễ giải thích cho nhân viên y tế và bệnh nhân."
1	Thứ hai, không cần giả định phân phối: không như logistic regression yêu cầu linearity, KNN hoạt động tốt với mối quan hệ phi tuyến phức tạp giữa các biến. Thứ ba, khả năng xử lý biên quyết định phức tạp: KNN tạo ra biên mềm (soft boundary) thay vì cứng, phù hợp với thực tế y học nơi ngưỡng bệnh/không bệnh không rõ ràng. Thứ tư, cập nhật dễ dàng: khi có dữ liệu mới, chỉ cần thêm vào training set mà không cần huấn luyện lại toàn bộ như neural network. Thứ năm, hiệu suất tốt: accuracy 89.3% cao hơn hầu hết phương pháp truyền thống. Cuối cùng, tốc độ inference 0.08s đủ nhanh cho ứng dụng real-time tại phòng khám.
1	Từ tháng 7-12/2024, chúng tôi triển khai pilot hệ thống KNN tại 3 trạm y tế phường ở quận 1, 7, và Thủ Đức, TP.HCM. Hệ thống được cài trên máy tính với giao diện đơn giản: nhân viên y tế nhập 11 chỉ số, hệ thống trả về dự đoán ngay lập tức kèm mức độ nguy cơ (thấp/trung bình/cao) và khuyến nghị (theo dõi/xét nghiệm thêm/chuyển tuyến). Trong 6 tháng, 2.341 người tham gia khám sàng lọc. Hệ thống phát hiện 547 ca nguy cơ cao (23.4%), trong đó 347 ca được xác nhận tiền tiểu đường hoặc tiểu đường qua xét nghiệm OGTT (positive predictive value 63.4%). So với phương pháp truyền thống chỉ dựa vào glucose đói, phát hiện được 156 ca (tăng 156%).
1	Nhân viên y tế đánh giá hệ thống dễ sử dụng (9.2/10 điểm), nhanh (8.7/10), và hữu ích (9.4/10). Không có ca nào bị bỏ sót nghiêm trọng. Chi phí triển khai chỉ 8.5 triệu đồng/trạm (máy tính + phần mềm + đào tạo), tiết kiệm 73% so với mua máy xét nghiệm OGTT. Nghiên cứu có một số hạn chế cần lưu ý. Thứ nhất, KNN yêu cầu lưu trữ toàn bộ training set (6.050 mẫu × 11 biến = 66.550 giá trị) để tính khoảng cách, tốn bộ nhớ hơn các mô hình parametric như logistic regression chỉ lưu vài hệ số.
1	"Tuy nhiên, với bộ nhớ hiện đại, đây không phải vấn đề lớn. Thứ hai, tốc độ inference chậm hơn các mô hình đã học (0.08s vs 0.02s của logistic regression) vì phải tính khoảng cách tới tất cả training samples. Khi tập dữ liệu tăng lên 100.000 mẫu, thời gian có thể tăng lên vài giây, cần tối ưu bằng KD-tree hoặc Ball-tree. Thứ ba, KNN nhạy cảm với imbalanced data, trong nghiên cứu này tỷ lệ 34:66 còn chấp nhận được nhưng nếu <10% sẽ cần SMOTE hoặc class weight. Thứ tư, curse of dimensionality: khi số chiều tăng cao (>50), khái niệm ""gần"" mất ý nghĩa. Thứ năm, dữ liệu chỉ từ TP.HCM, chưa đại diện cho vùng nông thôn hoặc dân tộc thiểu số."
1	Kết quả nghiên cứu có ý nghĩa quan trọng cho công tác phòng chống tiểu đường tại Việt Nam. Hệ thống KNN cho phép sàng lọc nguy cơ tiểu đường hiệu quả tại tuyến cơ sở với chi phí thấp, không cần xét nghiệm đắt tiền như OGTT (150.000đ) cho mọi người. Việc phát hiện sớm 347 ca tiền tiểu đường trong pilot 6 tháng cho thấy tiềm năng phát hiện hàng chục nghìn ca mỗi năm nếu triển khai toàn TP.HCM với 7 triệu dân. Can thiệp sớm bằng tư vấn dinh dưỡng, tập luyện, giảm cân có thể ngăn 58% trường hợp tiến triển thành tiểu đường, tiết kiệm hàng nghìn tỷ đồng chi phí điều trị. Hệ thống cũng giúp phân tầng nguy cơ, tập trung nguồn lực vào nhóm nguy cơ cao thay vì sàng lọc bừa bãi.
1	Dữ liệu thu thập được có thể dùng để nghiên cứu dịch tễ học, xác định yếu tố nguy cơ riêng của người Việt, và hoạch định chính sách y tế dựa trên bằng chứng. Để nâng cao hiệu quả, chúng tôi đề xuất các cải tiến sau. Thứ nhất, mở rộng tập dữ liệu lên 50.000+ mẫu từ nhiều tỉnh thành, bao gồm vùng nông thôn và miền núi để tăng tính đại diện. Thứ hai, bổ sung thêm biến số như tiền sử gia đình, chế độ ăn (calories, carb, fiber), hoạt động thể lực (MET-minutes/tuần), giờ ngủ, stress, để tăng độ chính xác lên 92-95%.
1	Thứ ba, kết hợp KNN với ensemble methods: dùng KNN làm base learner trong Random Forest hoặc Gradient Boosting để tận dụng ưu điểm của nhiều thuật toán. Thứ tư, phát triển ứng dụng mobile cho phép người dân tự đánh giá nguy cơ tại nhà, kết nối với bác sĩ gia đình qua telemedicine. Thứ năm, tích hợp dữ liệu gen (SNPs liên quan insulin resistance) và microbiome để dự đoán chính xác hơn. Thứ sáu, nghiên cứu longitudinal theo dõi 5-10 năm để đánh giá khả năng dự đoán biến chứng tim mạch, thận, mắt ở bệnh nhân tiểu đường.
1	Nghiên cứu đã chứng minh hiệu quả của thuật toán KNN trong dự đoán nguy cơ tiểu đường typ 2 với độ chính xác 89.3%, độ nhạy 87.6%, độ đặc hiệu 90.2%, và AUC 0.912 trên 8.642 người Việt Nam. Mô hình sử dụng 11 chỉ số sinh hóa và nhân trắc dễ thu thập, với HbA1c, glucose máu đói, và BMI là ba yếu tố quan trọng nhất. Thử nghiệm thực địa tại 3 trạm y tế đã phát hiện thêm 347 ca tiền tiểu đường trong 6 tháng, tăng 156% so với phương pháp truyền thống. Chi phí triển khai thấp (8.5 triệu/trạm) và dễ sử dụng làm KNN phù hợp cho sàng lọc quy mô lớn tại tuyến cơ sở. Phát hiện sớm và can thiệp kịp thời có thể giảm gánh nặng bệnh tật và chi phí y tế đáng kể.
1	Nghiên cứu này phát triển hệ thống hybrid kết hợp Random Forest và Convolutional Neural Network (CNN) để phát hiện và phân loại sâu răng từ hình ảnh X-quang panoramic nha khoa. Tập dữ liệu gồm 12.834 hình ảnh panoramic từ 9.276 bệnh nhân tại 15 phòng khám nha khoa ở Hà Nội, TP.HCM, và Đà Nẵng trong giai đoạn 2021-2024, chứa tổng cộng 387.412 răng được gán nhãn chi tiết. CNN (kiến trúc U-Net) được sử dụng để phân đoạn (segmentation) từng răng và vùng sâu, sau đó Random Forest với 500 decision trees phân loại mức độ sâu thành 5 cấp theo tiêu chuẩn ICDAS. Hệ thống đạt F1-score 91.7% cho task phát hiện sâu răng, accuracy 88.4% cho phân loại mức độ, và IoU (Intersection over Union) 0.843 cho segmentation.
1	Thời gian xử lý một hình ảnh panoramic trung bình 3.7 giây, nhanh hơn 94% so với nha sĩ (63 giây). Triển khai pilot tại 5 phòng khám giúp phát hiện thêm 423 răng sâu bị bỏ sót (8.2% tổng số răng khám), đặc biệt là sâu gian răng và sâu dưới nướu khó quan sát bằng mắt thường. Sâu răng (dental caries) là bệnh lý răng miệng phổ biến nhất trên toàn cầu, ảnh hưởng đến 2.3 tỷ người (35% dân số) theo WHO 2023. Tại Việt Nam, theo khảo sát sức khỏe răng miệng quốc gia năm 2023, tỷ lệ sâu răng ở người trưởng thành là 79.2%, ở trẻ em 5 tuổi là 73.4%, và ở người cao tuổi lên tới 94.7%.
1	Chỉ số DMFT (Decayed, Missing, Filled Teeth) trung bình ở người trưởng thành là 4.7, cao hơn khuyến nghị của WHO (<3). Sâu răng không được điều trị kịp thời dẫn đến đau, nhiễm trúng, mất răng, ảnh hưởng dinh dưỡng, chất lượng cuộc sống, và tốn khoảng 6.8 triệu đồng/răng cho điều trị tủy và phục hồi. Chi phí điều trị sâu răng toàn quốc ước tính 47.000 tỷ đồng/năm. Phát hiện sâu răng sớm giai đoạn ban đầu cho phép can thiệp đơn giản bằng fluoride hoặc trám nhỏ, chi phí chỉ 150.000-300.000đ, giảm 95% so với điều trị muộn. Tuy nhiên, việc phát hiện sâu răng phụ thuộc vào kinh nghiệm nha sĩ và điều kiện chiếu sáng, dẫn đến sai sót 12-28% theo các nghiên cứu.
1	X-quang nha khoa, đặc biệt là phim panoramic (chụp toàn cảnh), cho phép quan sát tất cả 32 răng, xương hàm, khớp thái dương hàm, và xoang hàm trong một ảnh duy nhất. Panoramic giúp phát hiện sâu gian răng (interproximal caries), sâu dưới nướu, sâu tái phát dưới trám cũ, và sâu chân răng mà khám lâm sàng khó thấy. Tuy nhiên, việc đọc phim X-quang tốn thời gian (trung bình 63 giây/phim) và đòi hỏi chuyên môn cao, đặc biệt khó phân biệt sâu giai đoạn sớm (chỉ mất khoáng men) với giai đoạn muộn (đã vào ngà hoặc tủy). Sự phát triển của machine learning và computer vision mở ra cơ hội tự động hóa quy trình này.
1	Random Forest phù hợp để phân loại dựa trên features được trích xuất từ ảnh, có khả năng xử lý nhiều đặc trưng phức tạp và cho kết quả ổn định. Kết hợp với deep learning cho segmentation, hệ thống có thể đạt độ chính xác cao hơn và giảm thiểu sai số của con người. Nghiên cứu sử dụng 12.834 hình ảnh X-quang panoramic số hóa từ 9.276 bệnh nhân (một số bệnh nhân có nhiều lần chụp trong quá trình điều trị) tại 15 phòng khám nha khoa ở ba thành phố lớn trong giai đoạn 2021-2024. Hình ảnh được chụp bằng 4 loại máy X-quang khác nhau: Planmeca ProMax (6.247 ảnh), Carestream CS 8100 (3.891 ảnh), Vatech PaX-i (2.156 ảnh), và Sirona Orthophos (540 ảnh). Độ phân giải dao động từ 2000×1000 đến 3000×1500 pixels.
1	Mỗi hình ảnh chứa trung bình 30.2 răng (một số bệnh nhân đã mất răng), tổng cộng 387.412 răng. Ba nha sĩ có trên 12 năm kinh nghiệm đã gán nhãn mỗi răng với các thông tin: vị trí răng (FDI numbering), tình trạng (khỏe/sâu/trám/mất), mức độ sâu theo ICDAS (0: khỏe, 1-2: sâu men, 3-4: sâu ngà nông, 5-6: sâu ngà sâu/tủy), và bounding box + segmentation mask cho vùng sâu. Kappa agreement giữa 3 nha sĩ là 0.847, cho thấy độ tin cậy cao. Dữ liệu được chia 70% training (8.984 ảnh, 271.188 răng), 15% validation (1.925 ảnh, 58.163 răng), và 15% test (1.925 ảnh, 58.061 răng).
1	Hệ thống được thiết kế theo pipeline 3 giai đoạn. Giai đoạn 1 - Tooth Detection: sử dụng YOLOv8 để phát hiện và định vị 32 răng trong ảnh panoramic, đạt mAP (mean Average Precision) 96.3% với IoU threshold 0.5. Mỗi răng được crop thành ảnh nhỏ 128×128 pixels để xử lý độc lập. Giai đoạn 2 - Caries Segmentation: mỗi ảnh răng được đưa vào mô hình U-Net (encoder-decoder CNN) để segmentation vùng sâu ở pixel level. U-Net có 4 lớp encoder với 64-128-256-512 filters và 4 lớp decoder tương ứng, sử dụng skip connections để bảo toàn thông tin chi tiết. Output là mask nhị phân cho vùng sâu, đạt IoU 0.843.
1	Giai đoạn 3 - Severity Classification: từ ảnh răng gốc và segmentation mask, trích xuất 127 features gồm texture (Haralick features: 13), shape (area, perimeter, circularity, solidity: 8), intensity statistics (mean, std, skewness, kurtosis cho các vùng: 24), và deep features (output của lớp pre-softmax của U-Net: 82). Random Forest với 500 trees phân loại mức độ sâu ICDAS 0-6. Mô hình U-Net được huấn luyện riêng trên 271.188 ảnh răng đã crop. Architecture gồm encoder dựa trên VGG16 pretrained trên ImageNet, decoder gồm 4 lớp upsampling với concatenation từ encoder. Loss function là Dice Loss + Binary Cross Entropy với trọng số 0.5:0.5, tối ưu cho segmentation bất cân bằng (vùng sâu chỉ chiếm 3.7% diện tích răng trung bình).
1	Optimizer Adam với learning rate 0.0001, batch size 32, huấn luyện 80 epochs trên 4 GPU V100. Data augmentation bao gồm rotation ±20°, flip, brightness ±0.2, contrast ±0.2, và elastic deformation để mô phỏng biến dạng tự nhiên của răng. Early stopping dựa trên validation Dice score. Mô hình tốt nhất đạt Dice score 0.878, IoU 0.843, precision 89.7%, recall 86.4% trên validation set. Thời gian inference 0.12 giây/răng trên GPU, 0.34 giây trên CPU. Phân tích cho thấy mô hình hoạt động tốt nhất với sâu ngà (IoU 0.891), kém hơn với sâu men sớm (IoU 0.762) do ranh giới mờ.
1	Từ mỗi ảnh răng 128×128 và segmentation mask, chúng tôi trích xuất 127 features. Texture features (13): Haralick features từ Gray Level Co-occurrence Matrix (GLCM) bao gồm contrast, correlation, energy, homogeneity, entropy đo tính chất bề mặt của vùng sâu so với răng khỏe. Shape features (8): area (diện tích vùng sâu), perimeter, circularity (4πA/P²), solidity (A/convex hull area), extent, major/minor axis length. Intensity features (24): mean, std, min, max, median, skewness, kurtosis, percentiles (25th, 75th) tính cho 3 vùng (vùng sâu, biên sâu 5-pixel, răng còn lại). Deep features (82): activation từ bottleneck layer của U-Net, capture high-level semantic information. Features được chuẩn hóa bằng StandardScaler. Random Forest với 500 trees, max_depth=30, min_samples_split=10, min_samples_leaf=4 được huấn luyện trên 271.188 mẫu.
1	Feature importance cho thấy top 5: deep feature 47 (15.3%), mean intensity vùng sâu (12.7%), area vùng sâu (11.2%), Haralick contrast (8.9%), và deep feature 23 (7.4%). Model đạt accuracy 88.4% trên validation set. Hệ thống được đánh giá toàn diện trên test set 1.925 ảnh (58.061 răng). Metrics bao gồm: (1) Detection - mAP, precision, recall cho task phát hiện răng; (2) Segmentation - IoU, Dice score, precision, recall cho từng class sâu; (3) Classification - accuracy, F1-score, confusion matrix cho 7 classes ICDAS. Chúng tôi cũng đánh giá per-tooth performance cho 5 loại răng: cửa (incisors), nanh (canines), tiền hàm (premolars), hàm nhỏ (first molars), hàm lớn (second/third molars). Phân tích error cases cho thấy hệ thống hay nhầm lẫn giữa ICDAS 2 (sâu men sâu) và ICDAS 3 (sâu ngà nông) do ranh giới mỏng giữa men và ngà trên X-quang.
1	Fine-tuning được thực hiện bằng cách tăng trọng số cho class 2 và 3 trong loss function, cải thiện accuracy từ 86.7% lên 88.4%. Chúng tôi cũng thử nghiệm ensemble 3 models U-Net trained trên different data splits, kết hợp bằng voting, cải thiện IoU từ 0.843 lên 0.856 nhưng tăng thời gian inference lên 3×. Trên test set 58.061 răng, hệ thống phát hiện đúng 55.347 răng (95.3%), bỏ sót 1.456 răng (2.5%, chủ yếu răng khôn bị che khuất), và false positive 1.258 răng (2.2%, nhầm tổn thương khác với răng). Overall precision 97.8%, recall 95.3%, F1-score 96.5%. Trong 17.234 răng sâu thật, hệ thống phát hiện đúng 15.803 răng (91.7%), bỏ sót 1.431 răng (8.3%, chủ yếu sâu ICDAS 1-2 giai đoạn rất sớm).
1	False positive 967 răng (5.6%, nhầm trám cũ hoặc tổn thương không sâu với sâu). Precision 94.2%, recall 91.7%, F1-score 92.9% cho detection sâu răng. Về segmentation, IoU trung bình 0.843, Dice score 0.878. Phân tích theo mức độ: ICDAS 1-2 (sâu men) đạt IoU 0.762, ICDAS 3-4 (sâu ngà nông) đạt 0.891, ICDAS 5-6 (sâu ngà sâu) đạt 0.914. Sâu giai đoạn sớm khó phân đoạn hơn do diện tích nhỏ (trung bình 23 pixels) và mờ nhạt. Thời gian xử lý: 0.23s cho detection, 0.34s×30 răng=10.2s cho segmentation, tổng 10.43s, nhưng tối ưu song song giảm xuống 3.7s.
1	Random Forest phân loại 17.234 răng sâu thành 7 classes ICDAS (0-6) với overall accuracy 88.4%, macro F1-score 85.7%, weighted F1-score 88.1%. Chi tiết cho từng class: ICDAS 0 (khỏe, 40.827 răng) - precision 96.7%, recall 97.2%, F1 96.9%; ICDAS 1 (sâu men khô, 3.421 răng) - P 78.3%, R 72.6%, F1 75.3%; ICDAS 2 (sâu men ướt, 4.156 răng) - P 82.1%, R 79.4%, F1 80.7%; ICDAS 3 (sâu ngà nông, 5.347 răng) - P 87.6%, R 88.9%, F1 88.2%; ICDAS 4 (sâu ngà trung bình, 3.124 răng) - P 89.3%, R 90.1%, F1 89.7%; ICDAS 5 (sâu ngà sâu, 978 răng) - P 91.7%, R 88.4%, F1 90.0%; ICDAS 6 (sâu tủy, 208 răng) - P 94.2%, R 86.5%, F1 90.2%.
1	Confusion matrix cho thấy nhầm lẫn chủ yếu giữa các class kề nhau (1↔2: 247 cases, 2↔3: 312 cases, 3↔4: 189 cases), hiếm khi nhầm lẫn cách >1 class (chỉ 34 cases). Kappa agreement với chẩn đoán của nha sĩ là 0.831, tốt hơn kappa giữa 2 nha sĩ khác nhau (0.794). Chúng tôi so sánh hệ thống với 5 approaches khác trên cùng test set. (1) Chỉ CNN end-to-end (ResNet50 phân loại toàn bộ ảnh răng): accuracy 82.3%, F1 79.7%, thời gian 0.18s. Ưu điểm nhanh nhưng kém chính xác và không cho segmentation mask. (2) U-Net segmentation + SVM classification: accuracy 86.1%, F1 83.4%, IoU 0.843, thời gian 0.42s.
1	SVM chậm hơn Random Forest và kém linh hoạt. (3) Faster R-CNN detection + CNN classification: accuracy 85.7%, F1 82.9%, thời gian 0.67s. Detection tốt nhưng classification kém hơn. (4) U-Net + XGBoost: accuracy 89.1% (cao nhất), F1 86.8%, IoU 0.843, thời gian 0.52s. XGBoost mạnh hơn Random Forest một chút nhưng chậm hơn và dễ overfit hơn. (5) U-Net + Random Forest (hệ thống của chúng tôi): accuracy 88.4%, F1 85.7%, IoU 0.843, thời gian 0.39s. Cân bằng tốt giữa accuracy, speed, và interpretability. Random Forest còn cho feature importance giúp hiểu model, trong khi XGBoost là black box hơn.
1	Hiệu suất khác nhau đáng kể giữa các loại răng. Răng cửa (incisors, 12 răng): detection recall 97.8%, segmentation IoU 0.867, classification accuracy 91.2%. Răng cửa dễ phát hiện vì rõ ràng trên ảnh, nhưng ít bị sâu (chỉ 18.7%). Răng nanh (canines, 4 răng): recall 96.4%, IoU 0.854, accuracy 89.7%, tương tự răng cửa. Răng tiền hàm (premolars, 8 răng): recall 95.1%, IoU 0.831, accuracy 87.3%. Răng hàm nhỏ (first molars, 4 răng): recall 93.7%, IoU 0.849, accuracy 88.9%. Răng hàm lớn (second/third molars, 4-8 răng): recall 91.2%, IoU 0.823, accuracy 85.6%. Răng hàm bị sâu nhiều nhất (53.4%) do có nhiều rãnh, mặt nhai phức tạp, nhưng cũng khó phát hiện hơn vì nằm sâu trong miệng, hay bị che khuất, và có cấu trúc phức tạp hơn.
1	Sâu gian răng (interproximal caries) chiếm 37.8% tổng số sâu, hệ thống phát hiện được 89.4%, cao hơn nhiều so với khám lâm sàng (chỉ 61.3%). Từ tháng 8-12/2024, chúng tôi triển khai hệ thống tại 5 phòng khám nha khoa ở Hà Nội (2), TP.HCM (2), và Đà Nẵng (1). Hệ thống chạy trên workstation với GPU RTX 3060, giao diện web cho phép nha sĩ upload ảnh panoramic và nhận kết quả trong 4-5 giây. Trong 5 tháng, 2.847 bệnh nhân được chụp X-quang panoramic. Nha sĩ khám lâm sàng trước, sau đó hệ thống AI phân tích ảnh panoramic. So sánh cho thấy: AI phát hiện thêm 423 răng sâu mà nha sĩ bỏ sót (8.2% tổng 5.156 răng sâu), chủ yếu là sâu gian răng (287 răng, 67.8%), sâu dưới nướu (89 răng, 21.0%), và sâu tái phát dưới trám cũ (47 răng, 11.2%).
1	Ngược lại, nha sĩ phát hiện 67 răng sâu mà AI bỏ sót (1.3%), chủ yếu sâu ICDAS 1 rất sớm chỉ thấy qua khám lâm sàng. Nha sĩ đánh giá hệ thống hữu ích 9.1/10, nhanh 9.4/10, chính xác 8.7/10, và 100% muốn tiếp tục sử dụng. Kết quả nghiên cứu chứng minh AI có thể hỗ trợ đắc lực cho nha sĩ trong chẩn đoán sâu răng. Việc phát hiện thêm 8.2% răng sâu bị bỏ sót có ý nghĩa quan trọng vì can thiệp sớm giúp bảo tồn răng tự nhiên, tránh điều trị tủy hoặc nhổ răng. Đặc biệt, sâu gian răng (67.8% răng bỏ sót) rất khó phát hiện bằng mắt thường vì nằm giữa hai răng, nhưng lại tiến triển nhanh và gây tổn thương hai răng cùng lúc.
1	AI với khả năng phân tích chi tiết từng pixel giúp phát hiện những thay đổi mật độ nhỏ nhất trên X-quang. Hệ thống cũng giúp chuẩn hóa chẩn đoán, giảm sự khác biệt giữa các nha sĩ (inter-observer variability 20.6% giảm xuống 11.3%). Thời gian xử lý 3.7 giây cho phép sử dụng trong quy trình làm việc thực tế mà không làm chậm. Chi phí 85 triệu đồng cho workstation + GPU + phần mềm, hoàn vốn trong 14 tháng nhờ phát hiện và điều trị sớm tăng doanh thu 6.1 triệu/tháng. Kiến trúc hybrid kết hợp U-Net và Random Forest mang lại nhiều lợi thế so với end-to-end deep learning. Thứ nhất, modular design cho phép cải tiến từng component độc lập: có thể thay U-Net bằng architecture mới hơn mà không ảnh hưởng Random Forest.
1	Thứ hai, interpretability: Random Forest cho feature importance, giúp hiểu model dựa vào đặc trưng nào (area, intensity, texture) để quyết định, trong khi end-to-end CNN là black box. Thứ ba, data efficiency: U-Net cần nhiều dữ liệu segmentation (271K răng), nhưng Random Forest hoạt động tốt với ít dữ liệu hơn nhờ features engineered. Thứ tư, robustness: khi U-Net segmentation có lỗi nhỏ, Random Forest vẫn phân loại đúng dựa vào features tổng thể, không nhạy cảm với từng pixel. Thứ năm, speed: Random Forest inference nhanh hơn nhiều lớp fully-connected của CNN. Tuy nhiên, hạn chế là pipeline phức tạp hơn end-to-end, và feature engineering tốn công sức.
1	Nghiên cứu gặp một số thách thức kỹ thuật và thực tế. Thứ nhất, quality control: chất lượng ảnh X-quang khác nhau giữa các máy, ảnh quá sáng/tối hoặc có nhiễu làm giảm accuracy 7-12%. Cần preprocessing robust hơn với histogram equalization, denoising, và brightness normalization. Thứ hai, edge cases: răng khôn mọc lệch, răng chồng lên nhau, implant hoặc crown kim loại gây artifact mạnh làm model bối rối. Cần thêm dữ liệu và special handling cho các trường hợp này. Thứ ba, annotation cost: việc gán nhãn chi tiết segmentation mask cho 387K răng tốn 4.200 giờ công (3 nha sĩ × 1.400 giờ), chi phí 840 triệu đồng. Thứ tư, regulatory approval: hệ thống AI y tế cần thông qua Bộ Y tế, quá trình xin phép phức tạp và tốn 18-24 tháng.
1	Thứ năm, user adoption: một số nha sĩ lớn tuổi e ngại công nghệ hoặc lo AI thay thế, cần đào tạo và thay đổi mindset. Kết quả của chúng tôi (F1-score 92.9% detection, accuracy 88.4% classification) xếp top tier so với các nghiên cứu quốc tế gần đây. Schwendicke et al. 2019 (Dentistry systematic review): tổng hợp 30 nghiên cứu, AI đạt sensitivity trung bình 87% (range 73-97%), specificity 88% (79-94%) cho detection sâu, ngang ngửa với nha sĩ. Lee et al. 2020 (Scientific Reports): CNN trên 3.000 ảnh panoramic đạt accuracy 82.0%, AUC 0.878, thấp hơn chúng tôi. Cantu et al. 2020 (JADA): Mask R-CNN đạt F1 88.3% cho detection, gần bằng chúng tôi nhưng không phân loại mức độ. Bayraktar & Ayan 2021: ensemble CNN + transfer learning đạt accuracy 94.2%, cao hơn nhưng chỉ test trên 500 ảnh bitewing (không phải panoramic), đơn giản hơn.
1	Zhang et al. 2023: Graph Neural Network kết hợp spatial relationship giữa các răng đạt F1 93.8%, tuy nhiên complex hơn và chưa có deployment thực tế. Nghiên cứu của chúng tôi vượt trội về quy mô dữ liệu (12.8K ảnh) và đã triển khai thực tế. Để nâng cao hiệu quả và mở rộng ứng dụng, chúng tôi đề xuất các hướng sau. Thứ nhất, multi-task learning: mở rộng hệ thống để detect và classify nhiều bệnh lý khác trên panoramic như viêm nha chu, nang, u hàm, nhiễm trùng chân răng, gãy xương. Một model thống nhất tiết kiệm thời gian và nguồn lực hơn nhiều model riêng lẻ.
1	Thứ hai, 3D imaging: áp dụng cho CBCT (Cone Beam CT) giúp phân tích 3D chính xác hơn, đặc biệt cho implant planning và phẫu thuật hàm mặt. Thứ ba, temporal analysis: theo dõi tiến triển sâu răng qua nhiều lần chụp, dự đoán tốc độ tiến triển và thời điểm cần can thiệp. Thứ tư, federated learning: huấn luyện model trên dữ liệu từ nhiều phòng khám mà không chia sẻ dữ liệu gốc, bảo vệ privacy và tăng diversity. Thứ năm, mobile app: phát triển app cho phép bệnh nhân tự chụp X-quang (tại phòng khám có máy) và nhận kết quả sơ bộ, khuyến khích khám sớm. Thứ sáu, explainable AI: cải thiện visualization, highlight vùng sâu trên ảnh gốc bằng heatmap, giúp nha sĩ dễ kiểm tra.
1	Nghiên cứu đã phát triển thành công hệ thống hybrid kết hợp U-Net deep learning và Random Forest machine learning để phát hiện và phân loại sâu răng tự động trên 12.834 hình ảnh X-quang panoramic. Hệ thống đạt F1-score 92.9% cho detection, accuracy 88.4% cho classification mức độ ICDAS, và IoU 0.843 cho segmentation vùng sâu. Thời gian xử lý 3.7 giây/ảnh nhanh hơn 94% so với nha sĩ, cho phép ứng dụng thực tế. Triển khai pilot tại 5 phòng khám đã phát hiện thêm 8.2% răng sâu bị bỏ sót, chủ yếu sâu gian răng và sâu dưới nướu. Kết quả vượt trội so với nhiều nghiên cứu quốc tế về quy mô dữ liệu, độ chính xác, và đã được validation trong thực tế lâm sàng.
0	Trong bài báo này, tác giả sử dụng mạng nơ-ron tích chập, một trong những kiến trúc phổ biến của học sâu để nhận diện và phân loại cảm xúc khuôn mặt. Một cách tổng quát, các mạng nơ-ron được chọn có cấu trúc phức tạp và có tham số lớn, tác giả tập trung vào việc xây dựng một mạng nơ-ron đơn giản hơn và phù hợp với bộ dữ liệu thông qua phương pháp so sánh và đánh giá. Ngoài ra, tác giả cũng tập trung vào việc thu thập một tập dữ liệu đủ lớn để đạt kết quả cao. Cụ thể, tác giả lựa chọn sử dụng nền tảng phần cứng nhúng Jetson TX2 của NVIDIA để tận dụng khả năng tính toán của GPU nhằm tối ưu thời gian tính toán và huấn luyện dữ liệu.
0	Trong đó, dữ liệu được sử dụng là FER2013 và RAF để huấn luyện và kiểm tra. Phương pháp sử dụng đã đạt được độ chính xác 72% trên tập dự liệu kiểm tra. Trong các hệ thống thông minh hỗ trợ tương tác người dùng, quá trình thu thập dữ liệu và đánh giá hành vi của khách hàng là rất cần thiết, việc này đòi hỏi hệ thống có thể ghi nhận được trạng thái cảm xúc của khách hàng thông qua nhận diện cảm xúc khuôn mặt là hết sức cần thiết. Thực vậy, các phương pháp và hệ thống nhận diện cảm xúc khuông mặt được sự quan tâm của nhiều nhà nghiên cứu trong và ngoài nước.
0	Trong [1], nhóm nghiên cứu thuộc trường đại học Nanchang, Trung Quốc đã áp dụng kiến trúc mạng nơ-ron tích chập, mạng VGG, và đạt được độ chính xác 73.06%. Mạng VGG được xem là có hiệu quả cao mặc dù có cấu trúc đơn giản hơn so với nhiều kiến trúc mạng CNN khác nhưng lại có số lượng tham số nhiều hơn và tăng chiều sâu hơn ở mỗi lớp. Mạng VGG có kích thước khá lớn và nhiều tham số (136 triệu) nên sẽ tốn rất nhiều thời gian huấn luyện dữ liệu. Bên cạnh đó, trong [2], các tác giả đã xây dựng ứng dụng nhận diện cảm xúc khuôn mặt của sinh viên sử dụng CNN với độ chính xác đạt được là 70%.
0	Ở trường đại học Hà Nội, Việt Nam, một nhóm nghiên cứu đã triển khai một dạng kiến trúc khác của CNN đó là BKStart với khoảng 7.17 triệu tham số nhưng vẫn đạt được độ chính xác khá cao (70,4%) trên FERC2013 dataset [3]. BKStart khác với VGG bằng việc sử dụng các kernel có kích thước lớn hơn tuy nhiên lại sử dụng stride bằng 1 để tăng hiệu quả trích xuất đặc trưng. Tác giả cũng đã áp dụng mạng Nơ-ron tích chập CNN, một trong những kiến trúc phổ biến trong học sâu, để nhận diện cảm xúc con người.
0	Bên cạnh sự phát triển của các thuật toán, một yếu tố quan trọng khác đó chính yếu tố phần cứng. Một nhóm nghiên cứu ở Luân đôn, Vương Quốc Anh cũng đã tiến hành thực hiện thiết kế kết hợp với phần cứng Xilinx Spartan-6 LX45 FPGA cho hệ thống nhận diện cảm xúc khuôn mặt [4]. Với phần cứng FPGA, tốc độ xử lý đã được cải thiện đáng kể, có thể đạt được được 30FPS. Tuy có thể cải thiện được khả năng tính toán nhưng lại không thể áp dụng được các thuật toán phức tạp nên dẫn đến độ chính xác còn khá thấp 51.28%. Phần cứng Jetson TX2 được lựa chọn để thực hiện mô hình nhận diện cảm xúc này.
0	Jetson TX2 đã được tích hợp vào đó các lõi CUDA cùng với hệ thống GPU mang lại tốc độ xử lý cao [5]. Ngoài ra Jetson TX2 đáp ứng được hầu hết các yêu cầu khắt khe về thời gian thực nhưng lại mang một kích thước nhỏ gọn, dễ dàng lắp đặt [6]. Tác giả thực hiện mô hình nhận diện cảm xúc khuôn mặt người sử dụng mạng Nơ-ron tích chập CNN trên phần cứng nhúng Jetson TX2 với các mục tiêu, phát triển mô hình nhận có khả năng nhận diện 7 loại cảm xúc: vui, buồn, kinh tởm, ngạc nhiên, bình thường, sợ hãi và giận dữ của con người.
0	Mô hình được xây dựng trên một hệ thống nhúng riêng biệt (Jetson TX2) và có thể đạt được độ chính xác cao (trên 70% cho cả quá trình phát hiện khuôn mặt người và nhận diện cảm xúc). Cảm xúc của con người là muôn hình vạn trạng. Mỗi người điều có mỗi cách bày tỏ cảm xúc của mình khác nhau. Chính vì thế, dữ liệu cần để huấn luyện cho mô hình nhận diện cảm xúc khuôn mặt người là rất lớn. Trong đề tài này, tập dữ liệu FER2013 được lựa chọn sử dụng, một phần tập dữ liệu RAF và các dữ liệu được thu thập từ internet. Hình 1 là tổng quan toàn bộ tập dữ liệu và cách xử lý từng loại dữ liệu để đưa bài toán nhận diện cảm xúc:
0	Tập dữ liệu nguồn mở FER2013.csv, được tạo ra cho một dự án bởi PierreLuc Carrier và Aaron Courville, được chia sẻ công khai trong cuộc thi Kaggle (2013). Dữ liệu trong tập FER2013.csv Bộ dữ liệu này bao gồm 35.887 ảnh xám: hình ảnh khuôn mặt kích thước 48x48 pixel từ nhiều góc độ khác nhau. Hình ảnh được phân loại thành một trong bảy lớp thể hiện cảm xúc khuôn mặt khác nhau, tất cả được gán nhãn từ 0 – 7 (0 = Giận dữ, 1 = Ghê tởm, 2 = Sợ hãi, 3 = Vui vẻ, 4 = Buồn, 5 = Ngạc nhiên, 6 = Bình thường). Gồm 8.989 ảnh ‘Happy’, 6.077 ảnh ‘Sad’, 6.198 ảnh ‘Neutral’, 4002 ảnh ‘Suprised’, 5121 ảnh ‘Scared’, 547 ảnh ‘Disgust’ và 4593 ảnh ‘Angry’ [4].
0	RAF-DB (Real-world Affective Faces Database) là một tập dữ liệu lớn gồm 15000 hình ảnh khuôn mặt với nhiều loại biểu cảm khác nhau được thu thập từ internet. Hình ảnh trong tập dữ liệu có sự thay đổi lớn về tuổi tác, dân tộc, tư thế đầu và điều kiện ánh sáng và đa dạng về phong cách (mang kính, đội nón) [5]. Hình 2 cho thấy quá trình xử lý tập dữ liệu RAF mà nhóm thức hiện để đưa tập dữ liệu này vào tập dữ liệu chung. Ban đầu, tập dữ liệu là tập hợp những hình ảnh cảm xúc khuôn mặt người chưa được phân loại và một tệp .txt chứa dữ liệu phân loại (tên ảnh, nhãn cảm xúc). Từ đó nhóm phân loại ra từng thư mục cảm xúc khác nhau.
0	Tiền xử lý hình ảnh (chuyển ảnh sang ảnh xám và giảm kích thước ảnh về 48x48), chuyển ảnh về dạng pixel và cập nhật vào file Dataset (.csv). Jetson TX2 là một máy tính nhúng có hiệu quả năng lượng cao của NVIDIA được sử dụng rộng rãi cho thị giác máy tính, trí tuệ nhân tạo, máy học. Jetson TX2 là một máy tính nhỏ nhưng khá mạnh mẽ. TX2 có 2 CPU: ARM Cortex-A57 (4 lõi) 2GHz và NVIDIA Denver2 (lõi kép) @ 2GHz [6]. Ngoài ra, ở một máy tính của NVIDIA, GPU là 1 thành phần không thể thiếu với Pascal 256 lõi 1300 MHz. Bộ nhớ 8Gb được chia sẻ giữa CPU và GPU. Tất cả những tác vụ tính toán có thể tiêu thụ chỉ 7,5W năng lượng.
0	Đây là phần cứng tốt để thực hiện các giải thuật học sâu trên nền tảng hệ thống nhúng [7]. Hình 3 cho thấy quá trình phát triển lõi GPU của NVIDIA đặc biệt là sự đột phá của NVIDIA Pascal ra mắt vào năm 2016 và sau đó đã được đem vào trong kiến trúc của phần cứng Jetson TX2. Mạng CNN được xây dựng trên ngôn ngữ Python và sử dụng thư viện Keras. Mạng CNN được xây dựng gồm sáu lớp tích chập, ba lớp Max-pooling và cuối cùng là hai lớp Full-connected. Số lượng filter tương ứng lần lượt là 32, 64 và 128. Kích thước của filter 3x3 và lớp Max-pooling có kích thước stride là 2x2.
0	Lớp Max-pooling được sử dụng sau mỗi hai lớp tích chập. Max pooling được dùng để giảm kích thước ma trận nhưng vẫn làm nổi bật lên được đặc trưng có trong ma trận đầu vào và giảm thiểu công việc tính toán cho phần cứng [9]. Sau mỗi lớp Max Pooling sẽ có một lớp Drop Out nhằm giảm hiện tượng overfitting bằng cách loại bỏ ngẫu nhiên một số unit. Để giữ được kích thước không gian của ngõ ra, các zero-padding được thêm vào. Sau khi qua các lớp tích chập, ngõ ra sẽ qua Flatten trở thành vector 1 chiều để đưa vào lớp dense (hay lớp full connected).
0	Giá trị 20 được sử dụng theo tập validation của FER-2013 [11]. Từ công thức (1), giá trị trả về thay vì bằng 0 khi 𝑥 < 0 như RELU, leaky RELU sẽ tạo thành đường dốc với giá trị giảm về nhỏ hơn 0. Ở mạng nơ-ron, kết quả đầu ra cần được chuẩn hóa sang dạng phần trăm để dự đoán giữa các lớp. Khi đó lớp cuối cùng của mạng CNN là lớp Softmax với 7 ngõ ra. Với 𝑎𝑘 là tỉ lệ dự đoán lớp thứ k, n là tổng số lớp được dự đoán (7 cảm xúc) và 𝑧𝑘 là hệ số của các node trước cho lớp thứ k.
0	Thuật toán sử dụng để cập nhật trọng số của mạng CNN là Adam. Thuật toán Adam nổi trội hơn một số thuật toán khác bởi khả năng vượt qua vùng local minimum và đạt tới điểm tối ưu nhất (flat minimum). Một đặc điểm ở thuật toán Adam đó chính là khả năng tận dụng “động lực” để hội tụ nhanh hơn. “Động lực” chính là khả năng tận dụng một số phần trong lần cập nhật trọng số trước để cập nhật cho trọng số hiện tại [11]. Kết quả cuối cùng của model cần được đánh giá để biết hiệu quả của quá trình training dữ liệu. Ở đây hàm mất mát chính là công cụ để thực hiện điều đó. Hàm mất mát sử dụng trong bài báo này là hàm “categorical_crossentropy”.
0	Với L là ký hiệu của hàm mất mát, yi là kết quả thực và 𝑦̂𝑖 là kết quả dự đoán. Biểu thức (3) chính là hàm tính sai số được áp dụng phổ biến và được sử dụng để vẽ đồ thị đánh giá kết quả cuối cùng. Biểu thức (4) là biểu thức tổng quát để tính độ chính xác, được sử dụng để tính độ chính xác cho quá trình đánh giá kết quả của hệ thống. Hình 5 cho thấy sự khác nhau trong quá trình huấn luyện dữ liệu giữa các bộ optimizer khác nhau bao gồm Adam, Adagrad và Sgd. Rõ ràng, từ hình 4 cho thấy thuật toán Adam cho kết quả với độ chính xác khá cao so với Sgd và Adagrad.
0	Trong khi đó, quá trình huấn luyện dữ liệu của Sgd và Adagrad lại cho thấy sự giao động khá lớn ở những epoch đầu tiên. Hình 5 trình bày tổng quát toàn bộ kiến trúc mạng CNN. Trong bảng 1, chúng tôi so sánh kết quả nhận diện đạt được giữa kiến trúc CNN ConvNet V2 với một số mô hình CNN khác. Đối với vác model có lượng Param thấp như Mini Xception nhưng lại đạt được độ chính xác tốt. Tuy nhiên vẫn chưa đạt được mục tiêu đề ra. Đối với model VGG 16 có lượng tham số cao nhưng đối với dataset có kích thước khá nhỏ khiển model này dễ dẫn đến hiện tượng overfitting.
0	Hình 6 trình bày kết quả nhận diện của 2 kiến trúc mạng CNN phổ biến, VGG16 và Big Xception, với độ chính xác lên tới 66%. Bảng 2 trình bày kết quả so sánh giữa các thông số ảnh hưởng kết quả huấn luyện dữ liệu. Đối với mỗi thông số khác nhau sẽ phù hợp với một bộ dataset và model khác nhau. Do đó, việc lựa chọn thông số là cần thiết để có thể đạt được hiệu suất tốt nhất trong quá trình huấn luyện dữ liệu. Việc đánh giá model được sử dụng sẽ dựa trên độ chính xác của tập Validation và tập test.
0	Hình 7 cho thấy độ chính xác của quá trình huấn luyện dữ liệu đã hội tụ sau khoảng 85 epoch và đạt được độ chính xác trên tập test là 72%. Với sự hội tụ của độ chính xác, hình 8 trình bày sai số của model trên tập test validation và tập train cũng đã hội tụ sau khoảng 90 epoch. Confusion matrix là một công cụ hữu ích trong các bài toán phân loại với việc xem xét cả những chỉ số về độ chính xác và độ bao quát của các dự đoán cho từng lớp. Confusion matrix trong bảng 3 cho thấy kết quả của quá trình đánh giá thực tế và kết quả dự đoán của mô hình.
0	Dựa trên kết quả thu được từ tập test, lớp Angry đạt được độ chính xác xấp xỉ 53%, Disgust đạt 73%, lớp Fear đạt 54%, lớp happy đạt 92%, lớp Sad đạt 77%, lớp Surprise đạt 52% và lớp Neutral đạt 80%. Ba lớp Angry, Fear và Surprise đạt hiệu suất thấp nhất và lớp Happy cùng với lớp Neutral đạt hiệu suất cao nhất. Để đánh giá model, kết quả thực nghiệm được xem xét trong những điều kiện khác nhau như được liệt kê trong bảng 4. Dựa vào bảng 4 có thể thấy kết quả nhận diện dễ bị ảnh hưởng bởi các điều kiện khác nhau.
0	Trong đó, khoảng cách càng xa hoặc độ sáng không đủ dễ làm giảm độ chính xác của mô hình nhận diện. Tương tự, trong điều kiện thiếu sáng hoặc quá sáng mô hình cho thấy kết quả nhận diện không tốt. Hơn nữa, đối với khoảng cách xa, kết quả phát hiện khuôn mặt sẽ bị ảnh hưởng đáng kể, phần lớn do độ phân giải của camera khá thấp khiến những bức ảnh xa có số lượng đặc trưng thấp ảnh hưởng tới kết quả nhận diện. Hình 9 và hình 10 tương ứng cho thấy kết quả nhận diện đúng và kết quả nhận diện sai trên tập dữ liệu test của FER2013 với khoảng hơn 7000 chiếm khoảng 20% tập dữ liệu của FER.
0	Hình 11 minh họa một số kết quả nhận diện 7 loại cảm xúc tương ứng bằng cách sử dụng trực tiếp camera được tích hợp sẵn trên phần cứng Jetson TX2. Trong đề tài này, tác giả đã triển khai một mô hình mạng Nơ-ron tích chập CNN trên nền tảng phần cứng nhúng Jetson TX2 để nhận diện 7 loại cảm xúc cơ bản của con người, với độ chính xác của mô hình đạt được 72%. Mô hình nhận diện được cảm xúc của khuôn mặt trong điều kiện vùng khuôn mặt sáng, khuôn mặt không bị che khuất, hướng khuôn mặt nhìn thẳng và nhìn nghiêng 45o.
0	Tỷ lệ nhận dạng chính xác các cảm xúc vui vẻ, ngạc nhiên và bình thường khá cao, cảm xúc buồn và giận dữ có tỷ lệ dự đoán thấp, riêng với cảm xúc ghê tởm tỷ lệ dự đoán là thấp nhất. Việc sử dụng phần cứng nhúng Jetson TX2 (GPU) cho thấy tốc độ xử lý nhanh hơn đáng kể so với laptop (CPU). Tuy nhiên, số lượng hình ảnh trong tập dữ liệu giữa các lớp có sự chêch lệch quá lớn, lớp khó chịu có ít dữ liệu nhất (1424 ảnh) trong khi đó lớp vui có nhiều dữ liệu nhất (14946 ảnh) trong tổng số 51226 ảnh, điều này dẫn đến sự mất cân bằng dữ liệu giữa các lớp. Từ đó có sự ảnh hưởng rất lớn đến độ chính xác của hệ thống.
0	Gần đây, nhiều botnet sử dụng thuật toán tạo tên miền bất thường tự động (DGA) để sinh và đăng ký nhiều tên miền bất thường ngẫu nhiên khác nhau cho máy chủ lệnh và điều khiển của chúng nhằm chống lại việc bị kiểm soát và đưa vào danh sách đen. Do đó, việc phát hiện tên miền bất thường đang là mối quan tâm nghiên cứu của nhiều nhà nghiên cứu trên toàn thế giới vì tính lan rộng, độ tinh vi cao và hậu quả nghiêm trọng đối với nhiều tổ chức và người dùng. Bài báo này tập trung nghiên cứu xây dựng mô hình phát hiện tên miền bất thường
0	dựa trên các kiến trúc Recurrent Neural Network (RNN), Long Short Term Memory (LSTM) và Convolutional Neural Network (CNN) của thuật toán học sâu và thử nghiệm đánh giá các mô hình này trên bộ dữ liệu gồm 16.7 triệu tên miền trong đó có 9 triệu tên miền bất thường và 7.7 tên miền lành tính. Kết quả thử nghiệm chỉ ra rằng mô hình học sâu sử dụng kiến trúc mạng học sâu LSTM cho tỷ lệ chính xác cao nhất. Ngoài ra, chúng tôi cũng đã đề xuất và triển khai thử nghiệm thành công giải pháp tích hợp mô hình phát hiện tên miền bất thường vào hệ thống giám sát cảnh báo sớm (SOC).
0	Trong cuộc sống hiện đại ngày nay, với việc mạng Internet ngày càng phát triển không ngừng, công nghệ thông tin được ứng dụng vào mọi mặt của đời sống, kinh tế, chính trị, xã hội đã giúp cho cá nhân, tổ chức, doanh nghiệp và các cơ quan hành chính nhà nước trên thế giới nói chung và Việt Nam nói riêng dễ dàng trao đổi thông tin và thực hiện các giao dịch được thuận lợi nhanh chóng. Cùng với sự phát triển của công nghệ thông tin và viễn thông đã mở ra không gian mới cho sự tấn công và đe dọa an ninh mạng.
0	Trong số những mối đe dọa này, là kẻ tấn công sử dụng thuật toán tạo tên miền (Domain Generation Algorithm- DGA) [6, 8] để tạo ra một số lượng lớn các tên miền bất thường ngẫu nhiên để kết nối với các máy chủ điều khiển và lệnh độc hại, chủ yếu được sử dụng trong các hình thức chính như sau: Phân phối mã độc: Các loại mã độc phổ biến được phân phối bởi tên miền bất thường bao gồm ransomware, phần mềm gián điệp, trojan và botnet. Tấn công từ chối dịch vụ (DDoS): Tên tên miền bất thường cũng được sử dụng để thực hiện các cuộc tấn công DDoS quy mô lớn, nhằm làm sập các trang web hoặc dịch vụ quan trọng.
0	Lừa đảo: Tên tên miền bất thường cũng được sử dụng để tạo ra các trang web giả mạo, nhằm lừa người dùng cung cấp thông tin cá nhân nhạy cảm. Việc tấn công này đã gây ra những thiệt hại không nhỏ về hệ thống mạng và sự mất mát dữ liệu của người dùng, dẫn đến thiệt hại về kinh tế, xã hội của các cá nhân, tổ chức, doanh nghiệp và cơ quan hành chính nhà nước. Do đó, phát hiện tên miền bất thường đang là mối quan tâm nghiên cứu của nhiều nhà nghiên cứu trên toàn thế giới vì mức độ lan rộng, độ tinh vi cao và hậu quả nghiêm trọng đối với nhiều tổ chức và người dùng.
0	Trong nghiên cứu này, chúng tôi tập trung đánh giá hiệu quả của việc xây dựng mô hình phát hiện tên miền bất thường dựa trên kỹ thuật học sâu và học máy để xác định ra mô hình kiến trúc LSTM có hiệu quả cao trong bài toán phát hiện tên miền bất thường. Yadav và cộng sự [1] đề xuất phương pháp phân biệt các tên miền sinh tự động bằng thuật toán thường được sử dụng trong các botnet với tên miền hợp lệ dựa trên phân tích sự phân bố các nhóm ký tự (1-gram, hoặc 2-gram liền kề) trong tên miền.
0	Các tác giả sử dụng độ đo phân kỳ Kullback-Leibler (K-L) để tính toán khoảng cách giữa các tên miền hợp lệ và các tên miền độc hại được sinh tự động. Hạn chế của phương pháp này là nó thường không phát hiện ra các họ DGA khác nhau. Đức và cộng sự [2] đã sử dụng mạng bộ nhớ ngắn hạn dài Long Short-Term Memory Network - LSTM để giải quyết cả hai bài toán DGA Botnet. Nhóm nghiên cứu đề xuất một thuật toán mới với tên gọi là LSTM.MI, kết hợp cả hai mô hình phân loại, kế thừa ưu điểm của LSTM truyền thống và các cải tiến để tăng cường độ chính xác với nhiễu.
0	Về dữ liệu thử nghiệm, các tên miền được nhóm tác giả thu thập từ thực tế, với 100.000 tên miền lành tính phổ biến nhất từ Alexa và 37 họ DGA Botnet được tổng hợp. Kết quả thử nghiệm cho thấy thuật toán đề xuất giúp nâng cao ít nhất 7% độ chính xác so với mô hình LSTM truyền thống. Nó cũng đạt độ chính xác cao trong bài toán phân lớp nhị phân với F1-score đạt 98,49%, đồng thời có khả năng nhận ra 05 họ DGA Botnet bổ sung. Hạn chế của nghiên cứu là bộ dữ liệu chưa thực sự đầy đủ và một số họ DGA Botnet gần như không thể phát hiện được. Curtin và cộng sự đã sử dụng mạng RNN để phát hiện và phân loại DGA Botnet [3].
0	Họ nhận ra rằng các tên miền được xây dựng dựa trên một không gian từ vừng, 32 có các nét đặc trưng khá giống với các tên miền lành tính, từ đó giúp tăng khả năng ẩn mình của các tên miền được sinh ra. Nhóm nghiên cứu đã đề xuất một khái niệm mới, gọi là thang điểm Smashword. Đây là thang đo lường mức độ giống nhau giữa tên miền của Botnet và các tên miền lành tính. Nhóm nghiên cứu áp dụng mô hình mạng Recurrent Neural Network và Side Information để áp dụng tiêu chuẩn đo lường trên. Bộ dữ liệu thử nghiệm với 1.000.000 tên miền Alexa, kết hợp với 41 họ DGA Botnet được nhóm nghiên cứu tổng hợp, với tổng cộng 2.300.000 tên miền cho cả hai nhãn.
0	Thực nghiệm cho thấy mô hình mới có tiềm năng ứng dụng cao cải tiến được độ chính xác hơn so với các mô hình trước đó. Một số họ DGA Botnet phức tạp như matsnu, suppobiox hay rovnix cũng được phát hiện bởi giải pháp trên. Qiao và cộng sự [4] đề xuất một kiến trúc học sâu phát triển dựa trên mạng LSTM và Attention. Kiến trúc mới có lõi gồm một lớp LSTM với một lớp Attention được bố trí tuần tự, kích thức đầu vào là 54 × 128. Nhóm tác giả đánh giá trên một bộ dữ liệu gồm 16 nhãn bao gồm cả nhãn lành tính. Kết quả thực nghiệm cho thấy mô hình đề xuất có F1-score đạt 94,58% cho bài toán phân loại.
0	Ưu điểm của phương pháp là đạt độ chính xác cao và loại bỏ được quá trình trích chọn các đặc trưng. Tuy nhiên, phương pháp đề xuất cũng chỉ phát hiện tốt các tên miền character-based DGA. Ngoài ra, tỷ lệ cảnh báo sai tổng cũng còn tương đối cao, khoảng 5% tính theo độ đo F1. Vinayakumar và nhóm cộng sự nghiên cứu bài toán phát hiện tên miền độc hại được sinh bởi Botnet hay thư điện tử, URL độc hại [5]. Một số kỹ thuật biểu diễn đặc trưng dựa trên n-gram đã được sử dụng để mô hình hóa bài toán. Bộ dữ liệu để đánh giá bao gồm các tên miền lành tính và độc hại được thu thập từ OpenDNS, Alexa và OSINT Feeds.
0	Kết quả so sánh thấy mô hình CNN-LSTM là hiệu quả nhất với F1-score đạt 96,3% cho bài toán phát hiện. Liu và cộng sự [6] lập luận rằng biểu diễn đặc trưng bằng các giá trị vô hướng có thể dẫn đến mất mát thông tin. Nhóm tác giả đề xuất một Capsule Network tuần tự (mạng học sâu được cải tiến từ CNN) dựa trên thuật toán k-means, gọi là LSTMCapsNet. Mô hình sử dụng một đơn vị LSTM hai chiều để trích xuất các thuộc tính cơ bản và sử dụng thuật toán k-mean để phân cụm thành hai nhãn độc hại và lành tính.
0	Đánh giá được thực hiện trên hai bộ dữ liệu, bao gồm bộ dữ liệu tên miền DGA từ mạng thực tế và tên miền DGA thu được thông qua thuật toán tạo tên miền tự động. Kết quả thực nghiệm cho thấy mô hình đề xuất đạt độ chính xác lần lượt là 99,17% và 97,75% trên hai bộ dữ liệu. Mô hình này không chỉ cải thiện khả năng nhận dạng 33 tên miền DGA và nhận dạng họ tên miền DGA trên lý thuyết mà còn thể hiện hiệu quả trong bộ dữ liệu thực tế. Các giai đoạn trong quá trình đánh giá thuật toán học máy đối với bài toán phân lớp nhị phân được thể hiện.
0	Theo đó, với dữ liệu đầu vào là các tên miền, bao gồm cả lành tính và độc hại đã được gán nhãn, sử dụng n-gram để tách các tên miền và kỹ thuật TF-IDF để biểu diễn các đặc trưng. Tiếp theo, các thuật toán học máy được sử dụng để huấn luyện, bao gồm: Support Vector Machines (SVM), Logistic Regression (LR), Naive Bayes (NB), Decision Trees (DT), Random Forests (RF), k-Nearest Neighbour (kNN), Adaptive Boosting (AB). Mô hình sau khi huấn luyện được dùng để giải quyết bài toán phân lớp nhị phân, với hai nhãn là độc hại và lành tính.
0	Trong nghiên cứu này sẽ tập trung xây dựng mô hình phát hiện bất thường mạng dựa trên các kiến trúc RNN, LSTM và CNN của mạng học sâu [9]. Bộ dữ liệu sẽ được tiền xử lý để có thể đáp ứng yêu cầu huấn luyện, cũng như cải thiện độ chính xác của mô hình về sau. Đây là bước hết sức quan trọng, vì dữ liệu càng có độ chi tiết cao thì kết quả mô hình đem lại càng tốt. Sau đó dữ liệu sẽ được tách thành 3 tập Train, Test và Validation Data người dùng. Train Data người dùng là tập dữ liệu sử dụng cho mục đích huấn luyện, chiếm tỉ lệ 80% so với tổng cả bộ dữ liệu.
0	Test Data người dùng là tập kiểm thử kết quả của mô hình, Validation Data người dùng là tập sử dụng vào mục đích giám sát quá trình huấn luyện. Hai tập Test và Validation chiếm tỉ lệ 10%. Sau khi dữ liệu đã tách thành 3 tập trên, chúng được sử dụng để huấn luyện và đánh giá các mô hình RNN, LSTM và CNN. Các mô hình đều được xây dựng dựa trên kiến trúc của Neural Network. Sau khi bộ dữ liệu đã được tiền xử lý, chúng được đưa qua lớp Reshape, sau đó đến lớp RNN/LSTM/CNN và đi đến các lớp Dense hay tên gọi khác là Fully-connected layer. Ngoài ra ở sau mỗi lớp Dense là một lớp Dropout Layer, thực hiện loại bỏ ngẫu nhiên tỉ lệ units ở lớp trước đó.
0	Cuối cùng kết quả của các mô hình là dự đoán cho các tên miền trong tập kiểm tra. Sơ đồ các mô hình như trong hình 3: Người dùng cần đưa dữ liệu gốc qua lớp Reshape Layer (vì lớp RNN Layer/LSTM Layer/CNN Layer yêu cầu dữ liệu phải có dạng 3 chiều [batch, timesteps, feature]. Trong đó: batch: batch_size (số mẫu dữ liệu đưa vào tại một thời điểm); timesteps: số bước nhớ của mô hình; features: số đặc trưng của dữ liệu. Trước khi qua Reshape Layer, người dùng quy định dữ liệu sẽ đi vào theo từng mẫu một.
0	Mỗi mẫu dữ liệu có 70 đặc trưng, vì vậy input_shape = (70). Vì vậy ở Reshape Layer người dùng cần dữ liệu vẫn đảm bảo có shape tương đương với input_shape (input_shape = timestep*features). Để dễ hình dung, người dùng đặt features = 70, timestep = 1. Sau khi qua lớp Reshape Layer, output sẽ được truyền vào lớp RNN/LSTM/GRU Layer, lớp này có chức năng phân tích các đặc trưng của dữ liệu. Output của lớp này có dạng 2 chiều (batch_size, units) với units là số unit có trong lớp. Ở các mô hình trên, output có dạng (None, 128). Hai lớp Dense Layer (tiếp theo (512 units và 64 units) tiếp tục phân tích các đặc trưng dữ liệu.
0	Ngoài ra sau mỗi Dense Layer có sử dụng một lớp Dropout Layer. Lớp Dropout Layer thực hiện loại bỏ ngẫu nhiên k % số unit ở lớp Dense Layer phía trước, mục đích của việc loại bỏ này nhằm tránh hiện tượng overfitting xảy ra trong quá trình huấn luyện. Lớp Dense Layer cuối cùng có 15 units, có nhiệm vụ biến đổi đầu vào dạng logits thành softmax (dạng xác suất). Kết quả đầu ra trả về kết quả dự đoán cho các tên miền trong tập kiểm tra. Các độ đo và ma trận nhầm lẫn (confusion matrix) được tính toán để đánh giá hiệu suất của mô hình.
0	Trong bài nghiên cứu này sẽ sử dụng bộ dữ liệu có 16 triệu tên miền được AAYUSH V. SHAH xây dựng, cập nhật năm 2020 và lưu trữ công khai trên Kaggle [7]. Bài toán phát hiện tên miền bất thường là bài toán phân lớp nhị phân, với nhãn 0 là tên miền lành tính, nhãn 1 là tên miền tên miền bất thường, được định nghĩa: TN: Số lượng mẫu tên miền là lành tính được phân loại đúng là lành tính. TP: Số lượng mẫu tên miền độc hại được phân loại đúng là độc hại. FP: Số lượng mẫu tên miền lành tính được phân loại sai thành độc hại.
0	FN: Số lượng mẫu tên miền độc hại được phân loại sai thành lành tính. Đánh giá kết quả thực nghiệm thông qua các tham số gồm Accuracy, Precision, Recall và F1-score, lần lượt được tính bằng các công thức (1), (2), (3) và (4) dưới đây. Accuracy giúp đánh giá độ chính xác chung của mô hình, bao gồm việc phát hiện đúng tên miền là lành tính hoặc tên miền là độc hại. Precision và Recall đánh giá riêng về khả năng phát hiện đúng một tên miền lành tính hay một tên miền độc hại trên tổng số lượng mẫu của tên miền đó.
0	Nhìn chung, việc phát hiện nhầm một tên miền lành tính thành độc hại sẽ ít gây nguy hiểm hơn là việc chấp nhận nhầm một tên miền độc hại là lành tính. F1-Score là đại lượng dùng chung để đánh giá một cách tổng thể giữa 1 Precison và Recall trong mô hình. Từ số liệu trên ta thấy: mô hình LSTM đều có các kết quả Precision/Recall/F1-Score cao hơn so với CNN và RNN và mô hình học máy. Ta có thể khẳng định các thuật toán học sâu LSTM hiệu quả trong bài toán phát hiện tên miền bất thường nhờ vào sự phù hợp với bộ dữ liệu. Mô hình thuật toán sau khi được huấn luyện sẽ được lưu lại để ứng dụng vào sản phẩm dự đoán tên miền.
0	Để dự đoán tên miền, nhóm đã tạo một ứng dụng cơ bản bằng windown form với phần intput để nhập tên miền cần kiểm tra, “Xem kết quả” để gửi phân tích và “Clear” để xóa tên miền hiện tại. Hiện trạng công việc giám sát SOC truyền thống: Người giám sát SOC thường không được trang bị đầy đủ để xử lý các mối đe dọa một cách hiệu quả, họ chỉ hiểu biết một phần nhất định, không thể nắm hết tất cả các lĩnh vực. Quá nhiều cảnh báo gây mệt mỏi cho người giám sát và khiến các cảnh báo quan trọng bị bỏ qua.
0	Tốn thời gian bởi các cảnh báo được tạo cần có sự can thiệp của con người để phân tích và thực hiện hành động nên quá trình giảm thiểu mối đe dọa bị chậm trễ. Việc triển khai tích hợp mô hình thuật toán học sâu vào việc phát hiện tên miền bất thường trong các hệ thống giám sát cảnh báo sớm (SOC) trong thực tế tại các đơn vị đem lại các lợi ích như sau: Xác định các mối đe dọa bảo mật với độ tin cậy cao, đồng thời giảm thời gian và kinh nghiệm cần thiết trong SOC. Toàn bộ quy trình có thể được sắp xếp hợp lý, giúp xác định các sự kiện quan trọng và tự động thực hiện biện pháp khắc phục.
0	Tích hợp mô hình phát hiện tên miền bất thường vào các hệ thống giám sát cảnh báo sớm (SOC) trong thực tế tại các đơn vị theo mô hình tổng thể như sau. Các thành phần trong mô hình: Server/Client windows: một người dùng hoặc máy chủ kết nối đến một tên miền nào đó. Network IDS Capture: gói tin sẽ được bắt lại. Machine Learning DGA Detection [10-13]: trích xuất domain từ log ids và dự đoán. SIEM/SOC: kết quả được gửi lên SIEM/SOC cảnh báo cho đội ngũ giám sát hệ thống. Kiến trúc phần mềm: Các thành phần chính của kiến trúc: IDS: thu thập các log dữ liệu trên máy tính, máy chủ và trên mạng mà nó giám sát.
0	Logtash: log sẽ được đưa đến logstash, logstash sẽ đọc những log này, thêm những thông tin như thời gian, IP, parse dữ liệu từ log (server nào, độ nghiêm trọng, nội dung log) ra, sau đó ghi xuống database là Elasticsearch. Mỗi dòng log của logstash được lưu trữ đưới dạng json. Aggregation query: để lấy danh sách domain trong vòng 24h từ logtash. Elasticsearch: cơ sở dữ liệu này được dùng để lưu trữ, tìm kiếm và query log. DGA_Detection & Alert: trích xuất domain từ log Elasticsearch chu kì 1 ngày, tiến hành đưa vào bộ phân loại để phát hiện domain bất thường, gửi lại kết quả phân loại lên cho đội ngũ giám sát.
0	Trong bài báo này, chúng tôi đã đề xuất và thực hiện các thử nghiệm đánh giá hiệu suất của các mô hình phát hiện tên miền bất thường dựa trên kỹ thuật học sâu và học máy. Kết quả chỉ ra rằng, mô hình kiến trúc mạng LSTM có kết quả độ chính xác cao nhất trong bài toán phát hiện tên miền bất thường. Đây là một con số ấn tượng, đem đến tiềm năng cho việc xây dựng một ứng dụng hỗ trợ phát hiện sớm tên miền bất thường. Chúng tôi cũng đã đề xuất và triển khai thử nghiệm thành công giải pháp tích hợp mô hình phát hiện tên miền bất thường vào hệ thống giám sát cảnh báo sớm (SOC) thực tế tại đơn vị.
0	Trong những năm gần đây, khoa học công nghệ đã được ứng dụng mạnh mẽ và góp phần phát triển kỷ nguyên chuyển đổi số. Dịch bệnh COVID-19 đã kiểm chứng việc ứng dụng thành tựu công nghệ trong thực tế là rất hữu ích. Ngày càng có nhiều ứng dụng thông minh trong lĩnh vực nông nghiệp để thay thế nông nghiệp truyền thống. Đặc biệt là các ứng dụng trí tuệ nhận tạo vào tự động hóa các khâu sản xuất và bảo quản nông sản Việt Nam. Trong bài báo này, nhóm tác giả trình bày giải pháp nhận dạng phát hiện trái dứa vào thời kỳ thu hoạch sử dụng mô hình Fast R-CNN cải tiến, thực nghiệm được thực hiện tại các vùng, vườn trồng dứa ở Việt Nam với sản lượng thu hoạch khá lớn.
0	Từ đó xây dựng hệ thống ứng dụng dự đoán trái dứa chín chạy trên nền tảng di động nhằm hỗ trợ người nông dân đạt hiệu quả hiệu quả kinh tế cao đối với cây dứa và các cây nông nghiệp chủ lực của Việt Nam. Trí tuệ nhân tạo (AI), Internet vạn vật (IoT), Điện toán đám mây (Cloud Computing) và dữ liệu lớn (Big Data) là những trụ cột của cuộc cách mạng công nghiệp 4.0. Dựa trên những trụ cột này đã tạo ra các mô hình ứng dụng mới trong sản xuất, cũng từ đấy đã thúc đẩy nhiều hoạt động, đã có tác động mạnh mẽ tới nền kinh tế số, chính trị số và đời sống xã hội số.
0	Có thể nói rằng thời điểm này trí tuệ nhân tạo đang nhanh chóng trở thành một trong những lĩnh vực khoa học được mong đợi nhất, vì khả năng mang lại lợi ích cho nhiều ngành, nhiều lĩnh vực như công nghiệp, nông nghiệp, y khoa, giáo dục. Ứng dụng của thành tựu trí tuệ nhân tạo cho ngành nông nghiệp chính xác (Precision Agriculture) được sử dụng rộng rãi và mang nhiều kết quả to lớn như: Máy bay, xe kéo không người lái, robot hỗ trợ thu hoạch tự động, hệ thống đo độ ẩm của đất phục vụ cho tưới tiêu nông sản...
0	Đặc biệt, nhiều nhà nghiên cứu đã chỉ ra rằng, việc sử dụng trí tuệ nhân tạo có thể làm tăng tốc độ và độ chính xác cho chẩn đoán, khuyến nghị người nông dân hạn chế sử dụng thuốc bảo vệ thực vật và đảm bảo được chất lượng an toàn thực phẩm. Xử lý hình ảnh và thị giác máy tính là những nền tảng cơ sở đã được sử dụng để đáp ứng các nhu cầu trên. Nhận dạng trái cây thời kỳ thu hoạch từ dữ liệu ảnh số ở vùng trồng qua thiết bị camera là một trong trong số đó. Hiện nay, những nghiên cứu áp dụng này đang được triển khai đưa vào sử dụng ở một số nước tiên tiến.
0	Tuy nhiên, do vấn đề về giá thành của sản phẩm, cũng như các thiết bị này không tích hợp được Tiếng Việt là một trong những hạn chế mà người nông dân vùng nông thôn không thể tiếp cận được. Việt Nam là nước nông nghiệp, hằng năm cung cấp một sản lượng lớn lúa, gạo phục vụ cho người dân thành phố và xuất khẩu ra thị trường thế giới. Ngoài mặt hàng lúa gạo trái cây cũng chiếm tỷ trọng lớn và đem lại thu nhập bình quân cho người nông dân khá cao so với các sản lượng nông sản khác.
0	Việt Nam đang khoanh vùng trồng các loại cây chủ lực này nhằm hướng tới xuất khẩu ra thị trường thế giới. Trong đó, trái dứa (khóm) đang được nhiều trồng nhiều ở Miền Trung và cả nước (Tiền Giang, Thanh Hóa, Quảng Nam.) và được đánh giá rất cao về giá trị dinh dưỡng, cũng như sản lượng thu hoạch. Vì thế, việc nghiên cứu áp dụng kỹ thuật học sâu để phát hiện và nhận dạng trái dứa trong thời kỳ chín có ý nghĩa khoa học, thực tiễn và có tính thời sự. Có thể nói nhận dạng trái cây, đặc biệt đối với trái dứa là một thách thức lớn vì dứa có những đặc điểm giống nhau về hình thái và màu sắc.
0	Nhận dạng hình ảnh cần nhiều sức mạnh tính toán với các dữ liệu thu thập lớn (Big Data). Phát hiện và phân loại đối tượng sử dụng các phép chuyển đổi không gian màu để tách các đối tượng ra khỏi môi trường xung quanh và cho phép lưu được những thông tin về cấu trúc của trái dứa. Sử dụng mạng nơ-ron tích chập nhằm trích xuất các đặc trưng phục vụ cho quá trình huấn luyện để tiến hành phân loại và dự đoán trái dứa. Trái dứa là một loại cây ăn quả nhiệt đới hiện được trồng nhiều ở các nước Nam Mỹ.
0	Một số nước nhiệt đới và một số nước cận nhiệt đới có sông ngòi tương đối nhiều như đảo Hawaii, Đài Loan, Việt Nam và các nước khác, chiếm 60% sản lượng dứa của thế giới. Dứa ở Việt Nam được phân thành nhiều nhóm dựa trên các đặc điểm hình thái, sự phát triển và chất lượng hương vị. Nền nông nghiệp của Việt Nam hiện đang xuất khẩu dứa loại 1 đi khắp thế giới. Đã góp phần tạo ra nền kinh tế phát triển thịnh vượng và mang lại thu thập cho nhiều hộ gia đình.
0	Các thuật toán cải tiến từ mạng nơ-ron tích chập đã chứng minh tính ưu việt cho phép đạt tới ngưỡng chính xác tuyệt đối, được sử dụng phổ biến và cho kết quả cao trong các bài toán phân loại hình ảnh. Mạng nơ-ron tích chập (CNNs) cho phép khả năng trích chọn đặc trưng của lớp tích chập, sử dụng nhiều lớp tích chập và lớp gộp để xử lý tăng độ sáng và chất lượng hình ảnh [1]. Thuật toán Fast R-CNN được cải tiến từ đó, với khả năng phát hiện đối tượng, thuật toán thực hiện 2 bước chính.
0	Đầu tiên, sử dụng tìm kiếm có chọn lọc để tìm những vị trí phù hợp nhất của hộp giới hạn (Region of Interest hay -ROI), sau đó sử dụng các mô hình CNNs để trích xuất và nhận dạng các hộp giới hạn đó [2]. Với mô hình R-CNN kết hợp với phương pháp tìm kiếm chọn lọc để phát hiện các khu vực được đề xuất và là cơ sở cho sự ra đời mô hình Fast R-CNN [3]. Byoungjun Kim và cộng sự [4], đã nghiên cứu đề xuất sử dụng mạng nơ-ron sâu để cải thiện khả năng phát hiện chính xác trái Dâu Tây trong thử nghiệm môi trường bị che khuất, xen lẫn trong lá cây.
0	Jose Luis Rojas-Aranda và cộng sự [5] sử dụng Deep Learning phân loại trái cây cho cửa hàng bán lẻ, với màu đơn sắc RGB và biểu đồ RGB từ phân cụm K-mean được sử dụng đầu mô hình. Joseph Redmon [6] đề xuất hướng tiếp cận mới để phát hiện đối tượng trong ảnh bằng cách sử dụng hộp giới hạn và xác suất lớp liên quan để tối ưu hóa hiệu xuất phát hiện đối tượng trực tiếp trên ảnh. Trong lĩnh vực nông nghiệp các nhà nghiên cứu hiện nay đang rất quan tâm tới việc ứng dụng công nghệ cao trong vào trong các giai đoạn từ ươm giống cho đến thu hoạch. Có thể thấy, đã có những sản phẩm công nghệ có thể hỗ trợ nông dân trong công việc chăm sóc định kỳ, tăng chất lượng đầu ra của trái cây.
0	Với mong muốn có được nền nông nghiệp phát triển bền vững, vươn tầm thế giới. Chính phủ Việt Nam luôn quan tâm và chú trọng đầu tư lĩnh vực nông nghiệp mũi nhọn này. Trên thế giới hiện nay, có các nhóm nghiên cứu của Horea và cộng sự [7], đã đề xuất một hệ thống huấn luyện theo kỹ thuật mạng nơ-ron sâu để xác định các loại quả từ hình ảnh. Nhóm Susovan Jana và cộng sự [8] đề xuất hệ thống tự động nhận dạng trái cây và rau quả sử dụng công nghệ thị giác máy tính và máy học. Nhóm nghiên cứu của Md Tohidul Islam và cộng sự [9] đề xuất một hệ thống sử dụng mạng nơ-ron phức hợp để phân loại hình ảnh thực phẩm.
0	Emmanuel Karlo Nyarko và cộng sự [10] đề xuất hệ thống nhận dạng quả đến kỳ để thu hoạch tự động bằng robot thông minh. Tao Yongting và Zhou Jun [11] đã đề xuất hệ thống nhận dạng để thu hoạch quả táo tự động bằng robot. Walter Maldonado Jr. và Jose Carlos Barbosa [12] đã đề xuất hệ thống chiết xuất các đặc điểm của quả xanh, vấn đề này được thực hiện với sự kết hợp của các kỹ thuật chuyển đổi mô hình màu, tạo ngưỡng, cân bằng biểu đồ, lọc không gian với các toán tử Laplace và Sobel và làm mờ Gaussian. Xiangqin Wei và cộng sự [13] đề xuất hệ thống hái trái cây trưởng thành bằng rô bốt.
0	Sakib và cộng sự [14] đã đề xuất hệ thống nhận dạng trái cây bằng cách sử dụng Mạng neuron tích chập (CNN) và thị giác máy tính. Một số nghiên cứu liên quan sử mô hình nhận dạng CNN và các mô hình cải tiến được đề xuất ứng dụng trong phát hiện và nhận dạng hình ảnh kỹ thuật số [15]–[20]. Qua đó cho thấy việc sử dụng trí tuệ nhân tạo, đặc biệt là xử lý hình ảnh trong lĩnh vực nông nghiệp, dịch vụ đang được chú trọng và sẽ có những bước đột phá thời gian đến.
0	Dữ liệu hình ảnh đầu vào được nhóm tác giả tự thu thập bằng các thiết bị quay chụp chuyên dụng. Số lượng hình ảnh đạt được là 5000 hình ảnh, được chia thành 3 tập dữ liệu nhỏ bao gồm: Tập huấn luyện, tập xác nhận và tập kiểm thử cùng với thẻ định danh thông tin. Trước khi sử dụng 1000 hình ảnh để đánh giá, trích 3800 hình ảnh đầu tiên để phục vụ cho huấn luyện. Tập dữ liệu kiểm thử được tạo bằng cách sử dụng 200 hình từ tập dữ liệu. Sơ đồ tập dữ liệu hình ảnh đầu vào được biểu thị theo Hình 1 dưới đây: Tiền xử lý là một bước quan trọng trong quá trình xử lý dữ liệu, giúp hình ảnh được đồng bộ về kích thước, chất lượng hình ảnh phục vụ cho việc huấn luyện.
0	Cần phải xử lý hình ảnh để làm sạch dữ liệu hình ảnh đầu vào nhằm làm giảm thời gian huấn luyện và cải thiện hiệu suất. Sử dụng hình ảnh nhỏ hơn để huấn luyện có thể cắt giảm thời gian đáng kể, làm giàu dữ liệu nhằm tạo ra các mẫu dữ liệu mới từ dữ liệu hiện có. Từ đó tổng quát hóa các tình huống có thể xảy ra cho phép mô hình huấn luyện thế hệ mới được học hỏi từ các tình huống rộng hơn. Các hình ảnh lớn được cắt thành các hình ảnh có độ phân giải (416x416). Mô hình có thể lưu trữ nhiều dữ liệu hơn và xử lý nhanh hơn. Các bước xử lý các giai đoạn biến đổi khác nhau như xoay, tạo nhiễu, thay đổi độ sáng.
0	Quá trình làm giàu dữ liệu hình ảnh trái dứa cần cung cấp hình ảnh đã xử lý thô và xây dựng mô hình huấn luyện gán nhãn và đánh dấu vị trí trái dứa có trong hình ảnh. Annotate với chức năng gán nhãn đối với trái dứa sau khi được vẽ hình hộp chữ nhật bao quanh có thể nhìn thấy được, qua đó không chỉ thu được các hộp giới hạn mà còn cả các pixel tương ứng của mỗi trái dứa. Nhiệm vụ của gán nhãn nhằm trích xuất các vùng đặc trưng dựa trên các đặc điểm hình thái như mắt, cuống, màu sắc, hình dạng, v.v. Kết quả cuối cùng sẽ là một tập dữ liệu định dạng gán nhãn được lưu ở định dạng Microsoft COCO.
0	Tại Hình 2 biểu diễn mật độ vị trí của trái dứa được gán nhãn được biểu diễn bằng sơ đồ nhiệt. Mạng CNN được xây dựng để phân loại hình ảnh sử dụng kỹ thuật học sâu. Xây dựng mô hình CNN dễ dàng hơn, cùng với việc sử dụng thư viện Keras với ngôn ngữ Python. Mô hình sử dụng các lớp CNN như Conv2D & MaxPooling2D. Conv2D còn được gọi là 2D Convolution Layer. Lớp này tạo ra hàng chục đầu ra bằng cách tạo ra nhân chập trùng với lớp đầu vào. MaxPooling2D là dùng cho hoạt động tổng hợp tối đa cho dữ liệu không gian. Dữ liệu không gian có thể được định nghĩa là biểu diễn thông tin về một đối tượng vật lý bằng các giá trị số.
0	Việc chọn phần tử tối đa từ vùng của bản đồ điện tử được bao phủ bởi bộ lọc là toán hạng được thực hiện bởi lớp tổng hợp tối đa. Để giảm kích thước của vùng đặc trưng (Feature Map), các lớp gộp được sử dụng. Do đó, có thể kết luận rằng, việc sử dụng các lớp tổng hợp làm giảm số lượng các tham số cần tìm hiểu và khả năng tính toán được thực hiện trong mạng nơ-rơn nhân tạo. Thuật toán Fast R-CNN được đánh giá là mô hình nhận dạng khá tốt về độ chính xác phát hiện và tốc độ phát hiện.
0	Đã có nhiều nghiên cứu đánh giá về độ chính xác mô hình này với các mô hình nhận dạng tương đồng khác như mô hình R-CNN [2]. Việc huấn luyện mô hình R-CNN có sẵn tốn khá nhiều tài nguyên tính toán, lưu trữ và thời gian vì mỗi hình ảnh yêu cầu phân loại các lớp với khoảng 2000 khu vực đề xuất, do đó, thời gian huấn luyện rất chậm và không thể áp dụng trong thời gian thực vì mỗi hình ảnh trong bộ thử nghiệm mất hơn 47 giây để xử lý và cho ra kết quả dự đoán. Trong khi đó, Fast R-CNN thực hiện song song việc trích xuất đặc trưng với việc tách các vùng đề xuất 2000 khu vực vào CNN và thực hiện trích xuất từ bản đồ đặc trưng.
0	Giảm được chi phí huấn luyện và thời gian đào tạo chỉ 0,32 giây trên cùng bộ dữ liệu như R-CNN, nên cho kết quả thực hiện nhanh hơn rất nhiều. Về thời gian phát hiện và nhận dạng bằng camera di động tương đồng với hình ảnh, vì mỗi giây camera sẽ gửi 24 khung hình/1 giây cho hệ thống. R-CNN xử lý 1 hình ảnh mất đến 47 giây là khá lâu nên không thể đáp ứng việc nhận dạng theo thời gian thực. Trong khi đó, Fast R-CNN chỉ mất khoảng 1s để trả về kết quả nên có khả năng xử lý camera di động và phát hiện đối tượng theo thời gian thực. Fast R-CNN sẽ tạo ra một vùng đối tượng đặc trưng phức tạp, tải toàn bộ hình ảnh vào ConvNet (lớp chập + lớp tổng hợp tối đa).
0	Sau khi kết hợp CNN, sẽ có một bản đồ đặc trưng nhỏ hơn đáng kể so với hình ảnh gốc, nhờ lớp ROI Pooling, lớp này sẽ đưa các vùng hình ảnh về kích thước hình vuông và định hình lại với cùng kích thước đầu ra. Tiếp tục qua các lớp được kết nối đầy đủ cho đến khi bạn đạt đến đầu ra vectơ tính năng ROI. Cuối cùng, để có được kết quả cuối cùng, mô hình sẽ dự đoán lớp và độ lệch của hộp giới hạn cho vùng phân mảnh đó. Dựa trên mô hình huấn luyện đã hoàn thiện, nhóm tác giả đã phát triển một hệ thống có khả năng phân tích và đánh giá hình ảnh đầu vào.
0	Bên cạnh đó, còn phát triển thêm giao diện người dùng thân thiện được xây dựng bằng Framework Django với ngôn ngữ lập trình Python. Người dùng chỉ cần chỉ định đường dẫn của hình ảnh đó trong máy tính, hệ thống sẽ đưa hình ảnh lên giao diện. Tiến hành nhận dạng, hệ thống sẽ kiểm tra hình ảnh có tồn tại không và tiến hành dự đoán với mô hình huấn luyện từ trước. Để đánh giá mô hình, nhóm tác giả so sánh kết quả từ mô hình Fast R-CNN trên tập xác nhận đã thu thập từ trước.
0	Hệ số Intersection Over Union - IoU đạt được 0,5. Trường hợp này được coi là vị trí đạt được tiêu chuẩn. IoU được xác định bằng cách chia số pixel trong giao điểm cho toàn bộ pixel. Độ chính xác trung bình (AP) là diện tích của biểu đồ bao gồm Precisions, Recall và mAP là diện tích trung bình của toàn bộ biểu đồ. Trong đó, đại lượng Precisions, Recall và F1 được sử dụng để đánh giá hiệu suất mô hình và được tính theo các công thức bên dưới: Trong đó: F1 là đại lượng đánh giá độ chính xác trung bình điều hòa giữa precision và recall; Precision phản ánh mức độ chuẩn xác của mô hình dự đoán đúng (nhóm positive); Tỷ lệ precision càng cao thì kết quả dự đoán của mô hình càng tốt trong việc phát hiện trái dứa chín.
0	Recall: đo lường tỷ lệ dự báo chính xác các trường hợp positive trên toàn bộ các dữ liệu mẫu dứa chín (positive). Để tính được recall thì phải biết trước nhãn của dữ liệu. Do đó, recall có thể được dùng để đánh giá trên tập train và validation vì đã biết trước nhãn. Trên tập test khi dữ liệu được coi như mới hoàn toàn và chưa biết nhãn thì sẽ sử dụng precision. Các chỉ số TP, TN đại diện cho giá trị dự đoán chính xác, FP, FN đại diện cho giá trị dự đoán sai lệch. Cụ thể: TP: Giá trị dự đoán chính xác; Khi mô hình dự đoán đúng (lớp Positive) cho cho trái dứa chín.
0	FP: Giá trị dự đoán sai lệch; Khi mô hình dự đoán đúng (lớp Positive) cho trái dứa chín. TN: Giá trị dự đoán đúng; Khi mô hình dự đoán sai (lớp Negative) cho cho trái dứa chưa chín. FN: Giá trị dự đoán sai lệch; Khi mô hình dự đoán sai (lớp Negative) cho cho trái dứa chưa chín. Hình 5 cho thấy, biểu đồ tính toán độ chính xác đạt tối đa từ 90% cho tới 95%. Với đường cong rõ rệt khi sử dụng α hệ số nhỏ để có thể tính toán và không bị vượt ngưỡng sau mỗi lần lặp. Sơ đồ biểu thị hàm tính toán mất mát và giá trị sai được giảm rõ rệt, gần đạt được ngưỡng tối ưu toàn cục.
0	Việc sử dụng nhiều hình ảnh trái dứa với kích cỡ khác nhau được cho trước sẽ giúp kết quả nghiên cứu có thể đạt được độ chính xác cao. Kết quả nhận dạng trái dứa bằng hình ảnh được thử nghiệm tại Hình 6 (a) và đầu ra được hiển thị nhận dạng trái dứa với độ chính xác đạt 95,624% trong Hình 6 (b). Ngoài ra, Hình 7 và Hình 8 thể hiện kết quả huấn luyện từ mô phỏng thuật toán Fast R-CNN với khả năng phát hiện và nhận dạng hình ảnh trái dứa nằm xen lẫn trong vùng lá dứa được chụp trên cánh đồng dứa với kết quả dự đoán đạt được độ chính xác cao, đạt tỷ lệ 95,62) cho phép để có thể thực nghiệm tại nông trường thực tế.
0	Bài báo tập trung cho việc ứng dụng kỹ thuật xử lý hình, thị giác máy tính và kỹ thuật học máy để nhận dạng hình ảnh trong lĩnh vực trái cây chủ lực của nông nghiệp Việt Nam với quy mô lớn. Kích thước, màu sắc, hình dạng, kết cấu và các điểm khuyết tật là những đặc điểm quan trọng của trái dứa trong việc trích xuất đặc trưng. Với mục đích giúp cho người nông dân, giảm sức lao động bằng tay chân thay vào đó là kiểm tra trái dứa bằng hệ thống quan sát. Hệ thống được triển khai cung cấp kết quả xác thực, độ chính xác và tốc độ xử lý nhanh.
0	Trong nghiên cứu này, nhóm tác giả tập trung sử dụng mô hình học sâu (Deep Learning) để nhận dạng, phát hiện trái dứa bằng cách triển khai Fast R-CNN cải tiến. Kết quả thử nghiệm cho thấy, hệ thống hoạt động tốt để có thể nhận dạng và sẵn sàng cho các ứng dụng thực tế với chất lượng phát hiện (mAP) có tỷ lệ cao hơn R-CNN, đồng thời giảm tải bộ nhớ đệm khi truy xuất dữ liệu. Qua phân tích kết quả thực nghiệm đã chỉ ra được những điểm mạnh, điểm yếu trong thuật toán nghiên cứu trước đây và nghiên cứu cải tiến của nhóm tác giả.
1	Điện toán đám mây đang trở thành nền tảng cốt lõi cho việc xây dựng các hệ thống đô thị thông minh nhờ khả năng lưu trữ, xử lý và phân tích dữ liệu lớn theo thời gian thực. Nghiên cứu này tập trung phân tích hiệu quả của mô hình điện toán đám mây trong quản lý giao thông, năng lượng và dịch vụ công tại các thành phố lớn. Dữ liệu nghiên cứu được thu thập từ 5 thành phố thí điểm với hơn 12 triệu bản ghi cảm biến IoT trong vòng 12 tháng. Kết quả cho thấy việc triển khai hạ tầng đám mây giúp giảm trung bình 28% chi phí vận hành hệ thống CNTT và cải thiện tốc độ xử lý dữ liệu lên đến 42%.
1	Ngoài ra, thời gian phản hồi dịch vụ công trực tuyến giảm từ 3,5 giây xuống còn 1,9 giây, góp phần nâng cao trải nghiệm người dân. Trong bối cảnh đô thị hóa nhanh chóng, các thành phố đang đối mặt với áp lực lớn về quản lý dữ liệu và cung cấp dịch vụ công hiệu quả. Điện toán đám mây được xem là giải pháp then chốt nhờ khả năng mở rộng linh hoạt và tối ưu tài nguyên. Theo báo cáo của IDC, quy mô thị trường điện toán đám mây toàn cầu đạt hơn 545 tỷ USD vào năm 2023 và dự kiến tăng trưởng trung bình 17% mỗi năm.
1	Việc áp dụng công nghệ này trong đô thị thông minh cho phép tích hợp dữ liệu từ nhiều nguồn như camera giao thông, cảm biến môi trường và hệ thống quản lý năng lượng. Nghiên cứu này nhằm đánh giá mức độ tác động thực tế của điện toán đám mây đến hiệu suất vận hành và chất lượng dịch vụ đô thị. Nghiên cứu sử dụng phương pháp thực nghiệm kết hợp phân tích định lượng. Hệ thống đám mây được triển khai trên nền tảng lai (hybrid cloud) bao gồm máy chủ nội bộ và dịch vụ đám mây công cộng.
1	Dữ liệu được thu thập từ hơn 3.200 cảm biến IoT đặt tại các khu vực giao thông, tòa nhà công cộng và trạm quan trắc môi trường. Các chỉ số đánh giá bao gồm độ trễ xử lý, mức sử dụng tài nguyên máy chủ, chi phí vận hành và mức độ hài lòng của người dùng. Hệ thống được vận hành liên tục trong 12 tháng và so sánh với mô hình hạ tầng truyền thống trước đó để đánh giá hiệu quả cải thiện. Kết quả thực nghiệm cho thấy hệ thống điện toán đám mây giúp giảm trung bình 35% tải cho máy chủ cục bộ và tăng khả năng mở rộng lên gấp 2,5 lần so với mô hình truyền thống.
1	Thời gian xử lý dữ liệu thời gian thực giảm từ 850 ms xuống còn 490 ms. Đặc biệt, chi phí bảo trì hạ tầng CNTT giảm khoảng 31% do giảm nhu cầu đầu tư phần cứng. Khảo sát người dùng với 1.200 người tham gia cho thấy 78% đánh giá mức độ hài lòng tăng lên rõ rệt khi sử dụng các dịch vụ đô thị trực tuyến. Tuy nhiên, thách thức về bảo mật dữ liệu và phụ thuộc nhà cung cấp vẫn cần được giải quyết trong các nghiên cứu tiếp theo. Nghiên cứu khẳng định điện toán đám mây đóng vai trò quan trọng trong việc nâng cao hiệu quả quản lý và chất lượng dịch vụ của đô thị thông minh.
1	Lập trình hướng đối tượng (OOP) là mô hình lập trình phổ biến trong phát triển phần mềm hiện đại nhờ khả năng tái sử dụng và bảo trì cao. Nghiên cứu này đánh giá hiệu quả của OOP trong việc xây dựng hệ thống quản lý giáo dục tại các trường đại học. Hệ thống thử nghiệm được triển khai tại 3 cơ sở đào tạo với hơn 25.000 sinh viên và 1.200 giảng viên. Kết quả cho thấy thời gian phát triển phần mềm giảm 22% so với mô hình lập trình thủ tục, trong khi số lỗi phát sinh trong quá trình bảo trì giảm tới 37%. Ngoài ra, khả năng mở rộng hệ thống được cải thiện rõ rệt khi số lượng người dùng tăng gấp đôi mà không làm giảm hiệu năng.
1	Sự phát triển nhanh chóng của công nghệ thông tin đã thúc đẩy nhu cầu xây dựng các hệ thống quản lý giáo dục hiệu quả và linh hoạt. Lập trình hướng đối tượng, với các đặc tính như đóng gói, kế thừa và đa hình, giúp mô hình hóa các thực thể trong giáo dục một cách trực quan. Theo khảo sát của Stack Overflow năm 2023, hơn 80% lập trình viên sử dụng các ngôn ngữ hướng đối tượng như Java, C++ và C#. Nghiên cứu này tập trung phân tích việc áp dụng OOP trong hệ thống quản lý sinh viên, học phần và kết quả học tập, từ đó đánh giá tác động của mô hình này đến hiệu suất phát triển và vận hành phần mềm.
1	Hệ thống quản lý giáo dục được thiết kế theo kiến trúc hướng đối tượng với các lớp đại diện cho sinh viên, giảng viên, môn học và đăng ký học phần. Dữ liệu thử nghiệm bao gồm hơn 1,8 triệu bản ghi học tập trong vòng 3 năm. Các chỉ số đánh giá gồm thời gian phát triển, số dòng mã, số lỗi phát hiện trong giai đoạn kiểm thử và thời gian phản hồi hệ thống. Kết quả được so sánh với một hệ thống tương tự được xây dựng theo mô hình lập trình thủ tục nhằm đánh giá sự khác biệt về hiệu quả và khả năng bảo trì.
1	Kết quả cho thấy hệ thống sử dụng OOP có số dòng mã giảm 18% nhưng vẫn đảm bảo đầy đủ chức năng. Thời gian phản hồi trung bình của hệ thống đạt 1,4 giây khi có 5.000 người dùng đồng thời, thấp hơn 0,6 giây so với hệ thống đối chứng. Số lỗi phát sinh trong giai đoạn bảo trì giảm đáng kể nhờ khả năng tái sử dụng và mở rộng lớp. Tuy nhiên, chi phí đào tạo lập trình viên ban đầu cao hơn khoảng 12%, cho thấy cần có chiến lược đào tạo phù hợp để khai thác tối đa lợi ích của OOP.
1	Lập trình hướng đối tượng chứng minh được hiệu quả rõ rệt trong phát triển hệ thống quản lý giáo dục quy mô lớn. Việc áp dụng OOP không chỉ giúp giảm chi phí bảo trì mà còn nâng cao khả năng mở rộng và ổn định của phần mềm, phù hợp với nhu cầu chuyển đổi số trong giáo dục hiện nay. Thông qua các cơ chế như đóng gói và kế thừa, hệ thống có thể dễ dàng tích hợp thêm các chức năng mới như học trực tuyến, đánh giá năng lực và phân tích dữ liệu học tập mà không làm ảnh hưởng đến các thành phần hiện có. Bên cạnh đó, cấu trúc hướng đối tượng giúp tăng tính nhất quán trong thiết kế, hỗ trợ làm việc nhóm hiệu quả và rút ngắn thời gian nâng cấp hệ thống.
1	Thuật toán K-Nearest Neighbors (KNN) là một trong những phương pháp học máy đơn giản nhưng hiệu quả trong phân loại và dự đoán. Nghiên cứu này đề xuất mô hình KNN để dự đoán hành vi mua sắm của khách hàng trong lĩnh vực bán lẻ. Dữ liệu nghiên cứu gồm 120.000 giao dịch từ 15 cửa hàng trong vòng 6 tháng. Kết quả cho thấy mô hình KNN đạt độ chính xác trung bình 87,6% khi dự đoán nhóm sản phẩm khách hàng có khả năng mua tiếp theo. Việc ứng dụng mô hình giúp tăng tỷ lệ chuyển đổi bán hàng lên 14% và giảm tồn kho khoảng 9% so với phương pháp truyền thống.
1	Trong bối cảnh cạnh tranh gay gắt của thị trường bán lẻ, việc hiểu và dự đoán hành vi tiêu dùng đóng vai trò quan trọng. Thuật toán KNN, dựa trên khoảng cách giữa các điểm dữ liệu, cho phép xác định hành vi của khách hàng mới dựa trên các khách hàng tương tự trong quá khứ. Theo báo cáo McKinsey, các doanh nghiệp ứng dụng phân tích dữ liệu hành vi khách hàng có thể tăng doanh thu từ 5–10%. Nghiên cứu này tập trung phân tích khả năng ứng dụng KNN trong thực tế, đặc biệt là trong môi trường dữ liệu lớn và đa chiều của ngành bán lẻ.
1	Dữ liệu đầu vào bao gồm độ tuổi, giới tính, tần suất mua hàng, giá trị đơn hàng và danh mục sản phẩm. Dữ liệu được chuẩn hóa và chia thành tập huấn luyện 75% và tập kiểm thử 25%. Khoảng cách Euclidean được sử dụng để tính độ tương đồng giữa các khách hàng. Giá trị K được thử nghiệm từ 3 đến 15 nhằm tìm ra cấu hình tối ưu. Các chỉ số đánh giá bao gồm độ chính xác, độ nhạy và F1-score để đảm bảo đánh giá toàn diện hiệu suất mô hình. Kết quả cho thấy mô hình đạt hiệu suất cao nhất khi K = 7, với độ chính xác 89,1% và F1-score đạt 0,88. Khi triển khai thực tế, hệ thống gợi ý sản phẩm dựa trên KNN giúp tăng giá trị đơn hàng trung bình từ 420.000 đồng lên 485.000 đồng.
1	Tuy nhiên, thời gian tính toán tăng đáng kể khi kích thước dữ liệu lớn, cho thấy cần kết hợp KNN với các kỹ thuật giảm chiều dữ liệu để tối ưu hiệu năng. Thuật toán KNN là giải pháp hiệu quả và dễ triển khai trong dự đoán hành vi tiêu dùng. Với độ chính xác cao và khả năng áp dụng linh hoạt, KNN góp phần nâng cao hiệu quả kinh doanh và hỗ trợ ra quyết định trong lĩnh vực bán lẻ hiện đại. Việc khai thác mối quan hệ tương đồng giữa các nhóm khách hàng giúp doanh nghiệp cá nhân hóa chiến lược tiếp thị, tối ưu hóa gợi ý sản phẩm và cải thiện trải nghiệm mua sắm tổng thể.
1	"Trong kỷ nguyên chuyển đổi số năm 2026, điện toán đám mây không còn là một lựa chọn mà đã trở thành nền tảng bắt buộc đối với các doanh nghiệp muốn tồn tại. Cloud Computing cho phép các đơn vị tiếp cận nguồn lực tính toán khổng lồ mà không cần đầu tư quá lớn vào phần cứng vật lý. Theo các báo cáo thị trường gần đây, việc chuyển dịch sang môi trường đám mây giúp các doanh nghiệp tiết kiệm trung bình khoảng 35% chi phí vận hành hàng năm. Thay vì duy trì các trung tâm dữ liệu cồng kềnh, doanh nghiệp có thể thuê dịch vụ theo mô hình ""Pay-as-you-go"". Điều này đặc biệt quan trọng đối với các startup, nơi nguồn vốn cần được tối ưu hóa cho các hoạt động kinh doanh cốt lõi hơn là quản lý hạ tầng IT phức tạp và tốn kém."
1	"Một nghiên cứu thực hiện trên 100 doanh nghiệp SMEs tại Việt Nam cho thấy sự khác biệt rõ rệt giữa việc dùng máy chủ vật lý và đám mây. Cụ thể, thời gian triển khai hệ thống mới giảm từ 4 tuần xuống còn chỉ vỏn vẹn 48 giờ khi sử dụng các dịch vụ như AWS hoặc Google Cloud. Tỉ lệ rủi ro mất mát dữ liệu cũng giảm mạnh tới 92% nhờ vào các cơ chế sao lưu tự động và dự phòng đa vùng. Về mặt tài chính, chi phí bảo trì hệ thống giảm 45%, trong khi hiệu suất làm việc của nhân viên IT tăng thêm 25% do không còn phải xử lý các lỗi phần cứng thủ công. Những con số này minh chứng cho thấy đám mây chính là ""đòn bẩy"" kinh tế mạnh mẽ nhất trong thời đại công nghệ hiện nay."
1	Ứng dụng của Cloud Computing trong đời sống vô cùng đa dạng, từ việc lưu trữ hình ảnh cá nhân đến quản lý các chuỗi cung ứng toàn cầu. Trong tương lai gần, sự kết hợp giữa Edge Computing và Cloud sẽ giúp giảm độ trễ dữ liệu xuống dưới mức 5ms, phục vụ cho các ứng dụng đòi hỏi tính thời gian thực cao như xe tự hành hay phẫu thuật từ xa. Đối với người dùng phổ thông, việc sử dụng các nền tảng SaaS (Software as a Service) đã giúp đơn giản hóa mọi quy trình làm việc từ soạn thảo văn bản đến chỉnh sửa video chuyên nghiệp. Việc phổ cập hóa công nghệ đám mây sẽ tiếp tục là xu hướng chủ đạo, góp phần xây dựng một xã hội số thông minh, nơi dữ liệu luôn sẵn sàng ở bất cứ đâu và vào bất cứ lúc nào.
1	"Lập trình hướng đối tượng (OOP) cung cấp một phương pháp luận mạnh mẽ để mô hình hóa các thực thể phức tạp trong thế giới thực vào trong mã nguồn máy tính. Thay vì viết các dòng lệnh tuần tự, OOP cho phép chúng ta định nghĩa các ""Object"" như Xe cộ, Đèn giao thông, hay Cảm biến đường phố với các thuộc tính và hành vi riêng biệt. Việc sử dụng bốn trụ cột chính của OOP bao gồm: Đóng gói, Kế thừa, Đa hình và Trừu tượng giúp hệ thống trở nên linh hoạt và dễ bảo trì hơn. Ví dụ, một đối tượng ""Xe máy"" có thể kế thừa các đặc tính từ lớp ""Phương tiện"", giúp giảm thiểu sự trùng lặp mã nguồn tới 40% trong quá trình phát triển các phần mềm quản lý đô thị hiện đại."
1	"Trong các dự án phần mềm quy mô lớn, tính bền vững của mã nguồn là yếu tố then chốt. Thực nghiệm cho thấy, các hệ thống được thiết kế theo cấu trúc OOP có khả năng tái sử dụng mã nguồn lên đến 60%. Khi cần thêm một loại phương tiện mới vào hệ thống quản lý giao thông, ví dụ như ""Xe điện tự hành"", lập trình viên chỉ cần tạo một lớp con mới mà không làm ảnh hưởng đến cấu trúc hiện tại của toàn bộ chương trình. Điều này giúp giảm thiểu rủi ro xuất hiện lỗi hệ thống (bugs) xuống mức dưới 5% mỗi khi có bản cập nhật mới."
1	Số liệu thống kê từ các nhóm phát triển phần mềm cho thấy thời gian gỡ lỗi và bảo trì hệ thống giảm 30% so với các phương pháp lập trình hướng thủ tục truyền thống trước đây. Nhờ có OOP, việc phát triển các ứng dụng di động mà chúng ta sử dụng hàng ngày như Grab, Shopee hay Facebook trở nên khả thi và hiệu quả hơn. Khả năng chia nhỏ bài toán thành các đối tượng độc lập giúp các nhóm lập trình viên hàng trăm người có thể làm việc song song trên cùng một dự án mà không gây xung đột. Điều này thúc đẩy tốc độ ra mắt sản phẩm nhanh hơn 1.5 lần so với các phương pháp cũ.
1	"Thuật toán K-Nearest Neighbors (KNN) là một trong những phương pháp phân loại và học máy đơn giản nhưng vô cùng hiệu quả dựa trên nguyên lý ""gần mực thì đen, gần đèn thì rạng"". KNN hoạt động bằng cách đo lường khoảng cách (thường là khoảng cách Euclidean) giữa điểm dữ liệu mới và các điểm dữ liệu có sẵn trong tập huấn luyện. Với giá trị k được chọn phù hợp, thuật toán sẽ dự đoán nhãn của dữ liệu mới dựa trên đa số các láng giềng gần nhất của nó. Trong môi trường thương mại điện tử, nếu một khách hàng có hành vi mua sắm tương đồng với 5 người khác (k=5), hệ thống sẽ tự động đề xuất các sản phẩm mà 5 người kia đã mua. Đây là nền tảng của các hệ thống gợi ý thông minh hiện nay."
1	Trong một thử nghiệm phân loại hành vi tiêu dùng với tập dữ liệu gồm 50.000 mẫu, thuật toán KNN cho thấy độ chính xác đạt mức 88.5% khi chọn giá trị k=7. Tuy nhiên, hiệu suất của thuật toán phụ thuộc nhiều vào việc làm sạch dữ liệu và chuẩn hóa các biến số. Khi áp dụng các kỹ thuật chuẩn hóa (Normalization), thời gian tính toán giảm được 20% và sai số giảm xuống chỉ còn 4%. Một điểm yếu cần lưu ý là KNN tiêu tốn khá nhiều bộ nhớ khi tập dữ liệu quá lớn, vì nó cần lưu trữ toàn bộ dữ liệu huấn luyện để so sánh.
1	Dù vậy, với sự hỗ trợ của phần cứng mạnh mẽ hiện nay, KNN vẫn là sự lựa chọn ưu tiên cho các bài toán phân loại nhanh trong lĩnh vực y tế (chẩn đoán bệnh dựa trên triệu chứng) và tài chính (phát hiện gian lận thẻ tín dụng). Ứng dụng rõ rệt nhất của KNN trong đời sống chính là các hệ thống lọc thư rác và nhận diện hình ảnh đơn giản. Khi bạn nhận được một email, thuật toán sẽ so sánh các đặc điểm của email đó với các mẫu thư rác đã biết để đưa ra quyết định phân loại.
1	"Trong lĩnh vực bán lẻ, việc áp dụng KNN giúp tăng tỉ lệ chuyển đổi đơn hàng lên thêm 15% thông qua việc ""gợi ý thông minh"". Người dùng cảm thấy được thấu hiểu hơn khi các ứng dụng luôn hiển thị đúng thứ họ cần. Trong tương lai, khi kết hợp với các kỹ thuật giảm chiều dữ liệu, KNN sẽ càng trở nên mạnh mẽ hơn, giúp các thiết bị IoT nhỏ gọn cũng có thể thực hiện các tác vụ học máy phức tạp ngay tại chỗ mà không cần gửi dữ liệu về máy chủ trung tâm."
1	Trong bối cảnh dữ liệu y tế ngày càng khổng lồ, việc sử dụng đơn lẻ Public Cloud (Đám mây công cộng) hay Private Cloud (Đám mây riêng) đều bộc lộ những hạn chế về chi phí hoặc tính bảo mật. Mô hình Hybrid Cloud nổi lên như một giải pháp tối ưu, cho phép các bệnh viện lưu trữ thông tin hành chính trên Public Cloud để tiết kiệm chi phí, trong khi các hồ sơ bệnh án nhạy cảm được bảo mật tuyệt đối tại Private Cloud. Việc phân tách dữ liệu này giúp hệ thống vừa tận dụng được sức mạnh tính toán linh hoạt, vừa tuân thủ các quy định khắt khe về quyền riêng tư như HIPAA hay GDPR.
1	Điều này tạo ra một hệ sinh thái số hóa y tế bền vững, nơi dữ liệu luôn được luân chuyển một cách an toàn và hiệu quả nhất. Kiến trúc của một hệ thống Hybrid Cloud trong y tế đòi hỏi sự kết nối chặt chẽ thông qua các cổng API và mạng riêng ảo (VPN) chuyên dụng. Cơ chế điều phối dữ liệu (Orchestration) đóng vai trò trung tâm, đảm bảo rằng khi nhu cầu truy cập tăng đột biến – chẳng hạn trong các đợt dịch bệnh – hệ thống sẽ tự động mở rộng tài nguyên từ đám mây riêng sang đám mây công cộng mà không làm gián đoạn dịch vụ.
1	Các thuật toán cân bằng tải được cấu hình để ưu tiên băng thông cho các tác vụ quan trọng như hội chẩn từ xa hoặc chẩn đoán hình ảnh AI. Việc thiết lập một hạ tầng như vậy đòi hỏi sự hiểu biết sâu sắc về cả phần cứng vật lý lẫn các lớp ảo hóa phần mềm phức tạp, nhằm đảm bảo tính toàn vẹn của dữ liệu y khoa. Dựa trên dữ liệu triển khai tại các hệ thống bệnh viện lớn vào đầu năm 2026, mô hình Hybrid Cloud đã giúp giảm độ trễ truy xuất dữ liệu xuống 40% so với mô hình truyền thống.
1	Về mặt kinh tế, tổng chi phí sở hữu (TCO) trong vòng 5 năm giảm trung bình 28%, chủ yếu nhờ giảm bớt việc đầu tư vào các máy chủ dự phòng đắt đỏ tại chỗ. Một con số đáng chú ý khác là khả năng khôi phục sau thảm họa (Disaster Recovery) đã cải thiện đáng kể, với thời gian phục hồi hệ thống (RTO) chỉ mất chưa đầy 15 phút, so với mức 4 giờ trước đây. Những số liệu này khẳng định rằng đầu tư vào hạ tầng đám mây lai không chỉ là bài toán công nghệ mà còn là chiến lược tài chính khôn ngoan cho ngành y tế.
1	Việc phổ cập Hybrid Cloud giúp việc chăm sóc sức khỏe trở nên công bằng hơn khi các bệnh viện tuyến dưới có thể tiếp cận dữ liệu chuyên gia từ tuyến trên gần như tức thời. Trong tương lai, việc tích hợp Blockchain vào lớp lưu trữ của Hybrid Cloud sẽ tạo ra một sổ cái không thể thay đổi, giúp minh bạch hóa toàn bộ quá trình điều trị của bệnh nhân. Điều này không chỉ ngăn chặn gian lận bảo hiểm mà còn tạo nền tảng cho y học cá nhân hóa, nơi các phác đồ điều trị được xây dựng dựa trên lịch sử dữ liệu chuẩn xác nhất. Với sự hỗ trợ của mạng 5G và 6G, khả năng kết nối giữa đám mây và các thiết bị đeo thông minh sẽ biến mỗi ngôi nhà thành một phòng khám từ xa hiện đại.
1	"Lập trình hướng đối tượng (OOP) đóng vai trò là ""xương sống"" cho các phần mềm điều khiển xe tự hành phức tạp. Bằng cách sử dụng tính đa hình (Polymorphism), các kỹ sư có thể định nghĩa một lớp trừu tượng ""Cảm biến"" chung cho tất cả các thiết bị như LiDAR, Radar và Camera. Mỗi loại cảm biến này sẽ có cách thực thi riêng cho phương thức ""quét môi trường"" nhưng vẫn được hệ thống điều khiển chính xử lý một cách thống nhất. Tính đóng gói (Encapsulation) đảm bảo rằng các trạng thái nội tại của động cơ hay hệ thống phanh được bảo vệ khỏi các can thiệp bên ngoài không mong muốn. Điều này không chỉ giúp mã nguồn trở nên gọn gàng mà còn cực kỳ an toàn, ngăn chặn các lỗi logic có thể dẫn đến tai nạn nghiêm trọng trong thực tế."
1	"Trong phát triển phần mềm xe tự hành, các Design Patterns như Strategy hay Observer được áp dụng triệt để để giải quyết bài toán thay đổi hành vi theo thời gian thực. Ví dụ, Pattern ""Strategy"" cho phép xe thay đổi thuật toán lái xe tùy thuộc vào điều kiện thời tiết: một chiến lược lái cho đường khô ráo và một chiến lược thận trọng hơn cho đường trơn trượt. Trong khi đó, Pattern ""Observer"" giúp hệ thống trung tâm nhận thông báo ngay lập tức từ các cảm biến khi phát hiện vật cản bất ngờ. Việc sử dụng các mẫu thiết kế chuẩn hóa này giúp đội ngũ lập trình viên giảm được 50% thời gian hội nhập cho nhân sự mới, do cấu trúc mã nguồn tuân theo các quy tắc thiết kế phần mềm phổ quát và dễ hiểu."
1	Độ tin cậy là yếu tố sống còn đối với xe tự hành, và OOP cung cấp môi trường lý tưởng cho việc kiểm thử đơn vị (Unit Testing). Nhờ cấu trúc module hóa, mỗi đối tượng có thể được kiểm tra độc lập trong các môi trường giả lập trước khi tích hợp vào hệ thống thật. Các báo cáo kỹ thuật cho thấy hệ thống được xây dựng trên nền tảng OOP chặt chẽ có mật độ lỗi (Bug Density) thấp hơn 35% so với các phương pháp lập trình tự do. Trong các bài thử nghiệm va chạm ảo, tỷ lệ phản ứng chính xác của các đối tượng điều khiển đạt 99.997%, một con số ấn tượng chứng minh khả năng xử lý logic của hướng đối tượng.
1	Sự ổn định này giúp các nhà sản xuất xe tự tin hơn khi đưa sản phẩm ra thị trường thương mại. Lợi ích lớn nhất của OOP chính là khả năng mở rộng hệ thống mà không cần đập đi xây lại từ đầu. Khi công nghệ pin mới hoặc các cảm biến thế hệ tiếp theo ra đời, các kỹ sư chỉ cần tạo ra các lớp đối tượng mới kế thừa từ các lớp cũ. Điều này giúp vòng đời của phần mềm xe tự hành kéo dài hơn, tiết kiệm hàng tỷ đô la chi phí nghiên cứu và phát triển cho các tập đoàn công nghệ.
1	"Trong việc giám sát môi trường, thuật toán K-Nearest Neighbors (KNN) được sử dụng để dự báo chỉ số chất lượng không khí (AQI) dựa trên các thông số như nồng độ bụi mịn PM2.5, CO2 và độ ẩm. Bước đầu tiên và quan trọng nhất là thu thập dữ liệu từ mạng lưới hàng ngàn cảm biến IoT rải rác trong thành phố. Do dữ liệu môi trường thường có nhiễu và các giá trị thiếu, việc tiền xử lý (Preprocessing) là bắt buộc để đảm bảo thuật toán KNN không bị sai lệch. Các kỹ thuật như làm mượt dữ liệu và chuẩn hóa thang đo (Min-Max Scaling) giúp các đặc trưng về nồng độ khí có trọng số tương đương nhau trong không gian vector. Đây là tiền đề để thuật toán có thể tìm ra những ""láng giềng"" thực sự tương đồng về điều kiện khí tượng."
1	Hiệu quả của KNN trong dự báo thời tiết phụ thuộc cực lớn vào việc chọn giá trị K và hàm đo khoảng cách. Qua các thực nghiệm chuyên sâu, các nhà nghiên cứu nhận thấy rằng đối với dữ liệu không khí biến động theo giờ, giá trị K nằm trong khoảng từ 5 đến 11 thường cho kết quả ổn định nhất. Thay vì chỉ sử dụng khoảng cách Euclidean truyền thống, việc áp dụng khoảng cách Manhattan hoặc Minkowski đôi khi mang lại độ chính xác cao hơn khi xử lý các thuộc tính dữ liệu có độ lệch lớn. Việc tối ưu hóa các tham số này được thực hiện thông qua quy trình Cross-Validation (Kiểm chéo), giúp hệ thống tránh được hiện tượng Overfitting (Quá khớp) và đảm bảo khả năng dự báo chính xác ngay cả với những biến động thời tiết cực đoan.
1	Kết quả thực nghiệm tại các thành phố thông minh cho thấy, mô hình KNN tối ưu hóa đạt độ chính xác dự báo AQI lên tới 91% trong khoảng thời gian 6 giờ tới. So với các mô hình hồi quy tuyến tính cũ, KNN vượt trội hơn ở khả năng nhận diện các mẫu dữ liệu phi tuyến tính phức tạp. Đặc biệt, thời gian phản hồi của hệ thống khi có sự thay đổi đột ngột về nồng độ ô nhiễm chỉ mất 2.5 giây, cho phép các cơ quan chức năng đưa ra cảnh báo kịp thời đến người dân thông qua ứng dụng di động. Tỷ lệ sai số tuyệt đối trung bình (MAE) được ghi nhận ở mức thấp kỷ lục là 3.2 đơn vị AQI, chứng minh rằng KNN là một công cụ đáng tin cậy trong quản lý rủi ro môi trường.
1	"Ứng dụng thực tế của KNN trong việc dự báo ô nhiễm không khí mang lại lợi ích trực tiếp cho sức khỏe cộng đồng, đặc biệt là những người có bệnh lý nền về hô hấp. Dựa trên dự báo từ thuật toán, hệ thống quản lý giao thông có thể điều tiết lưu lượng xe cộ tại các khu vực có nguy cơ ô nhiễm cao để giảm thiểu phát thải. Ngoài ra, dữ liệu từ KNN còn hỗ trợ các nhà quy hoạch đô thị trong việc xác định các ""điểm nóng"" ô nhiễm để trồng thêm cây xanh hoặc lắp đặt các hệ thống lọc khí công nghiệp. Trong tương lai, sự kết hợp giữa KNN và các mô hình Deep Learning sẽ còn đẩy xa giới hạn của việc dự báo, giúp con người chủ động hơn trước những thách thức của biến đổi khí hậu."
0	Trong bối cảnh thương mại điện tử bùng nổ năm 2026, kiến trúc Serverless đã trở thành giải pháp hàng đầu để giải quyết bài toán lưu lượng truy cập không ổn định. Thay vì phải quản lý các máy chủ ảo 24/7, doanh nghiệp chỉ cần đẩy mã nguồn lên các nền tảng như AWS Lambda hoặc Azure Functions. Hệ thống sẽ tự động kích hoạt tài nguyên khi có yêu cầu từ người dùng và tự động tắt khi không sử dụng. Điều này giúp loại bỏ hoàn toàn lãng phí tài nguyên trong những khung giờ thấp điểm. Khả năng tự động mở rộng (Auto-scaling) của Serverless cho phép các website bán lẻ xử lý hàng chục nghìn giao dịch mỗi giây trong các sự kiện giảm giá lớn mà không cần sự can thiệp thủ công của kỹ sư hệ thống.
1	Các số liệu thực tế từ các sàn thương mại điện tử quy mô trung bình cho thấy, việc chuyển đổi sang Serverless giúp giảm chi phí hạ tầng tới 60%. Thay vì trả phí cố định cho các gói máy chủ đắt đỏ, doanh nghiệp chỉ thanh toán cho thời gian thực thi mã nguồn tính bằng mili giây. Nghiên cứu cũng chỉ ra rằng, thời gian phản hồi của hệ thống (Latency) trong các tác vụ thanh toán giảm trung bình từ 250ms xuống còn 120ms nhờ vào khả năng phân phối mã nguồn gần sát với vị trí địa lý của người dùng qua Edge Locations. Với mỗi 100ms độ trễ được cắt giảm, tỷ lệ chuyển đổi đơn hàng của doanh nghiệp thường tăng thêm khoảng 1%, tạo ra lợi thế cạnh tranh trực tiếp trên thị trường số đầy khốc liệt.
1	"Dù mang lại nhiều lợi ích, kiến trúc Serverless cũng đặt ra những thách thức mới về bảo mật do bề mặt tấn công bị phân tán. Việc quản lý các ""Function"" nhỏ lẻ đòi hỏi một hệ thống giám sát cực kỳ chi tiết để phát hiện các lỗ hổng thực thi mã từ xa. Ngoài ra, do tính chất ""Stateless"" (không lưu trạng thái), việc kết nối giữa các hàm Serverless với các cơ sở dữ liệu truyền thống thường gặp vấn đề về giới hạn kết nối đồng thời. Để khắc phục, các doanh nghiệp thường phải triển khai thêm các lớp đệm dữ liệu (Caching) hoặc sử dụng các dịch vụ Database được thiết kế riêng cho đám mây. Việc cân bằng giữa tính linh hoạt và tính bảo mật chính là chìa khóa để triển khai Serverless thành công."
1	"Xu hướng tương lai sẽ chứng kiến sự kết hợp hoàn hảo giữa Serverless và AI để tạo ra các ứng dụng ""tự nhận biết"" tài nguyên. Các thuật toán học máy sẽ dự đoán lưu lượng truy cập trước 15-30 phút để chuẩn bị sẵn tài nguyên (Warm-up), giúp loại bỏ hoàn toàn hiện tượng ""Cold Start"" – điểm yếu lớn nhất của Serverless hiện nay. Khi chi phí tính toán đám mây tiếp tục giảm, chúng ta sẽ thấy sự ra đời của các nền tảng ""Zero-Ops"", nơi lập trình viên chỉ cần tập trung hoàn toàn vào logic kinh doanh mà không cần biết đến sự tồn tại của hạ tầng phía dưới. Điều này sẽ thúc đẩy làn sóng đổi mới sáng tạo, cho phép các ý tưởng mới được hiện thực hóa và đưa ra thị trường với tốc độ nhanh kỷ lục."
1	"Việc hiện đại hóa hệ thống ngân hàng đòi hỏi sự thay đổi từ cấu trúc khối (Monolith) sang kiến trúc dịch vụ nhỏ (Microservices). Lập trình hướng đối tượng (OOP) cung cấp nền tảng để chia nhỏ các nghiệp vụ ngân hàng phức tạp thành các dịch vụ độc lập như: quản lý thẻ, thanh toán hóa đơn, và tiết kiệm. Mỗi dịch vụ này được thiết kế như một ""Object"" lớn với các Interface (giao diện) giao tiếp rõ ràng. Tính trừu tượng của OOP cho phép che giấu các logic nghiệp vụ nội bộ, giúp các bộ phận khác nhau của ngân hàng có thể làm việc trên các dịch vụ riêng biệt mà không gây ảnh hưởng lẫn nhau. Điều này không chỉ tăng tốc độ phát triển mà còn giúp hệ thống vận hành ổn định hơn khi có sự cố xảy ra."
1	Tính đóng gói (Encapsulation) trong OOP đóng vai trò quan trọng trong việc bảo vệ dữ liệu khách hàng tại các ngân hàng số. Các thông tin nhạy cảm như số dư tài khoản hay mã PIN được bảo vệ nghiêm ngặt bên trong các lớp đối tượng, chỉ cho phép truy cập thông qua các phương thức được kiểm soát chặt chẽ. Theo thống kê từ các tổ chức an ninh mạng, các hệ thống áp dụng nguyên tắc OOP chặt chẽ giảm được 45% nguy cơ bị tấn công theo hình thức tiêm nhiễm mã độc (Injection) và rò rỉ dữ liệu. Việc định nghĩa các lớp quyền hạn (Permissions) dựa trên kế thừa giúp quản lý hàng triệu tài khoản người dùng một cách nhất quán, đảm bảo rằng mỗi khách hàng chỉ có thể thực hiện các hành động trong phạm vi quyền hạn được cấp.
1	Các báo cáo vận hành từ các ngân hàng lớn sau khi áp dụng Microservices dựa trên OOP cho thấy chỉ số Uptime (thời gian hoạt động liên tục) đạt mức 99.995%. Thời gian trung bình để khắc phục sự cố (MTTR) giảm từ 3 giờ xuống còn 15 phút, nhờ vào việc khoanh vùng lỗi trong từng dịch vụ cụ thể mà không làm sập toàn bộ hệ thống. Về mặt hiệu suất làm việc, các đội ngũ lập trình viên ghi nhận khả năng tái sử dụng mã nguồn tăng thêm 55%, giúp giảm chi phí bảo trì hàng năm khoảng 1.2 triệu USD cho mỗi hệ thống lõi. Những số liệu này minh chứng cho tính hiệu quả của việc kết hợp các nguyên lý lập trình chuẩn mực vào các lĩnh vực đòi hỏi độ tin cậy cực cao như tài chính.
1	Kiến trúc Microservices và OOP là nền tảng tất yếu để xây dựng hệ sinh thái ngân hàng mở, nơi các bên thứ ba có thể kết nối an toàn với dữ liệu ngân hàng qua các API chuẩn hóa. Việc sử dụng các mẫu thiết kế (Design Patterns) như Factory hay Singleton giúp việc tạo lập và quản lý các kết nối API trở nên dễ dàng và bảo mật hơn. Trong tương lai, sự kết hợp giữa OOP và các hợp đồng thông minh (Smart Contracts) sẽ tạo ra các giao dịch tài chính tự động hóa hoàn toàn nhưng vẫn đảm bảo tính minh bạch và an toàn tuyệt đối. Ngân hàng số khi đó sẽ không chỉ là nơi lưu trữ tiền tệ mà còn là một nền tảng dịch vụ thông minh, phục vụ mọi nhu cầu tài chính của người dân một cách tức thì.
1	Ngành xuất khẩu nông sản hiện nay đang đối mặt với những tiêu chuẩn khắt khe về chất lượng và độ đồng đều của sản phẩm. Thuật toán K-Nearest Neighbors (KNN) đã được ứng dụng thành công để phân loại trái cây dựa trên các đặc trưng vật lý như kích thước, màu sắc và độ ngọt (đo qua cảm biến). Thay vì dựa vào sức người vốn dễ sai sót và mệt mỏi, hệ thống thị giác máy tính kết hợp với KNN có thể hoạt động liên tục 24/7 với độ chính xác cao. Việc số hóa các tiêu chuẩn chất lượng giúp doanh nghiệp dễ dàng đáp ứng các yêu cầu từ các thị trường khó tính như EU hay Nhật Bản, nơi chỉ một vài sản phẩm không đạt chuẩn có thể dẫn đến việc bị trả lại toàn bộ lô hàng.
1	Để thuật toán KNN hoạt động hiệu quả, các hình ảnh nông sản từ camera được trích xuất thành các vector đặc trưng bao gồm các thông số về sắc thái màu (RGB), độ tròn và các tì vết trên bề mặt. Trong một thử nghiệm phân loại thanh long xuất khẩu, thuật toán KNN với k=5 đã cho thấy khả năng phân biệt 3 cấp độ chất lượng với độ chính xác đạt 94.2%. Các nghiên cứu thực nghiệm chỉ ra rằng, việc sử dụng KNN đơn giản hơn nhiều so với các mạng thần kinh sâu (Deep Learning) trong điều kiện tài nguyên máy tính tại các nhà kho nông sản có hạn, mà vẫn đảm bảo được tốc độ xử lý khoảng 5-7 sản phẩm mỗi giây trên một dây chuyền đơn lẻ.
1	Việc áp dụng phân loại tự động bằng KNN giúp các hợp tác xã nông nghiệp giảm tỷ lệ lãng phí nông sản do phân loại sai lên đến 20%. Trước đây, việc phân loại thủ công thường dẫn đến tình trạng hàng tốt bị xếp vào loại thường hoặc ngược lại, gây thiệt hại trực tiếp về doanh thu. Với hệ thống mới, giá trị xuất khẩu trung bình của mỗi tấn nông sản tăng thêm khoảng 15% nhờ vào tính đồng nhất của lô hàng. Chi phí nhân công cho khâu kiểm định cũng giảm được 70%, cho phép doanh nghiệp tái đầu tư vào các công nghệ bảo quản sau thu hoạch. Đây là minh chứng rõ nét cho việc công nghệ máy học bình dân như KNN có thể thay đổi bộ mặt của ngành nông nghiệp truyền thống.
1	"Trong tương lai, mô hình KNN không chỉ dừng lại ở khâu phân loại tại kho mà còn có thể tích hợp vào các robot thu hoạch trực tiếp tại cánh đồng. Bằng cách so sánh dữ liệu thực tế với ""láng giềng"" là các mẫu quả đã chín chuẩn, robot có thể đưa ra quyết định hái chính xác mà không làm hư hại cây trồng. Sự kết hợp giữa dữ liệu lớn (Big Data) về mùa vụ và thuật toán phân loại sẽ giúp nông dân dự báo được sản lượng và chất lượng sản phẩm từ sớm, từ đó chủ động hơn trong việc tìm kiếm thị trường tiêu thụ. Chuyển đổi số trong nông nghiệp thông qua các thuật toán máy học chính là con đường ngắn nhất để nâng tầm nông sản Việt Nam trên bản đồ thế giới."
1	Trong một thế giới đầy biến động, việc phụ thuộc vào một nhà cung cấp đám mây duy nhất (Vendor Lock-in) mang lại rủi ro rất lớn cho các tập đoàn logistics toàn cầu. Chiến lược Multi-Cloud cho phép doanh nghiệp phân bổ hạ tầng trên nhiều nền tảng như Google Cloud, AWS và Alibaba Cloud đồng thời. Điều này đảm bảo rằng nếu một khu vực của nhà cung cấp này gặp sự cố, hệ thống sẽ tự động chuyển hướng sang nhà cung cấp khác, giúp duy trì luồng thông tin hàng hóa không bao giờ bị gián đoạn. Việc tận dụng ưu thế riêng của từng nhà cung cấp ở các vùng địa lý khác nhau cũng giúp tối ưu hóa tốc độ truy cập dữ liệu cho các kho hàng và phương tiện vận tải trên toàn thế giới.
1	"Một trong những lợi ích lớn nhất của Multi-Cloud là khả năng giảm độ trễ mạng xuống mức tối thiểu bằng cách đặt máy chủ gần với người dùng cuối nhất có thể. Đối với các ứng dụng theo dõi đơn hàng thời gian thực, việc giảm độ trễ từ 500ms xuống còn 50ms giúp cải thiện trải nghiệm người dùng một cách rõ rệt. Về mặt tài chính, việc áp dụng chiến lược ""Cloud Arbitrage"" – lựa chọn nhà cung cấp có giá rẻ nhất cho từng loại tác vụ cụ thể – đã giúp các công ty vận tải tiết kiệm được khoảng 22% tổng chi phí hạ tầng IT hàng năm. Các công cụ quản lý tập trung cho phép các kỹ sư điều phối hàng nghìn máy chủ ảo trên khắp thế giới chỉ từ một bảng điều khiển duy nhất, tăng hiệu quả quản trị lên 35%."
1	Quản lý logistics xuyên biên giới đòi hỏi việc tuân thủ các quy định về chủ quyền dữ liệu của từng quốc gia. Hệ thống Multi-Cloud cho phép doanh nghiệp lưu trữ dữ liệu tại chính quốc gia đó để đáp ứng luật pháp, trong khi vẫn có thể tổng hợp báo cáo toàn cầu một cách nhanh chóng. Tính dự phòng dữ liệu cũng được nâng lên một tầm cao mới; ngay cả trong kịch bản xấu nhất là một nhà cung cấp đám mây bị tấn công mạng toàn diện, dữ liệu dự phòng trên các đám mây khác vẫn đảm bảo hoạt động kinh doanh diễn ra bình thường. Các số liệu cho thấy các doanh nghiệp sử dụng Multi-Cloud có khả năng phục hồi sau thảm họa nhanh hơn gấp 3 lần so với các mô hình đám mây đơn nhất truyền thống.
1	"Trong tương lai gần, mạng lưới Multi-Cloud sẽ là nền tảng để vận hành các đội xe vận tải và tàu biển tự hành. Sự kết hợp giữa Edge Computing và Đa đám mây sẽ cho phép xử lý dữ liệu từ hàng triệu cảm biến IoT trên các container theo thời gian thực, giúp tối ưu hóa lộ trình và tiết kiệm nhiên liệu. Ước tính, việc tối ưu hóa lộ trình bằng AI trên đám mây có thể giảm lượng khí thải carbon của ngành vận tải xuống 12% vào năm 2030. Chúng ta đang tiến tới một kỷ nguyên nơi mọi kiện hàng đều có ""bản sao số"" sống động trên mây, giúp con người có cái nhìn toàn cảnh và chính xác về dòng chảy vật chất trên toàn hành tinh, từ đó kiến tạo một nền kinh tế tuần hoàn và bền vững."
1	"Với sự gia tăng của các thiết bị IoT trong gia đình từ camera, bóng đèn đến tủ lạnh thông minh, nguy cơ bị tấn công mạng đã trở nên hiện hữu hơn bao giờ hết. Thuật toán KNN được ứng dụng để xây dựng các hệ thống phát hiện xâm nhập (IDS) nhẹ gọn, có thể chạy trực tiếp trên các bộ định tuyến (Router) gia đình. Hệ thống hoạt động bằng cách giám sát các lưu lượng mạng thông thường và dán nhãn chúng. Khi có một thiết bị bắt đầu gửi các gói tin có hành vi bất thường – giống với các mẫu tấn công đã biết trong cơ sở dữ liệu ""láng giềng"" – hệ thống sẽ ngay lập tức đưa ra cảnh báo hoặc chặn kết nối của thiết bị đó để bảo vệ toàn bộ mạng nội bộ."
1	Trong các thử nghiệm thực tế với tập dữ liệu tấn công mạng phổ biến như NSL-KDD, thuật toán KNN cho kết quả phát hiện mã độc và các cuộc tấn công từ chối dịch vụ (DoS) với độ chính xác đạt 92%. Một ưu điểm lớn của KNN là không yêu cầu quá trình huấn luyện phức tạp như các mạng Neuron, giúp nó cực kỳ phù hợp với các thiết bị có bộ nhớ và CPU hạn chế. Bằng cách sử dụng các kỹ thuật giảm chiều dữ liệu (như PCA), số lượng đặc trưng cần xử lý giảm từ 41 xuống còn 10, giúp tốc độ phân loại tăng gấp 4 lần trong khi độ chính xác chỉ giảm nhẹ khoảng 1%. Điều này cho phép hệ thống phản ứng với các mối đe dọa chỉ trong vòng vài mili giây sau khi chúng xuất hiện.
1	Việc tích hợp KNN vào các thiết bị mạng gia đình đã giúp giảm tỷ lệ lây nhiễm mã độc botnet xuống 50% đối với các hộ gia đình tham gia thử nghiệm. Người dùng cảm thấy an tâm hơn khi biết rằng các camera an ninh của mình không bị truy cập trái phép hoặc bị lợi dụng để tấn công các mục tiêu khác trên Internet. Các số liệu thống kê cho thấy, trung bình mỗi ngày hệ thống IDS dựa trên KNN có thể ngăn chặn thành công khoảng 15 nỗ lực dò tìm mật khẩu (Brute-force) từ các bot tự động trên mạng. Chi phí để triển khai giải pháp phần mềm này gần như bằng không đối với nhà sản xuất, nhưng lại mang lại giá trị gia tăng cực lớn về mặt thương hiệu và sự an toàn cho khách hàng cuối.
1	Trong tương lai, các hệ thống KNN trong nhà thông minh sẽ có khả năng tự học từ chính hành vi của chủ nhà để giảm thiểu các cảnh báo sai (False Positives). Ví dụ, nếu bạn thường xuyên truy cập camera vào lúc 2 giờ sáng khi đi làm về, hệ thống sẽ dần ghi nhận đó là hành vi bình thường thay vì coi đó là một vụ xâm nhập. Sự kết hợp giữa bảo mật ở tầng biên (Edge Security) và trí tuệ nhân tạo sẽ tạo ra một lớp giáp bảo vệ vô hình nhưng vô cùng chắc chắn cho cuộc sống số của chúng ta. Khi các thiết bị trở nên thông minh hơn, chúng ta sẽ không còn phải lo lắng về việc quản lý bảo mật phức tạp, mà có thể hoàn toàn tận hưởng sự tiện nghi của công nghệ mang lại.
1	Nghiên cứu này trình bày việc triển khai hệ thống quản lý bệnh án điện tử dựa trên nền tảng điện toán đám mây tại Bệnh viện Đa khoa tỉnh Bình Dương trong giai đoạn 2023-2024. Hệ thống được xây dựng trên Amazon Web Services với kiến trúc microservices, phục vụ 1.200 nhân viên y tế và quản lý hồ sơ của 45.000 bệnh nhân. Kết quả cho thấy thời gian truy xuất hồ sơ giảm từ 12 phút xuống còn 8 giây, chi phí vận hành giảm 42% so với hệ thống cũ, và tỷ lệ hài lòng của bác sĩ tăng từ 65% lên 91%. Nghiên cứu đề xuất mô hình triển khai phù hợp với điều kiện thực tế của các bệnh viện tuyến tỉnh tại Việt Nam.
1	Trong bối cảnh chuyển đổi số y tế, các bệnh viện tại Việt Nam đang đối mặt với nhiều thách thức trong việc quản lý thông tin bệnh nhân. Theo số liệu từ Bộ Y tế năm 2023, có 87% bệnh viện tuyến tỉnh vẫn sử dụng hồ sơ giấy kết hợp với các hệ thống cơ sở dữ liệu đơn lẻ, dẫn đến tình trạng mất mát, trùng lặp và khó khăn trong tra cứu thông tin. Bệnh viện Đa khoa tỉnh Bình Dương với quy mô 800 giường bệnh, tiếp nhận trung bình 2.500 lượt khám mỗi ngày, đã ghi nhận 156 trường hợp mất hồ sơ trong năm 2022. Điện toán đám mây với khả năng lưu trữ không giới hạn, truy cập từ xa và chi phí linh hoạt được xác định là giải pháp tiềm năng để giải quyết các vấn đề này.
1	Nghiên cứu nhằm thiết kế và triển khai hệ thống quản lý bệnh án điện tử trên nền tảng đám mây, đánh giá hiệu quả về mặt kỹ thuật và kinh tế sau 12 tháng vận hành. Các chỉ tiêu cụ thể bao gồm: giảm thời gian truy xuất hồ sơ xuống dưới 10 giây, đạt độ khả dụng 99.5%, giảm ít nhất 30% chi phí so với hệ thống truyền thống, và đạt tỷ lệ hài lòng của người dùng trên 85%. Nghiên cứu cũng xây dựng khung đánh giá rủi ro bảo mật và tuân thủ quy định về bảo vệ dữ liệu cá nhân theo Nghị định 13/2023/NĐ-CP. Kết quả kỳ vọng sẽ là cơ sở cho các bệnh viện khác trong quá trình chuyển đổi số.
1	Điện toán đám mây là mô hình cung cấp tài nguyên máy tính theo yêu cầu qua Internet, cho phép truy cập vào kho tài nguyên chia sẻ có thể cấu hình được. Theo định nghĩa của NIST (National Institute of Standards and Technology), điện toán đám mây có 5 đặc tính thiết yếu: tự phục vụ theo yêu cầu, truy cập mạng rộng rãi, gộp chung tài nguyên, khả năng co giãn nhanh và dịch vụ đo lường được. Ba mô hình dịch vụ chính bao gồm IaaS (Infrastructure as a Service) cung cấp tài nguyên phần cứng ảo hóa, PaaS (Platform as a Service) cung cấp môi trường phát triển ứng dụng, và SaaS (Software as a Service) cung cấp phần mềm hoàn chỉnh.
1	Trong y tế, mô hình Private Cloud được ưu tiên do yêu cầu bảo mật cao, tuy nhiên chi phí đầu tư lớn. Mô hình Hybrid Cloud kết hợp ưu điểm của Public và Private Cloud, cho phép lưu dữ liệu nhạy cảm trên Private Cloud và dữ liệu thông thường trên Public Cloud, phù hợp với điều kiện ngân sách hạn chế của bệnh viện công. Kiến trúc microservices chia nhỏ ứng dụng thành các dịch vụ độc lập, mỗi dịch vụ thực hiện một chức năng nghiệp vụ cụ thể và giao tiếp qua API. Trong hệ thống quản lý bệnh án, các microservices điển hình bao gồm: dịch vụ quản lý bệnh nhân, dịch vụ lưu trữ hình ảnh y khoa (PACS), dịch vụ kết quả xét nghiệm (LIS), dịch vụ đơn thuốc điện tử, và dịch vụ thanh toán.
1	Mỗi dịch vụ có thể được phát triển bằng ngôn ngữ lập trình khác nhau, triển khai độc lập và mở rộng theo nhu cầu riêng. Ví dụ, dịch vụ lưu trữ hình ảnh DICOM có thể sử dụng Python với thư viện PyDICOM và yêu cầu tài nguyên lưu trữ lớn, trong khi dịch vụ tra cứu thông tin bệnh nhân sử dụng Node.js với yêu cầu xử lý nhanh. Container hóa bằng Docker và quản lý bằng Kubernetes giúp triển khai và vận hành các microservices hiệu quả, với khả năng tự động mở rộng khi tải tăng cao.
1	Hệ thống được thiết kế trên nền tảng AWS với kiến trúc 3 tầng: tầng trình diễn sử dụng React.js cho giao diện web và React Native cho ứng dụng di động, tầng logic nghiệp vụ gồm 8 microservices chạy trên ECS (Elastic Container Service), và tầng dữ liệu sử dụng RDS PostgreSQL cho dữ liệu cấu trúc và S3 cho lưu trữ hình ảnh. Mỗi microservice được container hóa và triển khai trên cluster ECS với Auto Scaling, đảm bảo tự động tăng số lượng container khi CPU sử dụng vượt 70%. Load Balancer phân phối lưu lượng đồng đều, API Gateway quản lý các endpoint và thực hiện xác thực JWT. CloudFront CDN được cấu hình để phân phối nội dung tĩnh và hình ảnh với độ trễ thấp.
1	Dữ liệu được mã hóa AES-256 khi lưu trữ và TLS 1.3 khi truyền tải. Backup tự động được thực hiện mỗi 6 giờ với lưu trữ 30 ngày trên S3 Glacier để tiết kiệm chi phí. Nghiên cứu được triển khai từ tháng 1/2023 đến tháng 12/2023 với 3 giai đoạn: xây dựng (3 tháng), thử nghiệm (2 tháng), và vận hành chính thức (7 tháng). Dữ liệu định lượng được thu thập từ CloudWatch Logs, bao gồm thời gian phản hồi API (response time), số lượng request/giây, tỷ lệ lỗi, và mức sử dụng tài nguyên. Dữ liệu định tính thu thập qua khảo sát 450 nhân viên y tế bằng bảng hỏi Likert 5 điểm về mức độ hài lòng, phỏng vấn sâu 25 bác sĩ và điều dưỡng về trải nghiệm sử dụng.
1	Chi phí vận hành được tính toán chi tiết bao gồm chi phí EC2, RDS, S3, data transfer và support. Phương pháp phân tích thống kê mô tả và kiểm định t-test được sử dụng để so sánh các chỉ số trước và sau triển khai với mức ý nghĩa p<0.05. Sau 7 tháng vận hành, hệ thống xử lý trung bình 18.500 request mỗi ngày với thời gian phản hồi trung bình 1.2 giây cho tra cứu thông tin bệnh nhân, 2.8 giây cho tải hình ảnh X-quang kích thước 8MB, và 0.6 giây cho cập nhật thông tin. Thời gian truy xuất hồ sơ hoàn chỉnh giảm từ 12 phút (hệ thống cũ) xuống 8 giây, vượt mục tiêu đề ra.
1	Độ khả dụng đạt 99.7%, chỉ ghi nhận 3 lần downtime với tổng thời gian 5.2 giờ do sự cố mạng ISP. CPU utilization trung bình 45%, với peak 78% vào khung giờ 8-10h sáng khi lượng khám đông. Database connections ổn định ở mức 120/200 connections. Auto Scaling đã tự động tăng số container từ 4 lên 8 instances trong 15 lần peak load. Băng thông sử dụng trung bình 2.3TB/tháng với 85% là download hình ảnh y khoa. Kết quả cho thấy kiến trúc đám mây đáp ứng tốt nhu cầu thực tế. Tổng chi phí vận hành hệ thống đám mây trong 12 tháng là 456 triệu đồng (19.200 USD với tỷ giá 23.750 VND/USD), bao gồm: EC2 instances (8 t3.medium) 168 triệu, RDS PostgreSQL (db.m5.large) 142 triệu, S3 storage cho 12TB dữ liệu 38 triệu, CloudFront CDN 42 triệu,
1	backup và data transfer 48 triệu, AWS Support 18 triệu. So với hệ thống on-premise trước đây có chi phí 785 triệu đồng/năm (server 320 triệu, storage 180 triệu, điện năng 95 triệu, bảo trì 140 triệu, nhân sự IT 50 triệu), hệ thống đám mây giúp tiết kiệm 329 triệu đồng, tương đương 42%. Chi phí đầu tư ban đầu giảm từ 1.2 tỷ đồng xuống 280 triệu đồng chỉ cho phát triển phần mềm. Mô hình pay-as-you-go cho phép tối ưu chi phí theo nhu cầu thực tế. Reserved Instances giúp tiết kiệm thêm 31% cho các tài nguyên chạy liên tục.
1	Khảo sát 450 nhân viên y tế (tỷ lệ phản hồi 87.3%) cho thấy mức độ hài lòng tổng thể 4.2/5 điểm. Cụ thể: 91% đồng ý hệ thống giúp tra cứu thông tin nhanh hơn, 84% đánh giá giao diện thân thiện và dễ sử dụng, 78% cho rằng giảm được thời gian làm việc giấy tờ, 68% hài lòng với tốc độ tải hình ảnh. Tuy nhiên, 23% phản ánh gặp khó khăn khi mất kết nối Internet, 15% cho rằng cần thêm đào tạo về các tính năng nâng cao. Phỏng vấn sâu cho thấy bác sĩ đánh giá cao khả năng truy cập hồ sơ từ bất kỳ đâu, đặc biệt hữu ích khi hội chẩn từ xa.
1	Điều dưỡng viên đánh giá cao tính năng cảnh báo tự động về dị ứng thuốc và tương tác thuốc. Tỷ lệ lỗi nhập liệu giảm từ 8.2% xuống 2.1% nhờ validation tự động. Kết quả nghiên cứu phù hợp với xu hướng toàn cầu về triển khai EMR trên đám mây. Nghiên cứu của Zhang et al. (2022) tại 15 bệnh viện Trung Quốc ghi nhận giảm 38% chi phí và tăng 23% hiệu suất làm việc, tương đương với kết quả 42% và 28% của chúng tôi. Tuy nhiên, độ khả dụng 99.7% của chúng tôi cao hơn mức 98.9% trong nghiên cứu của Kumar et al. (2021) tại Ấn Độ, có thể do chúng tôi sử dụng kiến trúc Multi-AZ và có SLA tốt hơn từ AWS.
1	Thời gian phản hồi trung bình 1.2 giây nhanh hơn nghiên cứu của Nguyen và Pham (2020) tại Bệnh viện Chợ Rẫy (2.8 giây) nhờ tối ưu hóa database indexing và sử dụng Redis cache. Điểm khác biệt là chúng tôi tập trung vào bệnh viện tuyến tỉnh với quy mô vừa, trong khi các nghiên cứu trước thường tập trung vào bệnh viện tuyến trung ương. Nghiên cứu còn một số hạn chế: thời gian quan sát 12 tháng chưa đủ dài để đánh giá tác động lâu dài, chưa so sánh với các nền tảng đám mây khác như Google Cloud hoặc Azure, và chưa tích hợp AI/ML cho chẩn đoán hỗ trợ.
1	Vấn đề phụ thuộc vào kết nối Internet vẫn là thách thức, đặc biệt tại các khu vực có hạ tầng mạng yếu. Hướng phát triển trong tương lai bao gồm: triển khai mô hình edge computing để xử lý dữ liệu ngay tại bệnh viện khi mất kết nối, tích hợp module AI phân tích hình ảnh X-quang, mở rộng sang 3 bệnh viện khác trong tỉnh để tạo mạng lưới chia sẻ dữ liệu, và nghiên cứu blockchain để tăng cường bảo mật và khả năng truy xuất nguồn gốc. Chi phí có thể được tối ưu hơn bằng serverless architecture cho các chức năng không thường xuyên sử dụng.
1	Nghiên cứu đã chứng minh tính khả thi và hiệu quả của việc triển khai hệ thống quản lý bệnh án điện tử trên nền tảng điện toán đám mây tại bệnh viện tuyến tỉnh ở Việt Nam. Hệ thống đạt các chỉ tiêu đề ra: thời gian truy xuất giảm 98.9%, chi phí giảm 42%, độ khả dụng 99.7%, và mức độ hài lòng 91%. Kiến trúc microservices trên AWS cung cấp khả năng mở rộng linh hoạt và độ tin cậy cao. Mô hình này có thể nhân rộng cho các bệnh viện có quy mô và điều kiện tương tự, góp phần thúc đẩy chuyển đổi số y tế. Để triển khai thành công, cần đầu tư vào đào tạo nhân sự, đảm bảo hạ tầng mạng ổn định, và tuân thủ chặt chẽ các quy định về bảo mật dữ liệu y tế.
1	Bài báo trình bày quá trình phân tích, thiết kế và triển khai hệ thống quản lý thư viện số cho Thư viện Đại học Khoa học Tự nhiên TP.HCM phục vụ 8.500 sinh viên và 450 giảng viên. Hệ thống được xây dựng dựa trên nguyên lý lập trình hướng đối tượng (OOP) với mô hình kiến trúc Model-View-Controller (MVC), sử dụng Java Spring Boot cho backend và Angular cho frontend. Nghiên cứu tập trung vào việc áp dụng 4 tính chất cốt lõi của OOP (đóng gói, kế thừa, đa hình, trừu tượng) để xây dựng hệ thống có khả năng mở rộng và bảo trì cao. Sau 6 tháng triển khai, hệ thống xử lý trung bình 3.200 giao dịch mượn/trả mỗi ngày, giảm 67% thời gian xử lý so với hệ thống thủ công, và đạt 94% mức độ hài lòng người dùng.
1	Thư viện Đại học Khoa học Tự nhiên TP.HCM quản lý 125.000 đầu sách và 340.000 bản với hệ thống cũ được phát triển năm 2015 bằng PHP thuần không theo chuẩn OOP, dẫn đến nhiều vấn đề nghiêm trọng. Code base 45.000 dòng có độ phức tạp cyclomatic cao (trung bình 28.5), khó bảo trì và mở rộng. Mỗi lần thêm tính năng mới như quản lý tài liệu điện tử hoặc tích hợp thanh toán online mất 3-4 tuần do code phụ thuộc chặt chẽ lẫn nhau. Thống kê năm 2022 ghi nhận 1.850 lỗi runtime, 67% là do không validate đầu vào và xử lý ngoại lệ không nhất quán.
1	Thời gian xử lý một giao dịch mượn sách trung bình 4.2 phút, gây ùn tắc vào giờ cao điểm. Database không có chuẩn hóa, dẫn đến dư thừa dữ liệu và khó truy vấn. Yêu cầu cấp thiết là xây dựng lại hệ thống với kiến trúc tốt hơn, áp dụng các nguyên lý công nghệ phần mềm hiện đại. Lập trình hướng đối tượng được lựa chọn vì khả năng mô hình hóa thực thể nghiệp vụ một cách tự nhiên và rõ ràng. Các đối tượng như Book, Member, Transaction, Fine trong hệ thống thư viện có thể được biểu diễn thành các class với thuộc tính và phương thức riêng, dễ hiểu và gần gũi với thế giới thực.
1	Tính đóng gói giúp che giấu chi tiết triển khai, ví dụ logic tính tiền phạt phức tạp được đóng gói trong class FineCalculator, các module khác chỉ cần gọi phương thức calculateFine() mà không cần biết thuật toán bên trong. Kế thừa cho phép tái sử dụng code, ví dụ class EBook và PhysicalBook đều kế thừa từ class Book trừu tượng, giảm 40% code trùng lặp. Mô hình MVC tách biệt logic nghiệp vụ (Model), giao diện người dùng (View) và điều khiển luồng (Controller), cho phép phát triển song song và test độc lập từng thành phần, rút ngắn thời gian phát triển từ 8 tháng xuống 5 tháng.
1	Tính đóng gói (Encapsulation) là việc gói dữ liệu và phương thức thao tác trên dữ liệu đó vào một đơn vị duy nhất là class, đồng thời che giấu chi tiết triển khai bên trong thông qua access modifier (private, protected, public). Ví dụ, class Member có thuộc tính private memberID, name, email và các phương thức public getMemberID(), setName() để truy cập có kiểm soát, ngăn chặn việc sửa đổi trực tiếp dữ liệu từ bên ngoài. Tính kế thừa (Inheritance) cho phép class con thừa hưởng thuộc tính và phương thức từ class cha, ví dụ StudentMember và FacultyMember kế thừa từ Member, được thêm thuộc tính studentID hoặc department riêng. Tính đa hình (Polymorphism) cho phép cùng một phương thức có nhiều cách thực thi khác nhau, ví dụ phương thức calculateFine() cho StudentMember là 5.000đ/ngày, FacultyMember là 3.000đ/ngày nhờ override.
1	Tính trừu tượng (Abstraction) tập trung vào những gì đối tượng làm thay vì làm như thế nào, ví dụ interface INotification định nghĩa phương thức send() mà không quan tâm gửi qua email hay SMS. Model-View-Controller là mẫu kiến trúc phần mềm chia ứng dụng thành 3 thành phần tương tác: Model quản lý dữ liệu và logic nghiệp vụ, View hiển thị dữ liệu cho người dùng, Controller xử lý input và điều phối giữa Model và View. Trong hệ thống thư viện, Model bao gồm các entity class như Book, Member, Transaction và các service class như BookService, MemberService chứa business logic. View là các template HTML/Angular component hiển thị danh sách sách, form mượn trả.
1	"Controller (REST API endpoints) nhận request từ client, gọi service xử lý, và trả về response. Luồng hoạt động: người dùng click ""Mượn sách"" trên View → Angular gửi HTTP POST đến Controller → Controller gọi TransactionService.createTransaction() → Service validate, cập nhật database qua Repository → Controller trả JSON response → View cập nhật giao diện. Tách biệt này cho phép thay đổi giao diện từ web sang mobile app mà không ảnh hưởng business logic. Singleton Pattern đảm bảo một class chỉ có một instance duy nhất, áp dụng cho DatabaseConnection để tránh tạo nhiều kết nối database gây lãng phí tài nguyên. Factory Pattern tạo đối tượng mà không chỉ định cụ thể class, ví dụ NotificationFactory.createNotification(type) trả về EmailNotification hoặc SMSNotification dựa trên type, dễ mở rộng thêm PushNotification sau này."
1	Repository Pattern trừu tượng hóa data access layer, ví dụ BookRepository interface với các phương thức findById(), findByISBN(), save(), được implement bởi BookRepositoryImpl sử dụng JPA, dễ thay đổi từ SQL sang NoSQL database. Observer Pattern cho phép các đối tượng theo dõi và phản ứng với sự kiện, ví dụ khi sách quá hạn, TransactionObserver tự động gửi thông báo và tính phạt. Strategy Pattern định nghĩa họ thuật toán có thể thay thế lẫn nhau, ví dụ FineCalculationStrategy có StandardStrategy, StudentStrategy, FacultyStrategy, được inject vào FineCalculator để tính phạt linh hoạt theo loại thành viên. Hệ thống được thiết kế với 15 class chính được tổ chức theo 4 package: entity, service, repository, controller. Package entity chứa Book (abstract class) với các thuộc tính bookID: String, title: String, authors: List<String>, ISBN: String, publicationYear: int và các phương thức abstract isAvailable(): boolean, getDetails(): String.
1	"EBook extends Book thêm thuộc tính fileFormat: String, fileSize: long và override isAvailable() kiểm tra license còn hiệu lực. PhysicalBook extends Book thêm location: String, totalCopies: int, availableCopies: int. Member (abstract) có memberID: String, name: String, email: String, registrationDate: Date, status: MemberStatus với enum MemberStatus {ACTIVE, SUSPENDED, EXPIRED}. StudentMember extends Member thêm studentID, major: String và maxBorrowDays = 14. FacultyMember extends Member thêm department: String, maxBorrowDays = 30. Transaction class có transactionID, member: Member, book: Book, borrowDate: Date, dueDate: Date, returnDate: Date, fine: Fine với association đến Member và Book. Fine class quản lý tiền phạt với amount: double, status: FineStatus, calculationStrategy: IFineCalculationStrategy. Các relationship: Member ""1"" --- ""0.."" Transaction, Book ""1"" --- ""0.."" Transaction, Transaction ""1"" --- ""0..1"" Fine."
1	Database được thiết kế chuẩn hóa đến dạng chuẩn 3NF với 12 bảng chính. Bảng books (book_id PK, isbn UNIQUE, title, publication_year, book_type ENUM('PHYSICAL','EBOOK')) lưu thông tin chung. Bảng physical_books (book_id FK, location, total_copies, available_copies) và ebooks (book_id FK, file_format, file_size, license_expiry) kế thừa từ books theo mô hình Table Per Subclass. Bảng authors (author_id PK, name) có quan hệ N-N với books qua book_authors (book_id FK, author_id FK). Bảng members (member_id PK, name, email UNIQUE, registration_date, status, member_type ENUM('STUDENT','FACULTY')) với student_members (member_id FK, student_id, major) và faculty_members (member_id FK, department). Bảng transactions (transaction_id PK, member_id FK, book_id FK, borrow_date, due_date, return_date NULL) có index trên (member_id, return_date) để tối ưu query sách đang mượn. Bảng fines (fine_id PK, transaction_id FK UNIQUE, amount DECIMAL(10,2), status, created_date) với trigger tự động tính phạt khi return_date > due_date.
1	Bảng reservations (reservation_id, member_id FK, book_id FK, reservation_date, status) quản lý đặt trước sách đang được mượn hết. API được thiết kế theo chuẩn REST với các endpoint sau: GET /api/books?page=0&size=20&sort=title trả về danh sách sách phân trang, GET /api/books/{id} trả về chi tiết một cuốn sách, POST /api/books với body JSON tạo sách mới (role ADMIN). GET /api/books/search?query=java&type=PHYSICAL tìm kiếm sách với filter. GET /api/members/{id}/transactions?status=BORROWED trả về lịch sử mượn của thành viên. POST /api/transactions với body {memberId, bookId} tạo giao dịch mượn mới, validate member không bị suspend, sách còn available, member chưa mượn quá giới hạn (Student: 5 cuốn, Faculty: 10 cuốn). PUT /api/transactions/{id}/return xử lý trả sách, tự động tính phạt nếu trễ hạn. GET /api/fines?memberId=123&status=UNPAID trả về danh sách tiền phạt.
1	POST /api/fines/{id}/pay thanh toán tiền phạt. Mỗi response có format chuẩn: {success: boolean, data: object, message: string, timestamp: long}. Error handling nhất quán với HTTP status code: 200 OK, 201 Created, 400 Bad Request, 401 Unauthorized, 404 Not Found, 500 Internal Server Error. Authentication dùng JWT token với refresh token mechanism, authorization dùng Spring Security với role-based access control. Backend sử dụng Java 17 với Spring Boot 3.1.5 framework, Spring Data JPA cho ORM mapping entity sang database, Spring Security cho authentication/authorization, và Spring Validation cho data validation. Database là PostgreSQL 15.2 với connection pool HikariCP, cấu hình maximum pool size 20, minimum idle 5, connection timeout 30 giây.
1	Frontend sử dụng Angular 16.2 với TypeScript, Angular Material cho UI components, RxJS cho reactive programming, và NgRx cho state management. Build tool là Maven 3.9 cho backend và npm 9.8 cho frontend. Testing framework bao gồm JUnit 5 cho unit test, Mockito cho mocking, TestContainers cho integration test với PostgreSQL container. Code coverage đạt 87% nhờ yêu cầu minimum 80% mới được merge code. CI/CD pipeline dùng Jenkins, tự động build, test, và deploy lên server staging khi có commit mới vào branch develop. Production deployment dùng Docker container chạy trên Ubuntu 22.04 server với nginx reverse proxy, cấu hình rate limiting 100 requests/phút/IP để chống DDoS.
1	Tính đóng gói được implement triệt để: tất cả thuộc tính của entity class đều private, chỉ expose qua getter/setter public có validation. Ví dụ trong class Member, phương thức setEmail(String email) validate format email qua regex pattern trước khi gán giá trị, throw IllegalArgumentException nếu không hợp lệ. Business logic được đóng gói trong service layer, controller không chứa logic mà chỉ delegate sang service. Tính kế thừa: abstract class Book được PhysicalBook và EBook kế thừa, abstract class Member được StudentMember và FacultyMember kế thừa, code chung được đặt ở class cha giảm duplication. Tính đa hình: interface INotificationService có các implementation EmailNotificationService, SmsNotificationService, PushNotificationService, được inject vào TransactionService thông qua constructor injection, cho phép thay đổi cách thông báo mà không sửa code TransactionService. Phương thức calculateFine() được override ở StudentFineCalculator (5.000đ/ngày, tối đa 200.000đ) và FacultyFineCalculator (3.000đ/ngày, tối đa 150.000đ).
1	Tính trừu tượng: interface IFineCalculationStrategy định nghĩa contract calculate(Transaction t): double, các concrete class implement strategy cụ thể, được inject vào FineService dựa trên member type. Singleton: DatabaseConnectionPool được implement bằng enum với một instance duy nhất, lazy initialization khi được gọi lần đầu. Factory: NotificationServiceFactory có method createNotificationService(NotificationType type) return INotificationService tương ứng, sử dụng switch-case, dễ thêm Telegram notification sau này bằng cách thêm một case mới. Repository: BookRepository interface extends JpaRepository<Book, String> cung cấp sẵn các method findAll(), findById(), save(), deleteById(), thêm custom method findByISBN(String isbn): Optional<Book>, findByTitleContaining(String keyword): List<Book>. Implementation tự động generate bởi Spring Data JPA, không cần viết code SQL. Observer: TransactionEventPublisher publish event khi transaction được tạo hoặc trả sách, TransactionEventListener subscribe và xử lý gửi notification, tính phạt, cập nhật thống kê.
1	Strategy: FineCalculationContext class có setStrategy(IFineCalculationStrategy strategy) và calculateFine(Transaction t) delegate cho strategy hiện tại, strategy được chọn dựa trên member.getMemberType() tại runtime, áp dụng Open-Closed Principle: mở cho mở rộng (thêm strategy mới), đóng cho sửa đổi (không sửa code hiện tại). Sau  6 tháng triển khai (từ 01/07/2023 đến 31/12/2023), hệ thống xử lý tổng cộng 576.000 giao dịch mượn/trả với trung bình 3.200 giao dịch/ngày, peak 5.800 giao dịch vào ngày 15/09 (đầu học kỳ 1). Thời gian xử lý mượn sách trung bình 45 giây (bao gồm tìm sách, quét mã, xác nhận), giảm 82% so với 4.2 phút của hệ thống cũ. Thời gian trả sách trung bình 35 giây.
1	API response time: GET /api/books trung bình 180ms (p95: 320ms, p99: 580ms), POST /api/transactions trung bình 250ms nhờ optimize database query với eager loading và index. Database query performance: tìm kiếm sách theo từ khóa trên 125.000 records chỉ mất 85ms nhờ full-text search index. Concurrent user test với JMeter cho thấy hệ thống xử lý ổn định 500 concurrent users với throughput 1.200 requests/second, CPU utilization 68%, memory usage 4.2GB/8GB. Bug density giảm từ 41 bugs/KLOC (hệ thống cũ) xuống 8 bugs/KLOC nhờ OOP architecture, code coverage 87%, và comprehensive testing. System uptime đạt 99.2%, chỉ downtime 58 giờ cho maintenance và upgrade.
1	Code quality được đánh giá bằng SonarQube cho thấy các chỉ số: cyclomatic complexity trung bình 3.8/method (xuất sắc, so với 28.5 hệ thống cũ), code duplication 2.1% (mục tiêu <5%), technical debt ratio 0.8% (A rating), 0 critical/blocker issues. Maintainability index 82/100 (very good). Số dòng code giảm từ 45.000 xuống 28.000 dòng nhờ tái sử dụng code qua kế thừa và composition. Thời gian onboarding developer mới giảm từ 4 tuần xuống 1.5 tuần do code structure rõ ràng, naming convention nhất quán, và comprehensive documentation. Thời gian thêm tính năng mới: tích hợp thanh toán online VNPay chỉ mất 3 ngày (so với dự kiến 3-4 tuần hệ thống cũ) bằng cách tạo VNPayPaymentStrategy implements IPaymentStrategy, inject vào PaymentService, không ảnh hưởng code hiện tại. Thêm quản lý tạp chí điện tử mất 5 ngày bằng cách tạo class EMagazine extends EBook.
1	Regression test suite với 450 test cases (unit test 320, integration test 130) chạy tự động mỗi build, thời gian chạy 8.5 phút, phát hiện 23 regression bugs trước khi deploy production. Khảo sát 850 người dùng (562 sinh viên, 288 giảng viên, tỷ lệ phản hồi 82%) về mức độ hài lòng cho kết quả trung bình 4.5/5 điểm. Phân tích chi tiết: 94% hài lòng với tốc độ xử lý nhanh, 91% đánh giá giao diện thân thiện và dễ sử dụng, 88% hài lòng với tính năng tìm kiếm chính xác, 85% đánh giá cao tính năng đặt trước sách online, 78% hài lòng với thông báo email/SMS tự động về hạn trả sách.
1	Các góp ý cải thiện: 32% muốn có mobile app (hiện tại chỉ responsive web), 28% đề xuất thêm recommendation system gợi ý sách dựa trên lịch sử mượn, 15% muốn tích hợp thanh toán phạt qua ví điện tử. Thư viện viên đánh giá hệ thống giảm 70% khối lượng công việc thủ công, báo cáo thống kê tự động tiết kiệm 15 giờ/tháng. Số lượt mượn sách tăng 34% (từ 2.380 lượt/ngày lên 3.200 lượt/ngày) do quy trình đơn giản hơn. Tỷ lệ trả sách đúng hạn tăng từ 72% lên 89% nhờ hệ thống nhắc nhở tự động qua email/SMS trước hạn 3 ngày và 1 ngày.
1	Áp dụng OOP mang lại lợi ích rõ rệt về khả năng mở rộng và bảo trì. Tính đóng gói giúp isolate changes: khi thay đổi logic tính phạt từ 5.000đ/ngày lên 6.000đ/ngày, chỉ cần sửa trong StudentFineCalculator.calculate() mà không ảnh hưởng 15 class khác đang sử dụng. Tính kế thừa giảm code duplication: thuộc tính chung như memberID, name, email chỉ định nghĩa một lần ở class Member, StudentMember và FacultyMember tự động thừa hưởng. Khi thêm thuộc tính phoneNumber vào Member, tất cả subclass đều có. Tính đa hình cho phép viết code generic: TransactionService.sendNotification(Member member) nhận parameter kiểu Member, hoạt động với cả StudentMember và FacultyMember nhờ polymorphism, không cần if-else kiểm tra type.
1	Interface INotificationService cho phép dễ dàng mock trong unit test, inject EmailNotificationService trong production và MockNotificationService trong test. Design patterns làm code dễ hiểu hơn: đọc code thấy BookServiceFactory, developer ngay lập tức hiểu đây là Factory pattern, không cần đọc implementation. Code review và collaboration hiệu quả hơn khi team follow cùng OOP principles và patterns. So sánh định lượng giữa hệ thống mới (OOP) và cũ (procedural PHP): Lines of Code giảm 37.8% (28K vs 45K), cyclomatic complexity giảm 86.7% (3.8 vs 28.5), code duplication giảm 91% (2.1% vs 23%), bugs/KLOC giảm 80.5% (8 vs 41). Thời gian xử lý giao dịch giảm 82% (45s vs 4.2 phút), thời gian thêm feature giảm 85% (3-5 ngày vs 3-4 tuần).
1	Chi phí bảo trì giảm 68%: hệ thống cũ tốn trung bình 120 giờ/tháng fix bugs và maintenance, hệ thống mới chỉ 38 giờ/tháng. User satisfaction tăng 44.6% (94% vs 65%). System uptime tăng từ 96.8% lên 99.2%. Về định tính: hệ thống cũ có code spaghetti với business logic lẫn lộn với presentation và data access, khó test. Hệ thống mới tách biệt rõ ràng theo MVC và SOLID principles, mỗi class có single responsibility, dễ test và maintain. Developer experience tốt hơn nhiều: IDE autocomplete và refactoring tools hoạt động tốt với OOP, trong khi code procedural khó refactor. Onboarding time giảm đáng kể do code structure chuẩn, dễ navigate.
1	Một  số bài học rút ra từ quá trình triển khai: không nên over-engineering, ban đầu team áp dụng quá nhiều design patterns (15 patterns), làm code phức tạp không cần thiết, sau đó refactor lại chỉ giữ 7 patterns thực sự cần thiết. Cần balance giữa DRY (Don't Repeat Yourself) và readable code: có những chỗ duplicate 5-10 dòng code đơn giản thì nên để nguyên thay vì abstract thành method phức tạp. Testing rất quan trọng: nhờ test coverage 87% mà phát hiện được 67 bugs trước khi release, tránh được incident nghiêm trọng. Documentation không thể thiếu: mặc dù code tốt là self-documenting, nhưng vẫn cần JavaDoc cho public API và architecture decision records (ADR) để team mới hiểu design rationale. Code review nghiêm ngặt giúp maintain code quality: yêu cầu approval từ 2 senior developers, checklist gồm 25 items về naming, error handling, performance, security.
1	Migration từ hệ thống cũ cần kế hoạch chi tiết: chạy song song 2 hệ thống trong 1 tháng, migrate data theo từng module, rollback plan đầy đủ. Nghiên cứu đã chứng minh hiệu quả của việc áp dụng lập trình hướng đối tượng và mô hình MVC trong xây dựng hệ thống quản lý thư viện số. Bốn tính chất cốt lõi của OOP (đóng gói, kế thừa, đa hình, trừu tượng) kết hợp với các design patterns phù hợp tạo ra hệ thống có code quality cao, dễ bảo trì và mở rộng. Các chỉ số kỹ thuật như cyclomatic complexity, code duplication, test coverage đều đạt mức excellent. Performance và user satisfaction vượt mục tiêu đề ra.
1	Hướng phát triển trong tương lai bao gồm: phát triển mobile app cho iOS và Android sử dụng Flutter với backend API có sẵn, implement AI recommendation system dùng collaborative filtering gợi ý sách, tích hợp blockchain để tạo certificate cho sách điện tử chống sao chép trái phép, mở rộng sang hệ thống liên thư viện cho phép mượn sách giữa các thư viện đại học, áp dụng microservices architecture khi hệ thống phát triển lớn hơn. Nghiên cứu này có thể là tài liệu tham khảo cho các thư viện và tổ chức muốn triển khai hệ thống tương tự.
1	Nghiên cứu này áp dụng thuật toán phân loại K-Nearest Neighbors (KNN) để dự đoán nguy cơ mắc bệnh tiểu đường type 2 dựa trên các chỉ số y tế của bệnh nhân. Dữ liệu được thu thập từ 2.850 hồ sơ bệnh án tại Bệnh viện Đa khoa Thống Nhất TP.HCM trong giai đoạn 2021-2023, bao gồm 8 đặc trưng: tuổi, BMI, huyết áp, glucose máu đói, HbA1c, cholesterol, triglyceride và tiền sử gia đình. Sau quá trình tiền xử lý dữ liệu, chuẩn hóa và tối ưu tham số, mô hình KNN với k=7 và khoảng cách Euclidean đạt độ chính xác 87.3%, độ nhạy 84.6%, độ đặc hiệu 89.1% và F1-score 86.2% trên tập test.
1	So sánh với Decision Tree (82.5%) và Logistic Regression (81.8%), KNN cho kết quả tốt hơn. Hệ thống được triển khai thành ứng dụng web hỗ trợ bác sĩ đưa ra quyết định lâm sàng, giảm 35% thời gian sàng lọc ban đầu. Theo báo cáo của Tổ chức Y tế Thế giới (WHO) năm 2023, Việt Nam có khoảng 4.2 triệu người mắc bệnh tiểu đường, chiếm 5.8% dân số, và con số này dự kiến tăng lên 6.1 triệu vào năm 2030. Đặc biệt nghiêm trọng là 46% bệnh nhân không được chẩn đoán, dẫn đến biến chứng muộn như bệnh thận, mù mắt, hoại tử chi. Chi phí điều trị tiểu đường và biến chứng trung bình 28 triệu đồng/năm/bệnh nhân, gây gánh nặng cho hệ thống y tế và gia đình.
1	"Tại Bệnh viện Đa khoa Thống Nhất, thống kê năm 2022 ghi nhận 8.500 ca khám liên quan đến tiểu đường, trong đó 65% đã có biến chứng khi được phát hiện. Việc sàng lọc sớm và dự đoán nguy cơ là then chốt để can thiệp kịp thời. Tuy nhiên, phương pháp truyền thống dựa vào kinh nghiệm bác sĩ và các tiêu chuẩn cứng nhắc (như glucose ≥126 mg/dL) có độ chính xác chỉ 73-78%, bỏ sót nhiều ca tiềm ẩn. Machine learning, đặc biệt là KNN, có tiềm năng cải thiện đáng kể độ chính xác dự đoán. K-Nearest Neighbors là thuật toán học máy đơn giản nhưng hiệu quả, thuộc nhóm instance-based learning và lazy learning. KNN phân loại một điểm dữ liệu mới dựa trên k điểm gần nhất trong tập huấn luyện, sử dụng nguyên tắc ""vật giống nhau ở gần nhau""."
1	Ưu điểm của KNN trong bài toán y tế: không cần giả định về phân phối dữ liệu (non-parametric), hoạt động tốt với dữ liệu phi tuyến, dễ hiểu và giải thích cho bác sĩ (có thể show k bệnh nhân tương tự nhất), không cần training time (chỉ cần lưu data), phù hợp với dataset size vừa phải (2.850 samples). Nhược điểm là prediction time chậm với dataset lớn và nhạy cảm với irrelevant features. Trong y tế, KNN đã được áp dụng thành công: nghiên cứu của Sisodia và Sisodia (2018) trên Pima Indians Diabetes dataset đạt 76.3% accuracy, nghiên cứu của Choubey et al. (2020) trên 768 bệnh nhân đạt 82.5% accuracy. Nghiên cứu này kỳ vọng đạt accuracy >85% nhờ dataset lớn hơn, nhiều features hơn và kỹ thuật tối ưu hóa tốt hơn.
1	KNN là thuật toán supervised learning dùng cho cả classification và regression. Với classification, KNN hoạt động theo 4 bước: (1) Chọn số k neighbors, (2) Tính khoảng cách từ điểm mới đến tất cả điểm trong training set, (3) Chọn k điểm có khoảng cách nhỏ nhất, (4) Voting: class nào xuất hiện nhiều nhất trong k neighbors thì gán cho điểm mới. Khoảng cách thường dùng nhất là Euclidean: d(p,q) = √(Σ(pi-qi)²) cho n chiều. Ví dụ với bệnh nhân mới có glucose=140, BMI=28.5, age=45 và k=3, tính khoảng cách đến tất cả 2.850 bệnh nhân trong training set, lấy 3 bệnh nhân gần nhất có labels [1,1,0] (1=diabetes, 0=normal), voting cho 1 xuất hiện 2 lần > 0 xuất hiện 1 lần, nên dự đoán bệnh nhân mới có diabetes.
1	Các biến thể: Weighted KNN gán trọng số cho neighbors dựa trên khoảng cách (gần hơn = trọng số cao hơn), Distance-Weighted KNN dùng weight = 1/distance². Lựa chọn k quan trọng: k nhỏ (k=1,3) sensitive với noise, k lớn (k=20,30) có thể bị dominated bởi majority class. Thường chọn k lẻ để tránh tie-breaking, và k = √n với n là số samples. Độ đo khoảng cách ảnh hưởng lớn đến performance của KNN. Euclidean distance: d = √(Σ(xi-yi)²), phổ biến nhất, hoạt động tốt khi features có magnitude tương đương và quan hệ tuyến tính. Manhattan distance (L1): d = Σ|xi-yi|, ít nhạy cảm với outliers hơn Euclidean, phù hợp với high-dimensional data. Minkowski distance tổng quát hóa: d = (Σ|xi-yi|^p)^(1/p), với p=1 là Manhattan, p=2 là Euclidean.
1	Cosine similarity: 1 - (A·B)/(||A||·||B||), đo góc giữa 2 vectors, phù hợp với text data hoặc khi magnitude không quan trọng. Mahalanobis distance tính đến correlation giữa các features: d = √((x-y)ᵀ·S⁻¹·(x-y)) với S là covariance matrix, hiệu quả khi features có correlation cao nhưng computational cost lớn. Trong nghiên cứu này, thử nghiệm cả 4 distance metrics trên validation set để chọn metric tốt nhất. Kết quả Euclidean cho accuracy cao nhất (87.3%) so với Manhattan (86.1%), Minkowski p=3 (85.8%) và Cosine (83.2%), có thể do features y tế có relationship tuyến tính. Chuẩn hóa dữ liệu là bước tiền xử lý quan trọng cho KNN vì thuật toán dựa trên khoảng cách. Nếu một feature có range lớn (ví dụ glucose: 70-400 mg/dL) và feature khác có range nhỏ (ví dụ HbA1c: 4-14%), thì glucose sẽ dominate distance calculation, làm giảm ảnh hưởng của HbA1c.
1	Min-Max Scaling chuyển features về range [0,1]: x_scaled = (x - min)/(max - min), bảo toàn relationships nhưng sensitive với outliers. Standardization (Z-score normalization) chuyển về phân phối mean=0, std=1: x_scaled = (x - μ)/σ, robust hơn với outliers, phù hợp khi data có normal distribution. Trong nghiên cứu này sử dụng Standardization vì medical features thường có normal distribution và có outliers (ví dụ glucose 500 mg/dL ở bệnh nhân diabetes nặng). Curse of dimensionality là hiện tượng khi số chiều tăng, khoảng cách giữa các điểm trở nên tương đương nhau, làm KNN mất hiệu quả. Với 8 features, vấn đề này chưa nghiêm trọng, nhưng đã áp dụng feature selection để loại bỏ redundant features, giảm từ 12 features ban đầu xuống 8 features quan trọng nhất dựa trên correlation analysis và domain knowledge.
1	Dữ  liệu được thu thập từ hệ thống quản lý bệnh án điện tử của Bệnh viện Đa khoa Thống Nhất TP.HCM, bao gồm 2.850 hồ sơ bệnh nhân từ 01/01/2021 đến 31/12/2023 sau khi loại bỏ 340 hồ sơ có missing data >30%. Tiêu chí lựa chọn: bệnh nhân từ 18-75 tuổi, có đầy đủ xét nghiệm glucose máu đói, HbA1c trong vòng 3 tháng, không mắc các bệnh lý nặng khác (ung thư, suy thận giai đoạn cuối). Dataset gồm 1.520 bệnh nhân tiểu đường (53.3%) và 1.330 bệnh nhân không tiểu đường (46.7%), tương đối balanced. Tám features được chọn: Age (tuổi, range 18-75, mean 48.3±12.6), BMI (chỉ số khối cơ thể kg/m², range 17.2-42.8, mean 25.6±4.3), SystolicBP (huyết áp tâm thu mmHg, 90-180, mean 128.5±16.2), FastingGlucose (glucose máu đói mg/dL, 65-385, mean 132.8±45.3),
1	HbA1c (%, 4.2-13.8, mean 6.8±2.1), TotalCholesterol (mg/dL, 120-320, mean 205.4±38.7), Triglyceride (mg/dL, 50-450, mean 168.3±72.5), FamilyHistory (tiền sử gia đình, binary 0/1, 42% có). Label: Diabetes (1=có, 0=không) dựa trên tiêu chuẩn ADA 2023: fasting glucose ≥126 mg/dL hoặc HbA1c ≥6.5% hoặc đang điều trị insulin/thuốc hạ đường huyết. Xử  lý missing values: 8.2% records có missing data ở một vài features, sử dụng KNN Imputer (k=5) để fill missing values dựa trên 5 neighbors tương tự nhất, tốt hơn mean imputation (test accuracy 87.3% vs 84.1%). Xử lý outliers: detect bằng IQR method (Q1-1.5·IQR, Q3+1.5·IQR), tìm thấy 127 outliers chủ yếu ở glucose và triglyceride. Thay vì loại bỏ (có thể mất thông tin quan trọng), sử dụng Winsorization cap outliers ở percentile 1% và 99%.
1	Chuẩn hóa: áp dụng StandardScaler từ scikit-learn fit trên training set, transform cả train và test set để tránh data leakage. Feature engineering: tạo thêm 2 derived features là GlucoseHbA1cRatio = FastingGlucose/HbA1c và BMI_BP = BMI * SystolicBP/100, nhưng sau feature selection bị loại do không cải thiện accuracy. Encoding: FamilyHistory đã là binary nên không cần encode. Class balancing: thử nghiệm SMOTE (Synthetic Minority Over-sampling Technique) để tạo synthetic samples cho minority class, nhưng không cải thiện performance (accuracy 86.8% vs 87.3% without SMOTE) nên không sử dụng. Split data: 70% training (1.995 samples), 15% validation (428 samples), 15% test (427 samples) với stratified split để đảm bảo tỷ lệ classes giống nhau giữa các tập.
1	Sử dụng Grid Search với 5-fold Cross-Validation trên training set để tìm k tối ưu. Thử nghiệm k từ 1 đến 25 (odd numbers) với 4 distance metrics (Euclidean, Manhattan, Minkowski p=3, Cosine). Kết quả: k=7 với Euclidean distance cho CV accuracy cao nhất 88.1%. Phân tích chi tiết: k=1 (accuracy 82.3%) quá sensitive với noise, k=3 (85.6%) và k=5 (86.8%) tốt hơn nhưng chưa optimal, k=7 (88.1%) đạt peak, k=9 (87.5%) và k>9 giảm dần do over-smoothing. Weighted KNN với weight='distance' (trọng số = 1/distance) được test nhưng chỉ cải thiện 0.3% (88.4% vs 88.1%) không đáng kể, computational cost tăng 15% nên không sử dụng. Feature selection: sử dụng Recursive Feature Elimination (RFE) với cross-validation, bắt đầu với 12 features, eliminate features ít quan trọng nhất từng bước.
1	Kết quả: 8 features (như mô tả ở 3.1) cho accuracy tốt nhất, loại bỏ 4 features: WaistCircumference (correlation 0.91 với BMI), DiastolicBP (correlation 0.78 với SystolicBP), HDL và LDL (correlation với TotalCholesterol). Algorithm implementation: sử dụng scikit-learn KNeighborsClassifier với algorithm='ball_tree' (fast cho prediction) thay vì brute force (chậm với >1000 samples), n_jobs=-1 để parallel processing. Mô hình KNN (k=7, Euclidean) được đánh giá trên tập test (427 samples) cho kết quả: Accuracy = 87.3%, Precision = 88.5%, Recall (Sensitivity) = 84.6%, Specificity = 89.1%, F1-score = 86.2%, AUC-ROC = 0.92. Confusion Matrix: True Positive (TP) = 182 (diabetes dự đoán đúng), False Positive (FP) = 23 (dự đoán nhầm diabetes), False Negative (FN) = 33 (bỏ sót diabetes), True Negative (TN) = 189 (không diabetes dự đoán đúng).
1	Phân tích chi tiết: Precision cao (88.5%) nghĩa là khi mô hình dự đoán có diabetes thì 88.5% trường hợp đúng, giảm false alarm. Recall 84.6% nghĩa là trong 215 bệnh nhân thực sự có diabetes, mô hình phát hiện được 182 ca (84.6%), bỏ sót 33 ca. Trong y tế, Recall quan trọng hơn Precision (tốt hơn dự đoán nhầm hơn bỏ sót), nhưng 84.6% vẫn chấp nhận được. Specificity 89.1% nghĩa là trong 212 người không diabetes, mô hình dự đoán đúng 189 ca (89.1%). F1-score 86.2% là harmonic mean của Precision và Recall, đánh giá tổng thể. AUC-ROC 0.92 (excellent, >0.9) cho thấy khả năng phân biệt 2 classes tốt. Cross-validation: 5-fold CV trên toàn bộ dataset cho mean accuracy 87.8%±1.3%, chứng tỏ mô hình stable.
1	So sánh KNN với 5 thuật toán classification khác trên cùng dataset: Decision Tree (DT), Random Forest (RF), Logistic Regression (LR), Support Vector Machine (SVM), Naive Bayes (NB). Kết quả trên test set: KNN (87.3%) > RF (85.9%) > SVM (84.2%) > DT (82.5%) > LR (81.8%) > NB (78.6%). Chi tiết: Decision Tree đơn giản, interpretable nhưng accuracy thấp nhất (82.5%), dễ overfitting (training accuracy 95.2%, test 82.5%). Random Forest cải thiện bằng ensemble (85.9%) nhưng chậm hơn KNN (training time 12.3s vs 0.8s), prediction time tương đương. Logistic Regression nhanh nhưng giả định linear separability không đúng với data này (81.8%). SVM với RBF kernel cho kết quả khá tốt (84.2%) nhưng sensitive với parameter tuning và computational cost cao. Naive Bayes nhanh nhất nhưng accuracy thấp (78.6%) do giả định independence giữa features không đúng (ví dụ BMI và glucose có correlation).
1	Về Recall (quan trọng trong y tế): KNN (84.6%) > RF (83.2%) > SVM (81.5%) > DT (79.8%) > LR (78.3%) > NB (74.1%). Kết luận: KNN cho performance tốt nhất với moderate computational cost, phù hợp cho deployment. Phân tích contribution của từng feature bằng permutation importance: shuffle ngẫu nhiên giá trị của một feature, measure decrease in accuracy. Kết quả (importance score giảm dần): FastingGlucose (0.182) quan trọng nhất, HbA1c (0.165), BMI (0.128), Age (0.115), FamilyHistory (0.092), TotalCholesterol (0.083), SystolicBP (0.071), Triglyceride (0.064). Giải thích: glucose và HbA1c là indicators trực tiếp của diabetes nên importance cao nhất, BMI cao tăng nguy cơ insulin resistance, Age cao tăng nguy cơ do giảm chức năng tế bào beta.
1	Visualize decision boundary bằng PCA giảm 8 dimensions xuống 2D cho thấy 2 classes có separation khá rõ ràng nhưng có overlap vùng boundary (glucose 110-130 mg/dL, HbA1c 5.7-6.4%) - prediabetes zone, khó phân loại chính xác. Case study: bệnh nhân 52 tuổi, BMI 29.2, glucose 118 mg/dL, HbA1c 6.1% được dự đoán có diabetes (probability 0.71), khi show 7 nearest neighbors: 5 có diabetes, 2 không, distance trung bình 1.85 (trên scale chuẩn hóa), giúp bác sĩ hiểu tại sao có dự đoán này và quyết định làm thêm xét nghiệm OGTT để xác nhận. Hệ thống được xây dựng với kiến trúc client-server: Frontend là Single Page Application (SPA) dùng React.js, Material-UI cho components, Recharts cho visualization. Backend là RESTful API dùng Flask (Python 3.10), scikit-learn 1.3.0 cho KNN model, pandas cho data processing, joblib cho model serialization.
1	"Database là PostgreSQL 15 lưu patient records và prediction history. Model được train offline hàng tuần với data mới, save thành pickle file (knn_model.pkl size 8.2MB chứa 1.995 training samples). Deployment: Docker container chạy trên AWS EC2 t3.medium (2 vCPU, 4GB RAM), nginx reverse proxy, SSL certificate từ Let's Encrypt. CI/CD pipeline: GitHub Actions tự động test và deploy khi merge vào main branch. Workflow: (1) Bác sĩ nhập 8 features vào form, (2) Frontend validate và gửi POST request đến /api/predict, (3) Backend load model, standardize input bằng fitted scaler, (4) KNN.predict() và predict_proba() tính prediction và probability, (5) Lưu vào database, (6) Return JSON {prediction: 1, probability: 0.78, risk_level: ""High"", neighbors: [...]} , (7) Frontend hiển thị kết quả với visualization và explanation. Response time trung bình 280ms (50ms model inference, 230ms network và processing)."
1	"Trang chủ có form nhập liệu với 8 fields: Age (number input, validation 18-75), BMI (auto-calculate từ weight và height hoặc nhập trực tiếp), Blood Pressure (hai inputs cho systolic/diastolic, chỉ dùng systolic cho model), Fasting Glucose (number, unit mg/dL hoặc mmol/L auto-convert), HbA1c (number, %), Cholesterol (number, mg/dL), Triglyceride (number, mg/dL), Family History (checkbox). Real-time validation: glucose <50 hoặc >500 hiển thị warning ""Please check input"", BMI <15 hoặc >50 warning. Submit button disabled khi có validation error. Kết quả hiển thị: (1) Risk Level with color coding: Low (green, prob <0.3), Medium (yellow, 0.3-0.7), High (red, >0.7), (2) Probability score với progress bar, (3) Radar chart so sánh 8 features của bệnh nhân với mean values của diabetes group và normal group, (4) List 7 nearest neighbors với similarity score, (5) Feature contribution bar chart,"
1	(6) Recommendations dựa trên risk level: High risk → consult endocrinologist, OGTT test; Medium → lifestyle modification, retest 3 months; Low → maintain healthy lifestyle. Lưu history: mỗi prediction được save vào database với timestamp, input features, output, có thể xem lại trong tab History. Pilot test tại Khoa Nội tiết Bệnh viện Đa khoa Thống Nhất từ 01/01/2024-31/03/2024 với 15 bác sĩ và 420 bệnh nhân. Kết quả: thời gian screening giảm từ trung bình 12 phút (thủ công check criteria, tra cứu guideline) xuống 7.8 phút (35% reduction), bao gồm nhập data 5 phút, chờ prediction 0.3 phút, review result 2.5 phút.
1	Agreement giữa prediction và diagnosis cuối cùng (sau các xét nghiệm bổ sung): 89.3% (375/420 cases), trong đó 28 cases mô hình dự đoán High risk nhưng diagnosis là prediabetes (HbA1c 6.0-6.4%), 17 cases dự đoán Low nhưng phát hiện có diabetes sau OGTT test (bỏ sót). Feedback từ bác sĩ: 86% đánh giá tool hữu ích, 78% tin tưởng vào prediction, 92% thích feature visualization giúp hiểu rõ risk factors, 65% đề xuất thêm tính năng dự đoán biến chứng. Challenges: 23% bệnh nhân không có đủ xét nghiệm (thiếu HbA1c hoặc lipid panel), cần làm thêm xét nghiệm tốn thời gian và chi phí. Internet connection issue 8 lần trong 3 tháng gây delay prediction. 12% bác sĩ vẫn prefer clinical judgment hơn machine learning, cần thêm training về AI in medicine.
1	Ưu điểm: KNN với k=7 đạt accuracy 87.3% cao hơn baseline methods (LR 81.8%, DT 82.5%) và comparable với deep learning methods trong literature (Neural Network 88-90%) nhưng đơn giản hơn, không cần GPU, dễ interpret. Model stable (CV std 1.3%), không overfitting. Feature importance analysis giúp identify key risk factors (glucose, HbA1c, BMI) phù hợp với medical knowledge, tăng trust. Implementation đơn giản, inference nhanh (50ms), phù hợp real-time application. Hạn chế: Dataset size 2.850 chưa lớn, có thể improve accuracy nếu có thêm data từ nhiều bệnh viện khác. Chỉ 8 features, thiếu một số indicators quan trọng như insulin level, C-peptide, GAD antibody (không có trong routine checkup). Model chưa xét đến temporal aspect: diabetes progression over time, chỉ snapshot tại một thời điểm. Curse of dimensionality: nếu thêm nhiều features hơn (>20) thì KNN performance giảm, cần xem xét dimensionality reduction hoặc chuyển sang deep learning.
1	Computational cost: với training set lớn (>100K samples), prediction sẽ chậm, cần approximate nearest neighbors algorithms như Annoy hoặc FAISS. Nghiên cứu này trên 2.850 Vietnamese patients cho accuracy 87.3%, cao hơn so với một số nghiên cứu tương tự: Sisodia & Sisodia (2018) trên 768 Pima Indians dataset đạt 76.3%, Sarwar & Sharma (2020) trên 520 Pakistan patients đạt 79.5%, thấp hơn một chút so với Osman & Hassan (2021) trên 1.500 Sudan patients đạt 89.2%. Sự khác biệt có thể do: (1) Dataset size: nghiên cứu này lớn hơn Pima dataset gấp 3.7 lần, (2) Feature quality: sử dụng HbA1c - gold standard cho diabetes diagnosis, trong khi Pima dataset không có, (3) Population difference: Vietnamese có genetic và lifestyle khác Pima Indians, (4) Preprocessing techniques: KNN imputer + standardization + feature selection tốt hơn.
1	Tuy nhiên, một số nghiên cứu dùng ensemble methods hoặc deep learning đạt accuracy cao hơn: Random Forest 90.5% (Maniruzzaman et al., 2018), CNN 91.2% (Zou et al., 2018), nhưng computational cost và interpretability trade-off. Nhìn chung, KNN 87.3% là acceptable performance, balancing giữa accuracy và simplicity, phù hợp deployment trong clinical setting với limited computational resources. Mô hình KNN có thể hỗ trợ bác sĩ trong: (1) Screening sơ bộ ở primary care clinics, identify high-risk patients cần refer đến endocrinologist, giảm burden cho chuyên khoa, (2) Prioritize patients cho làm OGTT test (expensive và time-consuming), focus vào medium-high risk group, (3) Patient education: visualization của risk factors và nearest neighbors giúp bệnh nhân hiểu rõ nguy cơ, motivate lifestyle change,
1	(4) Population health management: run prediction trên electronic health records database để identify undiagnosed diabetes trong community. Khuyến nghị triển khai: (1) Không thay thế clinical judgment, chỉ là decision support tool, bác sĩ vẫn cần comprehensive assessment, (2) Regular model update: retrain hàng quý với data mới để maintain accuracy, monitor for concept drift (distribution changes over time), (3) Expand dataset: collaborate với nhiều bệnh viện để tăng size và diversity, improve generalizability, (4) Integrate với EMR system để automatic data extraction, giảm manual input, (5) Add explainability features: LIME hoặc SHAP để explain individual predictions, tăng trust, (6) Mobile app: develop smartphone app cho self-screening tại nhà, tuy nhiên cần careful validation và disclaimer.
1	Nghiên cứu đã chứng minh hiệu quả của thuật toán K-Nearest Neighbors trong dự đoán nguy cơ bệnh tiểu đường type 2 dựa trên dữ liệu y tế thực tế tại Việt Nam. Với k=7 và Euclidean distance, mô hình đạt accuracy 87.3%, sensitivity 84.6%, specificity 89.1% trên tập test 427 bệnh nhân, vượt trội so với các phương pháp baseline. Quy trình tiền xử lý dữ liệu cẩn thận (KNN imputation, standardization, feature selection) và tối ưu hóa tham số (grid search với cross-validation) đóng vai trò quan trọng trong việc đạt được performance cao. Hệ thống được triển khai thành công trong môi trường lâm sàng, giảm 35% thời gian screening và nhận được phản hồi tích cực từ bác sĩ. Tuy nhiên, cần mở rộng dataset, thêm features quan trọng, và nghiên cứu temporal models để dự đoán progression.
0	Cháy đang trở thành một trong những mối nguy hiểm tự nhiên lớn nhất đe dọa hệ sinh thái, nền kinh tế và đời sống của con người. Trong ngành Hàng hải, các sự cố cháy lớn trên tàu thủy gây ra những hậu quả nặng nề, thiệt hại lớn cho các hãng tàu và tính mạng của thuyền viên. Do đó, các hệ thống phát hiện cháy sớm đóng vai trò quan trọng để ngăn chặn đám cháy lan rộng ngoài tầm kiểm soát trong môi trường đặc biệt khắc nghiệt dưới tàu thủy. Trong khi đó, việc phát hiện các đối tượng tiềm ẩn nguy cơ cháy sớm sẽ giúp hạn chế tối đa thiệt hại trên toàn tàu.
0	Bài báo đề xuất phương pháp cảnh báo cháy sớm dựa trên các cảm biến thị giác và học sâu để phân tích các điểm ảnh. Đồng thời, ứng dụng mạng nơ-ron tích chập (CNN) để huấn luyện mô hình dựa trên dữ liệu đào tạo nhận dạng đám cháy cho các khu vực nguy hiểm trên tàu thủy. Dữ liệu cảnh báo tại mỗi thời điểm sẽ được thu thập và hiển thị tới trung tâm điều khiển thông qua hệ thống báo động chung. Qua đó, kết quả nghiên cứu cho thấy được ứng dụng mô hình cảnh báo cháy sớm trên tàu thủy sẽ giúp cho hệ thống báo cháy thông thường trở nên linh hoạt và thông minh hơn.
0	Những năm gần đây, vận chuyển hàng hóa bằng đường thủy đang là xu hướng lựa chọn của thương mại quốc tế. Với sự phát triển nhanh chóng của toàn cầu hóa kinh tế và ngành vận tải biển, vận tải đường thủy chiếm tỷ trọng ngày càng tăng trong thương mại quốc gia. Là phương tiện vận tải đường thủy chính, tàu thuyền đảm nhiệm 90% vận chuyển hàng hóa ngoại thương hàng năm [1]. Do đó, sự đổi mới này cần có những con tàu lớn hơn, nền kinh tế và quy mô vận tải đường dài. Bên cạnh đó, việc đảm bảo an toàn trên tàu thủy phòng ngừa tai nạn là nhiệm vụ và yêu cầu cấp bách hiện nay [2].
0	Do tính chất môi trường làm việc trên tàu khác nhau, cabin, buồng máy, hàng hóa và hầm nhiên liệu nên khi xảy ra cháy sẽ có hại cho sức khỏe và gây ra các mối đe dọa nguy hiểm. Trong khi đó, diện tích hoạt động, di chuyển và cứu hộ trên tàu là phức tạp, các lối thoát hiểm đông đúc nên rất khó sơ tán và cứu hộ kịp thời. Vì vậy, các tai nạn này thường gây ra những tổn thất lớn đến tính mạng thuyền viên, tài sản hàng hóa trên tàu [3, 4]. Hiện nay, các công nghệ hiện đại như xử lý ảnh ứng dụng công nghệ học sâu trong dự báo và phát hiện nguy cơ tiềm ẩn hỏa hoạn cho các đối tượng trên tàu giúp có thể phát hiện, ngăn chặn các nguy cơ cháy nổ xảy ra.
0	Trong nghiên cứu, Ronghui Ji đã đề xuất phương pháp phân tích tình trạng và đặc điểm đám cháy cơ bản dựa trên cảm biến truyền thống [5, 6]. Đồng thời, dựa trên phương pháp học máy nông (shallow machine learning) và học máy sâu để thử nghiệm phân tích video cháy trên tàu thủy [7]. Phương pháp phát hiện cháy bằng hình ảnh có nhiều ưu điểm, lắp đặt linh hoạt trong các môi trường và không gian khác nhau. Dựa trên công nghệ xử lý ảnh từ camera bằng các thuật toán xác định các điểm cháy và các nguy cơ tiềm ẩn cháy trong hình ảnh dựa trên dữ liệu luyện mạng.
0	Các mô hình xử lý, phân tích hình ảnh dựa trên mạng nơ-ron tích chập (CNN), mô hình GoogleNet, Modified GoogleNet ứng dụng cho máy bay không người lái hoặc các. Do đó, trong nghiên cứu này đề xuất mô hình CNN phân tích ảnh nhiệt dựa trên kỹ thuật học sâu, đồng thời do dữ liệu về cháy tại buồng máy đặc biệt nguy hiểm nên dữ liệu về đám cháy có thể lấy từ phần mềm mô phỏng 3D. Nội dung bài báo được chia thành các phần sau: Mục 2 trình bày cơ sở lý thuyết xây dựng mô hình CNN luyện mạng. Xây dựng thuật toán phát hiện cảnh báo cháy sớm thể hiện trong Mục 3. Mục 4 thực hiện các bước mô phỏng và đánh giá mô hình. Cuối cùng, phần Kết luận được mô tả ở Mục 5.
0	Trong phần này, nhóm tác giả đề xuất thuật toán phát hiện cháy bằng hình ảnh dựa trên mạng nơ-ron tích chập dựa trên trích xuất tại các vùng nguy cơ cháy. Đầu tiên, hình ảnh được thu thập từ camera IR tại các vị trí khảo sát để làm hình ảnh đầu vào các lớp tích chập. Tiếp theo, thuật toán CNN phát hiện các đối tượng hình ảnh, các điểm xuất hiện đám cháy có hoặc không. Đồng thời, hệ thống phát hiện đối tượng với các vùng đề xuất thông qua các lớp tích chập, lớp gộp, lớp kết nối đầy đủ.
0	Các mạng nơ-ron khác thường sử dụng trọng số để luyện mạng, nhưng đối với CNN lớp tích chập để tạo ra đặc điểm từ hình ảnh gốc. Lớp tích chập là một tập hợp các hạt nhân tích chập. Hạt nhân tích chập trượt trên hình ảnh và tính toán một pixel mới bằng tổng có trọng số của các pixel mà nó trôi qua để tạo bản đồ đặc điểm. Bản đồ đặc điểm phản ánh các đặc điểm của một khía cạnh cho hình ảnh gốc. Trong đó: x biểu diễn hình ảnh đầu vào với kích thước WxH; w - Hạt nhân tích chập có kích thước JxI; b - Độ lệch trong quá trình đào tạo và y - Biểu đồ đầu ra.
0	Đối với hệ số w và b được xác định trong quá trình luyện mạng. Dữ liệu đám cháy liên quan trên tàu thủy rất khó thu thập do môi trường làm việc, công tác chữa cháy khó khăn và mức độ nguy hiểm cao [8]. Do đó, để có bộ dữ liệu cháy trên tàu thủy, nhóm tác giả đã mô phỏng lại các tình huống cháy có thể xảy ra trên tàu với các kịch bản khác nhau như Hình 2.1. Để đào tạo các thuật toán dựa trên mô hình CNN đòi hỏi có bộ dữ liệu lớn. Tuy nhiên, các hình ảnh liên quan đến các đám cháy trên tàu hiện nay là rất nhỏ. Có hai loại đầu vào là hình ảnh và video các đám cháy có thể thu thập để luyện mạng [2, 9].
0	Trong nghiên cứu này, nhóm tác giả thu thập được 1.000 hình ảnh về cháy trên các vị trí trên tàu trong phần mềm UNITY 3D, một mô hình tàu được xây dựng giống với con tàu thực. Hình ảnh đầu vào có các nguồn khác nhau nên trong quá trình huấn luyện cần có khối thay đổi kích thước về dạng 64x64x3. Trong nghiên cứu này, nhóm tác giả sử dụng mô hình Faster R-CNN để luyện mạng cho hệ thống báo cháy. Bộ Faster R-CNN dựa trên bộ CNN nhưng thêm một lớp mạng đề xuất vùng (RPN) để tạo ra các vùng đề xuất thực tế giúp xử lý nhanh hơn và phù hợp với bộ dữ liệu [10]. Hình 2.2 thể hiện sơ đồ cấu trúc lớp mạng Faster R-CNN với đầu vào bao gồm tập hợp ảnh 3 kênh.
0	Số lượng ảnh phụ thuộc vào dữ liệu luyện mạng trong Mục 2.2. Đầu tiên, lớp tích chập sử dụng 8 bộ lọc đầu ra (3x3) theo sau đó là lớp chuẩn hóa và lớp gộp tối đa 2x2. Lớp chập thứ 2 sử dụng 16 bộ lọc đầu ra. Cuối cùng, đầu ra của CNN kết hợp với bộ RPN sẽ được gộp vào lớp phân tích ảnh để phân loại và kích hoạt báo động [10-12]. Ngoài ra, sau khi được đào tạo cho bộ CNN và xác thực để thể hiện phân loại nhị phân dựa trên một số dữ liệu sẽ được thử nghiệm. Kết quả sẽ đưa ra được mức độ có “cháy”, “không cháy”, “khói” và “không khói”.
0	Trong môi trường làm việc trên tàu thủy khu vực buồng máy có nhiệt độ cao hơn các khu vực khác nên việc phát hiện cháy dựa trên ảnh nhiệt cần yêu cầu xử lý cao. Do đó, một thuật toán phát hiện điểm cháy hoặc điểm có khả năng cháy dựa trên điểm ảnh nhiệt có nhiệt độ cao bất thường trong thời gian ngắn. Có một số yếu tố thể hiện mức độ phù hợp của mô hình thực tế trong quá trình xác định tính hợp lệ và đây là các tham số đánh giá trong các quy trình phát hiện đối tượng. Độ chính xác của mô hình là mức độ xác định giá trị chênh lệch giữa giá trị đo lường và giá trị thực tế.
0	Một số nghiên cứu trước đây sử dụng phục vụ cho quá trình đào tạo và mô hình hóa bằng Python 3.7, Tensorflow 2.4 và xử lý ảnh được thực hiện trên OpenCV cùng một số thư viện khác. Trong nghiên cứu này, nhóm tác giả sử dụng mạng Faster R-CNN một công nghệ học sâu thực hiện trên máy tính an Intel (R) Core (TM) i7-10700 CPU @ 2.90 GHz, 16 cores, 40 logical processors, and NVIDIA’s GeForce RTX 3060Ti Ram 128Gb. Giống như nhiều nghiên cứu khác về mạng nơ-ron thông thường sẽ sử dụng 80% mẫu dữ liệu để huấn luyện mạng và 20% còn lại sử dụng để thử nghiệm mô hình.
0	Với 1.000 mẫu dữ liệu, nhóm tác giả sẽ sử dụng 800 mẫu dữ liệu để huấn luyện mạng và 200 mẫu còn lại để thử nghiệm mô hình tương ứng với tỷ lệ 80:20 thể hiện trên Hình 4.2. Hình 4.3 thể hiện mức độ màu nhiệt của khu vực buồng máy trên tàu. Do buồng máy đang sửa chữa bảo dưỡng nên chưa hoạt động. Tuy nhiên, phần mềm vẫn nhận diện được khu vực có điểm nhiệt cao là R1 giá trị max là 417,60 C. Trong khi đó, Hình 4.4 cho thấy được mức độ dự đoán của mô hình đạt khoảng 85% do lượng dữ liệu đầu vào còn hạn chế.
0	Do điều kiện không được thử nghiệm cháy trên tàu thực, nhóm tác giả đã sử dụng phần mềm 3D mô phỏng các đám cháy khác nhau. Từ kết quả cho thấy mức độ dự báo smoke đạt 90,4565% và sát với dữ liệu thực tế. Bên cạnh đó, nhóm tác giả cũng sử dụng các mô hình tương đương R-CNN, SSD, YOLOV8 để kiểm tra kết quả và thời gian đáp ứng của thuật toán và kết quả đưa ra trong Bảng 4.1 [14]. Bài báo này đã đề xuất xây dựng được mô hình hệ thống báo cháy trên tàu thủy ứng dụng công nghệ TIC và trí tuệ nhân tạo,
0	đồng thời xây dựng được bộ dữ liệu cho luyện mạng tại các khu vực khi có cháy trên tàu thủy bằng phương pháp xử lý ảnh nhiệt kết hợp với dữ liệu trong quá khứ. Phương pháp của hệ thống bao gồm thu thập một tập dữ liệu với số lượng lớn hình ảnh về nhiều tình huống cháy khác nhau và xử lý trước tập dữ liệu đã thu thập, bao gồm các kỹ thuật tăng cường dữ liệu và đào tạo mô hình. Đánh giá mô hình được so sánh với các hệ thống phát hiện cháy hiện có và kết quả cho thấy với tốc độ xử lý và hiệu suất của hệ thống đã đạt hiệu quả cao trong môi trường hàng hải.
0	Ở bài báo này các tác giả trình bày một giải pháp ứng dụng học sâu (Deep Learning) để phát hiện và cảnh báo sớm các vật thể lạ (Foreign Object Debris - FOD) trên đường băng nhằm hạn chế các rủi ro xảy ra đối với máy bay khi cất và hạ cánh. Giải pháp của nhóm tác giả sử dụng camera PTZ để thu thập dữ liệu ảnh góc rộng, tiền xử lý hình ảnh, sau đó đưa vào xử lý học sâu bằng thuật toán Yolo-4. Kết quả thử nghiệm cho thấy mô hình có thể hoạt động với tốc độ 45 khung hình/giây, đạt độ chính xác 98% với cấu hình Intel core I7, GPU RTX 2080.
0	Các vật thể lạ (FOD) nói đến bất kỳ vật thể nào nằm trong và xung quanh sân bay (đặc biệt trên đường băng và đường lăn) có thể gây ra những tác động nguy hiểm đến máy bay hoặc nhân viên hàng không. Các vật thể lạ điển hình như: các kim loại xoắn, các bộ phận của máy bay hoặc phương tiện bị rơi trên đường băng như ốc, mảnh nhựa vỡ... FOD gây rủi ro cho các máy bay và gây thiệt hại kinh tế đáng kể đến các hãng hàng không trên thế giới. Ví dụ như tai nạn chuyến bay 4590 của Air France đã khiến 113 người thiệt mạng được xác định nguyên dân do một dải kim loại xoắn như Hình 1.
0	Để giảm thiểu và loại bỏ các nguy cơ do FOD gây nên, một số công ty đã tiến hành phát triển các sản phẩm, hệ thống phát hiện FOD như Tarsier của QinetiQ, FODetect của Xsight, và iFerret của Stratech [1]. Các hệ thống này đều sử dụng camera để chụp ảnh các FOD nghi vấn; sau đó các vật thể này được xác minh bởi các nhân viên giám sát hệ thống. Các hệ thống này hiện đã được triển khai thương mại ở một số sân bay nhưng chưa được triển khai rộng rãi trên toàn thế giới. Lý do chính là bởi các hệ thống này hoạt động này vẫn cần có sự xác thực của con người do đó tính tin cậy của hệ thống bị ảnh hưởng lớn bởi sự tin cậy của các cá nhân chịu trách nhiệm xác thực cho hệ thống.
0	Để giải quyết bài toán nhận diện vật thể lạ (FOD), có nhiều thuật toán hiệu quả đã được công bố gần đây [2-7]. Các thuật toán này thường được phát triển trên nhiều loại cảm biến khác nhau như: hệ thống cảm biến LiDar, hệ thống cảm biến radar FMCW hoạt động trên bước sóng milimet hoặc cảm biến Radar hoạt động trên tần số 96GHz. Các thuật toán này có thể đạt được kết quả cao trên nhiều môi trường hoạt động khác nhau. Ngoài ra, bằng cách sử dụng đa cảm biến, các hệ thống này có thể thực hiện phát hiện và nhận dạng các vật thể lạ.
0	Các thuật toán này dựa trên các mẫu vật thể lạ đã được thu thập trong quá khứ do vậy các phương pháp này có hiệu quả cao đối với các vật thể lạ quen thuộc nhưng hoạt động không hiệu quả đối với các FOD có nền đồng nhất và có nhiễu môi trường phức tạp. Ở bài báo này, nhóm tác giả đề xuất xây dựng một ứng dụng nhận diện và phát hiện vật thể lạ tự động thông qua mạng học sâu. Hệ thống có khả năng tích lũy các thông tin mà con người yêu cầu sau đó có khả năng hoạt động độc lập.
0	Học sâu là một công nghệ đang được quan tâm đầu tư nghiên cứu và phát triển trên toàn thế giới. Kể từ khi có sự cải thiện đáng kể trong ứng dụng phân loại hình ảnh, mạng Nơ-ron tích chập (CNN) đã được đưa vào ứng dụng rộng rãi trong các lĩnh vực thị giác máy tính như phân loại hình ảnh, xác minh khuôn mặt, phát hiện đối tượng và chú thích hình ảnh... Dựa trên các bộ dữ liệu (dataset) được công khai cho cộng đồng như ImageNet [8], Pascal VOC [9] and COCO [10], CNN đã chứng minh được khả năng hoạt động tốt hơn trong các bài toán về phát hiện và nhận dạng đối tượng so với các phương pháp xử lý truyền thống.
0	So với các thuật toán truyền thống, CNN thực hiện phát hiện và nhận diện các đối tượng dựa trên trích chọn đặc trưng do vậy thể hiện được năng lực tốt trong các bài toán về phát hiện vật thể lạ (FOD). Bài toán phát hiện vật thể lạ được chia làm 2 nhiệm vụ: Phát hiện, xác định vị trí của mục tiêu và phân loại các đối tượng trên mặt đường. Ở bài báo này, nhóm tác giả tập trung vào giải quyết vấn đề phát hiện và xác định vị trí của vật thể lạ. Sau đó thông tin về vật thể được đưa về hệ thống cảnh báo trung tâm để các cá nhân chịu trách nhiệm giám sát đưa ra quyết định về phân loại và xử lý.
0	Để giải quyết vấn đề này, nhóm tác giả đã sử dụng thuật toán RPN (Region Proposal Network) để thực hiện trên nền tảng YOLO v4. Mạng Yolo (You Only Look Once)[11-13] được ra đời nhằm cải thiển độ chính xác của các mạng CNN đang được áp dụng hiện nay cũng như khả năng hoạt động với các bài toán nhận diện yêu cầu thời gian thực. Yolo hướng tới khả năng mạng Neural có thể được huấn luyện cũng như thực thi các tính toán thời gian thực trên các nền tảng GPU thông thường. Mạng Yolo là một thuật toán phát hiện đối tượng một giai đoạn, sử dụng một mạng CNN duy nhất để thực hiện xử lý hình ảnh và có thể trực tiếp tính toán kết quả phân loại
0	và tọa độ vị trí của các đối tượng trong khung hình được đưa vào tính toán. Với việc sử dụng định vị và phân loại đối tượng end-to-end đã làm tăng tốc độ tính toán lên đáng kể qua các phiên bản. So với YOLO v3, YOLO v4 đã được tích hợp thêm thuật toán nâng cao dữ liệu thông qua thuật toán xử lý dữ liệu được tích hợp sẵn. Ngoài ra, các thuật toán về đào tạo mạng, hàm kích hoạt và hàm mất mát cũng được tối ưu hóa giúp YOLO v4 có thể hoạt động với tốc độ cao hơn và đạt được sự cân bằng giữa độ chính xác và tốc độ trong các bài toán yêu cầu tốc độ thời gian thực.
0	Mạng Yolov4 sử dụng CSPDarknet53, một mạng Neural mã nguồn mở làm mạng xương sống sử dụng để trào đào và tích xuất các đặc trưng của ảnh (Bochkovskiy et al., 2020), sau đó PANet được sử dụng để kết hợp tốt các thông tin đã được trích xuất (Wang and He, 2019) và sử dụng thuật toán được phát triển bởi nhà phát triển Yolo để nhận diện đối tượng trong ảnh (Redmon and Farhadi, 2018). Hiện nay, thuật toán CNN ngày càng trở nên phổ biến và được áp dụng nhiều trong các lĩnh vực khác nhau. CNN được phát triển dựa trên 2 phương pháp nghiên cứu chính: Thuật toán dựa trên đề xuất vùng và Thuật toán dựa trên đề xuất phi vùng.
0	Trong đó thuật toán dựa trên đề xuất vùng như Faster R-CNN, R-CNN, Mask R-CNN, bao gồm một bộ tạo vùng quan tâm và bộ phân loại đối tượng. Bằng cách sử dụng thuật toán Faster R-CNN cho phép việc khoanh vùng các đối tượng được thực hiện trong thời gian ngắn hướng tới ứng dụng trong các bài toán thời gian thực. Mô hình thuật toán phát hiện vật thể được xây dựng như hình 4, bao gồm 5 bước chính: 1> Điều khiển PTZ camera quay để quét toàn bộ bề mặt đường băng. 2> Thu nhận ảnh từ Camera PTZ, thực hiện thuật toán tăng cường ảnh. 3> Sử dụng thuật toán Yolov4 để thực hiện phát hiện các đối tượng trong ảnh. 4> Loại bỏ các cảnh báo giả. 5> Phát tín hiệu cảnh báo.
0	Bước 1: Điều khiển Camera PTZ: Các camera được khảo sát và lựa chọn sử dụng là camera PTZ có thể được thực hiện điều khiển linh hoạt. Camera được lắp đặt ở bên cạnh và dọc theo đường băng. Vị trí các camera được bố trí để đảm bảo toàn bộ bề mặt đường băng đều được bao phủ bởi hệ thống camera. Số lượng camera được tính toán dựa trên cơ sở khoảng cách từ camera đến đường băng và đảm bảo thời gian quét, xử lý dữ liệu của một camera tối đa là 4 phút. Camera được điều khiển thông qua 01 phần mềm trên máy tính đảm bảo thực hiện điều khiển đồng bộ các camera.
0	Nhóm tác giả đã thực hiện khảo sát và lựa chọn lên phương án sử dụng camera Hitachi KP-HD1005-S5 và tính toán thực hiện lắp đặt camera cách đường băng 300 m. Với cách lắp đặt này thì một vật có kích thước nhỏ nhất mà camera có thể thu được là 0.15x2 cm tương đương 1x1 pixel. Camera có độ nhạy sáng với ảnh màu là 0.009 lux và ảnh đen trắng là 0.005 lux có khả năng đáp ứng được điều kiện ánh sáng thấp. Bước 2: Lọc và tăng cường ảnh: Sau khi thực hiện thu nhận các ảnh được đưa về tiền xử lý để thực hiện các thuật toán cắt và điều chỉnh kích thước phù hợp với góc quay và vị trí của camera từ đó có thể đưa ảnh đồng nhất về đường băng.
0	Sau xử lý về kích thước, các ảnh được đưa qua một chương trình thực hiện tăng cường ảnh. Cụ thể, nhóm tác giả sử dụng thuật toán cân bằng thích nghi (Adaptive Histogram Equalization) nhằm nâng cao độ tương phản của ảnh sau đó được đưa qua thuật toán phân vùng, xác định vùng bề mặt để loại bỏ các khu vực ảnh không phải là khu vực đường băng. Do các vật thể lạ thường có kích thước nhỏ nên cần sử dụng thuật toán nâng cao độ tương phản nhằm cho phép nhìn rõ đối tượng trong điều kiện thời tiết không thuận lợi như sương mù, thiếu sáng, trời mưa hoặc trường hợp vật thể lạ có màu tương đối đồng nhất với đường băng.
0	Hình ảnh đầu ra của khối là ảnh có độ tương phản cao và đã được loại bỏ các khu vực có thể gây nhầm lẫn cho thuật toán học máy. Hình ảnh sau đó được đưa vào Yolov4 để thực hiện nhận diện các vật thể lạ bằng các thuật toán học máy. Bước 3: Thực hiện phát hiện đối tượng qua Yolov4: Dựa trên phân tích các thuật toán phổ biến hiện tại như SSD, Yolo, Faster R-CNN... nhóm tác giả đã sử dụng lựa chọn Yolov4 để thực hiện bài toán này. Yolov4 là một mã nguồn mở có thể dễ dàng triển khai cài đặt trên các môi trường bằng giao diện lập trình Python.
0	Ưu điểm của Yolov4 là thuật toán có độ chính xác cao và tốc độ xử lý nhanh phù hợp với các bài toán yêu cầu có tốc độ xử lý thời gian thực. Để hỗ trợ cho khả năng hoạt động của Yolov4, thuật toán này được thực hiện cài đặt trên một máy tính có cấu hình phù hợp (Intel core I7, GPU RTX 2080) để đảm bảo có khả năng đáp ứng được xử lý được hình ảnh từ nhiều camera cùng lúc. Ngoài ra, để Yolov4 đáp ứng được bài toán nhận diện vật thể lạ, nhóm tác giả đã thực hiện hiệu chỉnh khối Head của thuật toán để định nghĩa lớp đối tượng vật thể lạ cho mô hình.
0	Bước 4: Loại bỏ cảnh báo giả: Thuật toán Yolov4 được dùng để phát hiện các đối tượng xuất hiện trong khu vực ảnh đầu vào, nhóm tác giả đã có những hiệu chỉnh ở cấu trúc Head nhằm tạo ra mô hình mạng phù hợp với bài toán phát hiện vật thể lạ. Tuy nhiên, trong quá trình chạy thực nghiệm gặp phải các vấn đề như thuật toán phát hiện các vật thể nằm ngoài khu vực đường băng hoặc các vùng là thành phần của đường băng nhưng có màu sắc tương phản là vật thể lạ. Để giải quyết bài toán này, nhóm tác giả đã thực hiện 2 phương pháp cải tiến nhằm loại bỏ các cảnh báo giả này.
0	Cụ thể: Sử dụng thuật toán phân vùng để xác định vùng ảnh thuộc đường băng với giả thiết rằng mặt đường băng là vùng ảnh có màu sắc và cường độ sáng đồng nhất có diện tích lớn nhất trong bức ảnh đầu vào. Từ đó, kết hợp với vị trí của vật thể được xác định bởi Yolov4 để xác định vật thể được nhận định có nằm trên vùng cần cảnh báo không. Nếu nằm ngoài vùng này, hệ thống tự động bỏ loại bỏ cảnh báo. Bổ sung thêm các hình ảnh về vật thể lạ phát hiện sai vào tập dữ liệu để tăng cường khả năng nhận diện.
0	Trong trường hợp hệ thống nhận diện một khu vực trong đường băng là vật thể lạ, hệ thống cho phép người dùng bổ sung thêm hình ảnh nhận diện sai vào hệ thống để hệ thống tự động học hỏi cho các lần phát hiện sau từ đó giảm khả năng phát hiện và cảnh báo giả của hệ thống. Bước 5: Phát tín hiệu cảnh báo: Việc phát cảnh báo được thực hiện tại tại màn hình và hệ thống xử lý trung tâm của hệ thống. Khi có nghi ngờ một vật thể lạ, hệ thống phần mềm thực hiện điều khiển Camera PTZ vào vị trí nhận diện sau đó phát âm cảnh báo để người giám sát ra quyết định thực thi. Nếu đúng là vật thể lạ người giám sát có thể thực hiện điều phối các thao tác cần thiết nhằm xử lý.
0	Nếu đó là phát hiện giả, người giám sát cập nhật trực tiếp trên phần mềm để hệ thống tự động đưa vào tập dữ liệu chung và cải tiến tập dữ liệu. Với mô hình hệ thống đã được xây dựng ở mục này, nhóm tác giả tiến hành thực hiện thu thập dữ liệu, tiến hành huấn luyện, xây dựng phần mềm để thử nghiệm. Trong quá trình thử nghiệm nhóm nghiên cứu đã thực hiện nhiều tinh chỉnh để đạt được kết quả phù hợp. Quá trình thực nghiệm của nhóm tác giả được sự hỗ trợ từ Công ty TNHH Kỹ thuật Quản lý bay (ATTECH) để thực hiện thu thập dữ liệu và đưa ra những phân loại cơ bản về vật thể lạ làm tiền đề cho quá trình huấn luyện và đào tạo mạng Yolov4 phù hợp với bài toán nhận diện vật thể lạ.
0	Quá trình thực nghiệm chi tiết hệ thống nhận diện và cảnh báo vật thể lạ được trình bày chi tiết ở mục III. Để phục vụ cho quá trình huấn luyện mạng Yolov4 phù hợp với bài toán nhận diện vật thể lạ, đặc biệt là điều kiện các sân bay ở Việt Nam, nhóm tác giả đã làm việc với phòng nghiên cứu phát triển của Công ty TNHH Kỹ thuật Quản lý bay (ATTECH) nhằm thực hiện thu thập dữ liệu, lên danh sách các vật thể lạ thường gặp phải trên bề mặt đường băng và thử nghiệm tạo các nguồn dữ liệu hình ảnh và video dựa trên các thông tin trên.
0	Hình ảnh được thu thập bằng cách đưa các vật thể lạ trong danh sách lên trên bề mặt đường có kiến trúc tương tự với bề mặt đường băng và thu lại bằng camera kỹ thuật số. Ngoài ra, nhóm tác giả còn được công ty cung cấp thêm hình ảnh về các vật thể lạ trong quá khứ mà công ty đã xử lý, giúp cho kho dữ liệu hình ảnh được thêm đầy đủ. Nhóm tác giả đã phối hợp với Công ty TNHH Kỹ thuật Quản lý bay (ATTECH) thu thập được hơn 100 video với hơn 50.000 ảnh có chứa các đối tượng được định nghĩa là vật thể lạ từ các video tự thực hiện và các video do ATTECH cung cấp.
0	Để đảm bảo khả năng hoạt động của mô hình mạng trong các điều kiện khác nhau, 2 bên đã phối hợp thực hiện thu thập hình ảnh từ nhiều điều kiện thời tiết và nhiều góc quan sát khác nhau. Sau quá trình trích xuất hình ảnh từ các video, nhóm thu được bộ dữ liệu có tên là PTIT-FOD bao gồm hơn 50.000 ảnh có chứa các vật thể được định nghĩa là vật thể lạ. Tập hình ảnh thu thập được gồm các vật thể lạ như: Ống nhựa, ống sắt, ốc vít, đá, quả bóng, mảnh vỡ của miếng kim loại... có kích thước dao động trong khoảng từ 1x1x1cm đến 20x10x10cm.
0	Các vật thể được đặt ở nhiều tư thế và hướng đặt khác nhau phục vụ cho việc thu thập dữ liệu hình ảnh. Bộ hình ảnh mẫu sau khi thu thập được 2 nhóm phối hợp đánh giá là phù hợp để có thể tiến hành những thử nghiệm cơ sở làm nền tảng cho các nghiên cứu chuyên sâu hơn. Tập ảnh sau khi được thu thập được đưa qua chương trình phần mềm thực hiện các thao tác tiền xử lý trước khi đưa vào quá trình huấn luyện. Trong mô hình triển khai, thuật toán này được sử dụng ngay sau khi thu thập hình ảnh và được xử lý tại bộ xử lý trung tâm của hệ thống.
0	Chương trình tiền xử lý thực hiện các thuật toán sau: - Cân bằng thích nghi - Phân vùng ảnh theo vùng bề mặt. Sau khi được chạy qua thuật toán tiền xử lý, bức ảnh được cân bằng về độ sáng, độ tương phản và tự động được loại bỏ các vùng được nhận diện không phải là đường băng. Do đó, giúp cho chương trình Yolov4 hoạt động có độ chính xác cao hơn. Từ tập hình ảnh đã được thu thập ở mục B, nhóm tác giả đã tiến hành tạo nhãn cho tập ảnh bằng công cụ Yolo_mark, thực hiện khoanh vùng các khu vực có vật thể lạ cần được phát hiện.
0	Sau đó, nhóm tác giả thực hiện đưa ảnh vào công cụ phần mềm lọc, tăng cường ảnh và đưa vào huấn luyện trên mạng Yolov4 để huấn luyện mạng phát hiện các vật thể lạ theo hướng dẫn của Darknet [14]. Nhóm tác giả lần lượt điều chỉnh các hệ số mạng với tập hình ảnh đã được thu thập ở trên để tìm được bộ thông số hyper phù hợp. Sau khi thử nghiệm tìm kiếm bộ thông số trên máy tính với cấu hình Intel Core i7, GPU RTX 2080, nhóm tác giả đã thu được một mô hình mạng có khả năng đạt được độ chính xác nhận diện là 98% trên tập dữ liệu đã được thu thập.
0	Theo đánh giá, mô hình có khả năng nhận diện với độ chính xác cao đối với các vật thể có kích thước hướng chụp lớn 1.5x1.5cm trở lên và có màu sắc không đồng nhất với màu sắc của bề mặt nền đường băng. Với các vật thể có kích thước nhỏ (khoảng dưới 1.5x1.5cm) ở vị trí xa được đánh giá có tỉ lệ nhận diện lỗi cao hơn. Ngoài ra, với sự hỗ trợ của thuật toán tiền xử lý bao gồm: Cân bằng thích nghi và Phân vùng ảnh theo bề mặt, cho phép chương trình nâng cao được tỉ lệ nhận diện vật thể lạ so với khi không sử dụng.
0	2 thuật toán này đặc biệt có tác dụng đối với trường hợp ánh sáng môi trường yếu hoặc trường hợp các vật thể có màu sắc tương đồng với màu sắc của bề mặt nền. Sau quá trình đánh giá, nhóm tác giả đã tích hợp 2 thuật toán này nhằm nâng cao tỉ lệ xác định vật thể lạ của chương trình. Với cấu hình máy tính đã đề cập ở trên, chương trình xử lý do nhóm xây dựng có thể thực hiện xử lý được 45 khung hình/giây. Kết quả thử nghiệm của nhóm tác giả đã được cùng phân tích đánh giá cùng nhóm nghiên cứu của Công ty TNHH Kỹ thuật Quản lý bay.
0	Hai nhóm nhận định kết quả nghiên cứu là phù hợp và có khả năng tiếp tục nghiên cứu nâng cấp để hoàn thiện nhằm thực hiện trong ứng dụng thực tế. Ở giai đoạn kế tiếp 2 nhóm sẽ tiếp tục phối hợp thử nghiệm trên các hình ảnh được thu trực tiếp từ camera nhằm có những đánh giá và hiệu chỉnh phù hợp. Kết quả nghiên cứu, thử nghiệm cho thấy mô hình mà nhóm tác giả đề xuất thử nghiệm đã đáp ứng được các yêu cầu cơ bản của bài toán phát hiện vật thể lạ trên đường băng (FOD). Với cấu hình máy như đề xuất (Intel core I7, GPU RTX 2080) mô hình có thể hoạt động với tốc độ nhận diện và phát hiện vật thể lạ của 45 hình/giây với độ chính xác là 98%.
0	Trong giai đoạn tới nhóm tác giả tiếp tục liên kết với công ty ATTECH để phát triển hoàn thiện mô hình thuật toán và tiến hành thử nghiệm trong điều kiện thực tế nhằm đưa ra đánh giá về khả năng hoạt động trong điều kiện môi trường thực tế. Kiến quả nghiên cứu làm nền tảng để nhóm nghiên cứu kết hợp với doanh nghiệp tạo ra một sản phẩm hoàn thiện có khả năng giải quyết được một bài toán có tính thiết yếu cao trong lĩnh vực hàng không nhằm nâng cao tính an toàn của các chuyến bay.
1	Trong bối cảnh nền kinh tế số phát triển mạnh mẽ, ngành vận tải và logistics tại Việt Nam đang đối mặt với áp lực cực lớn về việc quản lý dòng hàng hóa khổng lồ. Việc duy trì các máy chủ vật lý tại chỗ (on-premise) không còn khả thi do chi phí bảo trì cao và khả năng mở rộng hạn chế khi nhu cầu tăng đột biến vào các mùa cao điểm như lễ tết. Điện toán đám mây nổi lên như một giải pháp cứu cánh, cho phép các doanh nghiệp thuê tài nguyên tính toán theo nhu cầu.
1	Nghiên cứu này tập trung vào việc áp dụng mô hình Hybrid Cloud để tối ưu hóa quy trình lưu kho và vận chuyển, giúp doanh nghiệp giảm thiểu rủi ro mất mát dữ liệu và tăng tốc độ xử lý đơn hàng lên gấp nhiều lần so với phương thức truyền thống. Hệ thống được thiết kế dựa trên nền tảng Amazon Web Services (AWS) kết hợp với hệ thống quản lý kho (WMS) sẵn có của doanh nghiệp. Chúng tôi sử dụng các dịch vụ như Amazon EC2 để tính toán lộ trình và Amazon S3 để lưu trữ dữ liệu lịch sử vận chuyển. Một điểm mấu chốt là việc áp dụng Auto-scaling, cho phép hệ thống tự động tăng thêm 50% tài nguyên máy chủ chỉ trong vòng 3 phút khi số lượng đơn hàng vượt ngưỡng 10.000 đơn/giờ.
1	Việc này giúp đảm bảo hệ thống không bao giờ bị treo (downtime), một yếu tố sống còn đối với các doanh nghiệp logistics trực tuyến hiện nay. Sự kết hợp này tạo ra một hệ sinh thái linh hoạt, cho phép các bên liên quan truy cập dữ liệu thời gian thực từ bất kỳ đâu. Dựa trên khảo sát thực tế tại một doanh nghiệp vận chuyển quy mô trung bình trong 6 tháng, kết quả cho thấy những chuyển biến tích cực rõ rệt. Chi phí vận hành công nghệ thông tin (IT Operations) giảm từ 1.2 tỷ VNĐ/tháng xuống còn 780 triệu VNĐ/tháng, tương đương mức giảm 35%.
1	Hiệu suất xử lý đơn hàng tăng từ 500 đơn/phút lên 1.200 đơn/phút mà không xảy ra hiện tượng nghẽn mạng cho thấy khả năng mở rộng linh hoạt và phân bổ tài nguyên thông minh của nền tảng điện toán đám mây. Nhờ cơ chế tự động cân bằng tải và xử lý song song, hệ thống có thể đáp ứng tốt các đợt cao điểm về lưu lượng truy cập, đặc biệt trong các giai đoạn khuyến mãi hoặc mùa mua sắm lớn. Đặc biệt, tỷ lệ sai sót trong phân loại hàng hóa giảm mạnh từ 4,2% xuống chỉ còn 0,8% nhờ vào khả năng đồng bộ hóa dữ liệu tức thời trên nền tảng đám mây, giúp các bộ phận kho vận, bán hàng và quản lý chuỗi cung ứng luôn làm việc trên cùng một nguồn dữ liệu nhất quán và cập nhật theo thời gian thực.
1	Điều này không chỉ hạn chế lỗi do dữ liệu trễ hoặc không đồng bộ, mà còn rút ngắn thời gian xử lý và giảm chi phí vận hành liên quan đến việc kiểm tra, sửa lỗi thủ công. Những con số này minh chứng rằng Cloud Computing không chỉ là một xu hướng công nghệ mang tính thời thượng, mà thực sự là một công cụ kinh tế hiệu quả, góp phần tối ưu hóa quy trình, nâng cao chất lượng dịch vụ khách hàng và trực tiếp thúc đẩy lợi nhuận cũng như năng lực cạnh tranh dài hạn cho doanh nghiệp trong bối cảnh chuyển đổi số hiện nay.
1	Ngành y tế hiện đại đòi hỏi sự chính xác tuyệt đối và khả năng quản lý thông tin bệnh nhân một cách khoa học. Việc sử dụng các phương pháp lập trình cũ khiến hệ thống trở nên cồng kềnh, khó nâng cấp và dễ xảy ra lỗi khi có sự thay đổi về quy trình khám chữa bệnh. Lập trình hướng đối tượng (OOP) với các đặc tính cốt lõi như tính đóng gói, tính kế thừa và tính đa hình cung cấp một khung sườn hoàn hảo để mô phỏng thực thể bệnh viện vào trong mã nguồn. Nghiên cứu này đề xuất một kiến trúc phần mềm quản lý bệnh viện, nơi mà mỗi thực thể như Bệnh nhân, Bác sĩ, Đơn thuốc hay Thiết bị y tế được quản lý như một đối tượng độc lập nhưng có mối liên kết chặt chẽ.
1	Trong hệ thống này, chúng tôi xây dựng lớp cơ sở (Base Class) là User, từ đó các lớp Doctor và Patient sẽ kế thừa để tái sử dụng mã nguồn, giúp giảm 40% thời gian viết code cho các tính năng đăng nhập và bảo mật. Tính đa hình được áp dụng triệt để trong phương thức Treat(), nơi mà mỗi bác sĩ chuyên khoa khác nhau sẽ có cách thực thi khám bệnh khác nhau trên cùng một đối tượng bệnh nhân. Việc này cho phép hệ thống dễ dàng mở rộng thêm các khoa mới (ví dụ: Khoa Nhi, Khoa Nội) mà không cần chỉnh sửa cấu trúc cốt lõi của phần mềm. Tính đóng gói giúp bảo vệ thông tin nhạy cảm của bệnh nhân, chỉ cho phép truy cập thông qua các hàm Getter/Setter được kiểm soát chặt chẽ bằng quyền hạn.
1	Sau khi triển khai thử nghiệm tại một phòng khám đa khoa, hệ thống cho thấy khả năng bảo trì (maintainability) cực cao. Khi cần cập nhật quy định mới về bảo hiểm y tế, đội ngũ kỹ thuật chỉ mất 2 ngày để điều chỉnh lớp Insurance thay vì 10 ngày như hệ thống cũ. Tốc độ truy xuất hồ sơ bệnh án trung bình đạt 0.5 giây/bệnh nhân, nhanh hơn 60% so với phương pháp lưu trữ thủ công hoặc các cơ sở dữ liệu phẳng không phân lớp. Tỷ lệ lỗi logic phát sinh trong quá trình vận hành giảm xuống dưới 1%, giúp y bác sĩ tập trung hoàn toàn vào công tác chuyên môn thay vì lo lắng về sự cố phần mềm. Điều này khẳng định OOP là nền tảng vững chắc cho các hệ thống phần mềm phức tạp và đòi hỏi độ tin cậy cao.
1	"Trong kỷ nguyên tài chính số, việc phê duyệt khoản vay nhanh chóng nhưng vẫn đảm bảo an toàn là thách thức lớn đối với các ngân hàng. Thuật toán K-Nearest Neighbors (KNN) là một phương pháp học máy giám sát hiệu quả, dựa trên nguyên lý ""những khách hàng có đặc điểm tương đồng thường có hành vi tài chính giống nhau"". Nghiên cứu này tập trung vào việc áp dụng KNN để phân loại khách hàng thành hai nhóm: ""Có khả năng trả nợ"" và ""Rủi ro cao"" dựa trên các biến số như thu nhập hàng tháng, độ tuổi, lịch sử tín dụng và nợ hiện tại. Việc tự động hóa quá trình này giúp ngân hàng tiết kiệm nhân lực và giảm thiểu sai sót do cảm tính của nhân viên thẩm định."
1	Thử nghiệm trên bộ dữ liệu gồm 5.000 hồ sơ vay vốn, mô hình KNN cho độ chính xác (Accuracy) lên tới 89.5%. Quan trọng hơn, độ nhạy (Recall) đối với nhóm khách hàng rủi ro đạt 92%, nghĩa là ngân hàng có thể phát hiện hầu hết các trường hợp có nguy cơ nợ xấu trước khi giải ngân. Thời gian để đưa ra quyết định cho một khoản vay giảm từ 24 giờ xuống còn dưới 10 giây. Với việc áp dụng thuật toán này, tỷ lệ nợ xấu của đơn vị thử nghiệm đã giảm từ 5.5% xuống còn 3.2% sau một năm vận hành. Kết quả này chứng minh rằng việc kết hợp toán học thuật toán vào quản trị rủi ro tài chính mang lại lợi ích kinh tế trực tiếp và giúp hệ thống ngân hàng trở nên minh bạch, an toàn hơn.
1	Trong những năm gần đây, điện toán đám mây (Cloud Computing) đã trở thành nền tảng công nghệ cốt lõi trong nhiều lĩnh vực của đời sống như giáo dục, y tế, thương mại điện tử và quản lý đô thị thông minh. Theo báo cáo của Gartner năm 2024, hơn 85% doanh nghiệp trên toàn cầu đã sử dụng ít nhất một dịch vụ đám mây trong hoạt động vận hành. Song song đó, lập trình hướng đối tượng (OOP) đóng vai trò quan trọng trong việc xây dựng các hệ thống phần mềm có khả năng mở rộng, bảo trì và tái sử dụng cao. Sự kết hợp giữa điện toán đám mây và OOP giúp tối ưu hóa việc quản lý dữ liệu lớn, giảm chi phí hạ tầng và nâng cao hiệu quả xử lý thông tin.
1	Nghiên cứu này tập trung phân tích cách hai công nghệ trên được ứng dụng trong quản lý dữ liệu đời sống, từ đó đánh giá hiệu quả thực tiễn và tiềm năng phát triển trong tương lai. Điện toán đám mây cung cấp các mô hình dịch vụ phổ biến như IaaS, PaaS và SaaS, cho phép người dùng truy cập tài nguyên tính toán thông qua Internet mà không cần đầu tư hạ tầng vật lý. Trong khi đó, lập trình hướng đối tượng tổ chức phần mềm dựa trên các đối tượng có thuộc tính và hành vi riêng biệt, giúp hệ thống dễ mở rộng và kiểm soát. Nghiên cứu sử dụng phương pháp phân tích hệ thống kết hợp với khảo sát thực nghiệm trên 120 ứng dụng quản lý dữ liệu sử dụng nền tảng đám mây.
1	Các ứng dụng này được xây dựng theo mô hình OOP, sử dụng các lớp, kế thừa và đa hình nhằm tối ưu cấu trúc phần mềm. Dữ liệu được thu thập trong vòng 6 tháng, tập trung vào hiệu suất xử lý, chi phí vận hành và mức độ ổn định của hệ thống. Kết quả nghiên cứu cho thấy việc áp dụng điện toán đám mây kết hợp OOP giúp giảm trung bình 32% chi phí vận hành hệ thống so với mô hình máy chủ truyền thống. Trong lĩnh vực giáo dục, các hệ thống quản lý học tập trực tuyến (LMS) sử dụng đám mây cho phép hơn 10.000 người dùng truy cập đồng thời mà không xảy ra gián đoạn.
1	Trong y tế, các phần mềm quản lý hồ sơ bệnh án điện tử được xây dựng theo mô hình hướng đối tượng giúp dễ dàng cập nhật, bảo mật và chia sẻ dữ liệu giữa các cơ sở khám chữa bệnh. Ngoài ra, các ứng dụng quản lý gia đình thông minh cũng tận dụng đám mây để lưu trữ dữ liệu cảm biến, từ đó nâng cao trải nghiệm người dùng và độ tin cậy của hệ thống. Nghiên cứu khẳng định rằng sự kết hợp giữa điện toán đám mây và lập trình hướng đối tượng mang lại hiệu quả cao trong quản lý dữ liệu đời sống. Không chỉ giúp tối ưu chi phí và tài nguyên, mô hình này còn nâng cao khả năng mở rộng và bảo trì hệ thống.
1	Thuật toán K-Nearest Neighbors (KNN) là một trong những thuật toán học máy đơn giản nhưng hiệu quả, được sử dụng rộng rãi trong phân loại và dự đoán dữ liệu. Theo thống kê từ Kaggle năm 2023, KNN nằm trong top 10 thuật toán được sử dụng nhiều nhất trong các bài toán phân tích dữ liệu cơ bản. Nhờ đặc tính dễ cài đặt và không yêu cầu giai đoạn huấn luyện phức tạp, KNN đặc biệt phù hợp với các bài toán trong đời sống như nhận dạng khuôn mặt, dự đoán bệnh lý và phân loại hành vi người dùng. Nghiên cứu này tập trung đánh giá khả năng ứng dụng của KNN trong phân tích dữ liệu đời sống, đồng thời phân tích hiệu quả thông qua các chỉ số độ chính xác và thời gian xử lý.
1	Thuật toán KNN hoạt động dựa trên nguyên tắc tìm ra K điểm dữ liệu gần nhất với mẫu cần dự đoán và xác định nhãn dựa trên đa số. Nghiên cứu sử dụng ba bộ dữ liệu thực tế gồm dữ liệu sức khỏe cộng đồng, dữ liệu hành vi mua sắm và dữ liệu giáo dục, với tổng số hơn 50.000 mẫu. Khoảng cách Euclidean được sử dụng để đo độ tương đồng giữa các mẫu dữ liệu. Các giá trị K khác nhau từ 3 đến 15 được thử nghiệm nhằm xác định cấu hình tối ưu. Phương pháp đánh giá bao gồm độ chính xác (Accuracy), độ nhạy (Recall) và thời gian xử lý trung bình trên mỗi truy vấn dữ liệu.
1	Kết quả cho thấy thuật toán KNN đạt độ chính xác trung bình 87% trong dự đoán bệnh tiểu đường type 2 dựa trên dữ liệu sức khỏe. Trong lĩnh vực thương mại điện tử, KNN giúp phân loại hành vi người dùng với độ chính xác lên đến 90%, từ đó hỗ trợ đề xuất sản phẩm phù hợp. Đối với dữ liệu giáo dục, thuật toán đạt hiệu quả cao trong việc dự đoán nguy cơ học sinh bỏ học sớm. Tuy nhiên, nghiên cứu cũng chỉ ra rằng thời gian xử lý của KNN tăng đáng kể khi số lượng dữ liệu lớn, đặc biệt với bộ dữ liệu trên 100.000 mẫu, thời gian truy vấn tăng trung bình 45%.
1	Thuật toán KNN chứng minh được tính hiệu quả và khả năng ứng dụng cao trong nhiều lĩnh vực đời sống. Mặc dù còn hạn chế về tốc độ xử lý khi dữ liệu lớn, nhưng với việc kết hợp các kỹ thuật tối ưu như giảm chiều dữ liệu hoặc sử dụng điện toán đám mây, KNN vẫn là giải pháp phù hợp cho nhiều bài toán thực tế. Trong tương lai, việc tích hợp KNN với các mô hình học sâu hoặc hệ thống phân tán sẽ mở ra nhiều hướng nghiên cứu mới, góp phần nâng cao chất lượng phân tích và dự đoán dữ liệu đời sống.
1	Trong bối cảnh hệ thống y tế đang chịu áp lực lớn, việc phát triển các giải pháp giám sát sức khỏe từ xa trở thành ưu tiên hàng đầu. Nghiên cứu này đề xuất một kiến trúc hệ thống tích hợp công nghệ điện toán đám mây (Cloud Computing) để lưu trữ và xử lý dữ liệu lớn, kết hợp với lập trình hướng đối tượng (OOP) để xây dựng cấu trúc phần mềm linh hoạt. Trọng tâm của hệ thống là việc ứng dụng thuật toán K-Nearest Neighbors (KNN) nhằm phân loại tình trạng bệnh lý của bệnh nhân dựa trên các chỉ số sinh tồn thời gian thực.
1	Kết quả thực nghiệm cho thấy hệ thống đạt độ chính xác phân loại lên tới 94,5% với độ trễ xử lý dữ liệu trung bình chỉ khoảng 120ms trên nền tảng đám mây, giúp đưa ra các cảnh báo kịp thời cho đội ngũ y tế trong các tình huống khẩn cấp. Sự bùng nổ của các thiết bị đeo thông minh (IoT) đã tạo ra một lượng dữ liệu sinh học khổng lồ, đòi hỏi những nền tảng lưu trữ có khả năng mở rộng cao. Điện toán đám mây đóng vai trò là xương sống hạ tầng, cho phép kết nối hàng triệu thiết bị mà không làm quá tải hệ thống cục bộ.
1	Tuy nhiên, thách thức lớn nhất hiện nay là làm thế nào để phân tích khối lượng dữ liệu này một cách nhanh chóng và chính xác để đưa ra chẩn đoán sơ bộ. Việc kết hợp KNN – một thuật toán học máy có giám sát đơn giản nhưng hiệu quả – cho phép hệ thống so sánh các chỉ số hiện tại của bệnh nhân với tập dữ liệu mẫu trong quá khứ để xác định các dấu hiệu bất thường như nhịp tim cao, huyết áp không ổn định hoặc nồng độ oxy trong máu (SpO2) giảm đột ngột.
1	Để đảm bảo tính bảo trì và khả năng mở rộng, hệ thống được thiết kế theo các nguyên tắc cốt lõi của lập trình hướng đối tượng (OOP). Các thực thể chính như Patient (Bệnh nhân), MedicalSensor (Cảm biến y tế), và DiagnosisResult (Kết quả chẩn đoán) được trừu tượng hóa thành các lớp (Classes) riêng biệt. Tính đóng gói (Encapsulation) được sử dụng để bảo vệ thông tin cá nhân của bệnh nhân, trong khi tính kế thừa (Inheritance) cho phép tạo ra các loại cảm biến khác nhau từ một lớp cơ sở chung. Việc áp dụng mô hình thiết kế (Design Patterns) kết hợp với OOP giúp đội ngũ phát triển dễ dàng cập nhật các module phân tích mới mà không làm ảnh hưởng đến cấu trúc tổng thể của hệ thống.
1	Thuật toán KNN được triển khai để phân loại trạng thái sức khỏe dựa trên khoảng cách Euclidean giữa các vector dữ liệu. Trong nghiên cứu này, chúng tôi đã tiến hành thử nghiệm trên tập dữ liệu gồm 10.000 mẫu bệnh án mẫu về tim mạch. Kết quả cho thấy khi chọn tham số K=5, hệ thống đạt được sự cân bằng tối ưu giữa độ chính xác và tốc độ tính toán. Số liệu thực tế ghi nhận tỷ lệ dương tính giả (False Positive) chỉ ở mức 3,2%, một con số rất ấn tượng đối với các hệ thống sàng lọc tự động. Khả năng xử lý song song của điện toán đám mây cho phép thuật toán KNN chạy trên nhiều nút tính toán đồng thời,
1	giúp giảm thời gian phản hồi từ 500ms xuống còn 115ms khi số lượng người dùng truy cập cùng lúc đạt ngưỡng 5.000 kết nối. Hệ thống giám sát sức khỏe tích hợp Cloud và KNN không chỉ mang lại hiệu quả về mặt kỹ thuật mà còn có ý nghĩa xã hội to lớn trong việc giảm tải cho các bệnh viện tuyến đầu. Việc sử dụng OOP giúp phần mềm có độ ổn định cao và dễ dàng bảo trì trong môi trường y tế nghiêm ngặt. Trong tương lai, chúng tôi dự kiến sẽ nâng cao độ chính xác của hệ thống bằng cách tích hợp thêm các trọng số cho thuật toán KNN dựa trên đặc điểm sinh lý riêng biệt của từng độ tuổi.
1	Nghiên cứu này tập trung vào việc xây dựng một hệ thống khuyến nghị sản phẩm thông minh dành cho các nền tảng thương mại điện tử quy mô lớn. Bằng cách tận dụng khả năng xử lý dữ liệu lớn của điện toán đám mây và tính linh hoạt của lập trình hướng đối tượng, chúng tôi triển khai thuật toán KNN để phân nhóm khách hàng dựa trên lịch sử mua sắm và hành vi duyệt web. Hệ thống được thiết kế để xử lý hàng triệu giao dịch mỗi ngày với độ ổn định cao. Kết quả thực nghiệm chỉ ra rằng mô hình đề xuất đã giúp tăng tỷ lệ chuyển đổi (Conversion Rate) thêm 18,7% và giảm tỷ lệ thoát trang (Bounce Rate) xuống còn 22% so với các phương pháp truyền thống.
1	Đây là một bước tiến quan trọng trong việc cá nhân hóa trải nghiệm khách hàng dựa trên dữ liệu thực tế. Trong kỷ nguyên kinh tế số, việc hiểu rõ nhu cầu của khách hàng là yếu tố sống còn đối với mọi doanh nghiệp bán lẻ trực tuyến. Tuy nhiên, dữ liệu khách hàng thường phân tán, không cấu trúc và có khối lượng cực lớn, gây khó khăn cho các hệ thống máy chủ vật lý thông thường. Điện toán đám mây cung cấp các dịch vụ lưu trữ như S3 hoặc BigQuery để quản lý tập trung nguồn dữ liệu này. Thách thức đặt ra là làm thế nào để xây dựng một thuật toán có khả năng phân loại khách hàng một cách thông minh trong thời gian thực.
1	"KNN xuất hiện như một giải pháp lý tưởng nhờ khả năng tìm kiếm các ""láng giềng"" có hành vi tương đồng, từ đó đưa ra các gợi ý sản phẩm mang tính cá nhân hóa cao, giúp doanh nghiệp tối ưu hóa doanh thu trên mỗi lượt truy cập. Để quản lý sự phức tạp của hệ thống khuyến nghị, chúng tôi áp dụng triệt để tư duy lập trình hướng đối tượng trong quá trình phát triển mã nguồn. Các module như UserSession, ProductCatalog, và RecommendationEngine được xây dựng dưới dạng các lớp độc lập, tương tác với nhau qua các giao diện (Interfaces) xác định. Tính đa hình (Polymorphism) cho phép hệ thống áp dụng các chiến lược phân loại khác nhau tùy thuộc vào loại sản phẩm hoặc phân khúc khách hàng mà không cần thay đổi logic cốt lõi."
1	Cách tiếp cận này giúp giảm thiểu 40% thời gian phát triển các tính năng mới và tăng khả năng tái sử dụng mã nguồn trong các dự án tương tự. OOP cũng giúp việc tích hợp các bộ thư viện học máy hiện đại trên đám mây trở nên đơn giản và mạch lạc hơn rất nhiều. Trong giai đoạn thử nghiệm kéo dài 6 tháng trên một nền tảng thương mại điện tử giả lập với 50.000 người dùng hoạt động, thuật toán KNN đã chứng minh được hiệu quả vượt trội. Với giá trị K tối ưu được xác định qua kiểm thử là 10, độ chính xác trong việc dự đoán sản phẩm khách hàng sẽ mua đạt mức 82%.
1	Về mặt hạ tầng, việc sử dụng điện toán đám mây giúp hệ thống tự động mở rộng (Auto-scaling) tài nguyên trong các giờ cao điểm, duy trì độ trễ dưới 200ms ngay cả khi lưu lượng truy cập tăng đột ngột 300%. Số liệu thống kê cho thấy giá trị đơn hàng trung bình (AOV) của nhóm khách hàng nhận được khuyến nghị từ hệ thống KNN cao hơn 12,5% so với nhóm đối chứng không sử dụng hệ thống, khẳng định giá trị kinh tế mà mô hình mang lại. Việc kết hợp điện toán đám mây, OOP và KNN tạo ra một hệ sinh thái mạnh mẽ cho các ứng dụng thương mại điện tử hiện đại.
1	Nghiên cứu đã chứng minh rằng sự kết hợp giữa kiến trúc phần mềm chuẩn mực và thuật toán học máy phù hợp có thể giải quyết tốt các bài toán kinh doanh phức tạp. Trong giai đoạn tiếp theo, chúng tôi đề xuất nghiên cứu việc áp dụng các kỹ thuật giảm chiều dữ liệu (như PCA) trước khi đưa vào KNN để tăng tốc độ tính toán hơn nữa. Đồng thời, việc bảo mật dữ liệu khách hàng trên đám mây theo các tiêu chuẩn quốc tế như GDPR cũng cần được chú trọng để đảm bảo sự phát triển bền vững của hệ thống trong môi trường kinh doanh toàn cầu đầy biến động hiện nay.
0	Nhận dạng khuôn mặt là một bài toán phổ biến đang đặt ra hiện nay. Tồn tại nhiều phương pháp và hướng tiếp cận đối với bài toán nhận dạng khuôn mặt: Tiếp cận theo đặc trưng toàn cục (sử dụng các đặc điểm toàn cục của khuôn mặt) và tiếp cận theo đặc trưng cục bộ (sử dụng các đặc điểm cục bộ của khuôn mặt). Tuy nhiên hiệu quả của các phương pháp nhận dạng này vẫn còn hạn chế và độ chính xác chưa cao khi dữ liệu đầu vào bị ảnh hưởng bởi các yếu tố khách quan của môi trường (độ sáng, hướng nghiêng, kích thước, ...).
0	Do đó, nhóm tác giả đề xuất xây dựng hệ thống nhận dạng khuôn mặt dựa trên thuật toán FaceNet và sử dụng Multi-task Cascaded Convolutional Networks phát hiện và xác định khuôn mặt cho phép nâng cao hiệu quả nhận dạng. Cùng với sự phát triển của xã hội, vấn đề an ninh bảo mật là một điều tất yếu hiện nay. Các hệ thống nhận dạng con người được ra đời với độ tin cậy ngày càng cao. Có thể kể đến như nhận dạng hình dáng, nhận dạng giọng nói, nhận dạng khuôn mặt, ... Trong đó, phổ biến và được ứng dụng nhiều hơn cả là bài toán nhận dạng khuôn mặt.
0	Hiện nay, tồn tại một số hướng tiếp cận đối với bài toán nhận dạng khuôn mặt: Tiếp cận theo đặc trưng toàn cục và tiếp cận theo đặc trưng cục bộ. Đối với phương pháp tiếp cận theo hướng toàn cục thì các đặc trưng chung của khuôn mặt sẽ được sử dụng để nhận dạng như: Màu sắc, hình dạng, các nét chính của khuôn mặt... Phương pháp được sử dụng phổ biến trong hướng tiếp cận này là Eigengaces-PCA và Fisherfaces. Phương pháp Eigenfaces sử dụng phép phân tích thành phần (Principal Components Analysis – PCA) cho phép giảm số chiều dữ liệu.
0	Với phương pháp này, sau quá trình chuẩn hoá, các đặc trưng toàn cục của khuôn mặt sẽ được biểu diễn thành các véc-tơ riêng. Tập hợp các véc-tơ này tạo thành không gian mới với số chiều dữ liệu giảm xuống mà các đặc trưng quan trọng của khuôn mặt vẫn được giữ lại trong quá trình nhận dạng. Trong không gian véc-tơ này, mỗi véc-tơ được gọi là Eigenfaces. Do PCA là thuật toán học không có giám sát nên có hạn chế trong trường hợp tập dữ liệu huấn luyện có nhiều hơn một mẫu cho mỗi lớp.
0	Phương pháp Fisherfaces là phương pháp phân tích tuyến tính khác biệt (Linear Discriminant Analysis – LDA) đã được sử dụng nhằm khai thác tốt hơn các thông 1 The University of Danang - University of Science and Technology (Mai Van Ha, Nguyen The Xuan Ly) tin về lớp nói trên [2]. LDA cho phép nhận diện khuôn mặt dựa trên một phép chiếu tuyến tính từ không gian hình ảnh vào một chiều không gian thấp hơn bằng cách tối đa giữa các lớp tán xạ và giảm thiểu phân tán trong lớp. Có thể thấy rằng, phương pháp LDA áp dụng các tiêu chuẩn phân biệt tuyến tính cho phép tối đa hóa tỷ lệ yếu tố quyết định của lớp giữa ma trận tán xạ của các lớp do đó cho phép khắc phục những nhược điểm của phương pháp Eigengaces-PCA.
0	Hướng tiếp cận nhận dạng dựa trên các đặc trưng cục bộ của khuôn mặt như: Các chi tiết như mắt, mũi, lông mày, điểm ảnh, ... Hướng tiếp cận này sử dụng hai phương pháp phổ biến là phương pháp lấy mẫu nhị phân cục bộ (Local Binary Pattern – LBP) và phương pháp biến đổi sóng nhỏ Gabor (Gabor wavelets) [3]. Trong LBP, bức ảnh sẽ được chia thành các vùng bằng nhau, tại mỗi vùng này có thể tính được 1 LBP histogram và dựa vào đó xác định được thông tin về vị trí mắt, mũi, miệng trên khuôn mặt.
0	Các thông tin này áp dụng trọng số lên histogram của các vùng chứa các đặc trưng quan trọng cho phép phân biệt giữa các khuôn mặt. Với phương pháp Gabor wavelets thì dữ liệu được chia thành các thành phần với tần số khác nhau và xem xét từng thành phần với độ phân giải thích hợp [4]. Với phương pháp này các ảnh khuôn mặt sẽ được trích chọn đặc trưng dựa vào biến đổi Gabor wavelet. Một tập các tần số và hướng của các điểm đặc trưng xác định bởi mạng wavelet sẽ là thông tin đặc trưng để biểu diễn ảnh.
0	Tuy nhiên, việc xây dựng hệ thống nhận dạng khuôn mặt với hiệu suất và độ chính xác cao là một thách thức rất lớn vì những yếu tố khách quan như môi trường, ánh sáng, độ nghiêng của khuôn mặt, độ tuổi, cảm xúc hay như việc bị che khuất. Vì vậy, việc xây dựng một hệ thống nhận dạng khuôn mặt hoạt động tốt dù khuôn mặt bị che lấp một phần hay bị ảnh hưởng bởi các yếu tố của môi trường xung quanh là cần thiết. Do đó, nhóm tác giả đề xuất sử dụng thuật toán FaceNet để nhận dạng khuôn mặt và ứng dụng Multi-task Cascaded Convolutional Networks (MTCNN) đối với việc phát hiện khuôn mặt trong bức ảnh.
0	Nhận dạng khuôn mặt người là một chủ đề nghiên cứu thuộc lĩnh vực thị giác máy được phát triển từ nhưng năm 90 của thế kỷ trước. Hiện nay, lĩnh vực nhận dạng được đẩy mạnh phát triển và nhận được sự quan tâm của nhiều nhà nghiên cứu từ nhiều lĩnh vực nghiên cứu khác nhau, đặc biệt là nhận dạng khuôn mặt. Bài toán nhận dạng khuôn mặt hướng tiếp cận cũng tương tự như hệ thống thị giác của con người khi cần nhận dạng một ai đó khi nhìn vào 1 bức ảnh. Hoạt động của hệ thống nhận dạng khuôn mặt có được triển khai chi tiết như sau: Bước 1: Phát hiện và xác định khuôn mặt trong bức ảnh.
0	Bước 2: Chuẩn hoá và trích chọn đặc trưng khuôn mặt đã được phát hiện trong bước 1. Bước 3: Tiến hành so sánh và nhận dạng các đặc trưng ở bước 2 với tập dữ liệu huấn luyện đã có để đưa ra kết quả kết luận nhận dạng. Vấn đề đầu tiên của nhận dạng khuôn mặt là phải phát hiện và xác định được vị trí khuôn mặt trong bức ảnh. Trong bài báo này nhóm tác giả đề xuất sử dụng MTCNN để phát hiện và xác định khuôn mặt người trong bức ảnh [5]. Về mặt cấu trúc, MTCNN bao gồm 3 mạng CNN (Convolutional Neural Networks) xếp chồng và đồng thời hoạt động khi phát hiện và xác định khuôn mặt.
0	Mỗi mạng CNN trong MTCNN có cấu trúc và vai trò khác nhau trong việc phát hiện khuôn mặt. Kết quả dữ liệu đầu ra của MTCNN là véc-tơ đặc trưng biểu diễn cho vị trí khuôn mặt được xác định trong bức ảnh (mắt, mũi, miệng, ...). MTCNN hoạt động theo 3 bước với 3 mạng nơ-ron riêng cho mỗi bước (P-Net, R-Net và O-Net). Khi sử dụng, MTCNN sẽ cho phép tạo ra nhiều bản sao của hình ảnh đầu vào, với các kích thước khác nhau để làm dữ liệu đầu vào. Tầng 1: Sử dụng mạng CNN, gọi là Mạng đề xuất (P-Net), để thu được các cửa sổ chứa khuôn mặt và các vectơ hồi quy trong các cửa sổ đó. Tiếp theo, các cửa sổ chứa khuôn mặt được hiệu chuẩn dựa trên các vector hồi quy.
0	Cuối cùng, những cửa sổ xếp chồng nhau tại một vùng được hợp nhất thành một cửa sổ. Kết quả đầu ra là các cửa sổ có thể chứa khuôn mặt. Mạng P-Net sử dụng kiến trúc CNN gồm 3 lớp tích chập và 1 lớp co. Đầu vào cửa sổ trượt với kích thước 12x12x3 (với 3 tương ứng với 3 màu: Đỏ, xanh lục, xanh lam trong hệ màu RGB thông thường). Kết quả của P-Net gồm 3 cụm như sau: Cụm thứ nhất có 2 bộ lọc kích thước 1x1 nhận dạng khuôn mặt; Cụm thứ hai có 4 bộ lọc kích thước 1x1 đóng khung 4 vị trí hộp giới hạn;
0	Cụm thứ ba có 10 bộ lọc kích thước 1x1 đóng khung 10 vị trí khuôn mặt. Tầng 2: Tất cả các cửa sổ chứa khuôn mặt từ tầng 1 sẽ được sàng lọc bằng cách đưa vào một CNN khác gọi là Mạng lọc (R-Net) để tiếp tục loại bỏ một số lượng lớn các cửa sổ không chứa khuôn mặt. Sau đó, thực hiện hiệu chuẩn với véc-tơ hồi quy và thực hiện hợp nhất các cửa sổ xếp chồng nhau tại một vùng. Trong bước R-Net sử dụng kiến trúc CNN gồm: 3 lớp tích chập, 2 lớp co và 1 lớp kết nối đầy đủ. Đầu vào cửa sổ trượt với kích thước 24x24x3 (3 tương ứng với 3 màu: Đỏ, xanh lục, xanh lam trong hệ màu RGB thông thường).
0	Kết quả của R-Net phân được 3 cụm: Cụm thứ nhất có 2 lớp nhận dạng khuôn mặt; Cụm thứ hai có 4 lớp đánh dấu vị trí hộp giới hạn; Cụm thứ ba có 10 lớp vị trí khuôn mặt. Tầng 3: Tầng này tương tự như tầng 2, sử dụng CNN chi tiết nhất được gọi là Mạng đầu ra (O-Net) để lọc kết quả một lần nữa và đánh dấu vị trí năm điểm chính trên khuôn mặt. Mạng O-Net sử dụng CNN gồm: 4 lớp tích chập, 2 lớp co, 1 lớp kết nối đầy đủ. Đầu vào cửa sổ trượt có kích thước 48x48x3 (trong đó số 3 tương ứng với 3 màu: Đỏ, xanh lục, xanh lam trong hệ màu RGB thông thường).
0	Kết quả của O-Net phân được 3 cụm: Cụm thứ nhất có 2 lớp nhận dạng khuôn mặt; Cụm thứ hai có 4 lớp đánh dấu vị trí hộp giới hạn; Cụm thứ ba có 10 lớp vị trí khuôn mặt. Ứng dụng MTCNN để phát hiện khuôn mặt cho phép xác định khuôn mặt trong bức ảnh tốt hơn so với các phương pháp khác. Các thuật toán nhận dạng khuôn mặt trước đây chủ yếu biểu diễn khuôn mặt bằng một véc-tơ đặc trưng và thông qua một lớp bottleneck để giảm số chiều dữ liệu. Tuy nhiên, số chiều dữ liệu của véc-tơ đặc trưng thường tương đối lớn nên sẽ làm cho tốc độ nhận dạng giảm xuống.
0	Vì vậy, thuật toán PCA thường được áp dụng để giảm số chiều dữ liệu của véc-tơ đặc trưng và tăng tốc độ nhận dạng. Đồng thời, trong các phương pháp nhận dạng thì hàm loss function thường chỉ xác định khoảng cách giữa 2 bức ảnh (đại lượng mô tả sự giống nhau của hai bức ảnh). Như vậy, xuất hiện vấn đề là trong một lần huấn luyện chỉ có thể học được một kết quả: Hoặc là giống nhau nếu hai bức ảnh cùng thuộc về một lớp, hoặc là khác nhau nếu hai bức ảnh thuộc về hai lớp riêng.
0	FaceNet là một thuật toán hỗ trợ cho việc nhận dạng và phân cụm khuôn mặt cho phép giải quyết các hạn chế nêu trên [6]. FaceNet sử dụng một mạng CNN và cho phép giảm số chiều dữ liệu của véc-tơ đặc trưng (thường sử dụng là 128 chiều). Do đó, cho phép tăng tốc độ huấn luyện và xử lý mà độ chính xác vẫn được đảm bảo. Đối với thuật toán FaceNet, hàm loss function sử dụng hàm triplet loss cho phép khắc phục hạn chế của các phương pháp nhận dạng trước đây, quá trình huấn luyện cho phép học được đồng thời: Sự giống nhau giữa hai bức ảnh (nếu hai bức ảnh cùng một lớp) và sự khác nhau giữa hai bức ảnh (nếu chúng không cùng một lớp).
0	FaceNet chính là một dạng siam network thường biểu diễn véc-tơ đặc trưng của các bức ảnh trong một không gian Euclidean n chiều (thường là 128 chiều). Việc biểu diễn thường tuân theo quy tắc: Nếu khoảng cách giữa các véc-tơ embedding càng nhỏ, thì mức độ tương đồng giữa chúng càng lớn và ngược lại. Tập hợp véc-tơ này sẽ là dữ liệu đầu vào cho hàm loss function để đánh giá chỉ số khoảng cách giữa các véc-tơ. FaceNet sử dụng CNN bằng cách dùng hàm f(x) và nhúng hình ảnh x vào không gian Euclidean d chiều sao cho khoảng cách giữa các hình ảnh của 1 người không phụ thuộc vào điều kiện bên ngoài, khoảng cách giữa các khuôn mặt giống nhau (của cùng một người là nhỏ) trong khi khoảng cách giữa các ảnh khác nhau sẽ có khoảng cách lớn.
0	Khi huấn luyện mô hình siam network với triplet loss cần phải xác định trước cặp ảnh (Xa, Xp) thuộc về cùng một người. Ảnh Xn là ảnh khác với ảnh gốc của người đó thường sẽ được lựa chọn ngẫu nhiên từ các bức ảnh thuộc các lớp còn lại. Do đó, tập hợp ảnh Xn thường được thu thập nhiều hơn 1 bức ảnh/1 người để có thể chuẩn bị được tập dữ liệu huấn luyện. Nếu 1 người chỉ có 1 ảnh thì có thể đưa những tập dữ liệu như vậy làm bộ ảnh Xn khi huấn luyện.
0	Như đã nêu trên có thể thấy, khi sử dụng triplet loss vào các mô hình CNN có thể tạo ra các véc-tơ đặc trưng tốt nhất cho mỗi một bức ảnh. Các véc-tơ đặc trưng này sẽ cho phép phân biệt rõ các ảnh Negative (ảnh khác với ảnh gốc) rất giống ảnh Positive (ảnh gần giống với ảnh gốc). Hơn nữa, khoảng cách giữa các bức ảnh thuộc cùng một lớp sẽ trở nên gần nhau hơn trong không gian chiếu Euclidean. Tuy vậy, việc sử dụng bộ ba như trên sẽ khiến cho quá trình hội tụ chậm.
0	Do đó, cần chọn bộ ba thích hợp trong quá trình huấn luyện để cải thiện được hiệu suất và độ chính xác của mô hình. Để khắc phục được việc hội tụ chậm, thường sẽ chọn bộ ba sai số sao cho khoảng cách giữa ảnh gốc và ảnh gần với ảnh gốc (ảnh của cùng 1 người) là lớn nhất và khoảng cách giữa ảnh gốc và ảnh của người khác là gần nhất: Lúc này ta sẽ huấn luyện làm sao cho biểu thức (3) trở về biểu thức (2). Việc huấn luyện sẽ giúp khoảng cách giữa hai ảnh của cùng 1 người là nhỏ nhất và ngược lại ảnh của 2 người sẽ có khoảng cách là lớn nhất.
0	Việc lựa chọn bộ ba sai số sẽ ảnh hưởng đến hiệu quả của mô hình, nếu giá trị bộ ba sai số được xác định tốt thì quá trình hội tụ khi huấn luyện sẽ nhanh hơn và kết quả sẽ cho độ chính xác cao hơn. Việc lựa chọn ngẫu nhiên bộ ba sai số có thể dẫn tới mô hình huấn luyện không thể hội tụ. Nhóm tác giả tiến hành xây dựng hệ thống nhận dạng trên cơ sở ứng dụng MTCCN và FaceNet sử dụng mạng nơ-ron tích chập và thuật toán softmax. Hệ thống gồm 02 phân quyền chính là Quản trị viên và Người dùng.
0	Phân quyền Người dùng có thể thực hiện các chức năng: Đăng nhập, quản lý tập hình ảnh huấn luyện, huấn luyện mô hình nhận dạng và nhận dạng khuôn mặt (nhận dạng thông qua hình ảnh hoặc nhận dạng trực tiếp: Sử dụng camera). Phân quyền quản trị thừa kế từ phân quyền người dùng và có thêm chức năng quản lý tài khoản người dùng. Bên cạnh đó, backend của hệ thống được xây dựng dựa trên API (sử dụng thư viện flask), sử dụng thư viện imgur để lưu trữ hình ảnh, Tensorflow để thực hiện các phép tính toán và sử dụng colaboratory của google để huấn luyện dữ liệu.
0	Để huấn luyện mô hình, nhóm tác giả sử dụng tập dữ liệu khuôn mặt người Việt từ Google Image (gồm 23105 khuôn mặt của 1020 người). Đặc điểm của bộ dữ liệu này các ảnh của một người được thu thập tại các thời kỳ, hoàn cảnh khác nhau. Nhóm tác giả lấy ảnh của 980 người để làm tập dữ liệu huấn luyện mô hình. Mô hình được huấn luyện với tham số như Hình 10 và thời gian huấn luyện mô hình rơi vào khoảng 60 phút. Tập dữ liệu để kiểm tra bao gồm 40 người với 874 bức ảnh.
0	Nhóm tác giả sử dụng 574 ảnh để huấn luyện và 300 ảnh để kiểm thử (tất cả các ảnh được điều chỉnh về kích thước 160x160) cho cả 2 thuật toán: Eigengaces-PCA (sử dụng haar cascade để phát hiện khuôn mặt) và FaceNet (sử dụng MTCNN để phát hiện khuôn mặt). Kết quả thực hiện nhận dạng được thể hiện ở Bảng 1. Dữ liệu thực tế từ Bảng 1 cho thấy, thời gian nhận dạng trung bình của phương pháp Eigengaces-PCA nhanh hơn với phương pháp đề xuất. Tuy nhiên, phương pháp nhận dạng khuôn mặt sử dụng thuật toán FaceNet và MTCNN để phát hiện khuôn mặt cho kết quả nhận dạng chính xác cao hơn.
0	Sự cần thiết của việc phát hiện, phân loại sớm tổn thương gan và việc nghiên cứu ứng dụng mô hình học sâu vào xử lý ảnh y khoa. Mục tiêu nghiên cứu: Thu thập dữ liệu, xây dựng, huấn luyện mô hình Faster R-CNN để phát hiện, phân loại các tổn thương khu trú thường gặp ở gan; Kiểm thử, đánh giá hiệu quả mô hình theo tiêu chí về thời gian, độ chính xác. Đối tượng và phương pháp nghiên cứu: Bộ dữ liệu ảnh chụp cắt lớp vi tính tiêm thuốc cản quang vùng bụng có tổn thương gồm nang gan, u mạch máu, ung thư tế bào gan nguyên phát; Áp dụng mô hình Faster R-CNN để phát hiện, phân loại tổn thương.
0	Kết quả: Bộ dữ liệu thu thập tại Bệnh viện Trường Đại học Y Dược Cần Thơ gồm 51 người bệnh có tổn thương khu trú thường gặp ở gan, với 2828 ảnh, 2836 vùng tổn thương được xác định bởi bác sĩ chẩn đoán hình ảnh. Trong đó, 11 người bệnh thuộc nhóm nang gan (440 vùng), 18 người bệnh thuộc nhóm u mạch máu (648 vùng), 21 người bệnh thuộc nhóm ung thư tế bào gan nguyên phát (1748 vùng) và 01 người bệnh có cả nang và u mạch máu; Mô hình Faster R-CNN cho kết quả độ chính xác mAP là 94%, thời gian huấn luyện 583 phút và thời gian xử lý 0,13 giây.
0	Kết luận: Bộ dữ liệu đã thu thập là nền tảng cho các nghiên cứu tiếp theo; Mô hình Faster R-CNN có thời gian huấn luyện ngắn, thời gian xử lý nhanh, độ chính xác cao, phù hợp và có thể áp dụng để triển khai các ứng dụng thực tế. Việc phát hiện và phân loại sớm tổn thương gan nhằm đưa ra giải pháp điều trị đúng đắn là vô cùng cần thiết. Để đánh giá chi tiết các tổn thương gan thì kỹ thuật chụp cắt lớp vi tính có tiêm thuốc cản quang là một lựa chọn hoàn hảo.
0	Tuy nhiên, kết quả chẩn đoán phụ thuộc nhiều vào tính tỉ mỉ, nhãn quan và kinh nghiệm của bác sĩ. Xuất phát từ thực tiễn, nghiên cứu ứng dụng mô hình học sâu để phát hiện và phân loại các tổn thương khu trú thường gặp ở gan trên ảnh chụp cắt lớp vi tính cần được triển khai thực hiện. Các đề tài ứng dụng máy học, đặc biệt là kỹ thuật học sâu để xử lý ảnh chụp cắt lớp vi tính có tiêm thuốc cản quang liên quan đến các tổn thương gan đã được nhiều nhà khoa học tham gia nghiên cứu.
0	Năm 2017, nhóm tác giả Changjian Sun đề xuất mô hình mạng học sâu kết nối đầy đủ đa kênh để vẽ đường biên bao quanh tổn thương [9]. Năm 2018, nhóm tác giả Sang-gil Lee sử dụng mô hình Grouped Single Shot MultiBox Detector để phát hiện các tổn thương gan [5]. Năm 2018, nhóm tác giả Dong Liang đề xuất mô hình Residual Convolutional Neural Networks để trích xuất đặc trưng và sử dụng thuật toán máy học véc tơ hỗ trợ để phân loại các tổn thương [6]. Đầu năm 2021, nhóm tác giả Jiarong Zhou sử dụng ba mô hình Faster R-CNN với mạng rút trích đặc trưng FPN riêng biệt để phân loại các tổn thương gan [10].
0	Các đề tài trên đều đạt được các kết quả khả quan, tuy nhiên vẫn chưa thể thực hiện được việc phân loại tổn thương hoặc chưa tận dụng được cả bốn thì chụp làm dữ liệu đầu vào. Từ những hạn chế trên, đề tài đề xuất giải pháp xây dựng bốn mô hình học sâu tương ứng cho bốn thì chụp, sử dụng mô hình học sâu hai bước là Faster R-CNN [8] với mạng rút trích đặc trưng ResNet-101 [4], áp dụng vào bộ dữ liệu ảnh cắt lớp vi tính có tiêm thuốc cản quang vùng bụng thu thập tại Bệnh viện Trường Đại học Y Dược Cần Thơ
0	để phát hiện và phân lớp ba loại tổn thương khu trú thường gặp ở gan bao gồm nang (Cyst), u mạch máu (Hemangioma) và ung thư tế bào gan nguyên phát (HCC), với các mục tiêu: (1) Thu thập tập dữ liệu, xây dựng và huấn luyện mô hình học sâu Faster R-CNN để phát hiện và phân loại các tổn thương khu trú thường gặp ở gan trên ảnh chụp cắt lớp vi tính. (2) Kiểm thử và đánh giá hiệu quả của mô hình Faster R-CNN theo các tiêu chí về thời gian và độ chính xác.
0	Đề tài tập trung vào việc phát hiện và phân loại các tổn thương khu trú thường gặp ở gan. Các hình ảnh chụp cắt lớp vi tính có tiêm thuốc cản quang vùng bụng liên quan đến ba loại tổn thương gồm nang gan, u mạch máu và ung thư tế bào gan nguyên phát được thu thập tại Bệnh viện Trường Đại học Y Dược Cần Thơ làm dữ liệu cho quá trình huấn luyện và kiểm thử trên mô hình học sâu là Faster R-CNN với mạng rút trích đặc trưng ResNet-101. Bài toán có đầu vào là ảnh chụp cắt lớp vi tính có tiêm thuốc cản quang vùng bụng tương ứng với các thì chưa tiêm thuốc cản quang, thì động mạch, thì tĩnh mạch và thì muộn.
0	Sau quá trình xử lý, kết quả đầu ra là các ảnh tương ứng trên đó đã thực hiện khoanh vùng và đưa ra dự đoán phân lớp thuộc một trong ba loại tổn thương khu trú thường gặp ở gan gồm nang, u mạch máu và ung thư tế bào gan nguyên phát. Đây là bài toán thuộc về lĩnh vực nhận dạng đối tượng (Object Detection [1]) trên ảnh. Để giải quyết bài toán thì các mô hình học sâu như R-CNN, Fast R-CNN, Faster R-CNN, R-FCN, YOLO, SSD... đều có tính khả thi và có thể mang lại hiệu quả cao.
0	Với đặc thù bài toán chuyên ngành y khoa cần độ chính xác cao, đề tài lựa chọn mô hình học sâu hai bước là Faster R-CNN với mạng trích xuất đặc trưng ResNet-101, áp dụng kỹ thuật làm giàu dữ liệu (Transfer learning) với bộ trọng số đã được huấn luyện trên tập dữ liệu lớn COCO Dataset [7] giúp giải quyết được vấn đề tập dữ liệu nhỏ đồng thời rút ngắn thời gian trong việc huấn luyện các mô hình. Sau đó sử dụng bộ trọng số đã được học tiếp tục huấn luyện và kiểm thử trên tập dữ liệu đã thu thập. Từ đó tiến hành đánh giá, so sánh hiệu quả về độ chính xác và thời gian của mô hình.
0	Quy trình đề xuất giải quyết bài toán gồm ba giai đoạn chính, đó là giai đoạn xây dựng bộ dữ liệu, giai đoạn huấn luyện và cuối cùng là giai đoạn kiểm thử đánh giá. Chi tiết được mô tả như sau: Giai đoạn xây dựng bộ dữ liệu: Tập dữ liệu ảnh DICOM chụp cắt lớp vi tính có tiêm thuốc cản quang bụng được thu thập tại Bệnh viện Trường Đại học Y Dược Cần Thơ. Mỗi tập tin gồm bốn tập ảnh tương ứng với thì chưa tiêm thuốc cản quang, thì động mạch, thì tĩnh mạch và thì muộn.
0	Giai đoạn tiền xử lý dữ liệu tiến hành chuyển đổi ảnh DICOM về các ảnh định dạng JPG có kích thước 512x512 nhằm tạo dữ liệu đầu vào phù hợp cho mô hình. Dữ liệu liên quan đến thông tin người bệnh đã được xóa hoàn toàn trong quá trình tiền xử lý để đảm bảo thông tin cá nhân được bảo mật. Công cụ sử dụng để đọc ảnh DICOM và chuyển đổi định dạng ảnh sang JPG là RadiAnt DICOM Viewer. Giai đoạn khoanh vùng, đánh nhãn phân loại tổn thương được hỗ trợ từ chuyên gia là bác sĩ chẩn đoán hình ảnh.
0	Quá trình khoanh vùng và đánh nhãn phân loại trên tập dữ liệu ảnh JPG được thực hiện bằng công cụ LabelImg. Các vùng tổn thương được gán nhãn thành một trong ba lớp là nang (nhãn là NAN), u mạch máu (nhãn HEM), ung thư tế bào gan nguyên phát (nhãn HCC). Tập dữ liệu gồm các hình ảnh thu được dưới định dạng JPG đã qua các bước tiền xử lý và khoanh vùng, đánh nhãn được chia thành tập dữ liệu dùng cho huấn luyện và tập dữ liệu dùng cho kiểm thử. Các tập dữ liệu này được sử dụng làm đầu vào cho các mô hình học sâu. Giai đoạn huấn luyện mô hình: Tiến hành xây dựng bốn mô hình tương ứng cho bốn thì chụp.
0	Giai đoạn huấn luyện được thực hiện trên tập dữ liệu huấn luyện. Kết quả thu được là các mô hình đã được huấn luyện. Một số tham số huấn luyện chính bao gồm: Sử dụng phương pháp làm giàu dữ liệu (Transfer learning) kế thừa bộ trọng số đã được huấn luyện trên tập dữ liệu lớn COCO. Số lượng phân lớp (num classes) trong đề tài là ba phân lớp, tương ứng với ba loại tổn thương khu trú thường gặp ở gan gồm nang (ký hiệu nhãn là NAN), u mạch máu (nhãn HEM) và ung thư tế bào gan nguyên phát (nhãn HCC).
0	Số bước học (num steps) là 100.000, con số này được lựa chọn sau quá trình theo dõi biểu đồ chỉ số Loss giảm dần đến khi không còn giảm được nữa và có dấu hiệu ổn định, khi đó mô hình huấn luyện đã đạt được kết quả tốt. Tham số mAP@IoU được dùng để đánh giá khả năng phân vùng bằng cách đo độ chồng lắp phù hợp giữa vùng dự đoán và vùng nhãn dữ liệu gốc. Nếu độ chồng lắp lớn hơn hoặc bằng 0,5 được xem là dự đoán chính xác và ngược lại nếu nhỏ hơn 0,5 thì dự đoán được xem là không chính xác. Giai đoạn kiểm thử: Sau khi thu được các mô hình đã được huấn luyện ở giai đoạn huấn luyện, giai đoạn kiểm thử được thực hiện cho bốn thì tương ứng với tập dữ liệu kiểm thử.
0	Dữ liệu cũng được chia làm bốn tập tương ứng với từng thì. Kết quả kiểm thử sẽ được ghi nhận với các thông số về độ chính xác và thời gian huấn luyện và xử lý. Môi trường cài đặt Tensorflow [3] là một nền tảng nguồn mở được phát triển bởi Google dành cho việc nghiên cứu về máy học và học sâu. Tensorflow có một hệ sinh thái toàn diện, linh hoạt gồm các công cụ, thư viện và tài nguyên cộng đồng cho phép các nhà nghiên cứu thúc đẩy công nghệ tiên tiến trong máy học và các nhà phát triển dễ dàng xây dựng và triển khai các ứng dụng máy học.
0	Google Colab (Colaboratory) [2] là một dịch vụ đám mây miễn phí, hỗ trợ cung cấp GPU và TPU là các thành phần xử lý đồ họa hỗ trợ cho nhu cầu tính toán lớn. Đây là một công cụ lý tưởng cho nghiên cứu và triển khai ứng dụng máy học, học sâu vì đã được cài đặt sẵn những thư viện rất phổ biến như PyTorch, TensorFlow, Keras và OpenCV. Các mô hình được xây dựng trên môi trường trực tuyến Google Colab với cấu hình mạnh phù hợp cho nhu cầu tính toán lớn, đi kèm nền tảng TensorFlow, ngôn ngữ Python.
0	Tập dữ liệu thu thập tại Bệnh viện Trường Đại học Y Dược Cần Thơ gồm hình ảnh chụp cắt lớp vi tính vùng bụng có tiêm thuốc cản quang của 51 người bệnh với chẩn đoán có tổn thương khu trú thường gặp ở gan. Số lượng người bệnh và các loại tổn thương được mô tả ở bảng bên dưới. Nhận xét: Số lượng người bệnh có tổn thương HCC chiếm số lượng cao nhất là 21 và có 01 người bệnh vừa có tổn thương nang gan vừa có u mạch máu. Quá trình tiền xử lý dữ liệu chuyển đổi định dạng từ DICOM sang JPG được thực hiện bằng công cụ Radiant Viewer.
0	Sau đó quá trình khoanh vùng đánh nhãn được thực hiện bằng công cụ LabelImg với sự hỗ trợ của chuyên gia là bác sĩ chẩn đoán hình ảnh. Kết quả số lượng vùng tổn thương tương ứng với từng phân loại và tập dữ liệu huấn luyện, kiểm thử được mô tả như bảng bên dưới. Nhận xét: Tổng số lượng vùng tổn thương là 2836 vùng. Trong đó, 2008 vùng tổn thương nằm ở tập dữ liệu huấn luyện và 828 vùng còn lại dùng cho tập kiểm thử. Số lượng vùng tổn thương nang gan là thấp nhất với 440 vùng và cao nhất là HCC với 1748 vùng.
0	Tương ứng bốn thì chụp, bốn mô hình Faster R-CNN với mạng rút trích đặc trưng ResNet-101 đã được xây dựng trên môi trường Google Colab, hệ điều hành Ubuntu với cấu hình với bộ vi xử lý, RAM, bộ xử lý đồ họa GPU mạnh hỗ trợ rất nhiều cho nhu cầu tính toán lớn với bài toán xử lý ảnh đầu vào kích thước 512x512, đi kèm nền tảng TensorFlow GPU 1.15 và ngôn ngữ Python 3.7. Tập dữ liệu huấn luyện và kiểm thử được sử dụng làm đầu vào tương ứng cho các mô hình đã xây dựng. Chi tiết thông số cấu hình hệ thống được mô tả như bảng bên dưới.
0	Nhận xét: Môi trường thực nghiệm trực tuyến với cấu hình mạnh hỗ trợ hiệu quả cho quá trình huấn luyện và kiểm thử với dữ liệu đầu vào ảnh kích thước 512x512. Quá trình huấn luyện với 100.000 bước học và kiểm thử mô hình Faster R-CNN trên tập dữ liệu đã thu thập, đề tài thu được một số kết quả thực nghiệm về độ đo giá trị mất mát (chỉ số Loss), thời gian huấn luyện, thời gian xử lý, kết quả phân loại tổn thương với độ đo chính xác được sử dụng là AP và mAP.
0	Nhận xét: Chỉ số Loss của mô hình Faster R-CNN với mạng rút trích đặc trưng ResNet-101 là 0,024122, chỉ số Loss thấp cho thấy mô hình rất phù hợp và có khả năng rút trích đặc trưng hiệu quả, từ đó cho độ chính xác mAP trung bình cao là 94%. Thời gian huấn luyện trung bình ngắn với 583 phút, tốc độ xử lý nhận dạng phân loại tổn thương của mô hình nhanh với 0,13 giây. Do số lượng tổn thương gan ngày càng tăng và việc chẩn đoán các tổn thương này bằng cách sử dụng kỹ thuật chụp cắt lớp vi tính có tiêm thuốc cản quang vùng bụng phụ thuộc nhiều vào tính tỉ mỉ và kinh nghiệm của bác sĩ,
0	việc ứng dụng kỹ thuật học sâu vào lĩnh vực xử lý ảnh y khoa hỗ trợ chẩn đoán ngày càng trở nên quan trọng nhằm tăng cường sự hiệu quả và giảm thiểu thời gian công sức. Trong nghiên cứu này, chúng tôi đã xây dựng bốn mô hình học sâu tương ứng cho cả bốn thì chụp cắt lớp vi tính có tiêm thuốc cản quang đa pha, sử dụng mô hình học sâu hai bước là Faster R-CNN với mạng rút trích đặc trưng ResNet-101, áp dụng vào bộ dữ liệu ảnh cắt lớp vi tính có tiêm thuốc cản quang vùng bụng thu thập tại Bệnh viện Trường Đại học Y Dược Cần Thơ để phát hiện và phân lớp ba loại tổn thương khu trú thường gặp ở gan bao gồm nang (Cyst),
0	u mạch máu (Hemangioma) và ung thư tế bào gan nguyên phát (HCC) và đạt được kết quả tốt. Ưu điểm của các hệ thống học sâu là chúng có thể liên tục nhận ra, trích xuất và tìm hiểu các đặc điểm phân cấp khác nhau. Điều này hỗ trợ các bác sĩ trong việc phát hiện và phân loại các tổn thương, đặc biệt là những tổn thương với kích thước nhỏ dễ bị bỏ sót. Một số nghiên cứu trước đây đã chỉ ra rằng các mô hình mạng học sâu có thể được sử dụng để phát hiện và khoanh vùng tổn thương gan khu trú trong hình ảnh chụp cắt lớp vi tính, điển hình với nghiên cứu của tác giả Changjian Sun cùng các cộng sự với việc sử dụng mô hình mạng học sâu kết nối đầy đủ đa kênh [9],
0	hay nghiên cứu của nhóm tác giả Sang-gil Lee với mô hình Grouped Single Shot MultiBox Detector [5]. Tuy nhiên, các đề tài này chỉ dừng lại ở việc phát hiện mà chưa thể thực hiện nhiệm vụ phân loại tổn thương. Việc áp dụng các mô hình máy học, học sâu vào xử lý ảnh y khoa thường có dữ liệu đầu vào là một hình ảnh duy nhất, quá trình xử lý và nhận dạng sẽ cho ra kết quả dự đoán trên ảnh này. Do đó, các quá trình xử lý máy học, học sâu thường được thực hiện trên ảnh X quang, siêu âm, ảnh chụp các tổn thương da.
0	Tiến thêm một bước, các đề tài của nhóm tác giả Dong Liang với mô hình Residual Convolutional Neural Networks và thuật toán máy học véc tơ hỗ trợ [6], hay đề tài của nhóm tác giả Jiarong Zhou cùng cộng sự với ba mô hình Faster R-CNN cùng mạng rút trích đặc trưng FPN riêng biệt [10], đã thực hiện thành công việc phân loại các tổn thương khu trú thường gặp ở gan trên ảnh cắt lớp vi tính đa pha có tiêm thuốc cản quang với đầu vào là các ảnh ở các thì chụp tương ứng với độ chính xác phát hiện và phân loại tương đối cao. Tuy nhiên, các đề tài vẫn còn hạn chế khi chỉ sử dụng ba trên bốn thì chụp cắt lớp vi tính có tiêm thuốc cản quang.
0	Khắc phục điểm yếu này, đề tài đề xuất giải pháp xây dựng bốn mô hình học sâu tương ứng để sử dụng đầy đủ cả bốn thì chụp của quá trình chụp cắt lớp vi tính có tiêm thuốc cản quang làm dữ liệu đầu vào là một điểm rất mới so với các nghiên cứu trước đây. Bốn mô hình tiến hành quá trình học các thuộc tính của tổn thương trong giai đoạn huấn luyện bằng cách trích xuất các đặc trưng tại mỗi thì chụp thông qua bốn ảnh đầu vào. Điều này góp phần mang đến hiệu quả cao hơn so với các nghiên cứu trước đây.
0	Với kết quả kiểm thử thực nghiệm, có thể thấy mô hình Faster R-CNN có thời gian huấn luyện ngắn (583 phút) và tốc độ xử lý rất nhanh (0,13 giây). Bên cạnh đó, mô hình có khả năng rút trích đặc trưng rất hiệu quả khi chỉ số Loss đạt giá trị thấp, từ đó độ chính xác thu được cao (94%), điều này đến từ việc mạng rút trích đặc trưng ResNet-101 hoạt động rất hiệu quả với loại dữ liệu ảnh chụp cắt lớp vi tính có tiêm thuốc cản quang đa pha. Với kết quả thu được về mặt thời gian và độ chính xác, mô hình Faster R-CNN cho thấy mình rất phù hợp với bài toán xử lý ảnh y khoa,
0	đặc biệt là bài toán phát hiện và phân loại các tổn thương khu trú thường gặp ở gan trên ảnh chụp cắt lớp vi tính. Tuy nhiên, vẫn còn nhiều mô hình học sâu khác bao gồm các mô hình thuộc nhóm học một bước (SSD, YOLO...) với thời gian xử lý nhanh hơn nhưng độ chính xác thấp hơn các mô hình học hai bước như R-FCN. Các mô hình này đã và đang chứng minh rất tốt tính hiệu quả trong lĩnh vực xử lý ảnh y khoa, do đó cần mở rộng nghiên cứu đánh giá trên các mô hình học sâu khác, lựa chọn phương pháp tối ưu với sự cân bằng giữa hai yếu tố là thời gian và độ chính xác nhằm tìm ra mô hình phù hợp cho giai đoạn xây dựng ứng dụng thực tế có hiệu quả cao.
0	Đặc tính hình ảnh chụp cắt lớp vi tính vùng bụng có tiêm thuốc cản quang bao gồm bốn thì chụp với số lượng ảnh nhiều và kích thước lớn là một thách thức. Quá trình thu thập và xây dựng tập dữ liệu tiêu tốn nhiều thời gian, công sức và có sự đóng góp rất lớn từ bác sĩ chẩn đoán hình ảnh. Tập dữ liệu gồm 51 người bệnh, trong đó có 11 người bệnh thuộc nhóm nang gan (440 vùng), 18 người bệnh thuộc nhóm u mạch máu (648 vùng), 21 người bệnh thuộc nhóm ung thư tế bào gan nguyên phát (1748 vùng) và 01 người bệnh có cả nang và u mạch máu.
0	Tại thời điểm nghiên cứu, chưa thể tìm được bất kỳ bộ dữ liệu chụp cắt lớp vi tính có tiêm thuốc cản quang vùng bụng có đánh nhãn tổn thương gan nào được chia sẻ công cộng để tiến hành các nghiên cứu ứng dụng các mô hình học sâu vào xử lý và so sánh đối chiếu các kết quả trên cùng tập dữ liệu. Tập dữ liệu đã thu thập được là nền tảng cho phép mở rộng hướng phát triển các nghiên cứu tiếp theo. Tuy nhiên, cần liên tục bổ sung tăng cường cho bộ dữ liệu, không chỉ tăng cường số lượng hình ảnh mà còn cung cấp nhiều phân loại tổn thương gan hơn kể cả tổn thương khu trú và lan tỏa, ngoài ra còn phát triển bổ sung tổn thương ở các cơ quan khác.
0	Ngoài ra có thể phát triển bài toán này trên môi trường tiếp cận dữ liệu lớn như Apache Spark khi bộ dữ liệu ngày càng được tăng cường và bổ sung. Khi đó các giải pháp tính toán thông thường sẽ không còn đạt hiệu quả cao với bộ dữ liệu cực lớn. Đề tài đã hoàn thiện bộ cơ sở dữ liệu ảnh chụp cắt lớp vi tính có tiêm thuốc cản quang vùng bụng thu thập tại Bệnh viện Trường Đại học Y Dược Cần Thơ gồm 51 trường hợp có một trong các chẩn đoán tổn thương khu trú thường gặp ở gan bao gồm nang,
0	u mạch máu và ung thư tế bào gan nguyên phát, với 2828 ảnh cùng 2836 vùng tổn thương được xác định bởi chuyên gia là bác sĩ chuyên ngành chẩn đoán hình ảnh. Áp dụng mô hình học sâu Faster R-CNN với mạng rút trích đặc trưng ResNet-101 để phát hiện và phân loại tổn thương khu trú thường gặp ở gan với kết quả độ chính xác mAP là 94%, thời gian huấn luyện 583 phút và tốc độ xử lý 0,13 giây. Mô hình Faster R-CNN cho thấy sự phù hợp và có thể được áp dụng để triển khai các ứng dụng thực tế trong tương lai.
1	Trong bối cảnh đô thị hóa nhanh, các thành phố lớn đang đối mặt với nhiều vấn đề nghiêm trọng liên quan đến ùn tắc giao thông, tai nạn và ô nhiễm môi trường. Thị giác máy tính (Computer Vision) nổi lên như một giải pháp hiệu quả giúp tự động hóa quá trình giám sát và phân tích giao thông theo thời gian thực. Nhờ sự phát triển của camera độ phân giải cao và các thuật toán học sâu, hệ thống có thể nhận diện phương tiện, phân tích mật độ lưu thông và phát hiện hành vi vi phạm.
1	Theo thống kê của IBM năm 2023, các thành phố ứng dụng hệ thống giám sát thông minh dựa trên thị giác máy tính đã giảm trung bình 28% thời gian ùn tắc và 21% số vụ tai nạn giao thông trong giờ cao điểm. Điều này cho thấy tiềm năng lớn của công nghệ trong quản lý giao thông hiện đại. Nghiên cứu này đề xuất mô hình giám sát giao thông sử dụng thuật toán YOLOv5 kết hợp với mạng CNN để nhận diện và phân loại phương tiện. Dữ liệu huấn luyện gồm hơn 120.000 ảnh giao thông thu thập từ camera đô thị tại ba thành phố lớn, bao gồm ô tô, xe máy, xe tải và xe buýt.
1	Mô hình đạt độ chính xác trung bình 94,6% trong việc nhận diện phương tiện và 91,2% trong việc phát hiện vi phạm như vượt đèn đỏ hay dừng sai làn. Hệ thống xử lý trung bình 25 khung hình mỗi giây, đáp ứng tốt yêu cầu thời gian thực. Việc triển khai mô hình này giúp giảm đáng kể sự phụ thuộc vào con người và tăng tính khách quan trong giám sát giao thông. Khi được triển khai thử nghiệm tại một quận trung tâm, hệ thống đã ghi nhận hiệu quả rõ rệt chỉ sau 6 tháng vận hành.
1	Thời gian xử lý vi phạm giảm từ trung bình 48 giờ xuống còn dưới 10 phút nhờ tự động hóa hoàn toàn quy trình nhận diện và lưu trữ bằng chứng. Số vụ vi phạm giao thông giảm 32% so với cùng kỳ năm trước. Ngoài ra, dữ liệu thu thập còn được sử dụng để điều chỉnh chu kỳ đèn tín hiệu, giúp lưu lượng xe tăng khoảng 18% trong giờ cao điểm. Những kết quả này chứng minh rằng thị giác máy tính không chỉ hỗ trợ giám sát mà còn góp phần tối ưu hóa toàn bộ hệ thống giao thông đô thị.
1	Các bệnh viện hiện đại phải xử lý khối lượng dữ liệu khổng lồ bao gồm hồ sơ bệnh án, kết quả xét nghiệm, lịch khám và thông tin thanh toán. Việc quản lý thủ công hoặc hệ thống phân tán gây ra tình trạng trùng lặp dữ liệu, sai sót và chậm trễ trong điều trị. Lập trình cơ sở dữ liệu đóng vai trò cốt lõi trong việc xây dựng hệ thống quản lý bệnh viện tập trung và hiệu quả. Theo báo cáo của WHO năm 2022, các bệnh viện áp dụng hệ thống quản lý dữ liệu tập trung đã giảm 35% thời gian chờ khám và giảm 27% sai sót y khoa liên quan đến nhập liệu. Điều này cho thấy tầm quan trọng của việc thiết kế cơ sở dữ liệu tối ưu trong lĩnh vực y tế.
1	Hệ thống được thiết kế dựa trên mô hình cơ sở dữ liệu quan hệ sử dụng MySQL kết hợp với các kỹ thuật chuẩn hóa dữ liệu ở mức 3NF. Cơ sở dữ liệu bao gồm hơn 40 bảng chính liên kết chặt chẽ như bệnh nhân, bác sĩ, hồ sơ khám bệnh và đơn thuốc. Các chỉ mục (index) được tối ưu nhằm tăng tốc truy vấn, giúp thời gian truy xuất hồ sơ bệnh án giảm từ 3,2 giây xuống còn 0,8 giây. Ngoài ra, việc sử dụng stored procedure và trigger giúp tự động kiểm tra tính hợp lệ của dữ liệu, giảm hơn 30% lỗi nhập sai thông tin so với hệ thống cũ. Đây là nền tảng quan trọng cho việc triển khai các ứng dụng y tế thông minh.
1	Sau khi triển khai tại một bệnh viện tuyến tỉnh với quy mô 500 giường, hệ thống đã cho thấy nhiều cải thiện rõ rệt. Thời gian xử lý thủ tục nhập viện giảm 40%, trong khi tỷ lệ trùng lặp hồ sơ bệnh nhân giảm xuống dưới 1%. Nhân viên y tế có thể truy cập thông tin bệnh nhân theo thời gian thực, giúp nâng cao chất lượng chẩn đoán và điều trị. Đặc biệt, dữ liệu tập trung còn hỗ trợ phân tích thống kê dịch bệnh, giúp bệnh viện dự báo nhu cầu giường bệnh với độ chính xác lên đến 92%. Điều này chứng minh rằng lập trình cơ sở dữ liệu là nền tảng không thể thiếu trong y tế hiện đại.
1	Chẩn đoán bệnh dựa trên hình ảnh y tế như X-quang, CT và MRI là một trong những thách thức lớn đối với ngành y học hiện đại. Việc phụ thuộc hoàn toàn vào bác sĩ có thể dẫn đến sai sót do yếu tố chủ quan và khối lượng công việc lớn. Mạng nơ-ron tích chập (CNN) đã chứng minh hiệu quả vượt trội trong việc trích xuất đặc trưng hình ảnh và hỗ trợ chẩn đoán tự động. Theo nghiên cứu của Nature Medicine năm 2023, các hệ thống sử dụng CNN đạt độ chính xác tương đương hoặc cao hơn bác sĩ trong một số bài toán chẩn đoán. Điều này thúc đẩy việc ứng dụng CNN trong hỗ trợ quyết định lâm sàng.
1	Nghiên cứu sử dụng kiến trúc ResNet-50 để phân loại bệnh phổi từ ảnh X-quang ngực. Bộ dữ liệu gồm 60.000 ảnh được gán nhãn bởi các chuyên gia y tế, trong đó 70% dùng để huấn luyện, 15% để xác thực và 15% để kiểm tra. Sau 50 epoch huấn luyện, mô hình đạt độ chính xác 96,1%, độ nhạy 94,8% và độ đặc hiệu 95,6%. Thời gian xử lý mỗi ảnh chỉ khoảng 0,04 giây, phù hợp cho ứng dụng lâm sàng. Kết quả này cho thấy CNN có khả năng hỗ trợ bác sĩ trong việc phát hiện sớm các bệnh nguy hiểm.
1	Khi được triển khai thử nghiệm tại một trung tâm chẩn đoán hình ảnh, hệ thống CNN giúp giảm trung bình 30% thời gian đọc phim X-quang cho mỗi ca bệnh. Tỷ lệ bỏ sót tổn thương nhỏ giảm từ 7% xuống còn 2,1%. Ngoài ra, hệ thống còn đóng vai trò như một công cụ đào tạo cho bác sĩ trẻ thông qua việc so sánh kết quả dự đoán với chẩn đoán thực tế. Việc kết hợp CNN và chuyên môn y khoa không nhằm thay thế con người mà giúp nâng cao độ chính xác và hiệu quả chẩn đoán, góp phần cải thiện chất lượng chăm sóc sức khỏe cộng đồng.
1	Ngành bán lẻ đang chuyển mình mạnh mẽ sang mô hình thông minh nhằm nâng cao trải nghiệm khách hàng và tối ưu vận hành. Việc kết hợp thị giác máy tính với hệ thống cơ sở dữ liệu cho phép theo dõi hành vi mua sắm, quản lý hàng tồn kho và phân tích xu hướng tiêu dùng theo thời gian thực. Theo báo cáo của McKinsey năm 2024, các doanh nghiệp bán lẻ ứng dụng AI và thị giác máy tính đã tăng doanh thu trung bình 15–20%. Nghiên cứu này nhằm đánh giá hiệu quả của hệ thống bán lẻ thông minh trong môi trường thực tế tại siêu thị.
1	Hệ thống sử dụng camera gắn trần kết hợp thuật toán CNN để nhận diện sản phẩm và hành vi khách hàng. Dữ liệu được lưu trữ trong cơ sở dữ liệu PostgreSQL với khả năng xử lý hơn 1 triệu bản ghi mỗi ngày. Hệ thống đạt độ chính xác 93% trong việc nhận diện sản phẩm được lấy khỏi kệ và tự động cập nhật tồn kho theo thời gian thực. Nhờ việc tối ưu truy vấn và lập chỉ mục, thời gian cập nhật dữ liệu giảm từ 5 giây xuống còn dưới 1 giây, đảm bảo trải nghiệm mượt mà cho khách hàng.
1	Sau 4 tháng triển khai thử nghiệm, hệ thống giúp giảm 25% tình trạng thiếu hàng đột xuất và giảm 18% chi phí quản lý kho. Doanh thu tăng 12% nhờ phân tích hành vi mua sắm để tối ưu bố trí sản phẩm. Ngoài ra, dữ liệu thu thập còn hỗ trợ xây dựng các chương trình khuyến mãi cá nhân hóa, làm tăng tỷ lệ quay lại của khách hàng lên 22%. Những kết quả này chứng minh rằng việc kết hợp thị giác máy tính và lập trình cơ sở dữ liệu là hướng đi tất yếu trong bán lẻ hiện đại.
1	Trong bối cảnh đô thị hóa nhanh chóng, việc quản lý lưu lượng xe tại các trung tâm thương mại trở nên cấp thiết. Nghiên cứu này đề xuất một giải pháp tích hợp mô hình YOLOv8 (một dạng CNN cải tiến) để nhận diện biển số xe trong thời gian thực. Hệ thống không chỉ dừng lại ở việc chụp ảnh mà còn tự động phân tách các ký tự trên biển số với độ chính xác cực cao. Sau khi nhận diện, dữ liệu chuỗi ký tự được đẩy trực tiếp vào hệ thống SQL Server thông qua một API trung gian. Quá trình này giúp tự động hóa việc ghi nhận giờ vào, vị trí đỗ và tính toán giá tiền mà không cần sự can thiệp thủ công của con người, giảm thiểu tối đa các sai sót trong khâu nhập liệu truyền thống.
1	Thực nghiệm trên tập dữ liệu 5.000 phương tiện tại TP.HCM cho thấy mô hình CNN đạt độ chính xác nhận diện biển số lên tới 98,7% trong điều kiện ánh sáng ban ngày và 94,2% trong điều kiện thiếu sáng hoặc có vật cản nhẹ. Thời gian xử lý trung bình cho một khung hình chỉ mất khoảng 45ms, đáp ứng tốt yêu cầu thời gian thực. Về phía cơ sở dữ liệu, việc sử dụng kỹ thuật Indexing và Stored Procedures giúp giảm thời gian truy vấn thông tin xe từ 1,2 giây xuống còn 0,15 giây cho mỗi lượt xe ra vào. Kết quả này chứng minh rằng sự kết hợp giữa thị giác máy tính và tối ưu hóa database có thể tăng hiệu suất vận hành bãi xe lên gấp 3 lần so với phương pháp thẻ từ thông thường.
1	Bệnh võng mạc tiểu đường là nguyên nhân hàng đầu gây mù lòa, nhưng có thể phòng tránh nếu phát hiện sớm. Nghiên cứu này tập trung vào việc xây dựng một mạng nơ-ron tích chập sâu dựa trên kiến trúc ResNet-50 để phân loại các tổn thương trên võng mạc từ ảnh chụp đáy mắt. Điểm đặc biệt của nghiên cứu là sự kết hợp với cơ sở dữ liệu NoSQL (MongoDB) để lưu trữ các tệp hình ảnh y tế dung lượng lớn cùng với lịch sử bệnh án của bệnh nhân dưới dạng tài liệu JSON linh hoạt. Việc lập trình cơ sở dữ liệu ở đây tập trung vào khả năng mở rộng, cho phép hệ thống lưu trữ hàng triệu bản ghi mà không làm giảm tốc độ truy xuất khi bác sĩ cần đối chiếu dữ liệu lịch sử.
1	Sau khi huấn luyện trên tập dữ liệu EyePACS với hơn 35.000 hình ảnh, mô hình đã đạt được chỉ số Sensitivity (độ nhạy) là 92,5% và Specificity (độ đặc hiệu) là 90,8%. Các con số này tương đương với kết quả chẩn đoán của các chuyên gia nhãn khoa có 10 năm kinh nghiệm. Đặc biệt, hệ thống có khả năng phát hiện các vi xuất huyết nhỏ mà mắt thường dễ bỏ sót với diện tích chỉ từ 5-10 pixels. Việc triển khai thực tế tại các phòng khám tuyến dưới cho thấy thời gian trả kết quả giảm từ 2 ngày xuống còn 3 phút, giúp bệnh nhân tiết kiệm được 40% chi phí đi lại và thăm khám ban đầu tại các bệnh viện trung ương lớn.
1	Ngành xuất khẩu trái cây đòi hỏi tiêu chuẩn phân loại nghiêm ngặt về kích thước và màu sắc. Nghiên cứu này giới thiệu hệ thống băng chuyền thông minh tích hợp camera công nghiệp sử dụng thuật toán Instance Segmentation (Mask R-CNN). Hệ thống có khả năng tách biệt từng quả cam hoặc táo trên băng chuyền, nhận diện các vết sẹo, nấm mốc hoặc sự không đồng đều về màu sắc. Toàn bộ dữ liệu về chất lượng của từng lô hàng được lập trình để cập nhật liên tục vào cơ sở dữ liệu PostgreSQL. Điều này cho phép doanh nghiệp truy xuất nguồn gốc sản phẩm và đánh giá chất lượng nhà cung cấp dựa trên các báo cáo thống kê tự động được trích xuất từ database theo thời gian thực.
1	Kết quả thử nghiệm tại một nhà máy đóng gói cho thấy hệ thống có khả năng phân loại 120 sản phẩm/phút với sai số dưới 2%, trong khi nhân công lành nghề chỉ đạt tối đa 40 sản phẩm/phút với sai số lên đến 15% do mệt mỏi. Về mặt kỹ thuật, mô hình CNN đạt chỉ số mAP (Mean Average Precision) là 0,89 trên tập dữ liệu nông sản nhiệt đới. Việc áp dụng lưu trữ dữ liệu tập trung giúp giảm thất thoát hàng hóa do phân loại sai xuống còn 1,5% so với mức 8% trước đây. Lợi nhuận ròng của doanh nghiệp ước tính tăng thêm 12% mỗi năm nhờ giảm chi phí nhân công và nâng cao uy tín chất lượng sản phẩm khi xuất khẩu sang các thị trường khó tính như EU hay Nhật Bản.
1	An ninh trong các tòa nhà thông minh đòi hỏi sự chính xác tuyệt đối và tốc độ phản ứng nhanh. Nghiên cứu này xây dựng một hệ thống nhận diện khuôn mặt sử dụng kiến trúc FaceNet kết hợp với bộ phân loại SVM (Support Vector Machine) ở lớp cuối cùng để tăng độ chính xác. Để giải quyết vấn đề nghẽn cổ chai dữ liệu khi có hàng ngàn người ra vào cùng lúc, chúng tôi sử dụng mô hình cơ sở dữ liệu phân tán (Distributed Database). Dữ liệu đặc trưng khuôn mặt (face embeddings) được lưu trữ dưới dạng các vector 128 chiều, giúp việc so khớp diễn ra cực nhanh bằng các phép toán khoảng cách Euclid thay vì so sánh hình ảnh thô, từ đó bảo vệ quyền riêng tư và tối ưu hóa tài nguyên máy chủ.
1	"Hệ thống đã được lắp đặt thử nghiệm tại một khu công nghệ cao với lưu lượng 10.000 lượt người/ngày. Kết quả cho thấy tỷ lệ chấp nhận sai (FAR) chỉ ở mức 0,001% và tỷ lệ từ chối sai (FRR) là 0,1%. Tốc độ truy vấn trong cơ sở dữ liệu chứa 50.000 mẫu khuôn mặt đạt mức dưới 100ms nhờ vào việc tối ưu hóa cấu trúc cây phân đoạn trong lập trình database. Ngoài ra, hệ thống còn tích hợp tính năng cảnh báo đối tượng trong ""danh sách đen"" với độ trễ từ khi camera bắt được hình ảnh đến khi gửi thông báo tới điện thoại bảo vệ là dưới 2 giây. Điều này tạo ra một lớp hàng rào an ninh chủ động, hiệu quả hơn hẳn các hệ thống camera giám sát thụ động chỉ dùng để ghi hình thông thường."
1	Nghiên cứu này trình bày phương pháp ứng dụng mạng nơron tích chập sâu (Deep Convolutional Neural Networks - CNN) trong việc phát hiện và phân loại các bệnh lý phổi từ hình ảnh X-quang ngực. Chúng tôi đã phát triển một mô hình CNN dựa trên kiến trúc ResNet-50 được tinh chỉnh để phân loại 5 loại bệnh lý phổi phổ biến bao gồm viêm phổi, lao phổi, ung thư phổi, tràn dịch màng phổi và phổi khỏe mạnh. Dữ liệu huấn luyện gồm 47,582 hình ảnh X-quang ngực được thu thập từ 3 bệnh viện lớn tại TP.HCM trong giai đoạn 2020-2024.
1	Mô hình đạt độ chính xác tổng thể 94.3%, độ nhạy 92.7%, độ đặc hiệu 95.8%, và điểm F1-score 93.5% trên tập kiểm tra gồm 9,516 hình ảnh. Thời gian xử lý trung bình cho mỗi hình ảnh là 0.43 giây trên GPU NVIDIA RTX 3090. Kết quả cho thấy hệ thống có khả năng hỗ trợ bác sĩ chẩn đoán hiệu quả, giảm 38% thời gian đọc phim và tăng 12% độ chính xác chẩn đoán so với phương pháp truyền thống. Nghiên cứu mở ra hướng ứng dụng AI trong y tế tại Việt Nam. Trong những năm gần đây, ứng dụng trí tuệ nhân tạo (AI) trong lĩnh vực y tế đã trở thành xu hướng toàn cầu với tiềm năng cách mạng hóa quy trình chẩn đoán và điều trị bệnh.
1	Theo thống kê của Tổ chức Y tế Thế giới (WHO), các bệnh lý đường hô hấp, đặc biệt là các bệnh về phổi, chiếm khoảng 15.2% tổng số ca tử vong toàn cầu hàng năm với con số lên tới 7.8 triệu người. Tại Việt Nam, số ca mắc bệnh phổi tăng trung bình 4.7% mỗi năm từ 2018 đến 2024, với khoảng 185,000 ca mới được chẩn đoán ung thư phổi và 142,000 ca viêm phổi nặng đòi hỏi nhập viện điều trị. Chẩn đoán sớm và chính xác các bệnh lý phổi đóng vai trò then chốt trong việc nâng cao tỷ lệ sống sót và cải thiện chất lượng cuộc sống của bệnh nhân.
1	Tuy nhiên, việc phân tích hình ảnh X-quang ngực thủ công bởi bác sĩ chuyên khoa thường tốn nhiều thời gian và phụ thuộc vào kinh nghiệm cá nhân, dẫn đến sai số chẩn đoán dao động từ 8% đến 15% trong các ca bệnh phức tạp. Mạng nơron tích chập (Convolutional Neural Network - CNN) là một kiến trúc học sâu đặc biệt hiệu quả trong xử lý dữ liệu dạng lưới như hình ảnh, được LeCun et al. giới thiệu lần đầu vào năm 1998 với mô hình LeNet-5. CNN bao gồm các lớp tích chập (convolutional layers) có khả năng tự động học các đặc trưng từ dữ liệu hình ảnh thông qua việc áp dụng các bộ lọc (filters) với kích thước thường là 3×3 hoặc 5×5 pixel.
1	Các lớp pooling giảm chiều dữ liệu và tạo tính bất biến với các phép biến đổi nhỏ. Trong y tế, CNN đã đạt được nhiều thành tựu đáng kể: mô hình của Esteva et al. (2017) phân loại ung thư da đạt độ chính xác 91.2% tương đương bác sĩ da liễu; hệ thống CheXNet của Rajpurkar et al. (2017) phát hiện 14 loại bệnh lý từ X-quang ngực với AUC trung bình 0.841. Tại Việt Nam, nghiên cứu của Trần et al. (2022) ứng dụng CNN phát hiện lao phổi đạt độ nhạy 88.3% nhưng còn hạn chế về tập dữ liệu với chỉ 12,450 hình ảnh.
1	Mục tiêu chính của nghiên cứu này là phát triển một hệ thống CNN tối ưu có khả năng phân loại chính xác 5 loại bệnh lý phổi phổ biến từ hình ảnh X-quang ngực với độ chính xác trên 93%, thời gian xử lý dưới 0.5 giây/hình ảnh, và khả năng triển khai thực tế tại các cơ sở y tế Việt Nam. Chúng tôi xây dựng tập dữ liệu lớn nhất tại Việt Nam với 57,098 hình ảnh X-quang được gán nhãn bởi 12 bác sĩ chuyên khoa có kinh nghiệm trên 10 năm. Nghiên cứu đóng góp: (1) Phương pháp tiền xử lý và tăng cường dữ liệu tối ưu cho hình ảnh X-quang Việt Nam; (2) Kiến trúc CNN cải tiến từ ResNet-50 với cơ chế attention và skip connections; (3) Phân tích chi tiết hiệu năng trên từng loại bệnh lý;
1	(4) Đánh giá khả năng triển khai thực tế qua thử nghiệm lâm sàng 6 tháng tại 3 bệnh viện. Dữ  liệu nghiên cứu được thu thập từ ba bệnh viện lớn tại TP.HCM bao gồm Bệnh viện Chợ Rẫy (32,450 hình ảnh), Bệnh viện Đại học Y Dược (15,870 hình ảnh), và Bệnh viện Nhân dân 115 (8,778 hình ảnh) trong khoảng thời gian từ tháng 01/2020 đến tháng 08/2024. Tổng cộng 57,098 hình ảnh X-quang ngực được thu thập với độ phân giải từ 2048×2048 đến 4096×4096 pixel, định dạng DICOM và PNG. Mỗi hình ảnh được gán nhãn độc lập bởi 3 bác sĩ chuyên khoa hô hấp có kinh nghiệm trung bình 12.5 năm, trường hợp có sự khác biệt được hội chẩn bởi một chuyên gia cấp cao để đảm bảo độ tin cậy.
1	Dữ liệu được phân chia theo tỷ lệ 70% huấn luyện (39,969 hình), 15% validation (8,565 hình), và 15% kiểm tra (8,564 hình). Phân bố theo nhãn: viêm phổi (18,245 ca - 32.0%), lao phổi (11,892 ca - 20.8%), ung thư phổi (8,567 ca - 15.0%), tràn dịch màng phổi (7,234 ca - 12.7%), và phổi bình thường (11,160 ca - 19.5%). Quy trình tiền xử lý gồm 5 bước chính để chuẩn hóa dữ liệu đầu vào. Bước 1: Chuyển đổi tất cả hình ảnh DICOM sang định dạng PNG 16-bit với chuẩn hóa giá trị pixel theo phương pháp rescale slope và intercept.
1	Bước 2: Resize hình ảnh về kích thước chuẩn 512×512 pixel bằng thuật toán Lanczos interpolation để giữ nguyên chất lượng chi tiết. Bước 3: Áp dụng CLAHE (Contrast Limited Adaptive Histogram Equalization) với clip limit 2.0 và tile grid size 8×8 để tăng cường độ tương phản cục bộ, đặc biệt quan trọng với tổn thương mờ. Bước 4: Chuẩn hóa giá trị pixel về khoảng [0,1] theo công thức x_norm = (x - mean) / std với mean=0.485 và std=0.229 tính từ ImageNet. Bước 5: Data augmentation bao gồm random rotation (±15 độ), horizontal flip (xác suất 0.5), random zoom (0.9-1.1), brightness adjustment (±0.2), và gaussian noise (sigma=0.01) để tăng tính đa dạng và khả năng tổng quát hóa của mô hình, mở rộng tập huấn luyện lên 119,907 hình ảnh.
1	Chúng tôi phát triển mô hình dựa trên kiến trúc ResNet-50 pretrained trên ImageNet với 23,508,032 tham số, được tinh chỉnh cho bài toán y tế cụ thể. Kiến trúc gồm: (1) Lớp đầu vào 512×512×3 với normalization layer; (2) 4 residual blocks với số lượng filters tăng dần 64-128-256-512, mỗi block có 3-6 convolutional layers với kernel size 3×3, stride 1-2, và ReLU activation; (3) Attention mechanism được thêm sau block thứ 3 với 512 channels để mô hình tập trung vào vùng quan trọng, tăng 2.8% accuracy; (4) Global Average Pooling 2D giảm chiều từ 16×16×512 về vector 512 chiều; (5) Fully connected layers gồm Dense 512 units + Dropout 0.5, Dense 256 units + Dropout 0.4, và output layer 5 units với Softmax activation.
1	Loss function sử dụng categorical cross-entropy có trọng số để xử lý mất cân bằng dữ liệu: viêm phổi (1.0), lao phổi (1.35), ung thư phổi (1.62), tràn dịch (1.78), bình thường (1.24). Optimizer Adam với learning rate 0.0001, beta1=0.9, beta2=0.999, batch size 32, và 150 epochs với early stopping patience 15 epochs. Hệ thống được triển khai trên máy chủ GPU với cấu hình: CPU Intel Xeon Gold 6248R 3.0GHz (24 cores), RAM 128GB DDR4, GPU 2× NVIDIA RTX 3090 24GB, SSD NVMe 2TB, và hệ điều hành Ubuntu 22.04 LTS. Framework phát triển sử dụng TensorFlow 2.14.0, Keras 2.14.0, CUDA 12.2, cuDNN 8.9, Python 3.11.5, và các thư viện hỗ trợ như OpenCV 4.8.1, NumPy 1.24.3, Pandas 2.0.3, Scikit-learn 1.3.0. Quá trình huấn luyện mất 47.5 giờ cho 150 epochs với GPU utilization trung bình 87%.
1	Inference được tối ưu hóa với TensorRT 8.6.1 giảm latency từ 0.68s xuống 0.43s/hình ảnh. Docker container được sử dụng để đảm bảo tính nhất quán môi trường khi triển khai. Monitoring system gồm Prometheus và Grafana theo dõi performance metrics realtime. API endpoints được xây dựng bằng FastAPI 0.104.1 với throughput 145 requests/second trong điều kiện production. Mô hình CNN đạt được hiệu năng vượt trội trên tập kiểm tra gồm 8,564 hình ảnh với độ chính xác tổng thể (overall accuracy) 94.3%, độ nhạy (sensitivity/recall) 92.7%, độ đặc hiệu (specificity) 95.8%, độ chính xác dương (precision) 93.1%, và điểm F1-score 93.5%. Diện tích dưới đường cong ROC (AUC) đạt 0.972, cho thấy khả năng phân biệt xuất sắc giữa các lớp bệnh lý.
1	"Confusion matrix cho thấy mô hình có khả năng phân loại tốt nhất với nhãn ""viêm phổi"" (accuracy 96.1%) và thấp nhất với ""tràn dịch màng phổi"" (accuracy 91.2%) do đặc điểm hình ảnh có thể nhầm lẫn với các tổn thương khác. Thời gian xử lý trung bình 0.43 giây/hình ảnh với độ lệch chuẩn 0.08 giây, nhanh hơn 68% so với thời gian đọc phim của bác sĩ trung bình là 1.35 giây. Top-2 accuracy đạt 98.7%, nghĩa là trong 98.7% trường hợp, nhãn đúng nằm trong 2 dự đoán có xác suất cao nhất. So sánh với baseline ResNet-50 không tinh chỉnh cho thấy cải thiện 5.8% accuracy, 7.2% sensitivity, và giảm 32% false negative rate."
1	Viêm phổi: Đạt accuracy 96.1%, sensitivity 95.3%, specificity 96.5%, precision 94.8%, F1-score 95.0%, và AUC 0.983. Mô hình phát hiện tốt các đặc trưng như mờ thâm nhiễm phế nang, mờ lan tỏa, và air bronchogram. False negative rate 4.7% (81/1,724 ca) chủ yếu ở các ca viêm phổi giai đoạn sớm với tổn thương nhỏ dưới 1.5cm. False positive rate 3.5% (58/1,657 ca) thường nhầm với tràn dịch màng phổi hoặc xẹp phổi một phần. Lao phổi: Accuracy 93.8%, sensitivity 91.2%, specificity 94.9%, precision 92.5%, F1-score 91.8%, AUC 0.968. Mô hình nhận diện tốt các nốt lao, tổn thương hang động, và dày màng phổi đỉnh. Khó khăn với 8.8% ca nhầm lẫn với ung thư phổi do hình ảnh tổn thương tương đồng. Ung thư phổi: Accuracy 92.5%, sensitivity 89.7%, specificity 93.8%, precision 90.3%, F1-score 90.0%, AUC 0.961.
1	Phát hiện tốt khối u trung tâm và ngoại biên, tuy nhiên 10.3% false negative ở các khối u nhỏ dưới 2cm hoặc vị trí sau tim. Tràn dịch màng phổi: Accuracy thấp nhất 91.2% do đặc điểm tràn dịch đa dạng. Phổi bình thường: Accuracy cao nhất 97.4% với specificity 98.2%. Chúng tôi áp dụng kỹ thuật Gradient-weighted Class Activation Mapping (Grad-CAM) để trực quan hóa các vùng mà mô hình tập trung khi đưa ra dự đoán, tăng tính minh bạch và khả năng giải thích cho quyết định của AI. Phân tích 500 hình ảnh ngẫu nhiên từ mỗi loại bệnh cho thấy: với viêm phổi, mô hình tập trung vào vùng phế nang bị thâm nhiễm (87.3% trường hợp),
1	trùng khớp với vùng bác sĩ chẩn đoán với IoU trung bình 0.742; với lao phổi, attention map highlight vùng đỉnh phổi và các nốt lao (82.6% trường hợp) với IoU 0.718; với ung thư phổi, mô hình chú ý đến khối u và vùng xung quanh (78.9% trường hợp) với IoU 0.693. Trong 156 ca false positive, Grad-CAM cho thấy 83.3% trường hợp mô hình tập trung vào artifacts hoặc vùng nhiễu không liên quan đến bệnh lý, gợi ý cần cải thiện data cleaning. Heatmap visualization giúp bác sĩ hiểu và tin tưởng kết quả AI, với 91.7% bác sĩ tham gia khảo sát (n=24) đánh giá Grad-CAM hữu ích trong quyết định lâm sàng.
1	Nghiên cứu của chúng tôi đạt hiệu năng vượt trội so với các công trình trước đây. CheXNet (Rajpurkar et al., 2017) trên tập ChestX-ray14 đạt AUC 0.841 cho 14 loại bệnh nhưng không tập trung vào các bệnh phổ biến tại Việt Nam. Nghiên cứu của Wang et al. (2021) sử dụng DenseNet-121 đạt accuracy 91.3% trên 85,000 hình X-quang Trung Quốc, thấp hơn 3.0% so với mô hình của chúng tôi. Tại Việt Nam, Trần et al. (2022) với tập dữ liệu 12,450 hình đạt accuracy 88.3% chỉ cho bài toán phân loại nhị phân (lao/không lao), trong khi mô hình của chúng tôi giải quyết bài toán 5 lớp phức tạp hơn với accuracy 94.3%. Về thời gian xử lý, mô hình của chúng tôi (0.43s/hình) nhanh hơn 34% so với EfficientNet-B7 của Nguyen et al. (2023) (0.65s/hình).
1	Điểm nổi bật là tập dữ liệu 57,098 hình lớn nhất tại Việt Nam và phương pháp attention mechanism cải tiến giúp tăng 2.8% accuracy so với ResNet-50 baseline. Hệ thống được triển khai thử nghiệm tại 3 bệnh viện trong 6 tháng (09/2024 - 02/2025) với 5,847 ca bệnh thực tế. Kết quả cho thấy: (1) Độ chính xác trong môi trường thực tế đạt 93.1%, chỉ giảm 1.2% so với tập kiểm tra do chất lượng hình ảnh đa dạng hơn và điều kiện chụp không đồng nhất; (2) Thời gian chẩn đoán trung bình giảm từ 8.5 phút xuống 5.3 phút (giảm 37.6%) khi bác sĩ có hỗ trợ AI, giúp xử lý thêm 142 ca/ngày/bác sĩ;
1	(3) Tỷ lệ đồng thuận giữa AI và bác sĩ đạt 91.4%, trong 8.6% trường hợp khác biệt có 62.3% là AI đúng sau khi xét nghiệm bổ sung; (4) Giảm 12.3% tỷ lệ chẩn đoán nhầm so với phương pháp thủ công, đặc biệt với các ca lao phổi giai đoạn sớm; (5) Mức độ hài lòng của bác sĩ đạt 8.7/10 điểm với 94.1% cho biết muốn tiếp tục sử dụng. Chi phí triển khai 18,500 USD bao gồm máy chủ, license, và đào tạo, hoàn vốn sau 14 tháng nhờ tăng throughput và giảm sai sót. Nghiên cứu đã thành công phát triển một hệ thống CNN hiệu năng cao cho bài toán phân loại 5 loại bệnh lý phổi từ hình ảnh X-quang ngực với độ chính xác 94.3%,
1	vượt trội so với các nghiên cứu trước đây tại Việt Nam. Mô hình ResNet-50 cải tiến với attention mechanism và data augmentation tối ưu đã chứng minh khả năng học các đặc trưng phức tạp từ hình ảnh y tế, đạt sensitivity 92.7% và specificity 95.8% trên tập dữ liệu lớn nhất Việt Nam với 57,098 hình ảnh. Thử nghiệm lâm sàng 6 tháng tại 3 bệnh viện với 5,847 ca bệnh thực tế chứng minh tính khả thi và hiệu quả của hệ thống trong môi trường thực tế, giảm 37.6% thời gian chẩn đoán và 12.3% tỷ lệ sai sót. Kỹ thuật Grad-CAM tăng tính minh bạch và sự tin tưởng của bác sĩ vào quyết định AI với 91.7% đánh giá tích cực.
1	Nghiên cứu đóng góp quan trọng vào lĩnh vực AI y tế Việt Nam với dataset chuẩn, phương pháp tiền xử lý tối ưu cho hình ảnh địa phương, và quy trình triển khai thực tế có thể nhân rộng ra các bệnh viện khác. Mặc dù đạt được kết quả khả quan, nghiên cứu vẫn tồn tại một số hạn chế cần được khắc phục trong tương lai. Thứ nhất, tập dữ liệu chủ yếu thu thập từ 3 bệnh viện tại TP.HCM, có thể không đại diện hoàn toàn cho đặc điểm bệnh lý ở các khu vực khác với thiết bị và điều kiện chụp khác nhau. Phân bố dân số nghiên cứu thiên về độ tuổi trung bình 48.5±16.2 tuổi, chưa có đủ dữ liệu cho nhóm trẻ em (dưới 15 tuổi) chiếm chỉ 3.2% mẫu.
1	Thứ hai, mô hình hiện tại chỉ xử lý hình X-quang tư thế thẳng, chưa hỗ trợ tư thế nghiêng (lateral view) chiếm 18% ca chụp thực tế hoặc các phương thức chẩn đoán hình ảnh khác như CT scan, MRI. Thứ ba, accuracy giảm 1.2% trong môi trường production do chất lượng hình ảnh không đồng nhất và artifacts, cần cải thiện preprocessing pipeline. Thứ tư, thời gian inference 0.43s/hình có thể chưa đủ nhanh cho các trường hợp cấp cứu đòi hỏi kết quả tức thì dưới 0.2 giây. Cuối cùng, chi phí triển khai 18,500 USD có thể là rào cản đối với các bệnh viện tuyến dưới có ngân sách hạn chế.
1	Dựa trên kết quả và hạn chế hiện tại, chúng tôi đề xuất các hướng phát triển tiếp theo. Mở rộng dữ liệu: Thu thập thêm 80,000 hình ảnh từ 10 bệnh viện trên cả nước trong 24 tháng tới để tăng tính đại diện, đặc biệt tập trung vào nhóm trẻ em (mục tiêu 15,000 hình) và người cao tuổi trên 70 tuổi. Cải thiện kiến trúc: Thử nghiệm các kiến trúc mới như Vision Transformer (ViT), EfficientNetV2-L, và ensemble learning kết hợp 3-5 models để tăng accuracy lên 96%+. Tích hợp multi-modal learning kết hợp hình ảnh X-quang với dữ liệu EHR (electronic health records), triệu chứng lâm sàng, và kết quả xét nghiệm để cải thiện 3-5% accuracy.  Xử lý đa view: Phát triển model xử lý đồng thời cả X-quang thẳng và nghiêng với correlation learning giữa 2 views.
1	Tối ưu hóa: Áp dụng model compression (pruning, quantization) và edge computing để giảm latency xuống 0.15s, giảm 70% dung lượng model từ 98MB xuống 29MB cho phép chạy trên thiết bị y tế cầm tay. Mở rộng ứng dụng: Nghiên cứu phát hiện và segmentation tổn thương chính xác vị trí với Mask R-CNN, phát triển module theo dõi tiến triển bệnh qua thời gian với temporal CNN, và xây dựng hệ thống hỗ trợ quyết định điều trị tích hợp với clinical guidelines. Triển khai rộng rãi: Phát triển cloud-based platform cho phép các bệnh viện nhỏ sử dụng qua API với chi phí 2-3 USD/ca, xây dựng mobile app cho bác sĩ và continuous learning system tự động cập nhật model từ dữ liệu mới với human-in-the-loop validation.
0	Hệ thủy lực động đã được nhiều nhà nghiên cứu xây dựng mô hình toán học mô tả với nhiều trạng thái khác nhau. Phương trình Navier-Stokes cho dòng chảy qua khe hẹp mô tả hiện tượng dòng chảy qua cửa cống gây hiện tượng sói mòn hai bên bờ cần được nghiên cứu và xử lý. Báo cáo này trình bày phương pháp ứng dụng công nghệ CNN giải phương trình Navierstock. Việc giải các PDE đã được nhóm nghiên cứu thực thi thành công và công bố [1,6], tuy nhiên ở đây tập trung vào hai vấn đề là mở rộng không gian tính toán (vì tài nguyên FPGA kích thước hạn chế, yêu cầu thực tế lại cần không gian tính toán lớn); trao đổi dữ liệu theo thời gian thực từ máy PC hay từ cảm biến đo lường với chip CNN.
0	Hướng giải quyết là chia tách không gian thành nhiều không gian con, xử lý biên để đồng bộ tính toán giữa các miền; xây dựng các vùng đệm để trao đổi dữ liệu với bên ngoài. Các kết quả thực nghiệm mô phỏng trên chip XCVL240T-1FFG1156 cho thấy tính hiệu quả, tin cậy của giải pháp. Phương trình Navier–Stokes là một trong các phương trình toán học mô tả hiện tượng dòng chảy thuỷ lực đã được xây dựng từ rất lâu và đã được phát triển thay đổi theo mỗi mô hình ứng dụng thực tế. Phương trình này cũng đã được nhiều nhà nghiên cứu thực hiện giải bằng một số phương pháp thủ công cũng như trên máy tính.
0	Tuy nhiên ở báo cáo này triển khai giải quyết một trong những dạng phương trình Navier-Stokes cụ thể đó là dạng tính toán cho dòng chảy đi qua một khe hẹp như cống xả lũ đổ ra hạ lưu. Công nghệ mạng nơ ron tế bào (CNN) là kiến trúc nhỏ gọn dạng onchip có khả năng tính toán song song quá trình vật lý kích thước lớn [2,3]. Mạng CNN được triển khai chế tạo đơn giản trên công nghệ chip lập trình được FPGA, đồng thời có khả năng ghép nối với thiết bị vào/ra thứ cấp để xử lý [1,4]. Cho đến nay nhiều lĩnh vực ứng dụng của công nghệ CNN được công bố tại các Hội thảo chuyên đề Ứng dụng công nghệ CNN (CNNA).
0	Trên thế giới, việc giải phương trình đạo hàm riêng (PDE) trên công nghệ CNN đã được thực hiện với nhiều dạng khác nhau. Tại Việt Nam cũng đã có một số nghiên cứu chế tạo mạng CNN bằng công nghệ FPGA giải thành công phương trình đạo hàm riêng Saint venant [1,6]. Hiện nay với nền tảng công nghệ chế tạo mạch bằng phương pháp cấu hình trên ngôn ngữ đặc tả phần cứng VHDL và nhiều chip trắng FPGA có kích thước tài nguyên lớn cho phép việc chế tạo chip CNN rất dễ dàng, thuận lợi. Tuy nhiên cần phát triển hơn về vấn đề tối ưu sử dụng tài nguyên trên chip, mở rộng không gian tính toán, nâng cao độ chính xác, kiến trúc vật lý gọn nhẹ, tiêu thụ ít năng lượng...
0	Báo cáo chia làm 5 phần: phần 1 giới thiệu chung; phần 2 giới thiệu phương trình Navier-Stokes cho dòng chảy qua khe hẹp; phần 3 Phân tích hệ phương trình Navier-Stokes để tìm mẫu làm cơ sở cho việc thiết kế mạng CNN; phần 4 Các giải pháp thiết kế, cấu hình chế tạo mạng CNN dựa trên chip FPGA Vertex 6 XCVL240T-1FFG1156 đảm bảo tối ưu tài nguyên và tốc độ tính toán; phần 5 là kết quả thự nghiệm cài đặt tạo ra mạng CNN thực hiện tính toán; phần cuối cùng là kết luận và hướng phát triển.
0	Phương trình Navier–Stokes được nghiên cứu ở đây là hệ 3 phương trình đạo hàm riêng của các ẩn hàm biểu diễn chiều cao mực nước, vận tốc dòng chảy theo hai chiều x, y. Mô hình thực nghiệm là một đoạn kênh thủy lực dòng chảy qua một cửa cống hẹp sau đó phân tán theo hai chiều Ox, Oy. Việc giải phương trình Navier–Stokes trên thiết bị điện tử chúng ta phải rời rạc hóa mô hình liên tục bằng phương pháp sai phân. Để đảm bảo độ chính xác thì các bước sai phân phải đủ nhỏ, tuy nhiên bước sai phân quá nhỏ khối lượng công việc tính toán tăng lên nhiều dẫn đến độ phức tạp tính toán và thời gian tính toán tăng lên đáng kể.
0	Nếu sử dụng công nghệ tính toán song song vật lý trên mạng CNN thì sẽ khắc phục được các khó khăn trên. Tuy nhiên với những không gian tính toán quá lớn việc tính toán song song cũng bị hạn chế bởi tài nguyên tính toán. Hiện nay có chip FPGA đời mới có tới hàng triệu phần tử lô gic (LE) nhưng cũng không phải là vô hạn. Do vậy, cần có giải pháp kỹ thuật chia không gian tính toán thành nhiều không gian con và tích hợp lại để đảm bảo kết quả tính toán được liên tục và chính xác nhất là với bài toán thời gian thực.
0	Để đơn giản các biến trong hệ thương trình Navier-Stokes trên, ta đặt: độ cao mặt nước h = zw; vận tốc theo chiều trục x là u = qx; theo chiều trục y là v = qy. Giả thiết lượng nước bay hơi hay thẩm thấu là không đáng kể hay qA = 0; ảnh hưởng động năng của giá trị rối giữa vận tốc theo chiều 0y sang 0x (hay 0x sang 0y) là không đáng kể do vận tốc ngang nhỏ có thể bỏ qua. Theo các mẫu tìm được trong (3.7), (3.8), (3.9) ta có thể thiết kế kiến trúc cho mạng CNN. Kiến trúc mạng nơ ron thực hiện giải phương trình NavierStokes là mạng CNN ba lớp 2D.
0	Từ mô hình thiết kế tổng thể trên ta có thể chế tạo từng khối tính toán cho mỗi lớp và liên kết để thực hiện tính toán song song trên toàn chip. Vấn đề thực nghiệm chế tạo mạng cần phải giải quyết là: Thứ nhất, do tài nguyên của chip FPGA hữu hạn, trong khi không gian cần tính toán lớn do vậy phải chia vùng tính toán để phối hợp tính toán tuần tự và song song; Thứ hai là các điểm biên tiếp giáp các vùng và biên của lưới sai phân cần xử lý. Việc chia và phối hợp các vùng biên sao cho liên tục và không gây kết quả sai do phối hợp thời điểm tính toán;
0	Thứ ba là việc trao đổi dữ liệu theo thời gian thực mà trong các báo cáo trước đây chưa được giải quyết. Với mạng CNN trong báo cáo này đã giải quyết được những vấn đề đặt ra ở trên. Khối điều khiển làm nhiệm vụ điều khiển hoạt động của toàn bộ hệ thống theo thuật toán đã đặt ra. Khi chia không gian tính toán thành các vùng sẽ có các vùng đệm (biên). Phần màu trắng là vùng đệm vào, phần màu xám là phần cần xử lý bằng CNN. Khối tính toán CNN có kích thước (M-2)x(N-2) tế bào xử lý dữ liệu cho phần màu xám bên trong vùng đệm vào. Với mục đích cài đặt đơn giản, chúng tôi chọn M = 3. N được chọn tùy thuộc vào tài nguyên của chip FPGA mà chúng ta sử dụng.
0	Giả sử chọn N = 4. Biên dọc tạm thời được đưa vào để được cấu trúc dữ liệu giống đệm vào CNN. Biên ngang tạm thời là không cần thiết do thuật toán truy xuất dữ liệu của khối điều khiển. Dữ liệu sau khi đã được thêm biên dọc tạm thời sẽ được chuyển vào khối bộ nhớ vào h, u, v. Như ta đã biết CNN làm việc theo cơ chế toàn cục và cục bộ. Khối điều khiển toàn cục điều khiển toàn bộ các thao tác vào/ra, xử lý tính toán. Đặc biệt ở đây là việc chia vùng không gian tính toán và phối hợp đồng bộ về thời gian và quét hết không gian. Các khối này sẽ được đưa lần lượt tới khối tính toán CNN để được xử lý theo cơ chế pipline.
0	Kết quả xử lý được ghi vào khối bộ nhớ ra h, u, v. Tiếp đó, kết quả xử lý trên khối bộ nhớ ra h, u, v sẽ được cập nhật lại biên dọc tạm thời rồi được đưa trở về khối bộ nhớ vào h, u, v cho lần xử lý tiếp theo. Để ý rằng bước thời gian rất nhỏ tính bằng xung đồng hồ do vậy cần có sự đồng bộ cao theo từng xung nếu lệch một xung giá trị hiện tại và quá khứ hoặc tương lai sẽ làm sai đáng kể kết quả tính toán. Kết quả mô phỏng tính toán cho thấy tính đúng đắn và hiệu quả của phương pháp cài đặt.
0	Chi phí cho tính toán khối MxN đầu tiên lấy từ bộ nhớ h, u, v vào là 9clk, trong đó 3clk cho cập nhật đệm vào CNN, 6clk cho tính toán lần đầu tiên. Mỗi khối MxN kế tiếp chỉ mất 1clk để tính toán do sử dụng cơ chế pipline để cập nhật đệm vào CNN và tính toán tại khối tính toán CNN. Bài báo giới thiệu giải pháp phân tích thiết kế và cấu hình chip CNN giải phương trình Navier Stokes. Những nghiên cứu trước [1,6] đã giải quyết vấn đề về độ chính xác, độ tin cậy, tốc độ tính toán do vậy báo cáo này chỉ tập trung trình bày việc giải quyết ba vấn đề:
0	chia không gian tính toán thành nhiều miền con để kết hợp xử lý song song và tuần tự nhằm đảm bảo tốc độ chấp nhận được và tiết kiệm tài nguyên; ghép nối không gian tính toán và đồng bộ thời điểm tính toán; thiết kế bộ nhớ đệm cho việc trao đổi dữ liệu giữa chip CNN với bên ngoài một cách linh hoạt để thực thi một hệ thống tính toán, liên kết trao đổi thông tin thời gian thực. Với dữ liệu vào là số thực 32 bit dấu chấm động, sử dụng chip FPGA Vertex 6 XCVL240T-1FFG1156 của hãng Xilinx, nhóm đã cài đặt được một mạng CNN gồm 1x12 tế bào đảm bảo các yêu cầu đặt ra như phân tích trên. Từ mô hình này có thể triển khai cho một hệ thống xử lý điều khiển tự động trong thực tiễn.
0	Khởi tạo trọng số sử dụng phương pháp Glorot là một trong những phương pháp hữu hiệu cho mạng nơ ron tích chập (CNN). Ngoài ra những nghiên cứu trước đây cũng đã sử dụng phương pháp này cho CNN số phức. Tuy nhiên, không có nghiên cứu nào khẳng định được phương pháp khởi tạo trọng số Glorot có hiệu quả tốt nhất cho CNN số phức. Nghiên cứu này tập trung vào việc tối ưu CNN số phức dựa trên cách thức khởi tạo trọng số kết nối của các tế bào nơ ron. Bài báo này đề xuất sử dụng giải thuật di truyền (GA) để tìm tham số tối ưu cho phân bố Rayleigh để khởi tạo trọng số trước khi huấn luyện CNN số phức.
0	Thực nghiệm cho thấy GA đã tìm ra tham số tốt hơn khi sử dụng phương pháp Glorot như các nghiên cứu trước đây. Mạng nơron tích chập số phức [1, 2] (Complex Valued Convolutional Neural Networks – CNN số phức) là trong những phương pháp hiệu quả hiện nay trong nhận dạng [3], xử lý nhiễu ảnh [4, 5] hay xử lý tín hiệu [6]. Có 2 hướng tiếp cận chính đối với tế bào nơ ron trong mô hình CNN số phức [7]. Hướng thứ nhất là tách riêng thành 2 tế bào nơ ron riêng biệt, nơ ron phụ trách tính toán phần thực và nơ ron phụ trách tính toán phần ảo.
0	Hướng thực hiện này có thể giúp cho các nghiên cứu có kế thừa mô hình CNN số thực một cách dễ dàng nhưng nó làm mất đi tính chất đặc biệt của số phức là tích 2 số phức hoạt động như một phép biến đổi hình học trong không gian 2 chiều. Hướng tiếp cận thứ 2 là áp dụng số phức cho từng nơ ron trong mạng CNN số phức [8]. Các tín hiệu đầu vào, đầu ra và trọng số kết nối giữa các lớp trong mạng CNN số phức đều sử dụng số phức. Hướng tiếp cận này cho thấy sự hiệu quả trong các bài toán có tín hiệu đầu vào hay đầu ra là số phức như kết hợp với phương pháp biến đổi Fourier nhanh (Fast Fourier Transform – FFT) để khử nhiễu hay nhận dạng tín hiệu, hình ảnh [7].
0	Bài báo này tập trung vào phân tích hướng tiếp cận thứ 2 là áp dụng số phức vào từ nơ ron trong mạng CNN số phức. Để huấn luyện 1 mạng CNN số phức, các phương pháp trước đây chú trọng việc xây dựng cấu trúc mạng, việc khởi tạo các trọng số bằng số phức ít được chú ý tới. Một số phương pháp [9, 10] có đề xuất sử dụng phương pháp Glorot [11] trong việc khởi tạo trọng số của các mạng CNN số thực để áp dụng cho CNN số phức. Tuy nhiên các nghiên cứu trước đây chưa chứng minh được việc khởi tạo như vậy là tối ưu nhất.
0	Nghiên cứu này đề xuất phương pháp khởi tạo trọng số cho mạng CNN số phức sử dụng giải thuật di truyền [12]. Bài báo này gồm 5 mục, mở đầu nêu bối cảnh và lý do lựa chọn phương pháp đề xuất. Tiếp theo bài báo giới thiệu các nghiên cứu liên quan tới mạng CNN số phức và giải thuật di truyền. Tiếp đó là giải pháp đề xuất và thực nghiệm và đánh giá kết quả. Cuối cùng là kết luận. Nghiên cứu này chú trọng việc khởi tạo trọng số cho mạng CNN số phức sử dụng giải thuật di truyền. Vì vậy trong mục này sẽ giới thiệu mạng CNN số phức, trong đó có cách thức khởi tạo trước đây.
0	Tiếp theo, mục này sẽ giới thiệu giải thuật di truyền trong các bài toán tối ưu các tham số trong các mô hình học máy, đặc biệt là mạng nơ ron nhân tạo. Giải thuật di truyền (Genetic Algorithm – GA) [15] là một phương pháp tối ưu tổ hợp dựa trên cơ chế di truyền của sinh vật. Giải thuật di truyền thường được sử dụng để tìm ra giải pháp tối ưu cho các bài toán NP khó. Giải thuật di truyền được mô tả như sau: Bước 1: Tạo tập nghiệm ngẫu nhiên. Bước 2: Đánh giá các nghiệm hiện có. Bước 3: Nếu nghiệm tốt nhất thỏa mãn yêu cầu thì nhảy đến Bước 7.
0	Bước 4: Lựa chọn các nghiệm tốt để nhân bản và các nghiệm xấu để loại bỏ. Bước 5: Lai ghép các cặp nghiệm để tạo ra nghiệm mới. Bước 6: Đột biến một số nghiệm trong tập nghiệm. Bước 7: In ra kết quả tốt nhất. Để hiểu rõ hơn về giải thuật di truyền, ta lấy ví dụ sử dụng giải thuật di truyền để giải bài toán tìm nghiệm của phương trình: 𝑥² − 12𝑥 + 36 = 0 (13). Bước 1: Thiết kế bộ gen là các số trong hệ nhị phân. Phương án nghiệm của bài toán là 1 số khi đổi qua cơ số thập phân của bộ gen.
0	Ví dụ, ta có thể tạo ngẫu nhiên tập nghiệm ban đầu như sau: 0100, 1011, 0101, 1010. Bước 2: Cần 1 tiêu chí đánh giá các nghiệm hiện có. Ta có thể chọn hàm đánh giá 𝑓(𝑥) = |𝑥² − 12𝑥 + 36| vì 𝑓(𝑥) càng nhỏ thể hiện phương án nghiệm càng gần với kết quả chính xác. Khi đã có hàm đánh giá, ta tính giá trị 𝑓 đối với tất cả các phương án nghiệm đang có. Ví dụ: 𝑓(0100) = 4, 𝑓(1011) = 25, 𝑓(0101) = 1, 𝑓(1010) = 16. Bước 3: Kiểm tra kết thúc việc thực thi hay không. Giả sử tiêu chí kết thúc thuật toán 𝑓(𝑥) = 0 thì chưa có nghiệm nào thỏa mãn tiêu chí kết thúc. Bước 4: Lựa chọn các nghiệm tốt để nhân bản và các nghiệm xấu để loại bỏ.
0	Ví dụ: nghiệm 1011 có 𝑓(1011) = 25 bị loại bỏ và nghiệm 0101 có 𝑓(0101) = 1 được nhân bản. Sau khi kết thúc bước này, tập hợp nghiệm sẽ là: 0101, 0101, 0100, 1010. Bước 5: Lai ghép. Có nhiều hình thức lai ghép như lai ghép 1 điểm, lai ghép nhiều điểm hay lai ghép đồng nhất. Giả sử ta chọn lai ghép 1 điểm tại vị trí 𝑘 = 1 thì có nghĩa là bít đầu nghiệm thứ nhất sẽ ghép với 3 bít sau của nghiệm thứ hai và ngược lại. Trong quá trình lai ghép theo cách thức trên, nếu cặp nghiệm 0101 và 1010 được lai ghép thì kết quả sẽ được 2 nghiệm mới là 1|101 và 0|010.
0	Ta thay thế 2 nghiệm được lai ghép thành 2 nghiệm mới trong tập nghiệm. Bước 6: Đột biến. Tại một nghiệm bất kỳ, đổi giá trị 1 thành 0 hoặc 0 thành 1 ở tại 1 bít nào đó. Ví dụ 0101 được chọn và đột biến thành 0100, hoặc 0010 được chọn và đột biến thành 0110. Quá trình từ Bước 2 đến Bước 6 sẽ được lặp đi lặp lại đến khi tìm được cá thể thỏa mãn tiêu chí của bài toán. Trong ví dụ trên thì nghiệm 0110 chính là nghiệm tốt nhất của bài toán.
0	Việc áp dụng GA trong việc tối ưu các tham số hay cấu trúc mô hình học máy được thực hiện rất nhiều trong nghiên cứu trước đây, đặc biệt là đối với các mô hình mạng nơ ron nhân tạo. Whitley Darrell [Whitley 1990] đề xuất sử dụng GA nhằm tối ưu cách thức kết nối giữa các tế bào nơ ron trong mô hình ANN. Cook [16] đề xuất phương pháp ứng dụng GA nhằm tối ưu các tham số trong mô hình ANN. Karegowda [17] đề xuất phương pháp tối ưu số kết nối trong mô hình ANN sử dụng GA. Aszemi [18] đề xuất phương pháp tối ưu tham số cho mô hình CNN sử dụng GA. Chung [19] đề xuất phương pháp sử dụng GA trong mô hình CNN nhiều kênh (Multi-channel CNN).
0	Bài báo này đề xuất mô hình huấn luyện CNN số phức kết hợp với GA nhằm tối ưu tham số 𝜎 của phân bố Rayleigh trong quá trình khởi tạo trọng số đựa trên Glorot đối với CNN số phức. Hình 1 thể hiện mô hình khởi tạo trọng số cho CNN số phức sử dụng GA. Vì thế, phương pháp đề xuất sẽ có kết quả không thể thấp hơn phương pháp dựa trên Glorot. Tuy nhiên, về mặt thời gian để tìm ra tham số tối ưu của phương pháp đề xuất thì rõ ràng phải thực hiện rất nhiều lần việc huấn luyện mạng ứng với từng tham số 𝜌 trong giải thuật di truyền.
0	Bài báo này chỉ chú trọng việc chứng minh rằng phương pháp khởi tạo trọng số dựa trên Glorot chưa phải là phương pháp tốt nhất, nên không chú trọng đến việc tối ưu về mặt thời gian huấn luyện. Để đánh giá mô hình đề xuất, nghiên cứu đã sử dụng dữ liệu MNIST Handwritten Digit Database [20] để sinh ngẫu nhiên 1000 mẫu cho tập huấn luyện và 5000 mẫu cho tập kiểm thử. Nghiên cứu sử dụng GA để tìm tham số 𝜌 ứng với 2 mô hình CNN số phức như ở Hình 2 (Model 1) và Hình 3 (Model 2). Hình 4 là kết quả nhận dạng đúng của 2 mô hình CNN số phức qua các thế hệ GA.
0	Hình 4 cho thấy kết quả nhận dạng đúng của cả 2 mô hình CNN số phức tăng dần qua từng thế hệ trong GA. Hình 5 và Hình 6 là kết quả so sánh tỉ lệ nhận dạng đúng biến thiên theo từng epoch giữa mô hình CNN số phức tốt nhất do GA đưa ra và mô hình CNN số phức trước đây (𝜌 = 0.5). Kết quả so sánh cho thấy mô hình CNN số phức tốt nhất do GA đưa ra đều tốt hơn mô hình CNN số phức với tham số được đề xuất bởi Glorot. Điều đó cho thấy việc khởi tạo trọng số sử dụng phương pháp Glorot chưa phải phù hợp nhất cho mô hình CNN số phức. Cả hai mô hình tốt nhất đều có khuynh hướng 𝜌𝐺𝐴 < 𝜌𝐺𝑙𝑜𝑟𝑜𝑡. Hình 5.
0	Tỉ lệ nhận dạng đúng biến thiên theo epoch đối với mô hình CNN số phức (Model 1) với tham số 𝜌 được đề xuất bởi phương pháp trước đây và tham số tốt nhất do GA tìm được. Hình 6. Tỉ lệ nhận dạng đúng biến thiên theo epoch đối với mô hình CNN số phức (Model 2) với tham số 𝜌 được đề xuất bởi phương pháp trước đây và tham số tốt nhất do GA tìm được. Bài báo này đã đề xuất mô hình sử dụng thuật toán di truyền để tìm tham số tối ưu cho việc khởi tạo trọng số trong mô hình mạng nơ ron tích chập số phức.
0	Kết quả thực nghiệm đã cho thấy việc khởi tạo với tham số được đề xuất bởi mô hình đề xuất tốt hơn so với việc khởi tạo bằng phương pháp Glorot. Việc sử dụng thuật toán di truyền để tìm tham số bắt buộc phải xây dựng rất nhiều mô hình mạng nơ ron tích chập số phức nên việc tìm ra mô hình tối ưu nhất là điều có thể lường trước được. Tuy nhiên, việc thực nghiệm với thuật toán di truyền này đã chứng minh được rằng phương pháp khởi tạo trọng số Glorot chưa phù hợp với mô hình số phức. Hướng phát triển trong thời gian tới của nghiên cứu này là tìm ra được công thức phù hợp hơn cho tham số khởi tạo trọng số của mô hình mạng nơ ron số phức để thay thế cho phương pháp Glorot.
1	Trong bối cảnh đô thị hóa nhanh chóng, tình trạng ùn tắc và tai nạn giao thông ngày càng gia tăng, đặt ra yêu cầu cấp thiết về các giải pháp giám sát thông minh. Thị giác máy tính (Computer Vision) kết hợp với mạng nơ-ron tích chập (CNN) đã trở thành hướng tiếp cận hiệu quả trong việc phân tích hình ảnh và video giao thông. Theo thống kê của Bộ Giao thông Vận tải Việt Nam năm 2024, trung bình mỗi ngày có hơn 65 vụ tai nạn giao thông tại khu vực đô thị, trong đó 42% liên quan đến vi phạm làn đường và tín hiệu đèn.
1	Việc áp dụng CNN cho phép hệ thống tự động nhận diện phương tiện, biển số và hành vi vi phạm với độ chính xác cao, góp phần giảm tải cho lực lượng giám sát thủ công và nâng cao hiệu quả quản lý giao thông. Nghiên cứu sử dụng tập dữ liệu gồm 120.000 khung hình video thu thập từ 35 camera giao thông tại TP. Hồ Chí Minh trong khoảng thời gian 6 tháng. Mô hình CNN được xây dựng dựa trên kiến trúc YOLOv5 với 53 lớp tích chập, được huấn luyện trong 200 epoch. Dữ liệu được chia theo tỷ lệ 70% cho huấn luyện, 15% cho kiểm thử và 15% cho đánh giá.
1	Hệ thống cơ sở dữ liệu MySQL được sử dụng để lưu trữ thông tin phương tiện, thời gian vi phạm và vị trí camera. Kết quả huấn luyện cho thấy mô hình đạt độ chính xác trung bình 94,6% trong nhận diện phương tiện và 91,2% trong phát hiện hành vi vượt đèn đỏ. Sau khi triển khai thử nghiệm trong 30 ngày tại 5 nút giao thông lớn, hệ thống ghi nhận khả năng phát hiện trung bình 3.200 lượt vi phạm mỗi ngày, tăng 38% so với phương pháp giám sát truyền thống. Thời gian xử lý mỗi khung hình giảm xuống còn 0,18 giây, đáp ứng yêu cầu xử lý thời gian thực.
1	Đặc biệt, tỷ lệ sai sót trong nhận diện biển số chỉ còn 3,5%, thấp hơn nhiều so với mức 9,8% khi sử dụng các thuật toán truyền thống. Kết quả cho thấy CNN kết hợp với thị giác máy tính không chỉ nâng cao độ chính xác mà còn giúp tối ưu hóa chi phí vận hành hệ thống giám sát giao thông.Nghiên cứu khẳng định tiềm năng lớn của thị giác máy tính và CNN trong lĩnh vực quản lý giao thông đô thị. Việc tích hợp với hệ thống cơ sở dữ liệu giúp lưu trữ, truy xuất và phân tích dữ liệu vi phạm một cách hiệu quả. Trong tương lai, mô hình có thể được mở rộng để dự đoán ùn tắc giao thông theo thời gian thực, từ đó hỗ trợ điều tiết luồng xe hợp lý.
1	Chẩn đoán hình ảnh y tế đóng vai trò quan trọng trong phát hiện sớm và điều trị bệnh. Tuy nhiên, áp lực công việc lớn khiến bác sĩ dễ xảy ra sai sót trong quá trình phân tích hình ảnh. Theo Tổ chức Y tế Thế giới, tỷ lệ chẩn đoán sai trong hình ảnh X-quang và CT có thể lên đến 15% tại các bệnh viện đông bệnh nhân. Thị giác máy tính kết hợp CNN được xem là giải pháp hỗ trợ hiệu quả, giúp tự động phân tích hình ảnh y tế với độ chính xác cao. Nghiên cứu này tập trung vào việc ứng dụng CNN trong phát hiện tổn thương phổi từ ảnh X-quang, kết hợp với hệ thống cơ sở dữ liệu để quản lý hồ sơ bệnh nhân.
1	Tập dữ liệu gồm 50.000 ảnh X-quang phổi được thu thập từ 3 bệnh viện lớn, trong đó 28.000 ảnh có tổn thương và 22.000 ảnh bình thường. Mô hình CNN được xây dựng dựa trên kiến trúc ResNet-50, huấn luyện trong 150 epoch. Hệ quản trị cơ sở dữ liệu PostgreSQL được sử dụng để lưu trữ hình ảnh, kết quả phân tích và thông tin bệnh nhân. Sau quá trình huấn luyện, mô hình đạt độ chính xác 96,1%, độ nhạy 95,4% và độ đặc hiệu 94,8%. Thời gian xử lý trung bình mỗi ảnh chỉ 0,25 giây.
1	Khi triển khai thử nghiệm tại bệnh viện, hệ thống giúp giảm thời gian đọc ảnh trung bình từ 7 phút xuống còn 2 phút mỗi ca. Tỷ lệ bỏ sót tổn thương giảm từ 12% xuống còn 3,1%. Ngoài ra, việc tích hợp cơ sở dữ liệu cho phép truy xuất lịch sử bệnh án nhanh hơn 45% so với hệ thống cũ. Các bác sĩ đánh giá hệ thống như một công cụ hỗ trợ hữu ích, giúp nâng cao độ tin cậy trong chẩn đoán, đặc biệt trong các ca bệnh phức tạp hoặc số lượng bệnh nhân lớn.
1	Bên cạnh những kết quả tích cực đã đạt được, việc ứng dụng CNN trong chẩn đoán hình ảnh y tế còn mở ra nhiều hướng phát triển tiềm năng trong tương lai. Cụ thể, mô hình có thể được huấn luyện với các tập dữ liệu đa dạng hơn như ảnh MRI, CT-scan hoặc siêu âm nhằm phát hiện đồng thời nhiều loại bệnh lý khác nhau, bao gồm ung thư, bệnh tim mạch và các rối loạn thần kinh. Khi tích hợp sâu với hệ thống cơ sở dữ liệu y tế điện tử, toàn bộ quá trình lưu trữ, truy xuất và phân tích dữ liệu bệnh nhân sẽ được tự động hóa, giúp giảm thiểu sai sót do con người và tăng tính nhất quán của thông tin.
1	Ngành bán lẻ đang chứng kiến sự chuyển đổi mạnh mẽ nhờ ứng dụng công nghệ số. Theo báo cáo của Nielsen năm 2024, hơn 68% doanh nghiệp bán lẻ tại Việt Nam đã bắt đầu áp dụng trí tuệ nhân tạo để tối ưu hóa hoạt động kinh doanh. Thị giác máy tính kết hợp CNN cho phép tự động nhận diện hành vi khách hàng, quản lý hàng hóa và giảm thất thoát. Nghiên cứu này tập trung vào việc xây dựng hệ thống giám sát thông minh trong siêu thị, sử dụng camera và CNN để nhận diện sản phẩm, kết hợp cơ sở dữ liệu nhằm phân tích hành vi mua sắm.
1	Hệ thống được triển khai tại một siêu thị với diện tích 2.500 m², sử dụng 40 camera độ phân giải cao. Tập dữ liệu gồm 85.000 hình ảnh sản phẩm thuộc 120 loại hàng hóa khác nhau. Mô hình CNN dựa trên kiến trúc EfficientNet-B4 được huấn luyện trong 180 epoch, đạt độ chính xác nhận diện sản phẩm 93,7%. Cơ sở dữ liệu MongoDB được sử dụng để lưu trữ thông tin sản phẩm, lượt tương tác và thời gian khách hàng dừng lại tại mỗi quầy. Dữ liệu được cập nhật theo thời gian thực với độ trễ dưới 1 giây.
1	Sau 3 tháng vận hành, hệ thống giúp giảm tỷ lệ thất thoát hàng hóa từ 2,9% xuống còn 0,9%. Doanh thu trung bình tăng 17% nhờ tối ưu cách trưng bày sản phẩm dựa trên dữ liệu hành vi khách hàng. Thời gian kiểm kê hàng hóa giảm 60% so với phương pháp thủ công. Phân tích dữ liệu cho thấy 54% khách hàng có xu hướng dừng lại lâu hơn tại các quầy được tối ưu bằng gợi ý từ hệ thống thị giác máy tính, chứng minh hiệu quả rõ rệt trong việc nâng cao trải nghiệm mua sắm.
1	Ngoài những lợi ích đã được chứng minh, việc kết hợp thị giác máy tính, CNN và hệ thống cơ sở dữ liệu còn tạo nền tảng cho quá trình chuyển đổi số toàn diện trong ngành bán lẻ. Dữ liệu thu thập từ hành vi mua sắm của khách hàng có thể được phân tích theo thời gian thực, từ đó hỗ trợ doanh nghiệp tối ưu chiến lược trưng bày, quản lý tồn kho và cá nhân hóa trải nghiệm khách hàng. Hơn nữa, khi được tích hợp với các công nghệ như trí tuệ nhân tạo dự đoán nhu cầu và thanh toán tự động, hệ thống bán lẻ không thu ngân sẽ giúp rút ngắn thời gian mua sắm, giảm thiểu sai sót trong thanh toán và nâng cao mức độ hài lòng của khách hàng.
1	Trong bối cảnh hệ thống y tế đang đối mặt với sự thiếu hụt nhân lực chuyên khoa, việc ứng dụng trí tuệ nhân tạo, đặc biệt là thị giác máy tính, đã trở thành một giải pháp đột phá. Ung thư da và các bệnh lý da liễu mãn tính thường có những biểu hiện phức tạp trên bề mặt biểu mô, gây khó khăn cho việc quan sát bằng mắt thường ở giai đoạn khởi phát. Nghiên cứu này tập trung vào việc xây dựng một mô hình dựa trên mạng nơ-ron tích chập (CNN) để phân tích các đặc điểm hình thái học của thương tổn.
1	Mục tiêu là tạo ra một công cụ hỗ trợ các bác sĩ sàng lọc nhanh chóng, giảm thiểu sai sót chủ quan và tối ưu hóa quy trình tiếp nhận bệnh nhân tại các bệnh viện tuyến đầu, nơi lưu lượng người khám bệnh luôn ở mức quá tải. Nghiên cứu sử dụng tập dữ liệu chuẩn hóa từ kho lưu trữ ISIC (International Skin Imaging Collaboration) với hơn 25.000 hình ảnh lâm sàng được gắn nhãn bởi các chuyên gia. Chúng tôi triển khai kiến trúc ResNet-50 kết hợp với kỹ thuật Transfer Learning để tận dụng các đặc điểm đã học từ tập dữ liệu ImageNet. Quá trình xử lý dữ liệu bao gồm các bước chuẩn hóa kích thước ảnh về 224x224 pixels, cân bằng trắng và tăng cường dữ liệu (Data Augmentation) bằng cách xoay ảnh, lật ảnh và điều chỉnh độ tương phản.
1	Việc sử dụng hàm mất mát Categorical Cross-entropy kết hợp với bộ tối ưu hóa Adam giúp mô hình hội tụ nhanh hơn. Hệ thống được lập trình trên nền tảng Python, sử dụng thư viện TensorFlow và PyTorch để đảm bảo tính linh hoạt trong việc tinh chỉnh các tham số lớp tích chập. Sau 100 chu kỳ huấn luyện (epochs), mô hình đã đạt được những chỉ số ấn tượng với độ chính xác (Accuracy) lên đến 94,8% trên tập kiểm tra độc lập. Đặc biệt, độ nhạy (Sensitivity) đối với các ca u hắc tố ác tính đạt 91,2%, cao hơn khoảng 5% so với mức trung bình của các bác sĩ nội trú có ít hơn 3 năm kinh nghiệm trong các bài kiểm tra mù.
1	Sự bùng nổ của các đô thị thông minh dẫn đến nhu cầu cấp thiết về một hệ thống giám sát giao thông thời gian thực có khả năng xử lý hàng triệu bản ghi mỗi phút. Thách thức lớn nhất hiện nay không chỉ nằm ở việc nhận diện phương tiện qua camera mà còn ở cách thức lập trình cơ sở dữ liệu để lưu trữ và truy xuất thông tin một cách hiệu quả. Các hệ thống truyền thống thường gặp hiện tượng thắt nút cổ chai khi số lượng camera tăng lên vượt mức 5.000 thiết bị. Nghiên cứu này đề xuất một kiến trúc cơ sở dữ liệu lai (Hybrid Database) kết hợp giữa SQL để quản lý thông tin phương tiện cố định và NoSQL cho các luồng dữ liệu phi cấu trúc từ video.
1	Việc tối ưu hóa này giúp giảm độ trễ truy vấn và đảm bảo tính nhất quán của dữ liệu trong điều kiện tải cao. Để giải quyết bài toán hiệu năng, chúng tôi áp dụng kỹ thuật phân mảnh dữ liệu (Sharding) theo khu vực địa lý và lập chỉ mục (Indexing) dựa trên thời gian thực. Trong phần lập trình cơ sở dữ liệu, các thủ tục lưu trữ (Stored Procedures) được viết bằng PL/SQL để tự động hóa việc tổng hợp dữ liệu theo giờ, giúp giảm tải cho tầng ứng dụng. Chúng tôi cũng triển khai một lớp đệm (Caching layer) sử dụng Redis để lưu trữ các trạng thái giao thông tại các nút giao trọng điểm. Điều này cho phép hệ thống phản hồi các truy vấn về mật độ giao thông chỉ trong vòng dưới 100ms.
1	Ngoài ra, việc sử dụng các cấu trúc dữ liệu tối ưu giúp giảm dung lượng lưu trữ thực tế xuống 30% so với phương pháp lưu trữ thông thường, nhờ vào các thuật toán nén dữ liệu nhị phân chuyên dụng. Thử nghiệm thực tế tại một khu vực mô phỏng với 1.200 nút giao thông cho thấy hệ thống có khả năng xử lý đồng thời 8.500 truy vấn mỗi giây mà không xảy ra hiện tượng treo máy. Số liệu từ thực nghiệm chỉ ra rằng việc áp dụng cơ sở dữ liệu tối ưu giúp giảm thời gian chờ đợi tại các đèn tín hiệu khoảng 18% thông qua các thuật toán điều phối tự động dựa trên dữ liệu lịch sử.
1	Trong kỷ nguyên nông nghiệp 4.0, việc theo dõi sức khỏe cây trồng theo cách thủ công đã trở nên lạc hậu và kém hiệu quả đối với các trang trại có quy mô hàng trăm héc-ta. Việc sử dụng thị giác máy tính (Computer Vision) kết hợp với các thiết bị bay không người lái (UAV) mở ra một hướng đi mới trong việc phát hiện sớm sâu bệnh và thiếu hụt dinh dưỡng. Bài nghiên cứu này tập trung vào việc phát triển một hệ thống tự động có khả năng phân loại các giai đoạn phát triển của cây trồng và nhận diện các dấu hiệu bất thường trên lá.
1	Bằng cách áp dụng các thuật toán học sâu, chúng tôi hướng tới việc giảm thiểu lượng thuốc trừ sâu sử dụng đại trà, từ đó bảo vệ môi trường và nâng cao chất lượng nông sản xuất khẩu theo các tiêu chuẩn quốc tế khắt khe. Chúng tôi đã xây dựng một bộ dữ liệu riêng biệt gồm 40.000 hình ảnh độ phân giải cao về các loại cây công nghiệp phổ biến như cà phê và hồ tiêu trong nhiều điều kiện ánh sáng khác nhau. Mô hình được lựa chọn là EfficientNet-B0 do có sự cân bằng tốt giữa độ chính xác và tài nguyên tính toán, phù hợp để chạy trên các thiết bị nhúng (Edge Computing) tích hợp trên drone.
1	"Trong quá trình huấn luyện, chúng tôi áp dụng kỹ thuật ""Label Smoothing"" để giảm thiểu hiện tượng Overfitting và sử dụng hàm kích hoạt Mish để cải thiện khả năng học của các lớp sâu. Quá trình lập trình được thực hiện trên môi trường Ubuntu với card đồ họa NVIDIA RTX 3090, giúp rút ngắn thời gian huấn luyện từ 15 ngày xuống còn 48 giờ cho toàn bộ tập dữ liệu khổng lồ này. Kết quả triển khai thực tế tại các vùng nguyên liệu cho thấy mô hình đạt điểm F1-score là 0,92 trong việc nhận diện bệnh rỉ sắt trên lá cà phê."
1	Số liệu thống kê sau một vụ mùa áp dụng hệ thống cho thấy năng suất cây trồng tăng thêm 12,5% nhờ vào việc can thiệp đúng lúc tại các điểm dịch bệnh mới phát sinh. Quan trọng hơn, lượng thuốc bảo vệ thực vật sử dụng đã giảm tới 25%, giúp tiết kiệm chi phí sản xuất trung bình khoảng 15 triệu đồng trên mỗi héc-ta cho nông dân. Nghiên cứu này không chỉ mang lại giá trị về mặt kỹ thuật mà còn có ý nghĩa kinh tế - xã hội to lớn, góp phần thúc đẩy quá trình chuyển đổi số trong nông nghiệp và tạo ra các sản phẩm nông sản sạch, an toàn cho người tiêu dùng.
1	Trong kỷ nguyên Công nghiệp 4.0, việc duy trì tiêu chuẩn chất lượng khắt khe trong sản xuất linh kiện điện tử là yếu tố sống còn đối với các doanh nghiệp công nghệ. Các lỗi vi mô như vết nứt trên bảng mạch in (PCB), mối hàn không đủ nhiệt, hoặc sự thiếu hụt các linh kiện dán bề mặt (SMD) thường rất khó nhận biết bằng mắt thường. Kiểm tra thủ công không chỉ gây ra sự chậm trễ trong dây chuyền mà còn tiềm ẩn rủi ro sai sót do sự mệt mỏi và tính chủ quan của con người, dẫn đến tỷ lệ hàng lỗi (Defect Rate) có thể lên tới 5-7% ở các khâu kiểm soát cuối cùng nếu không có sự can thiệp của máy móc.
1	Nghiên cứu này đề xuất giải pháp ứng dụng thị giác máy tính kết hợp với trí tuệ nhân tạo để tự động hóa quy trình hậu kiểm. Việc triển khai hệ thống này nhằm mục tiêu tối ưu hóa hiệu suất vận hành, đảm bảo độ chính xác tuyệt đối và giảm thiểu chi phí bảo hành phát sinh do sản phẩm lỗi lọt ra thị trường tiêu dùng. Hệ thống được thiết kế dựa trên thuật toán YOLOv8 (You Only Look Once) phiên bản mới nhất, tối ưu cho việc nhận diện đa đối tượng trong thời gian thực với độ trễ cực thấp. Quy trình bắt đầu bằng việc thu thập hình ảnh từ camera công nghiệp có độ phân giải 4K, được chiếu sáng bởi hệ thống đèn LED vòng chuyên dụng để loại bỏ bóng đổ trên bề mặt linh kiện.
1	"Dữ liệu hình ảnh sau đó được xử lý qua các lớp tích chập của mạng CNN để trích xuất đặc trưng hình thái của các lỗi điển hình như hở mạch hay đoản mạch. Ở tầng lưu trữ, chúng tôi lập trình cơ sở dữ liệu sử dụng PostgreSQL với các bảng được phân vùng (Partitioning) theo mã lô sản xuất và mốc thời gian thực. Điều này cho phép hệ thống truy xuất lịch sử lỗi của từng dây chuyền chỉ trong vài mili giây. Việc tích hợp các ""Trigger"" trong SQL giúp tự động gửi cảnh báo đến bộ phận kỹ thuật ngay khi tỷ lệ lỗi vượt ngưỡng cho phép là 0,2%, đảm bảo tính kịp thời trong phản ứng sản xuất."
1	Kết quả thử nghiệm trên dây chuyền sản xuất thực tế với 50.000 sản phẩm mẫu cho thấy mô hình đạt độ chính xác trung bình (mAP) là 98,5% đối với các loại lỗi phổ biến nhất. Tốc độ kiểm tra được cải thiện đáng kể, giảm từ mức trung bình 4,2 giây mỗi sản phẩm khi kiểm tra bằng tay xuống còn 0,08 giây với hệ thống tự động hóa hoàn toàn. Về mặt kinh tế, doanh nghiệp có thể tiết kiệm được khoảng 35% chi phí nhân công cho khâu kiểm tra và giảm tỷ lệ sản phẩm lỗi bị trả về từ khách hàng xuống dưới mức 0,1%.
1	Ngoài ra, kho dữ liệu khổng lồ thu thập được từ hệ thống còn cung cấp những thông tin giá trị giúp các kỹ sư phân tích nguyên nhân gốc rễ của lỗi (Root Cause Analysis), từ đó tinh chỉnh quy trình sản xuất đầu vào một cách khoa học. Thông qua việc khai thác dữ liệu lịch sử, dữ liệu cảm biến theo thời gian thực và nhật ký vận hành thiết bị, các mô hình Deep Learning có khả năng phát hiện những mối quan hệ tiềm ẩn mà phương pháp phân tích truyền thống khó nhận ra.
1	Điều này cho phép doanh nghiệp chủ động dự đoán rủi ro, giảm thiểu thời gian dừng máy và tối ưu chi phí bảo trì. Bên cạnh đó, quản trị dữ liệu thông minh giúp đảm bảo tính toàn vẹn, nhất quán và khả năng truy xuất nhanh chóng của dữ liệu, tạo nền tảng vững chắc cho các quyết định chiến lược. Nghiên cứu khẳng định rằng việc kết hợp chặt chẽ giữa Deep Learning và quản trị dữ liệu thông minh không chỉ nâng cao chất lượng sản phẩm mà còn gia tăng năng suất, khả năng thích ứng và năng lực cạnh tranh toàn cầu cho ngành công nghiệp điện tử trong bối cảnh chuyển đổi số hiện nay.
0	Nấm từ lâu đã được biết đến như một nguồn thực phẩm giàu dinh dưỡng, giàu chất chống oxi hóa và chất xơ. Đặc biệt là một nguồn nhiều vitamin B, selen, kẽm và đồng - các chất quan trọng trong việc sản xuất năng lượng trong tế bào, cần thiết cho một hệ thống miễn dịch mạnh mẽ. Đối với con người, nấm trở thành món ăn ưa chuộng trên toàn thế giới. Tuy nhiên, việc tiêu thụ nhầm nấm độc có thể dẫn đến hậu quả nghiêm trọng, bao gồm buồn nôn, nôn mửa, suy nhược thần kinh, rối loạn, thiếu máu cấp tính, thậm chí có thể dẫn tới tử vong nếu không cấp cứu kịp thời.
0	Nhưng không phải ai cũng biết cách nhận biết nấm độc, chất độc và nấm ăn được, và rất khó để chúng ta phân biệt nấm ăn được hay nấm không ăn được, từ hình dáng bên ngoài giống nhau của chúng. Bài viết này nhằm giải quyết vấn đề bằng cách sử dụng mô hình học máy Faster Region-based Convolutional Neural Network (Faster R-CNN) để phân loại nấm ăn được và nấm độc. Mô hình Faster R-CNN được huấn luyện trên tập dữ liệu hình ảnh nấm đa dạng, tập trung vào các đặc điểm hình dạng, màu sắc và kết cấu. Sau quá trình huấn luyện, mô hình đã đạt độ chính xác ấn tượng lên đến 99,10% trong việc phân loại nấm.
0	Kết quả này chứng minh tiềm năng của mô hình Faster R-CNN trong việc hỗ trợ người dùng nhận diện nấm an toàn, góp phần giảm thiểu nguy cơ ngộ độc và tử vong do nấm độc. Nấm có tên khoa học là Fungi tiếng Anh là Mushroom, bao gồm các sinh vật nhân chuẩn dị dưỡng có tế bào cấu tạo bởi Kitin. Chúng không phải thực vật, mà thuộc giới riêng biệt. Nấm có vai trò quan trọng trong hệ sinh thái, tham gia vào quá trình phân hủy các chất hữu cơ và tạo nên mối quan hệ cộng sinh với nhiều loại thực vật.
0	Giới nấm bao gồm rất nhiều loại khác nhau từ nấm men đơn bào đến nấm lớn như nấm rơm, nấm mỡ, nấm hương…, cho đến gần đây nhiều loại nấm đã được miêu tả dựa trên những đặc điểm hình thái, như kích cỡ và hình dạng các bào tử hay quả thể (Hibbett, Binder, Bischoff và cs., 2007). Sinh sản của nấm thường thông qua bào tử, được tạo ra trên các cấu trúc đặc biệt hoặc quả thể (như tai nấm, mũ nấm). Một số loài mất khả năng tạo ra cấu trúc sinh sản chuyên biệt và nhân lên bằng sinh sản sinh dưỡng.
0	Các loài phổ biến nhất được ăn như rau trong nhiều bữa ăn trên khắp thế giới, một số loại nấm phổ biến nhất bao gồm: nấm mỡ, nấm rơm, nấm hương, nấm kim châm, nấm bào ngư, nấm dai, nấm đùi gà…, (Nguyễn Lân Dũng, 2004; Friedman, 2015). Nấm có khả năng thích nghi đáng kinh ngạc, chúng phân bố trên toàn thế giới và phát triển ở nhiều loại môi trường sống khác nhau, bao gồm cả sa mạc, những nơi có nồng độ muối cao, bức xạ ion hóa (Vaupotic và cs., 2008; Dadachova và cs., 2007) cũng như trầm tích biển sâu (Raghukumar và cs., 1998).
0	Dựa vào tỉ lệ giữa số loài nấm với số loài thực vật ở trong cùng một môi trường, người ta ước tính giới nấm có khoảng 1,5 triệu loài (Hawksworth, 2006), trong đó có khoảng hơn 300 loài nấm dược liệu được ghi nhận trong tổng số hơn 2000 loài nấm ăn được được xác định, trong đó khoảng 20 loài có thể được nuôi trồng: nấm mỡ, nấm rơm, nấm hương, nấm sò, mộc nhĩ, nấm kim châm…, (Friedman, 2015; Nguyễn Lân Dũng, 2004). Các loài nấm quả thể được biết đến với hai dạng: nấm ăn được và nấm không ăn được, thông thường ta có thể nhận dạng nấm ăn được và không ăn được bằng cách quan sát hình thái bên ngoài của nấm (Vũ Văn Hùng và cs., 2023).
0	Nấm ăn được: thường không có màu sắc sặc sỡ, không có bao gốc nấm và vòng cuống nấm, không có độc tố hoặc rất ít độc tố không gây hại, nấm độc: thường có màu sắc sặc sỡ hơn, có bao gốc nấm và có thêm vòng cuống nấm bao quanh thân nấm ở dưới phiến mũ nấm rõ ràng, độc tố từ ít đến cao vô cùng, sẽ gây hại nghiêm (Vũ Văn Hùng và cs., 2023). Tuy nhiên, việc nhận dạng nấm ăn được và nấm độc chỉ dựa vào hình thái bên ngoài là không đủ và có thể gây nguy hiểm. Có một số loại nấm không ăn được rất giống nấm ăn được, khiến việc phân biệt bằng mắt thường trở nên khó khăn.
0	Ví dụ, nấm rơm có thể bị nhầm với một số loại nấm độc, đặc biệt là trong giai đoạn nấm còn non và chưa nở hoàn toàn như nấm độc tán trắng, nấm mũ tử thần. Để giải quyết vấn đề này và giảm thiểu số ca tử vong do ăn phải nấm độc, bài viết này đề xuất sử dụng một mô hình cải tiến dựa trên mạng nơ-ron tích chập Faster Region-based Convolutional Neural Networks (Faster R-CNN). Mô hình này được kỳ vọng sẽ tăng tốc độ và duy trì độ chính xác cao trong việc phân loại nấm không ăn được và nấm ăn được. Các nghiên cứu về sử dụng mô hình Học sâu được sử dụng cho việc nhận dạng nấm được mô tả trong bảng 1.
0	Bài viết này trình bày một chiến lược nhận dạng đối tượng để nhận dạng nấm ăn được và không ăn được từ hình ảnh trong bộ dữ liệu huấn luyện. Đầu ra yêu cầu là xác định vị trí của nấm ăn được trong ảnh và liệu chúng có thực sự ăn được hay không. Đầu vào có thể là hình ảnh hoặc video chứa một hoặc nhiều đối tượng, từ đó các đặc trưng có thể được xuất ra. Bằng cách xử lý các hình ảnh đầu vào với nhiều lớp tích chập (convolutional layers) và lớp tổng hợp (pooling layers), mạng nơ-ron tích chập (CNN) có thể trích xuất các đặc trưng hình ảnh quan trọng, chẳn hạn như hình dáng, màu sắc, kết cấu.
0	Các đặc trưng này được học từ dữ liệu và có thể được sử dụng để phân loại hoặc nhận dạng các đối tượng trong ảnh. Trong trường hợp phân loại nấm ăn được và nấm không ăn được, CNN có thể học cách nhận biết các đặc điểm đặc trưng của các loại nấm (mũ nấm, phiến nấm, màu sắc), sau khi được huấn luyện trên tập dữ liệu nấm đã được gắn nhãn, CNN có thể dự đoán xem một hình ảnh nấm mới là nấm ăn được hay nấm độc với độ chính xác cao. Fast R-CNN (Shaoqing Ren và cs., 2016) được tạo ra từ điều này, bao gồm hai giai đoạn chính.
0	Tìm kiếm bộ lọc được sử dụng trước tiên để tìm các hộp giới hạn phù hợp nhất (được gọi là Vùng quan tâm hoặc RoI), sau đó CNN được sử dụng để trích xuất các hộp giới hạn. Để tìm các khu vực được đề xuất, R-CNN được kết hợp với một chiến lược tìm kiếm chọn lọc, đóng vai trò là nền tảng cho Fast R-CNN. (Inkyu Sa và cs., 2016) đề xuất sử dụng phương pháp mạng nơ-ron tích chập để huấn luyện mô hình nhận dạng trái cây trong “Hệ thống phát hiện trái cây sử dụng mạng nơ-ron sâu”.
0	(Byoungjun Kim và cs., 2021) đề xuất một phương pháp cải tiến dựa trên thị giác để phát hiện bệnh Dâu tây bằng cách sử dụng mạng nơ-ron sâu (DNN) có khả năng được tích hợp vào hệ thống robot tự động. Jose Luis Rojas-Aranda và cs. (2020) đã trình bày một phương pháp phân loại hình ảnh, dựa trên mạng nơ-ron tích chập (CNN) nhẹ để tăng độ chính xác phân loại, các đặc trưng đầu vào khác nhau được thêm vào kiến trúc CNN. (Changqing Cao và cs., 2019) đề xuất một thuật toán cải tiến dựa trên CNN dựa trên vùng nhanh hơn (Faster R-CNN) để phát hiện vật thể nhỏ.
0	Tang Yunchao và cs. (2020) đề xuất một phương pháp đào tạo dựa trên mạng nơ-ron sâu để phát hiện trái cây từ ảnh. Muresan và Oltean (2018) đề xuất sử dụng mạng nơ-ron tích chập (CNN) với thị giác máy tính để tạo ra một hệ thống nhận dạng trái cây. Có rất nhiều nghiên cứu sử dụng mô hình nhận dạng CNN và các mô hình cải tiến được đề xuất để áp dụng trong phát hiện và nhận dạng hình ảnh kỹ thuật số (Yuting Zhang và cs., 2015; Nguyễn Hà Huy Cường và cs., 2021). Việc nhận dạng và phân loại nấm là một công việc khó khăn thử thách, cần phải có một công cụ nhận dạng hiệu quả để nhận dạng nấm độc và không độc.
0	Trong bài viết này, đề xuất một giải pháp công nghệ có sử dụng Faster R-CNN bổ sung tham khảo để xây dựng mô hình dự đoán nấm độc hay nấm ăn được. Mô hình phân loại được xây dựng bằng Faster R-CNN có thể phân loại trong hoàn cảnh thực tế, chẳng hạn như khi thực hiện các thử nghiệm về nông nghiệp thì thủ tục phân loại sẽ được thực hiện trên bộ phân loại đã được huấn luyện để tìm nấm độc hay không độc. Grishick (2021), Shaoqing Ren và Kaiming He (2015) bổ sung tham khảo để xây dựng mô hình dự đoán nấm độc hay nấm ăn được.
0	Trong bài viết này, toàn bộ quá trình được chia thành ba giai đoạn: i) tiền xử lý dữ liệu; ii) huấn luyện mô hình; iii) đánh giá kết quả, sơ đồ được hiển thị trong hình bên dưới. i) Tiền xử lý dữ liệu: Giai đoạn này bao gồm các bước như thu thập dữ liệu hình ảnh nấm, làm sạch dữ liệu, chuẩn hóa kích thước ảnh và tăng cường dữ liệu để làm phong phú thêm tập dữ liệu huấn luyện. ii) Huấn luyện mô hình: Ở giai đoạn này, mô hình Faster R-CNN cải tiến sẽ được huấn luyện trên tập dữ liệu đã qua tiền xử lý.
0	Quá trình huấn luyện sẽ điều chỉnh các tham số của mô hình để nó có thể học cách phân biệt các đặc trưng của nấm ăn được và nấm độc. iii) Đánh giá kết quả: Sau khi huấn luyện, mô hình sẽ được đánh giá trên một tập dữ liệu riêng biệt để kiểm tra hiệu suất và độ chính xác của nó trong việc phân loại nấm. Trong giai đoạn này, tôi đã thu thập một lượng lớn hình ảnh nấm ăn được và nấm không ăn được từ nguồn (Marcos Volpato, 2021), tập dữ liệu có hơn 3.000 hình ảnh của nấm ăn được và không ăn được: gồm 1.181 nấm ăn được và 2.220 nấm không ăn được.
0	Để tăng cường khả năng khái quát hóa của mô hình, nghiên cứu này đã áp dụng các kỹ thuật tăng cường dữ liệu như xoay ảnh, cắt ảnh, làm sắc nét, điều chỉnh độ tương phản và độ sáng. Công cụ Roboflow Annotate đã được sử dụng để tự động gán nhãn cho các hình ảnh, giúp tiết kiệm thời gian và công sức so với việc gán nhãn thủ công. Công cụ Roboflow Annotate được sử dụng để tự động gắn nhãn dữ liệu, giúp đơn giản hóa việc xử lý ảnh thô thành mô hình thị giác máy tính để triển khai đào tạo và dùng đặt tên cho các ảnh thu thập được. Tập dữ liệu nấm được chia thành ba phần: tập huấn luyện, tập xác định và tập kiểm tra, với các tỷ lệ lần lượt là 0,8; 0,1 và 0,1.
0	Mục tiêu chính của học máy là xây dựng mô hình có khả năng khái quát hóa tốt, tức là hoạt động tốt trên dữ liệu mới chưa từng gặp trong quá trình huấn luyện. Việc chia dữ liệu thành các tập riêng biệt giúp đánh giá khả năng này một cách khách quan hơn. Tất cả hình ảnh đã được xử lý để loại bỏ dữ liệu sai, sau đó được xác định và lưu vào các thư mục khác nhau. Đây là một tỷ lệ phân chia khá phổ biến và thường được sử dụng khi kích thước tập dữ liệu đủ lớn. Tỷ lệ này đảm bảo tập huấn luyện đủ lớn để mô hình học được các đặc trưng quan trọng, đồng thời tập xác thực và tập kiểm tra đủ lớn để đánh giá mô hình một cách đáng tin cậy.
0	Việc lựa chọn tỷ lệ này dựa trên kinh nghiệm thực tế và các nghiên cứu trước đây trong lĩnh vực học máy. Tỷ lệ 80% cho tập huấn luyện đảm bảo mô hình có đủ dữ liệu để học các đặc trưng quan trọng. Tập xác thực (10%) giúp đánh giá mô hình trong quá trình huấn luyện và điều chỉnh các siêu tham số. Cuối cùng, tập kiểm tra (10%) được sử dụng để đánh giá hiệu suất cuối cùng của mô hình trên dữ liệu chưa từng thấy. Tập huấn luyện được sử dụng để phù hợp với mô hình, tập xác thực được sử dụng để điều chỉnh các tham số của mô hình và đánh giá sơ bộ hiệu năng của mô hình, bộ kiểm tra cuối cùng được sử dụng để xác nhận khả năng khái quát của mô hình.
0	Công cụ này có thể tạo ra một hình chữ nhật xung quanh nấm có thể nhìn thấy được. Kết quả không chỉ thu được các hộp giới hạn mà còn có các pixel của nấm dựa trên đặc điểm hình thái của chúng như: vảy nấm, mũ nấm, phiến nấm, màu sắc... Đầu ra cuối cùng sẽ là tập dữ liệu có chú thích được lưu dưới dạng Microsoft COCO từ Roboflow. Tiếp theo sẽ thay đổi kích thước của ảnh thành 224×224×3 làm đầu vào của mô hình. Mục tiêu của nghiên cứu này là trình bày một phương pháp kỹ thuật hiệu quả để nhận dạng nấm, đặc biệt tập trung vào việc sử dụng và cải tiến mô hình Faster R-CNN nhằm tăng tốc độ phát hiện đối tượng và đảm bảo độ chính xác cao trong quá trình phân loại nấm ăn được và nấm độc.
0	Faster R-CNN là một mô hình học sâu mạnh mẽ trong lĩnh vực thị giác máy tính, đặc biệt là trong bài toán phát hiện đối tượng. Mô hình này sử dụng một mạng lưới đề xuất vùng (Region Proposal Network - RPN) để tạo ra các vùng ứng viên có khả năng chứa đối tượng, sau đó sử dụng một mạng phân loại để xác định xem vùng đó có chứa đối tượng hay không và thuộc lớp nào. Hiệu suất của phương pháp đề xuất được đánh giá bằng cách sử dụng độ chính xác phân loại (Classification Accuracy) làm thước đo chính. Độ chính xác phân loại được định nghĩa là tỷ lệ phần trăm số lượng hình ảnh nấm được phân loại chính xác trên tổng số hình ảnh trong tập dữ liệu kiểm tra.
0	Trong đó: TP (True Positive): Số điểm ảnh được phân loại đúng là dương tính (ví dụ: điểm ảnh thuộc về một đối tượng). FP (False Positive): Số điểm ảnh bị phân loại sai là dương tính (ví dụ: điểm ảnh nền được phân loại là một phần của đối tượng). FN (False Negative): Số điểm ảnh bị phân loại sai là âm tính (ví dụ: điểm ảnh thuộc về một đối tượng nhưng bị phân loại là nền). Công thức Accuracy là cách tính độ chính xác trong các bài toán phân loại của học máy (Machine learning). Cụ thể, độ chính xác được tính bằng tỷ lệ giữa số lượng mẫu được phân loại đúng (correct classification) và tổng số lượng mẫu trong tập dữ liệu (the number of entire instance).
0	IoU: công thức tính Intersection over Union (IoU), một độ đo được sử dụng để đánh giá độ chính xác của các mô hình trong các tác vụ thị giác máy tính như phát hiện đối tượng và phân đoạn ngữ nghĩa. Tỷ lệ giữa diện tích giao nhau (Area of Overlap) của vùng được dự đoán và vùng ground truth (nhãn thực tế), chia cho diện tích hợp (Area of Union) của cả hai vùng. Average Precision (AP) là diện tích của đồ thị bao gồm Precisions, Recall và mAP là diện tích trung bình của toàn bộ đồ thị. Nghiên cứu này đã trình bày một hệ thống nhận dạng nấm dựa trên mô hình học máy Faster R-CNN.
0	Hệ thống này đã được huấn luyện trên một tập dữ liệu ảnh nấm đa dạng và đã thể hiện khả năng phân tích, đánh giá hình ảnh nấm một cách hiệu quả. Kết quả thực nghiệm cho thấy mô hình có thể hỗ trợ mọi người trong việc phân loại nấm ăn được và nấm độc, góp phần giảm thiểu nguy cơ ngộ độc nấm và mang lại lợi ích thiết thực cho sức khỏe cộng đồng. Nghiên cứu này đã chỉ ra rằng quá trình trích xuất đặc trưng có ảnh hưởng đáng kể đến độ chính xác của mô hình nhận dạng nấm. Việc lựa chọn mô hình trích xuất đặc trưng phù hợp là rất quan trọng để đạt được hiệu suất tối ưu.
0	Tuy nhiên, cần lưu ý rằng không phải tất cả các mô hình có độ chính xác cao đều phù hợp để sử dụng trong các ứng dụng thời gian thực, do sự đánh đổi giữa độ chính xác và tốc độ xử lý. Hình 6 trình bày một biểu đồ 2D so sánh độ chính xác và tốc độ của các mô hình trích xuất đặc trưng khác nhau. Kết quả cho thấy mô hình Faster R-CNN cải tiến đạt được độ chính xác cao nhất (99.60%) trong số các mô hình được đánh giá, đồng thời vẫn đảm bảo tốc độ xử lý đủ nhanh để đáp ứng yêu cầu của ứng dụng nhận dạng nấm thời gian thực. Biểu đồ 2D này thể hiện quá trình huấn luyện một mô hình học máy qua các epochs.
0	"Biểu đồ này cho thấy độ chính xác phân loại của mô hình tăng dần theo số lượng epochs. Ban đầu, độ chính xác tăng nhanh, sau đó chậm lại và đạt giá trị cuối cùng là 0.996. Điều này cho thấy mô hình đã học được cách phân loại dữ liệu một cách hiệu quả. Để đánh giá hiệu suất của mô hình, tôi đã sử dụng độ chính xác phân loại (Classification Accuracy) làm thước đo chính. Kết quả được trình bày dưới dạng các chỉ số đánh giá khác nhau như AP (Average Precision), AR (Average Recall) và F1-score. Có thể thấy rằng, đây là kết quả đánh giá cho thấy mô hình hoạt động khá tốt trong việc phát hiện các đối tượng thuộc lớp ""mushroom-rqSL"", đặc biệt là khi yêu cầu về độ chồng lấp (IoU) không quá cao."
0	Các kết quả của mô hình đề xuất còn được so sánh với kết quả dự đoán cùng chức năng như nghiên cứu của tác giả Devika và Asha Gowda Karegowda (2021), Norbert Kiss và László Czúni (2021), Orawan haowalit, Fuangfar Pensiri và Porawat Visutsak (2020), Wacharaphol Ketwongsa (2022). Các độ đo hiệu năng của mô hình đề xuất được liệt kê chi tiết ở bảng 2. Bằng cách sử dụng và cải tiến mô hình Faster R-CNN, hệ thống đã đạt được độ chính xác cao (99.10%) trong việc phân loại nấm ăn được và nấm độc, đồng thời có tốc độ xử lý nhanh (xấp xỉ 0.1 giây/ảnh).
0	Kết quả này cho thấy tiềm năng ứng dụng thực tiễn của hệ thống trong việc hỗ trợ nông dân và người tiêu dùng nhận biết nấm một cách an toàn và hiệu quả, góp phần giảm thiểu nguy cơ ngộ độc nấm. Mô hình Faster R-CNN có khả năng vượt trội trong việc phát hiện và phân loại đối tượng với độ chính xác cao, đặc biệt khi được cải tiến để phù hợp với bài toán cụ thể. Tuy nhiên, mô hình này cũng có một số hạn chế như yêu cầu tài nguyên tính toán lớn và có thể gặp khó khăn khi xử lý các hình ảnh có chất lượng thấp hoặc bị nhiễu.
1	Trí tuệ nhân tạo (Artificial Intelligence – AI) đang trở thành công cụ quan trọng trong lĩnh vực y tế hiện đại, đặc biệt trong phân tích và xử lý hình ảnh y khoa. Trong số các kỹ thuật AI, mạng nơ-ron tích chập (Convolutional Neural Network – CNN) cho thấy hiệu quả vượt trội nhờ khả năng tự động trích xuất đặc trưng từ dữ liệu hình ảnh. Theo thống kê của Tổ chức Y tế Thế giới, hơn 70% quyết định chẩn đoán lâm sàng hiện nay vẫn phụ thuộc nhiều vào đánh giá hình ảnh. Việc áp dụng CNN giúp giảm tải cho bác sĩ, đồng thời nâng cao độ chính xác trong phát hiện bệnh sớm.
1	Nghiên cứu này tập trung phân tích việc ứng dụng CNN trong chẩn đoán hình ảnh y tế, đặc biệt là X-quang và CT scan, qua đó đánh giá tác động của AI đối với chất lượng chăm sóc sức khỏe cộng đồng. Mô hình CNN được xây dựng gồm nhiều lớp tích chập, lớp gộp và lớp kết nối đầy đủ nhằm xử lý dữ liệu ảnh y tế có độ phân giải cao. Dữ liệu huấn luyện bao gồm hơn 15.000 ảnh X-quang phổi được thu thập từ các bệnh viện tuyến trung ương. Các ảnh được chuẩn hóa kích thước và tăng cường dữ liệu bằng kỹ thuật xoay, lật và điều chỉnh độ sáng. Quá trình huấn luyện diễn ra trong 50 epoch với tốc độ học 0.001.
1	Kết quả cho thấy mô hình CNN đạt độ chính xác trung bình 94.6% trong phát hiện viêm phổi, cao hơn 12% so với phương pháp trích xuất đặc trưng thủ công truyền thống. Điều này chứng minh khả năng học sâu của CNN trong việc nhận dạng các mẫu hình phức tạp. Kết quả thực nghiệm cho thấy việc tích hợp CNN vào hệ thống hỗ trợ chẩn đoán giúp rút ngắn thời gian phân tích ảnh từ trung bình 10 phút xuống còn dưới 30 giây cho mỗi ca bệnh. Tỷ lệ bỏ sót tổn thương giảm từ 8.3% xuống còn 1.7%, góp phần nâng cao độ an toàn trong điều trị. Ngoài ra, hệ thống có thể triển khai trên nền tảng đám mây, cho phép các bệnh viện tuyến dưới tiếp cận công nghệ chẩn đoán tiên tiến mà không cần đầu tư hạ tầng lớn.
1	Trong bối cảnh thương mại điện tử phát triển mạnh mẽ, việc hiểu và dự đoán hành vi tiêu dùng đóng vai trò then chốt trong chiến lược kinh doanh. Các thuật toán học máy như K-Nearest Neighbors (KNN) và Artificial Neural Network (ANN) được sử dụng rộng rãi để phân tích dữ liệu khách hàng. Theo báo cáo ngành bán lẻ, hơn 60% doanh nghiệp ứng dụng AI đã ghi nhận mức tăng trưởng doanh thu từ 15–30%. Nghiên cứu này tập trung đánh giá hiệu quả của KNN và ANN trong việc dự đoán xu hướng mua sắm, từ đó hỗ trợ doanh nghiệp cá nhân hóa dịch vụ và tối ưu chiến dịch tiếp thị.
1	Dữ liệu nghiên cứu gồm 120.000 giao dịch mua sắm trực tuyến trong vòng 12 tháng, bao gồm các thuộc tính như độ tuổi, giới tính, lịch sử mua hàng và tần suất truy cập. Thuật toán KNN được sử dụng để phân loại khách hàng dựa trên mức độ tương đồng hành vi, trong khi ANN đảm nhiệm việc dự đoán khả năng mua lại sản phẩm. Kết quả cho thấy KNN đạt độ chính xác 87.2% trong phân nhóm khách hàng, còn ANN đạt độ chính xác 91.8% trong dự đoán hành vi mua hàng. Việc kết hợp hai mô hình giúp cải thiện hiệu suất hệ thống tổng thể lên hơn 10% so với việc sử dụng từng thuật toán riêng lẻ.
1	Hệ thống dự đoán hành vi tiêu dùng dựa trên KNN và ANN giúp doanh nghiệp tăng tỷ lệ chuyển đổi đơn hàng từ 2.4% lên 4.1%. Đồng thời, chi phí quảng cáo không hiệu quả giảm gần 18% nhờ khả năng nhắm đúng đối tượng khách hàng tiềm năng. Ngoài ra, mô hình còn hỗ trợ xây dựng chương trình khuyến mãi cá nhân hóa, góp phần nâng cao trải nghiệm người dùng. Kết quả nghiên cứu cho thấy AI không chỉ mang lại lợi ích về mặt kỹ thuật mà còn tạo ra giá trị kinh tế bền vững cho doanh nghiệp trong môi trường cạnh tranh khốc liệt.
1	Giao thông đô thị đang đối mặt với nhiều thách thức như ùn tắc, tai nạn và ô nhiễm môi trường. Việc ứng dụng trí tuệ nhân tạo, đặc biệt là ANN và CNN, mở ra hướng tiếp cận mới trong xây dựng hệ thống giao thông thông minh. Theo thống kê, các thành phố áp dụng AI trong quản lý giao thông đã giảm trung bình 20–30% thời gian ùn tắc. Nghiên cứu này đề xuất mô hình kết hợp CNN để nhận dạng phương tiện và ANN để dự đoán mật độ giao thông theo thời gian thực, nhằm nâng cao hiệu quả điều hành đô thị.
1	CNN được sử dụng để xử lý dữ liệu hình ảnh từ hơn 200 camera giao thông, với nhiệm vụ nhận dạng xe máy, ô tô và xe tải. Dữ liệu đầu ra được đưa vào ANN để dự đoán lưu lượng giao thông trong 15–30 phút tiếp theo. Hệ thống được huấn luyện trên tập dữ liệu gồm hơn 2 triệu khung hình thu thập trong 6 tháng. Kết quả cho thấy độ chính xác nhận dạng phương tiện đạt 96.1%, trong khi ANN dự đoán mật độ giao thông với sai số trung bình dưới 5%. Điều này cho thấy tính khả thi cao trong môi trường thực tế.
1	Việc triển khai hệ thống AI trong giám sát giao thông giúp giảm thời gian chờ đèn đỏ trung bình 22%, đồng thời giảm số vụ tai nạn tại các nút giao trọng điểm khoảng 15%. Hệ thống còn hỗ trợ cơ quan quản lý đưa ra quyết định điều phối giao thông kịp thời trong giờ cao điểm. Ngoài ra, dữ liệu thu thập được có thể dùng cho quy hoạch đô thị dài hạn. Nghiên cứu khẳng định rằng sự kết hợp giữa ANN và CNN là giải pháp hiệu quả, góp phần xây dựng đô thị thông minh và nâng cao chất lượng cuộc sống người dân.
1	Trong kỷ nguyên số hóa y tế hiện nay, việc phân tích hình ảnh y khoa đóng vai trò then chốt trong việc phát hiện sớm các bệnh lý nguy hiểm, đặc biệt là ung thư phổi và các tổn thương não bộ. Mạng nơ-ron tích chập (Convolutional Neural Networks - CNN) đã nổi lên như một công cụ đắc lực, vượt trội hơn các phương pháp xử lý ảnh truyền thống nhờ khả năng tự động trích xuất các đặc trưng không gian từ dữ liệu đầu vào. Thay vì dựa vào các thuật toán thiết kế thủ công dễ sai sót, CNN sử dụng các lớp tích chập (convolutional layers) để quét và nhận diện các mẫu hình từ đơn giản như đường cạnh, góc khúc đến phức tạp như khối u hay sự biến đổi mô tế bào.
1	Việc áp dụng CNN không chỉ giúp giảm tải áp lực làm việc cho các bác sĩ chẩn đoán hình ảnh mà còn nâng cao độ chính xác, giảm thiểu rủi ro bỏ sót bệnh trong giai đoạn đầu, tạo tiền đề cho các phác đồ điều trị hiệu quả hơn. Để xây dựng một mô hình chẩn đoán hiệu quả, quá trình huấn luyện CNN đòi hỏi một tập dữ liệu khổng lồ và được gán nhãn chính xác bởi các chuyên gia y tế hàng đầu. Trong nghiên cứu này, chúng tôi tập trung vào kiến trúc ResNet-50, một biến thể mạnh mẽ của CNN, để xử lý tập dữ liệu gồm 112,000 ảnh X-quang lồng ngực được thu thập từ các bệnh viện lớn trong giai đoạn 2020-2024.
1	"Cơ chế ""skip connections"" của ResNet cho phép đào tạo các mạng sâu hơn mà không gặp phải vấn đề biến mất đạo hàm (vanishing gradient), giúp mô hình học được các đặc điểm tinh vi của nhu mô phổi. Quá trình này bao gồm việc tiền xử lý ảnh để loại bỏ nhiễu, chuẩn hóa độ sáng và tăng cường dữ liệu (data augmentation) bằng cách xoay, lật ảnh để mô hình có thể nhận diện bệnh lý ở nhiều góc độ khác nhau, đảm bảo tính tổng quát hóa cao khi áp dụng vào thực tế lâm sàng tại các cơ sở y tế địa phương."
1	Kết quả thực nghiệm sau 50 chu kỳ huấn luyện (epochs) cho thấy sự vượt trội đáng kể của mô hình CNN so với phương pháp chẩn đoán truyền thống của các bác sĩ có dưới 5 năm kinh nghiệm. Cụ thể, mô hình đạt độ chính xác (accuracy) lên tới 94.5% trong việc phân loại các nốt phổi lành tính và ác tính, trong khi tỷ lệ chẩn đoán đúng trung bình của con người trong cùng điều kiện chỉ đạt khoảng 88.2%. Đáng chú ý hơn, chỉ số độ nhạy (sensitivity) – khả năng phát hiện đúng người mắc bệnh – của mô hình đạt 96.8%, giúp giảm thiểu tối đa tỷ lệ âm tính giả (bỏ sót bệnh).
1	Trong bối cảnh cạnh tranh khốc liệt của thị trường thương mại điện tử, việc thấu hiểu và đáp ứng nhu cầu cá nhân của khách hàng là chìa khóa để duy trì lòng trung thành và gia tăng doanh thu. Thuật toán K-láng giềng gần nhất (K-Nearest Neighbors - KNN) được ứng dụng rộng rãi trong các hệ thống gợi ý (Recommender Systems) nhờ vào sự đơn giản nhưng vô cùng hiệu quả của phương pháp lọc cộng tác (Collaborative Filtering). Nguyên lý cốt lõi của KNN trong ngữ cảnh này là giả định rằng những người dùng có hành vi mua sắm tương tự nhau trong quá khứ sẽ có xu hướng thích những sản phẩm giống nhau trong tương lai.
1	Bằng cách tính toán khoảng cách vector giữa các người dùng hoặc sản phẩm trong không gian đa chiều, hệ thống có thể đề xuất các mặt hàng phù hợp nhất với thị hiếu của từng cá nhân, từ đó biến trải nghiệm mua sắm thụ động thành một hành trình khám phá đầy thú vị và mang tính cá nhân hóa cao độ. Việc triển khai KNN trên quy mô lớn đòi hỏi sự xử lý tinh tế đối với ma trận thưa (sparse matrix) của dữ liệu người dùng và sản phẩm, nơi mà hầu hết người dùng chỉ tương tác với một phần nhỏ trong hàng triệu sản phẩm có sẵn.
1	Nghiên cứu sử dụng tập dữ liệu hành vi của 500,000 người dùng trên một nền tảng bán lẻ trực tuyến, bao gồm lịch sử xem, thêm vào giỏ hàng và lịch sử giao dịch thực tế. Chúng tôi áp dụng thước đo khoảng cách Cosine Similarity thay vì khoảng cách Euclidean truyền thống để đo lường độ tương đồng, giúp loại bỏ sự sai lệch do độ dài của vector (số lượng sản phẩm đã mua) gây ra. Đồng thời, giá trị K tối ưu được xác định là K=50 sau quá trình tinh chỉnh (hyperparameter tuning), nghĩa là hệ thống sẽ xem xét 50 người dùng có hành vi giống nhất với khách hàng mục tiêu để đưa ra danh sách gợi ý sản phẩm tiềm năng nhất.
1	Kết quả triển khai thực tế trong vòng 6 tháng đã chứng minh sức mạnh của hệ thống gợi ý dựa trên KNN đối với các chỉ số kinh doanh quan trọng. Tỷ lệ chuyển đổi (Conversion Rate) của nhóm người dùng được tiếp cận với hệ thống gợi ý mới đã tăng 23.4% so với nhóm đối chứng không sử dụng thuật toán. Đặc biệt, giá trị đơn hàng trung bình (Average Order Value - AOV) cũng ghi nhận mức tăng trưởng 18.7%, do khách hàng có xu hướng mua thêm các sản phẩm đi kèm (cross-sell) được gợi ý chính xác. Hơn nữa, thời gian trung bình (Time on Site) mà khách hàng lưu lại trên ứng dụng đã tăng từ 4.5 phút lên 7.2 phút, cho thấy nội dung được cá nhân hóa đã giữ chân người dùng hiệu quả hơn.
1	Sự bùng nổ của thanh toán kỹ thuật số và ngân hàng trực tuyến đã mở ra nhiều tiện ích, nhưng đồng thời cũng tạo ra những lỗ hổng an ninh nghiêm trọng, dẫn đến sự gia tăng của các hoạt động gian lận tài chính tinh vi. Các hệ thống phát hiện gian lận dựa trên quy tắc (rule-based) truyền thống ngày càng trở nên lỗi thời vì chúng không thể bắt kịp với các thủ đoạn thay đổi liên tục của tội phạm mạng. Mạng nơ-ron nhân tạo (Artificial Neural Networks - ANN), với khả năng mô phỏng cấu trúc xử lý thông tin của não bộ con người, cung cấp một giải pháp linh hoạt và mạnh mẽ hơn.
1	ANN có khả năng học các mối quan hệ phi tuyến tính phức tạp giữa hàng trăm biến số đầu vào như thời gian giao dịch, địa chỉ IP, thói quen chi tiêu và số tiền chuyển khoản, từ đó phát hiện các điểm bất thường (anomalies) mà con người hoặc các thuật toán tuyến tính khó có thể nhận ra. Trong nghiên cứu này, chúng tôi thiết kế một mạng nơ-ron đa tầng (Multi-Layer Perceptron - MLP) bao gồm một lớp đầu vào với 45 biến đặc trưng, ba lớp ẩn (hidden layers) với số lượng nơ-ron lần lượt là 128, 64 và 32, cùng một lớp đầu ra sử dụng hàm kích hoạt Sigmoid để đưa ra xác suất gian lận từ 0 đến 1.
1	Dữ liệu huấn luyện bao gồm 2.5 triệu giao dịch thẻ tín dụng được thu thập từ một ngân hàng quốc tế, trong đó tỷ lệ giao dịch gian lận chiếm 0.17% (tập dữ liệu mất cân bằng nghiêm trọng). Để giải quyết vấn đề mất cân bằng dữ liệu, kỹ thuật SMOTE (Synthetic Minority Over-sampling Technique) đã được áp dụng để sinh ra các mẫu gian lận nhân tạo, giúp mô hình không bị thiên lệch về phía các giao dịch hợp lệ. Quá trình huấn luyện sử dụng thuật toán tối ưu hóa Adam và hàm mất mát Binary Cross-entropy để tinh chỉnh trọng số qua hàng nghìn vòng lặp, đảm bảo mô hình hội tụ ở điểm tối ưu toàn cục.
1	Khi áp dụng vào môi trường thực tế (production environment), hệ thống ANN đã thể hiện khả năng vượt trội trong việc bảo vệ tài sản của khách hàng và ngân hàng. Mô hình đạt độ chính xác tổng thể (Accuracy) là 99.92%, tuy nhiên con số quan trọng hơn là chỉ số F1-Score đạt 0.89 cho lớp gian lận, cao hơn đáng kể so với mức 0.65 của các mô hình Logistic Regression trước đó. Hệ thống đã chặn thành công 92% các giao dịch gian lận ngay tại thời điểm thực hiện (real-time), giúp ngân hàng ngăn chặn khoản thất thoát ước tính lên tới 5.4 triệu USD chỉ trong một quý vận hành.
1	Tại các đô thị lớn với mật độ dân cư cao, việc giám sát và xử lý vi phạm giao thông bằng sức người đang gặp phải những giới hạn nghiêm trọng về khả năng bao quát và tính liên tục. Các hệ thống camera giám sát truyền thống thường chỉ ghi hình thụ động, đòi hỏi nhân viên an ninh phải xem lại hàng giờ video để phát hiện lỗi, dẫn đến độ trễ lớn và bỏ sót nhiều hành vi vi phạm như vượt đèn đỏ, đi sai làn đường hay không đội mũ bảo hiểm. Sự ra đời của các mô hình học sâu, đặc biệt là kiến trúc YOLO (You Only Look Once), đã tạo ra bước đột phá trong bài toán phát hiện đối tượng thời gian thực (Real-time Object Detection).
1	Khác với các mô hình CNN truyền thống quét từng vùng ảnh, YOLO xem xét toàn bộ bức ảnh trong một lần chạy mạng duy nhất, cho phép xử lý video với tốc độ cực cao mà vẫn đảm bảo độ chính xác, đáp ứng nhu cầu cấp thiết về một hệ thống giao thông thông minh (ITS) tự động hóa hoàn toàn. Nghiên cứu này đề xuất việc tinh chỉnh (fine-tuning) mô hình YOLOv8 trên bộ dữ liệu giao thông đặc thù của Việt Nam, bao gồm 85,000 hình ảnh được trích xuất từ camera an ninh tại các ngã tư trọng điểm trong điều kiện ánh sáng và thời tiết đa dạng (nắng gắt, mưa lớn, ban đêm).
1	"Chúng tôi tập trung gán nhãn cho 4 lớp đối tượng chính: xe máy, ô tô, người đi bộ và biển báo đèn tín hiệu. Quá trình huấn luyện sử dụng kỹ thuật ""Mosaic Data Augmentation"" – ghép 4 ảnh ngẫu nhiên thành một để tăng cường khả năng nhận diện các đối tượng có kích thước nhỏ ở xa. Hệ thống máy chủ huấn luyện được trang bị 2 GPU NVIDIA Tesla V100, chạy với kích thước batch (batch size) là 64 và tốc độ học (learning rate) khởi điểm là 0.01, giảm dần theo hàm cosine qua 300 epochs. Hàm mất mát (Loss function) được tùy chỉnh để ưu tiên độ chính xác của hộp bao (bounding box) đối với các phương tiện di chuyển ở tốc độ cao."
1	Hệ thống sau khi triển khai thử nghiệm tại 5 nút giao thông lớn đã cho thấy hiệu suất ấn tượng vượt trội so với các phiên bản YOLOv5 trước đó. Độ chính xác trung bình (mAP@0.5) đạt tới 92.7% cho lớp phương tiện xe máy và 95.1% cho ô tô, với khả năng phát hiện hành vi vượt đèn đỏ chính xác đến 98.3%. Về tốc độ xử lý, mô hình đạt được 65 FPS (khung hình trên giây) trên phần cứng biên (Edge AI devices) như NVIDIA Jetson Xavier, đảm bảo khả năng giám sát thời gian thực mà không có độ trễ đáng kể. Thống kê trong 3 tháng vận hành cho thấy hệ thống đã tự động ghi nhận 12,400 trường hợp vi phạm, giúp giảm tải 80% khối lượng công việc cho lực lượng cảnh sát giao thông tại trung tâm điều hành.
1	Trong bối cảnh chuyển đổi năng lượng toàn cầu, việc cân bằng giữa cung và cầu điện năng là bài toán sống còn để đảm bảo an ninh năng lượng và tối ưu hóa chi phí vận hành lưới điện quốc gia. Sự biến động thất thường của nhu cầu sử dụng điện, chịu ảnh hưởng bởi các yếu tố thời tiết, hành vi con người và các sự kiện xã hội, khiến các mô hình thống kê tuyến tính truyền thống như ARIMA hay Hồi quy tuyến tính trở nên kém hiệu quả. Mạng nơ-ron hồi quy (RNN), cụ thể là kiến trúc Bộ nhớ dài-ngắn (Long Short-Term Memory - LSTM), được xem là giải pháp tối ưu cho các bài toán chuỗi thời gian (time-series) nhờ khả năng ghi nhớ các phụ thuộc dài hạn và loại bỏ các thông tin nhiễu không cần thiết.
1	Việc áp dụng LSTM giúp các nhà quản lý lưới điện có thể dự đoán chính xác đỉnh tải (peak load), từ đó lên kế hoạch huy động nguồn phát hợp lý, tránh lãng phí nhiên liệu và giảm thiểu nguy cơ quá tải gây rã lưới. Dữ liệu đầu vào cho mô hình bao gồm chuỗi thời gian tiêu thụ điện năng theo từng giờ trong suốt 5 năm (2019-2024) của một thành phố công nghiệp, kết hợp với dữ liệu khí tượng thủy văn (nhiệt độ, độ ẩm, lượng mưa) tương ứng. Chúng tôi thiết kế một mạng Stacked LSTM gồm 3 lớp ẩn, mỗi lớp chứa 128 tế bào nhớ (memory cells), giúp mô hình học được các đặc trưng phức tạp và tính chu kỳ của dữ liệu (theo ngày, tuần, mùa).
1	Dữ liệu được chuẩn hóa (normalized) về khoảng [0, 1] bằng phương pháp Min-Max Scaler để tăng tốc độ hội tụ. Để tránh hiện tượng quá khớp (overfitting) thường gặp khi dữ liệu có nhiều nhiễu, kỹ thuật Dropout với tỷ lệ 20% được áp dụng sau mỗi lớp LSTM. Mô hình sử dụng cửa sổ trượt (sliding window) với độ dài 168 giờ (tương đương 1 tuần quá khứ) để dự báo nhu cầu tiêu thụ cho 24 giờ tiếp theo, cho phép hệ thống cập nhật liên tục theo diễn biến thực tế. Kết quả kiểm chứng trên tập dữ liệu kiểm tra (test set) cho thấy mô hình LSTM đề xuất đạt chỉ số Sai số căn quân phương (RMSE) là 0.45 MW, thấp hơn 35% so với mô hình ARIMA truyền thống và thấp hơn 12% so với mạng RNN cơ bản.
1	Chỉ số Sai số phần trăm trung bình tuyệt đối (MAPE) duy trì ở mức 2.8%, một con số rất ấn tượng trong lĩnh vực dự báo năng lượng, nơi mà mức sai số dưới 5% được coi là xuất sắc. Đặc biệt, trong các ngày nắng nóng cực đoan khi nhu cầu sử dụng điều hòa tăng đột biến, mô hình vẫn bám sát biểu đồ thực tế với độ trễ pha không đáng kể. Ứng dụng kết quả dự báo này vào bài toán điều độ kinh tế (Economic Dispatch) giúp công ty điện lực tiết kiệm ước tính 1.2 tỷ đồng mỗi tháng nhờ tối ưu hóa lịch vận hành các tổ máy phát điện đắt tiền và giảm thiểu việc phải mua điện giá cao từ thị trường bên ngoài vào giờ cao điểm.
1	Nhận dạng khuôn mặt là một trong những lĩnh vực ứng dụng nổi bật của trí tuệ nhân tạo trong đời sống hiện đại, đặc biệt trong an ninh, kiểm soát ra vào và xác thực danh tính. Các hệ thống truyền thống dựa trên đặc trưng hình học thường gặp hạn chế khi điều kiện ánh sáng hoặc góc chụp thay đổi. Việc kết hợp CNN và ANN giúp hệ thống học được các đặc trưng trừu tượng từ dữ liệu hình ảnh, từ đó nâng cao độ chính xác nhận dạng. Theo thống kê, thị trường nhận dạng khuôn mặt toàn cầu tăng trưởng hơn 17% mỗi năm, cho thấy nhu cầu ứng dụng AI trong lĩnh vực này ngày càng lớn. Nghiên cứu tập trung đánh giá hiệu quả của CNN và ANN trong xây dựng hệ thống nhận dạng khuôn mặt tự động.
1	Hệ thống sử dụng CNN để trích xuất đặc trưng khuôn mặt từ ảnh đầu vào, sau đó ANN được dùng để phân loại danh tính người dùng. Dữ liệu huấn luyện gồm 25.000 ảnh khuôn mặt của 500 cá nhân, thu thập trong nhiều điều kiện ánh sáng và biểu cảm khác nhau. Các ảnh được chuẩn hóa kích thước 224×224 pixel và tăng cường dữ liệu bằng kỹ thuật xoay và làm mờ. Sau 60 epoch huấn luyện, mô hình đạt độ chính xác nhận dạng 95.3%, cao hơn 14% so với phương pháp PCA truyền thống. Kết quả cho thấy sự kết hợp CNN–ANN mang lại hiệu suất ổn định và khả năng tổng quát hóa cao.
1	Hệ thống được thử nghiệm trong môi trường kiểm soát ra vào của một tòa nhà văn phòng với hơn 1.000 lượt nhận dạng mỗi ngày. Thời gian xác thực trung bình chỉ 0.8 giây cho mỗi người, nhanh hơn 65% so với phương pháp dùng thẻ từ. Tỷ lệ nhận dạng sai giảm xuống còn 1.9%, góp phần nâng cao mức độ an toàn. Ngoài ra, hệ thống còn có khả năng mở rộng để tích hợp với camera giám sát hiện có, giúp giảm chi phí triển khai ban đầu. Nghiên cứu chứng minh rằng AI đóng vai trò quan trọng trong việc xây dựng các hệ thống an ninh thông minh và hiệu quả.
1	Chẩn đoán bệnh sớm dựa trên dữ liệu sinh học là một trong những hướng nghiên cứu quan trọng của trí tuệ nhân tạo trong y học hiện đại. Các dữ liệu như chỉ số sinh hóa máu, huyết áp và nhịp tim thường có mối quan hệ phi tuyến phức tạp. Thuật toán KNN và ANN được đánh giá cao nhờ khả năng xử lý dữ liệu đa chiều và phát hiện các mẫu ẩn trong tập dữ liệu lớn. Theo thống kê, việc ứng dụng AI trong chẩn đoán có thể giảm tới 30% tỷ lệ chẩn đoán sai. Nghiên cứu này tập trung phân tích hiệu quả của KNN và ANN trong dự đoán nguy cơ mắc bệnh tim mạch.
1	Dữ liệu nghiên cứu gồm 18.000 hồ sơ bệnh án với 14 thuộc tính sinh học khác nhau. KNN được sử dụng để phân loại bệnh nhân dựa trên mức độ tương đồng về chỉ số sức khỏe, trong khi ANN đảm nhiệm việc dự đoán nguy cơ bệnh trong tương lai. Các mô hình được đánh giá bằng phương pháp cross-validation 10-fold. Kết quả cho thấy KNN đạt độ chính xác 85.6%, trong khi ANN đạt 92.4% trong dự đoán bệnh tim. Khi kết hợp hai mô hình, độ chính xác tổng thể tăng lên 94.1%, cho thấy hiệu quả rõ rệt của phương pháp lai.
1	Hệ thống hỗ trợ chẩn đoán giúp bác sĩ rút ngắn thời gian đánh giá hồ sơ bệnh nhân từ 15 phút xuống còn khoảng 3 phút. Tỷ lệ phát hiện sớm bệnh tim tăng từ 68% lên 89%, góp phần giảm nguy cơ biến chứng nghiêm trọng. Ngoài ra, hệ thống có thể triển khai tại các cơ sở y tế tuyến cơ sở, nơi thiếu nhân lực chuyên môn cao. Nghiên cứu khẳng định rằng AI không chỉ hỗ trợ bác sĩ trong chẩn đoán mà còn nâng cao chất lượng chăm sóc sức khỏe cộng đồng một cách bền vững.
1	Quản lý rác thải là thách thức lớn tại các đô thị đông dân, đặc biệt khi tỷ lệ phân loại rác tại nguồn còn thấp. Theo báo cáo môi trường, hơn 60% rác thải sinh hoạt chưa được phân loại đúng cách, gây lãng phí tài nguyên và ô nhiễm môi trường. Trí tuệ nhân tạo, đặc biệt là CNN và KNN, mở ra giải pháp tự động hóa quá trình phân loại rác dựa trên hình ảnh. Nghiên cứu này tập trung xây dựng hệ thống phân loại rác thông minh nhằm nâng cao hiệu quả tái chế và giảm tải cho các bãi chôn lấp.
1	CNN được sử dụng để trích xuất đặc trưng hình ảnh từ các loại rác như nhựa, kim loại, giấy và rác hữu cơ. Sau đó, KNN đảm nhiệm vai trò phân loại dựa trên vector đặc trưng đầu ra. Tập dữ liệu gồm hơn 30.000 ảnh rác thải được thu thập trong điều kiện thực tế. Kết quả huấn luyện cho thấy hệ thống đạt độ chính xác trung bình 93.2%, trong đó rác tái chế được nhận dạng chính xác hơn 95%. So với phương pháp phân loại thủ công, sai số giảm hơn 20%, chứng minh hiệu quả vượt trội của AI.
1	Hệ thống phân loại rác thông minh được thử nghiệm tại một khu dân cư với công suất xử lý đạt khoảng 1.500 vật phẩm mỗi giờ, cho thấy khả năng vận hành ổn định trong điều kiện thực tế. Kết quả triển khai cho thấy tỷ lệ rác tái chế thu hồi tăng đáng kể từ 35% lên 62%, qua đó giúp giảm lượng rác thải phải chôn lấp và hạn chế phát sinh khí nhà kính từ các bãi rác truyền thống. Bên cạnh lợi ích môi trường, hệ thống còn mang lại hiệu quả kinh tế rõ rệt khi chi phí nhân công cho khâu phân loại giảm khoảng 30% sau 6 tháng vận hành liên tục.
1	Trong kỷ nguyên mạng xã hội bùng nổ, hàng triệu bình luận và phản hồi của khách hàng được tạo ra mỗi ngày trên các nền tảng như Facebook, Shopee hay Tiki. Việc phân tích thủ công khối lượng dữ liệu khổng lồ này để hiểu được thái độ của người dùng (tích cực, tiêu cực hay trung lập) là nhiệm vụ bất khả thi đối với con người. Đặc biệt, tiếng Việt là một ngôn ngữ đơn lập với cấu trúc ngữ pháp lỏng lẻo, nhiều từ lóng (slang), teencode và ngữ nghĩa phụ thuộc nặng nề vào ngữ cảnh, khiến các mô hình dịch máy hoặc xử lý ngôn ngữ tự nhiên (NLP) truyền thống như Bag-of-Words hay LSTM thường gặp khó khăn trong việc nắm bắt ý định thực sự.
1	"Sự ra đời của kiến trúc Transformer, điển hình là BERT (Bidirectional Encoder Representations from Transformers), đã tạo ra cuộc cách mạng nhờ cơ chế ""Self-Attention"", cho phép mô hình hiểu được mối quan hệ hai chiều của từ ngữ trong câu. Nghiên cứu này tập trung ứng dụng PhoBERT – phiên bản được tối ưu riêng cho tiếng Việt – để giải quyết bài toán phân tích cảm xúc (Sentiment Analysis) trong lĩnh vực chăm sóc khách hàng. Chúng tôi sử dụng bộ dữ liệu gồm 150,000 bình luận được thu thập từ các trang thương mại điện tử lớn tại Việt Nam trong năm 2024. Dữ liệu được gán nhãn thủ công bởi đội ngũ ngôn ngữ học với 3 nhãn: Tích cực (Positive), Tiêu cực (Negative) và Trung lập (Neutral)."
1	"Quá trình tiền xử lý bao gồm việc chuẩn hóa Unicode, tách từ (word segmentation) sử dụng VnCoreNLP và loại bỏ các ký tự đặc biệt nhưng giữ lại các biểu tượng cảm xúc (emojis) vì chúng mang giá trị biểu đạt cao. Mô hình PhoBERT-base được sử dụng làm nền tảng (pre-trained model), sau đó chúng tôi tiến hành tinh chỉnh (fine-tuning) thêm một lớp phân loại tuyến tính (Linear Classifier) ở đầu ra. Quá trình huấn luyện diễn ra trên Google Colab Pro với GPU Tesla T4 trong 20 epochs, sử dụng hàm tối ưu AdamW với tốc độ học (learning rate) là 2e-5 để tránh làm hỏng các trọng số đã được học trước đó của mô hình gốc. Kỹ thuật ""Early Stopping"" cũng được áp dụng để dừng huấn luyện khi độ chính xác trên tập kiểm thử không còn cải thiện sau 3 vòng lặp liên tiếp."
1	"Kết quả thực nghiệm cho thấy mô hình PhoBERT sau khi tinh chỉnh đạt độ chính xác (Accuracy) lên tới 93.8%, vượt xa so với mô hình LSTM hai chiều (Bi-LSTM) chỉ đạt 84.5%. Chỉ số F1-Score cho lớp ""Tiêu cực"" – lớp quan trọng nhất để phát hiện phàn nàn của khách hàng – đạt 0.91, chứng tỏ mô hình rất nhạy bén trong việc phát hiện các từ ngữ mang sắc thái chê trách dù được viết dưới dạng ẩn ý hay châm biếm. Khi tích hợp vào hệ thống Chatbot chăm sóc khách hàng tự động, mô hình đã giúp giảm 75% thời gian phản hồi trung bình bằng cách tự động phân loại và chuyển tiếp các khiếu nại khẩn cấp đến nhân viên trực tổng đài."
1	"Trong marketing hiện đại, chiến lược ""một kích cỡ vừa cho tất cả"" (one-size-fits-all) đã trở nên lỗi thời và kém hiệu quả, gây lãng phí ngân sách quảng cáo khổng lồ. Để tối ưu hóa lợi nhuận, các doanh nghiệp bán lẻ cần hiểu rõ đặc điểm hành vi của từng nhóm khách hàng khác nhau để thiết kế các chương trình khuyến mãi phù hợp. Thuật toán phân cụm K-Means (K-Means Clustering), một kỹ thuật học máy không giám sát (Unsupervised Learning), là công cụ mạnh mẽ để giải quyết bài toán này. Khác với các thuật toán phân loại có giám sát (như SVM hay CNN) cần dữ liệu có nhãn, K-Means tự động tìm kiếm các cấu trúc ẩn trong dữ liệu thô để gom nhóm các đối tượng có đặc điểm tương đồng nhau."
1	Mục tiêu của nghiên cứu là áp dụng K-Means kết hợp với mô hình RFM (Recency - Frequency - Monetary) để phân chia tập khách hàng của một chuỗi siêu thị lớn thành các nhóm riêng biệt, phục vụ cho các chiến dịch tiếp thị cá nhân hóa (Personalized Marketing). Dữ liệu đầu vào bao gồm lịch sử giao dịch của 78,000 khách hàng thành viên trong vòng 12 tháng, được chuyển đổi thành 3 chỉ số vector RFM: Recency (Số ngày từ lần mua cuối cùng), Frequency (Tần suất mua hàng) và Monetary (Tổng số tiền đã chi tiêu). Do biên độ giá trị của ba chỉ số này rất khác nhau (ví dụ: số tiền có thể lên đến hàng triệu, trong khi tần suất chỉ là vài lần), chúng tôi sử dụng phương pháp chuẩn hóa Standard Scaler để đưa dữ liệu về cùng một phân phối chuẩn (mean=0, std=1).
1	"Thách thức lớn nhất của K-Means là xác định số lượng cụm K tối ưu. Chúng tôi áp dụng phương pháp ""Khuỷu tay"" (Elbow Method), vẽ đồ thị biểu diễn sự biến thiên của tổng bình phương sai số trong cụm (WCSS) theo giá trị K. Tại điểm gãy khúc (elbow point) của đồ thị, chúng tôi xác định K=4 là số lượng cụm tối ưu nhất, đảm bảo sự cân bằng giữa độ nén của dữ liệu và khả năng diễn giải ý nghĩa kinh doanh của từng nhóm. Kết quả phân cụm đã chia tệp khách hàng thành 4 nhóm rõ rệt với các chiến lược tiếp cận riêng biệt. Nhóm 1 (chiếm 20%) là ""Khách hàng VIP"" với tần suất mua cao và chi tiêu lớn; chiến lược cho nhóm này là các chương trình tri ân đặc quyền để duy trì lòng trung thành."
1	"Nhóm 2 (chiếm 35%) là ""Khách hàng tiềm năng"" có tần suất mua trung bình nhưng giá trị đơn hàng thấp; chiến lược là gợi ý sản phẩm bán chéo (cross-selling) để tăng giá trị giỏ hàng. Nhóm 3 (chiếm 15%) là ""Khách hàng sắp rời bỏ"" đã lâu không quay lại; hệ thống tự động gửi voucher giảm giá sâu để lôi kéo họ trở lại. Nhóm 4 là khách hàng vãng lai. Sau 3 tháng áp dụng chiến lược marketing riêng biệt cho từng cụm (Targeted Marketing), doanh thu từ nhóm ""Khách hàng sắp rời bỏ"" đã hồi phục được 18.5%, và doanh thu tổng thể của siêu thị tăng 12.4% so với cùng kỳ năm trước. Chi phí marketing giảm 30% do không còn gửi tin nhắn spam đại trà đến những người không có nhu cầu."
1	"Ngành logistics và quản lý kho vận đang chứng kiến sự chuyển dịch mạnh mẽ sang tự động hóa với sự tham gia của các robot tự hành (AGV - Automated Guided Vehicles). Tuy nhiên, các phương pháp tìm đường cổ điển như thuật toán A* hay Dijkstra thường gặp hạn chế trong các môi trường kho hàng phức tạp và biến động liên tục (dynamic environments), nơi vị trí hàng hóa thay đổi và có nhiều robot cùng di chuyển gây tắc nghẽn. Để giải quyết vấn đề này, Học tăng cường (Reinforcement Learning - RL) mang lại một cách tiếp cận đột phá: thay vì lập trình cứng nhắc các quy tắc di chuyển, chúng ta cho phép robot tự ""học"" cách di chuyển tối ưu thông qua cơ chế Thử và Sai (Trial and Error)."
1	Nghiên cứu này đề xuất sử dụng mạng Deep Q-Network (DQN) – sự kết hợp giữa Q-Learning và Deep Neural Networks – để điều khiển đội hình robot lấy hàng, giúp chúng tự động tìm đường đi ngắn nhất và tránh va chạm trong thời gian thực. Chúng tôi xây dựng một môi trường giả lập kho hàng 2D kích thước 50x50 lưới (grids), mô phỏng các kệ hàng, trạm sạc và các chướng ngại vật di động. Tác tử (Agent) là robot có thể thực hiện 4 hành động: lên, xuống, trái, phải. Trái tim của hệ thống RL là thiết kế Hàm phần thưởng (Reward Function): robot nhận được +100 điểm khi đến đích thành công, -10 điểm nếu va chạm vào kệ hàng hoặc robot khác, và -0.1 điểm cho mỗi bước di chuyển để khuyến khích nó tìm đường ngắn nhất.
1	"Mạng nơ-ron được sử dụng là một mạng tích chập (CNN) nhận đầu vào là trạng thái hình ảnh cục bộ của robot và trả về giá trị Q-value cho từng hành động. Chúng tôi sử dụng kỹ thuật ""Experience Replay"" với bộ nhớ đệm dung lượng 10,000 mẫu để lưu trữ các trải nghiệm quá khứ, giúp phá vỡ sự tương quan thời gian giữa các mẫu dữ liệu liên tiếp, từ đó giúp quá trình huấn luyện hội tụ ổn định hơn. Tham số Epsilon-Greedy được thiết lập giảm dần từ 1.0 xuống 0.05 để cân bằng giữa việc khám phá môi trường (Exploration) và khai thác tri thức đã học (Exploitation)."
1	"Sau 50,000 tập (episodes) huấn luyện, mô hình DQN đã thể hiện khả năng thích ứng tuyệt vời. So sánh với thuật toán tìm đường truyền thống A*, robot sử dụng DQN có khả năng hoàn thành nhiệm vụ nhanh hơn 18% trong các tình huống có chướng ngại vật di động xuất hiện bất ngờ, vì nó không cần tính toán lại toàn bộ đường đi từ đầu mà chỉ cần phản ứng theo trạng thái hiện tại. Tỷ lệ va chạm trong kịch bản đa robot (Multi-agent scenario) giảm xuống còn dưới 0.5%. Đáng chú ý, robot đã tự ""sáng tạo"" ra các chiến thuật di chuyển thông minh như nhường đường tại các ngã tư hẹp để tránh tắc nghẽn cục bộ (deadlock) – một hành vi chưa từng được lập trình trước."
1	Các tư liệu hình ảnh về lịch sử Việt Nam, đặc biệt là giai đoạn đầu thế kỷ 20, đang đứng trước nguy cơ hư hại nghiêm trọng do sự lão hóa vật lý của phim tráng và điều kiện bảo quản không tối ưu trong thời chiến. Các phương pháp phục chế kỹ thuật số truyền thống (như phép nội suy song tuyến tính) thường làm ảnh bị mờ nhòe (blur), mất đi độ sắc nét của các chi tiết quan trọng như hoa văn trang phục hay biểu cảm khuôn mặt nhân vật lịch sử. Mạng đối nghịch sinh (Generative Adversarial Networks - GANs), cụ thể là kiến trúc Super-Resolution GAN (SRGAN), mang đến một hướng tiếp cận đột phá.
1	"Thay vì chỉ tính toán trung bình cộng các điểm ảnh lân cận, SRGAN có khả năng ""tưởng tượng"" và tái tạo lại các chi tiết bị mất dựa trên việc học hàng triệu mẫu vân bề mặt (texture) từ ảnh hiện đại, giúp biến các tấm ảnh đen trắng mờ nhạt, vỡ hạt thành các bức ảnh độ phân giải cao, sống động như vừa được chụp. Hệ thống được xây dựng dựa trên sự cạnh tranh giữa hai mạng nơ-ron sâu: Mạng Sinh (Generator) và Mạng Phân biệt (Discriminator). Mạng Sinh cố gắng tạo ra một bức ảnh độ phân giải cao (4K) từ ảnh đầu vào chất lượng thấp (Low-res), trong khi Mạng Phân biệt đóng vai trò như một ""nhà giám định nghệ thuật"" khó tính, cố gắng phát hiện xem bức ảnh đó là thật hay do máy tạo ra."
1	Dữ liệu huấn luyện bao gồm 45,000 cặp ảnh chất lượng cao và thấp, được bổ sung thêm tập dữ liệu riêng gồm 2,000 ảnh tư liệu Đông Dương. Chúng tôi sử dụng hàm mất mát Perceptual Loss (dựa trên đặc trưng VGG-19) thay vì hàm mất mát điểm ảnh (Pixel-wise MSE) thông thường. Điều này giúp mô hình tập trung vào sự tương đồng về mặt thị giác và cấu trúc ngữ nghĩa thay vì chỉ sai số toán học, giúp các chi tiết như vân gỗ, nếp nhăn trên da hay ký tự trên biển hiệu được tái tạo sắc sảo và tự nhiên.
1	"Kết quả phục chế trên tập dữ liệu kiểm thử gồm 500 ảnh tư liệu Hà Nội xưa cho thấy sự vượt trội của SRGAN. Chỉ số Tỷ lệ tín hiệu trên nhiễu đỉnh (PSNR) đạt 28.5 dB và Chỉ số tương đồng cấu trúc (SSIM) đạt 0.89, cao hơn lần lượt 4dB và 0.15 so với các thuật toán CNN thông thường. Quan trọng hơn, trong bài kiểm tra định tính (MOS - Mean Opinion Score) với sự tham gia của 20 nhà sử học và chuyên gia lưu trữ, 92% số người đánh giá cho rằng ảnh được phục chế bởi GAN giữ được ""hồn"" của bức ảnh gốc và chi tiết chân thực hơn hẳn."
1	Việt Nam là một trong những quốc gia xuất khẩu gạo hàng đầu thế giới, nhưng năng suất lúa gạo tại Đồng bằng sông Cửu Long thường xuyên bị đe dọa bởi các loại dịch bệnh như bệnh đạo ôn (Blast) và đốm nâu (Brown spot). Việc phát hiện bệnh bằng mắt thường của nông dân thường diễn ra khi bệnh đã lây lan rộng, dẫn đến việc lạm dụng thuốc bảo vệ thực vật, gây ô nhiễm môi trường và tăng chi phí sản xuất. Ứng dụng Trí tuệ nhân tạo trong Nông nghiệp chính xác (Precision Agriculture), cụ thể là sử dụng ảnh viễn thám từ vệ tinh hoặc flycam kết hợp với thuật toán học máy, cho phép giám sát sức khỏe cây trồng trên diện rộng.
1	Thuật toán Rừng ngẫu nhiên (Random Forest), một phương pháp học tập kết hợp (Ensemble Learning) từ nhiều Cây quyết định (Decision Trees), được lựa chọn nhờ khả năng xử lý tốt dữ liệu nhiễu và độ chính xác cao trong bài toán phân loại đa lớp mà không bị quá khớp (overfitting). Dữ liệu nghiên cứu được thu thập từ vệ tinh Sentinel-2 kết hợp với ảnh chụp từ drone tại 5 tỉnh miền Tây trong 3 vụ lúa liên tiếp (2023-2025), bao phủ diện tích 10,000 hecta. Chúng tôi trích xuất các chỉ số thực vật quan trọng như NDVI (Normalized Difference Vegetation Index) và EVI (Enhanced Vegetation Index) từ các kênh phổ hồng ngoại và cận hồng ngoại.
1	"Mô hình Random Forest được thiết lập với 500 cây quyết định (n_estimators=500), sử dụng tiêu chí Gini Impurity để phân chia các nút. Điểm mạnh của Random Forest là nó đánh giá được mức độ quan trọng của từng đặc trưng (Feature Importance); kết quả cho thấy chỉ số phản xạ ánh sáng ở bước sóng ""Red-Edge"" là yếu tố quan trọng nhất để phát hiện sớm sự thay đổi diệp lục trong lá lúa khi mới chớm bệnh, trước cả khi mắt người có thể nhìn thấy các vết đốm. Mô hình đã đạt được độ chính xác tổng thể là 96.4% trong việc phân loại 4 trạng thái: Lúa khỏe, Nhiễm đạo ôn, Nhiễm đốm nâu và Thiếu dinh dưỡng."
1	So với phương pháp thăm đồng truyền thống chỉ đạt độ chính xác khoảng 70-75%, hệ thống này cung cấp bản đồ cảnh báo dịch bệnh sớm trước 10-14 ngày. Thử nghiệm thực địa tại tỉnh An Giang cho thấy, các hộ nông dân áp dụng bản đồ này để phun thuốc cục bộ (chỉ phun vùng bị bệnh) đã giảm được 40% lượng thuốc trừ sâu sử dụng, tương đương tiết kiệm 3 triệu đồng/hecta/vụ. Năng suất lúa thu hoạch tăng trung bình 15% do ngăn chặn kịp thời sự bùng phát của dịch bệnh. Hệ thống xử lý dữ liệu đám mây có khả năng phân tích và gửi báo cáo về điện thoại của nông dân chỉ trong vòng 30 phút sau khi nhận dữ liệu hình ảnh, giúp việc ra quyết định trở nên nhanh chóng và khoa học.
1	"Tin giả (Fake News) lan truyền trên các mạng xã hội như Facebook, Twitter (X) có cơ chế lây lan tương tự như virus sinh học, gây ra những hậu quả nghiêm trọng về chính trị, kinh tế và xã hội. Các phương pháp phát hiện tin giả truyền thống thường chỉ tập trung phân tích nội dung văn bản (Text-based) bằng NLP. Tuy nhiên, tin giả ngày nay được viết rất tinh vi, khó phân biệt thật giả nếu chỉ đọc nội dung. Điểm khác biệt cốt lõi nằm ở ""cấu trúc lan truyền"": tin giả thường được chia sẻ dồn dập bởi các nhóm tài khoản ảo (botnet) hoặc lan truyền theo các cụm cộng đồng phân cực."
1	"Mạng nơ-ron đồ thị (Graph Neural Networks - GNN), cụ thể là Graph Convolutional Networks (GCN), cho phép mô hình hóa bài toán dưới dạng đồ thị, trong đó mỗi bài đăng, người dùng là một ""nút"" (node) và hành động chia sẻ, bình luận là các ""cạnh"" (edge), từ đó khai thác được các đặc trưng cấu trúc ẩn sâu của mạng lưới lan truyền. Chúng tôi xây dựng tập dữ liệu đồ thị gồm 200,000 bài đăng và 1.5 triệu người dùng liên quan, được gán nhãn dựa trên dữ liệu từ các trang xác thực tin tức (Fact-checking sites)."
1	"Mô hình GCN được thiết kế với cơ chế ""tích hợp lân cận"" (Neighbor Aggregation): đặc trưng của một bài đăng không chỉ được xác định bởi nội dung của nó mà còn được tổng hợp từ đặc điểm của những người chia sẻ nó (độ uy tín, lịch sử hoạt động). Nếu một bài tin được chia sẻ bởi hàng loạt tài khoản mới lập, có hành vi giống hệt nhau trong một thời gian ngắn, mô hình đồ thị sẽ nhận diện đây là dấu hiệu bất thường. Chúng tôi sử dụng ma trận kề (Adjacency Matrix) để biểu diễn mối quan hệ và áp dụng 3 lớp tích chập đồ thị để lan truyền thông tin qua các nút, kết hợp với các đặc trưng văn bản được trích xuất từ BERT để tăng cường ngữ nghĩa."
1	"Hệ thống GNN đề xuất đạt độ chính xác phát hiện tin giả lên tới 95.2%, vượt trội so với các mô hình chỉ dựa trên nội dung (khoảng 85%) hoặc chỉ dựa trên thông tin người dùng. Đặc biệt ấn tượng là khả năng ""Phát hiện sớm"" (Early Detection): mô hình có thể xác định tin giả với độ tin cậy trên 90% chỉ sau 2 giờ kể từ khi bài viết bắt đầu lan truyền và chưa đạt 50 lượt chia sẻ. Phân tích đồ thị cũng chỉ ra rằng tin giả thường có cấu trúc lan truyền hình ""sao"" (từ một nguồn phát tán ra nhiều nhánh đơn lẻ) hoặc hình ""chuỗi dài"" (qua các bot), khác hẳn với cấu trúc lan truyền hữu cơ và dày đặc của tin thật."
1	"Trong mô hình giáo dục đại học hiện đại, việc quản lý nề nếp và chất lượng giờ học tại các giảng đường lớn (sức chứa từ 100-200 sinh viên) luôn là một thách thức đối với giảng viên và phòng công tác sinh viên. Phương pháp điểm danh truyền thống bằng giấy hoặc gọi tên không chỉ tiêu tốn từ 15 đến 20 phút thời gian vàng ngọc của tiết học mà còn dễ xảy ra tình trạng ""điểm danh hộ"" hoặc gian lận. Bên cạnh đó, việc đánh giá chất lượng bài giảng dựa trên phản hồi cuối kỳ thường mang tính chủ quan và có độ trễ lớn."
1	Nhu cầu cấp thiết đặt ra là phải xây dựng một hệ thống giám sát tự động, khách quan, có khả năng nhận diện danh tính sinh viên nhanh chóng và phân tích hành vi (sự tập trung, ngủ gật, sử dụng điện thoại) để giảng viên có thể điều chỉnh phương pháp dạy học kịp thời. Sự kết hợp giữa MTCNN (Multi-task Cascaded Convolutional Networks) để phát hiện khuôn mặt và FaceNet để trích xuất đặc trưng đang là giải pháp tối ưu cho bài toán này trong môi trường Smart Campus. Hệ thống được xây dựng trên mô hình đường ống (pipeline) hai giai đoạn. Giai đoạn 1 sử dụng MTCNN để phát hiện và căn chỉnh khuôn mặt (face alignment) từ luồng video trực tiếp của camera lớp học, đảm bảo loại bỏ các khuôn mặt bị che khuất quá 50% hoặc quá mờ.
1	Giai đoạn 2 là trái tim của hệ thống: mô hình FaceNet sử dụng kiến trúc Inception-ResNet-v1 để chuyển đổi mỗi khuôn mặt thành một vector đặc trưng 128 chiều (128-D embedding) trên không gian Euclid. Điểm đột phá của FaceNet nằm ở hàm mất mát Triplet Loss, giúp mô hình học cách thu nhỏ khoảng cách giữa các vector của cùng một người (Anchor - Positive) và nới rộng khoảng cách giữa các vector của người khác nhau (Anchor - Negative). Cơ sở dữ liệu huấn luyện bao gồm 10,000 ảnh đa góc độ của 2,000 sinh viên đang theo học, được lưu trữ và truy xuất thông qua hệ quản trị cơ sở dữ liệu SQL Server (tích hợp với C# .NET cho giao diện quản lý), đảm bảo tốc độ truy vấn danh tính dưới 0.5 giây.
1	Kết quả triển khai thí điểm tại 10 giảng đường trong học kỳ I năm học 2024-2025 cho thấy những con số ấn tượng. Độ chính xác nhận diện danh tính (Identity Accuracy) đạt 99.2% trong điều kiện ánh sáng tiêu chuẩn và 94.5% khi sinh viên đeo khẩu trang y tế (nhờ tập trung vào đặc trưng vùng mắt và trán). Hệ thống có khả năng điểm danh đồng thời 30 sinh viên trong một khung hình chỉ trong vòng 1.2 giây, giúp rút ngắn quy trình điểm danh xuống gần như bằng 0. Ngoài ra, mô đun phân tích hành vi sử dụng mạng CNN phụ trợ đã phát hiện chính xác 88% các trường hợp sinh viên mất tập trung (ngủ gật, nhìn điện thoại liên tục).
0	Phân loại video bằng học máy trở thành một lĩnh vực tiềm năng, giúp tự động nhận dạng và phân loại vào các danh mục tương ứng. Quá trình này bắt đầu bằng việc tiền xử lý dữ liệu video để trích xuất và chuyển đổi thông tin thành đặc trưng số học. Đặc biệt, các thuật toán học máy như KNN, SVM, CNN và PhoBERT được sử dụng để xử lý và phân tích nội dung video cũng như thông tin ngôn ngữ trong video. Trong thực nghiệm, dữ liệu được thu thập từ hệ thống lưu trữ nội bộ của Đài Phát thanh và Truyền hình thành phố Cần Thơ, với mỗi video có độ dài trung bình khoảng 3 phút. Các thuật toán đã được triển khai và đánh giá trên tập dữ liệu này để đo lường và so sánh hiệu suất.
0	Kết quả của thuật toán PhoBERT, với độ chính xác đạt tới 98%. Từ kết quả cho thấy khả năng vượt trội của PhoBERT trong việc xử lý và nhận dạng nội dung video, tạo điều kiện thuận lợi cho việc phát triển hệ thống phân loại video tự động. Trong những năm qua, sự tiến bộ đột phá của công nghệ và sự lan tỏa mạnh mẽ của mạng xã hội đã biến Internet thành một nguồn thông tin đa dạng, từ sách, báo, hình ảnh đến video và âm nhạc. Trong đó, video đóng vai trò ngày càng quan trọng, tác động sâu rộng vào xã hội.
0	Điều này làm cho việc phân loại video dựa trên nội dung trở nên cực kỳ quan trọng, đặc biệt trong lĩnh vực bản tin thời sự truyền hình, việc tự động phân loại chủ đề đóng vai trò then chốt để cung cấp thông tin chính xác và đáng tin cậy. Công nghệ trí tuệ nhân tạo (AI) và xử lý ngôn ngữ tự nhiên (NLP) đã đạt được sự phát triển đáng kể, mở ra khả năng tự động hóa việc phân loại video. Ở Việt Nam, việc áp dụng AI trong lĩnh vực truyền thông không chỉ góp phần thúc đẩy sự tiến bộ của ngành mà còn tối ưu hóa quá trình sản xuất nội dung, nâng cao trải nghiệm người xem.
0	Trên phạm vi toàn cầu, việc ứng dụng AI để tự động phân loại chủ đề đã thu hút sự quan tâm đặc biệt, mang lại những tiến bộ đột phá cho ngành truyền thông. Nghiên cứu của (Ahmed và cộng sự, 2020) đã giới thiệu một phương pháp tiên tiến để phát hiện tin giả thông qua kỹ thuật học máy, chủ yếu tập trung vào xử lý ngôn ngữ tự nhiên. Nghiên cứu này tập trung vào việc phát triển các thuật toán có khả năng phân tích nội dung văn bản để phân biệt giữa thông tin đáng tin cậy và thông tin không chính xác.
0	Bằng cách kết hợp phương pháp rút trích đặc trưng TF-IDF (Term Frequency-Inverse Document Frequency) với các thuật toán Naïve Bayes, Passive Aggressive và SVM. Kết quả thực nghiệm trên nhiều tập dữ liệu cho thấy, mô hình này đạt được độ chính xác lên đến 93%. Trong lĩnh vực phân loại video, Gao (2021) đã tiến hành nghiên cứu chi tiết bằng việc áp dụng mô hình kiến trúc ResNet-v2. Tác giả đã đặt nền tảng nghiên cứu trên sự kết hợp và cải tiến của thuật toán Adam cùng thuật toán Gradient Descent, nhằm tối ưu hóa hiệu suất học. Kết quả thực nghiệm đã cho thấy thuật toán Adam cải tiến hiệu quả trong việc cập nhật trọng số mạng và đạt được sự hội tụ nhanh chóng.
0	Đặc biệt, mô hình Inception-ResNet-v2 sau khi được cải tiến đã vượt trội so với các mô hình mạng nơron tích chập (CNN) thông thường, với tỷ lệ chính xác phân loại lên tới 91,47% trên tập dữ liệu video tin tức. Luo (2021) triển khai phương pháp máy véc-tơ hỗ trợ (SVM) và các kỹ thuật học máy khác để phân loại văn bản. Sử dụng các phương pháp Naive Bayes, SVM, Logistic Regression, Logistic Regression CrossValidation (LRCV) để phân loại trên 3 bộ dữ liệu khác nhau. Kết quả cho thấy rất khá tốt, SVM đạt mức đánh giá chính xác khoảng 90% trong cả 3 tập dữ liệu. Dựa trên các công trình nghiên cứu hiện có, đã có nhiều phương pháp đề xuất để phân loại video dựa trên nội dung và tính chất của video.
0	Mỗi phương pháp đều mang lại những ưu điểm và hạn chế riêng. Trong nghiên cứu này, chúng tôi đã sử dụng kỹ thuật học máy và phân tích thống kê, cùng với việc chuẩn hóa dữ liệu. Mục tiêu chính là phát triển một phương pháp phân loại video chính xác và hiệu quả hơn dựa trên nội dung và đặc điểm của video. Mô hình đề xuất cho hệ thống tự động phân loại chủ đề được biểu diễn trong Hình 1. Mô hình này được thiết kế với 4 giai đoạn: (1) Chuyển đổi và tiền xử lý dữ liệu; (2) Rút trích đặc trưng dữ liệu và giảm chiều dữ liệu; (3) Huấn luyện và phân loại bằng cách sử dụng KNN, SVM, CNN, và PhoBERT;
0	(4) Đánh giá hiệu suất của các thuật toán tại giai đoạn (3) dựa trên các tiêu chí như Accuracy, Precision, Recall, F1-Score. Chuyển đổi dữ liệu sang dạng văn bản và tiền xử lý là bước quan trọng trong quá trình khai thác dữ liệu, giúp dễ dàng quan sát và khám phá. Mỗi ngôn ngữ trong việc phân loại đều có đặc trưng và yêu cầu tiền xử lý khác nhau, nhằm tối ưu hiệu suất và đơn giản hóa thuật toán huấn luyện. Tùy thuộc vào mục đích của phân loại, chúng ta sẽ có các phương pháp xử lý trước khác nhau, chẳng hạn như: Chuyển văn bản thành chữ thường và sữa lỗi chính tả. Tách từ và chuẩn hóa các từ.
0	Xóa các ký tự đặc biệt ([], [.], [,], [:], [”], [”], [;], [/], [[]], [˜], [´], [!], [@], [#], [$], [%], [ˆ], [&], [*], [(], [)]). Tách các từ bằng từ ghép (Tiếng Việt). Loại bỏ các từ dừng (Stop words). Loại bỏ các từ trùng lặp (Remove duplicates). Loại bỏ các biểu tượng cảm xúc (Remove emojis). Chuyển đổi văn bản thành vectơ làm đầu vào cho máy học phân loại. kNN là thuật toán học máy giám sát được sử dụng trong phân loại và dự báo (Guo và cộng sự, 2003). kNN phân loại dữ liệu chưa được gán nhãn bằng cách tính khoảng cách giữa mỗi điểm dữ liệu không được gán nhãn và tất cả các điểm khác trong tập dữ liệu. Công thức tìm khoảng cách đường thẳng là phương pháp phổ biến nhất để tìm khoảng cách trong kNN.
0	Việc phân loại được thực hiện dựa trên k láng giềng gần nhất (khoảng cách nhỏ nhất), trong đó k là số láng giềng gần nhất. Nhãn lớp được gán cho mẫu thử nghiệm được tính theo công thức (5). Trong mô hình mạng nơron tích chập (CNN) (Krizhevsky và cộng sự, 2017) được đề xuất với kiến trúc mạng trong Hình 2, có 4 lớp tích chập (Conv2D) và 3 lớp MaxPool2D, được xếp chồng lên nhau trong mạng. Lớp tích chập đầu tiên (Conv2D1) trong mô hình CNN mà chúng tôi đề xuất là lớp đầu vào liên kết trực tiếp với lớp tích chập tiếp theo (Conv2D2). Lớp (Conv2D1) sử dụng 32 bộ lộc, mỗi bộ lộc với kernel có kích thước là (3 x 3), sử dụng stride là 1 và áp dụng hàm kích hoạt ReLU.
0	Lớp tích chập thứ 2 (Conv2D2) của mô hình bao gồm 64 bộ lộc với kernel kích thước (3 x 3), sử dụng stride là 1 và hàm kích hoạt ReLU. Tiếp theo, sau lớp (Conv2D2) là lớp MaxPool2D (MP1) với kích thước (2 x 2) nhằm lựa chọn đặc trưng quan trọng. Để giảm hiện tượng overfitting, chúng tôi đã thêm lớp Dropout (DR1) với tỷ lệ 20% (dựa trên việc điều chỉnh mô hình cho từng giá trị khác nhau), giúp loại bỏ 20% nơron trong quá trình huấn luyện. Lớp tích chập thứ 3 (Conv2D3) sử dụng 128 bộ lộc với kernel kích thước (3 x 3), sau đó áp dụng lớp MaxPool2D (MP2) với kích thước (2 x 2) và lớp Dropout (DR2) với tỷ lệ 20%.
0	Còn lớp tích chập thứ 4 (Conv2D4) bao gồm 256 bộ lộc với kernel (3 x 3), được tiếp tục bởi lớp MaxPool2D (MP3) (2 x 2) và Dropout (DR3) tỷ lệ 20%. Lớp Flatten (FL) được sử dụng trong giai đoạn này để chuyển đổi dữ liệu ma trận hai chiều thành vector, theo sau là lớp Dense (Dense1) với hàm kích hoạt ReLU và Dropout (DR4) 50% để thu được đầu ra cuối cùng sẽ được xử lý cho các lớp tiếp theo. Lớp Dense thứ 2, 3 (Dense2, Dense3) nhận đầu ra của lớp Dense đầu tiên với hàm kích hoạt ReLU, lớp này kết nối sử dụng hàm kích hoạt Softmax để phân phối xác suất cho từng chủ đề.
0	PhoBERT (Dat và cộng sự, 2020) là một mô hình tiếng Việt dựa trên kiến trúc được cải tiến từ RoBERT so với BERT. Mô hình này được huấn luyện chuyên biệt cho Tiếng Việt, sử dụng khoảng 20GB dữ liệu từ bộ văn bản Wikipedia và các trang tin tức Tiếng Việt khác, nhằm giải quyết hiệu quả các thách thức trong xử lý ngôn ngữ tự nhiên cho Tiếng Việt. Mô hình PhoBERT bao gồm hai phiên bản: PhoBERT-Base với 12 lớp và PhoBERT-Large với 24 lớp. Đầu vào của PhoBERT cần điều chỉnh để hoàn thiện tính tương thích với nhiệm vụ phân loại.
0	Dựa trên kiến trúc của PhoBERT, văn bản đầu vào của mô hình cần được chuyển đổi thành chuỗi token và được bổ sung thêm hai token quan trọng là [CLS] và [SEP] để đánh dấu cuối câu. Trong nhiệm vụ phân loại, trạng thái ẩn tương ứng với token [CLS] là đại diện cho toàn bộ câu, được sử dụng để thực hiện các nhiệm vụ phân loại, khác với vectơ trạng thái ẩn liên quan đến token biểu diễn từ thông thường. Do đó, khi đưa vào mô hình một câu trong quá trình huấn luyện, nhận được đầu ra là một vectơ trạng thái ẩn tương ứng với token đó. Lớp bổ sung thêm vào mô hình bao gồm các nơron tuyến tính chưa được huấn luyện, có kích thước là [kích thước vectơ trạng thái ẩn, số chủ đề],
0	có nghĩa là đầu ra của PhoBERT khi kết hợp với lớp phân loại là một vectơ chứa hai số, đại diện cho điểm số sử dụng làm cơ sở cho quá trình phân loại câu. Trong thử nghiệm của bài báo này, chúng tôi sử dụng mô hình PhoBERT-Base và tinh chỉnh các tham số được sử dụng trong mô hình như: max_length; pad_to_max_length = true (tự động thêm khoảng đệm vào phía sau [SEP]); learning_rate (adam); epochs; batch_size; cross_entropy. Dữ liệu được thu thập từ hệ thống lưu trữ nội bộ của Đài Phát thanh và Truyền hình thành phố Cần Thơ, với mỗi đoạn video có thời lượng trung bình là 3 phút. Tổng cộng, bộ dữ liệu bao gồm 14.503 mẫu, được phân thành 11 chủ đề chính được mô tả chi tiết trong Bảng 1.
0	Quá trình thực nghiệm mô hình được thực hiện dựa trên máy tính cá nhân, với cấu hình như sau: Intel® Core™ i5-7200U CPU @ 2.50GHz 2.70GHz, RAM 8GB. Kết quả thực nghiệm cho thấy hiệu quả phân loại của các thuật toán là tương đối tốt. Trong đó, kNN thực nghiệm với nhiều tham số k tương ứng {1, 2, 3, 4, 5, 6, 7}, và p tương ứng {1, 2} kết hợp với GridSearchCV tự động kiểm tra tìm ra giá trị tối ưu của k và hàm khoảng cách nhằm tăng hiệu suất mô hình. Kết quả thu được từ mô hình với độ chính xác 83% (k = 5, p = 1).
0	Tiếp theo, SVM sử dụng GridSearchCV áp dụng cho các bộ thông số khác nhau của mô hình được thiết lập, qua đó tìm được bộ thông số tối ưu nhất của hàm kernel = RBF, C = 10, Gamma = 1 đạt được kết quả cao nhất trong các trường hợp thực nghiệm với độ chính xác lên đến 91%. Trong mô hình CNN dựa trên các giá trị của tham số như: Batch_Size, Optimizer {RMSprop, Adam, Nadam, SGD}, Epoch tăng dần và điều chỉnh các thông số trong mô hình đã thay đổi đáng kể về tốc độ học, tỉ lệ lỗi, độ chính xác của mô hình. Kết quả thực nghiệm cho thấy giá trị Batch_Size = 32, Optimizer = SGD, Epoch = 23 cho kết quả với độ chính xác là 89%.
0	Mô hình PhoBERT để tìm kiếm siêu tham số, nhóm tác giả điều chỉnh kết hợp với siêu tham số tối ưu hóa Adam được đề xuất từ tác giả (Nguyen, D. Q và cộng sự, 2020), learning_rate {2e-5, 3e-5, 5e-5}, batch_size {128, 256}, Epoch. Ở giai đoạn thử nghiệm, các tiêu chí đánh giá mô hình trên tập dữ liệu kiểm tra đạt được kết quả mong đợi với độ chính xác 98% ứng với learning_rate = 5e-5, batch_size = 256, epoch = 5. So sánh kết quả cho thấy được các thuật toán có độ chính xác khi phân lớp có thể xem là tương đối tốt. Nhưng PhoBERT cho kết quả tốt hơn trên tập dữ liệu.
0	Trong nghiên cứu này, chúng tôi giới thiệu một giải pháp phân loại chủ đề tự động cho bản tin thời sự truyền hình, sử dụng các thuật toán học máy kết hợp với phương pháp trích đặc trưng dữ liệu TF-IDF, và SVD giảm chiều dữ liệu để tối ưu hóa tính toán nhanh và hiệu quả. Kết quả thực nghiệm thu được từ KNN, SVM, CNN, và PhoBERT cho thấy các kỹ thuật học máy có thể dễ dàng áp dụng vào các bài toán phân loại. So sánh hiệu suất giữa các mô hình cho thấy phương pháp PhoBERT đạt được kết quả tốt nhất. Trong thời gian tới, chúng tôi dự kiến sẽ phát triển, và cải tiến thuật toán, cũng như sử dụng phương pháp lai giữa các thuật toán để tối ưu hóa tốc độ quá trình huấn luyện và phân loại.
0	Bài báo đề xuất một mô hình học sâu dựa trên kiến trúc mạng CNN để xác định tâm làn đường đối với các thiết bị tự hành, đảm bảo độ chính xác trong nhiều điều kiện môi trường về ánh sáng và địa hình; yêu cầu độ phức tạp tính toán thấp, dễ dàng xây dựng tập dữ liệu huấn luyện. Trong những năm gần đây, cùng với sự phát triển của công nghệ thông tin, trí tuệ nhân tạo, lĩnh vực tự động hóa cũng đang phát triển hết sức mạnh mẽ. Nhu cầu về xe không người lái, các thiết bị tự hành ngày càng lớn. Trong đó, nhận dạng, xác định làn đường đối với các thiết bị tự hành (Autonomous vehicles) cũng như các hệ thống hỗ trợ lái xe (Driver Assistance Systems) là bài toán đặc biệt quan trọng,
0	đòi hỏi ngày càng cao về sự chính xác, an toàn ngay cả trong nhiều điều kiện khác nhau như ánh sáng, thời tiết thay đổi, sự đa dạng về môi trường hoạt động. Đối với các thiết bị tự hành trong công nghiệp trước đây, dữ liệu đầu vào thường lấy từ các cảm biến như cảm biến hồng ngoại, cảm biến từ trường... cùng với những thiết kế cố định đối với môi trường hoạt động như ánh sáng, line đường..., điều đó làm giới hạn phạm vi hoạt động cũng như tính đa dụng của các thiết bị. Những thiết bị tự hành hiện đại hiện nay như ô tô không người lái, máy bay không người lái, robot vận chuyển hàng hóa, drone giao hàng tự động thường sử dụng các hệ thống camera để thu thập dữ liệu cho việc xác định quỹ đạo di chuyển.
0	Dữ liệu từ cảm biến camera có thể được xử lý bằng các thuật toán xử lý ảnh thông thường như lọc màu, phát hiện cạnh, xoay ảnh, lọc nhiễu... để bóc tách được các line đường từ đó xác định được làn đường và tâm đường. Tuy nhiên, những thuật toán đó thường thiếu ổn định với nhiễu do độ sáng thay đổi, làn đường xuất hiện bóng cây, làn đường bị mưa ướt và thậm chí không thể xác định được làn đường khi line đường bị mất... Một ví dụ minh họa về sử dụng xử lý ảnh trong phát hiện line đường như trong hình 1.
0	Tuy nhiên, trên thực tế, vạch kẻ đường không phải lúc nào cũng rõ ràng như trong ví dụ trên, line đường có thể bị mờ hoặc bị các phương tiện khác che khuất dẫn đến thuật toán không thể bóc tách được. Đây là hạn chế rất lớn đối với các thuật toán xử lý ảnh thông thường. Thời gian gần đây, lĩnh vực trí tuệ nhân tạo nói chung và học sâu nói riêng đã đạt được rất nhiều thành tựu đột phá trong nhiều lĩnh vực của đời sống, công nghệ. Học sâu (deep learning) là một nhánh của ngành máy học dựa trên một tập hợp các thuật toán để cố gắng mô hình dữ liệu trừu tượng hóa ở mức cao
0	bằng cách sử dụng nhiều lớp xử lý với cấu trúc phức tạp, hoặc bằng cách khác bao gồm nhiều biến đổi phi tuyến nhằm xấp xỉ một hàm số bất kì giữa đầu ra (output) và đầu vào (input) [1]. Học sâu là một tập các kỹ thuật học máy mạnh sử dụng mạng neuron nhân tạo nhiều lớp. Nhiều kiến trúc học sâu khác nhau như mạng tri giác đa lớp (MLP: Multi Layers Perceptron), mạng neuron tích chập (CNN-Convolution Neutral Network) [2], mạng tin cậy sâu (DBN-Deep Belief Network) và mạng neuron hồi quy (RNN-Recurrent Neutral Network)
0	đã được áp dụng cho các lĩnh vực như thị giác máy tính, tự động nhận dạng giọng nói, xử lý ngôn ngữ tự nhiên, nhận dạng âm thanh ngôn ngữ và tin sinh học, chúng đã được chứng minh là tạo ra các kết quả rất tốt đối với nhiều nhiệm vụ khác nhau. Đã có rất nhiều nghiên cứu nhằm ứng dụng deep learning vào bài toán xác định làn đường và cho kết quả rất tốt [3, 4, 5]. Với các thuật toán sử dụng mô hình deep learning, các mô hình thường được xây dựng theo kiến trúc giống với các mạng Segmentation [6], từ hình ảnh đầu vào, mô hình sẽ phân loại các pixel có phải làn đường hay không.
0	Tuy nhiên, các mô hình này vẫn có một vài hạn chế: Xây dựng dữ liệu huấn luyện mô hình khó khăn, công việc gán nhãn cho các pixel trong ảnh có phải làn đường không tốn nhiều thời gian, dễ xảy ra nhầm lẫn. Mô hình có độ phức tạp tính toán lớn, cần nhiều lớp convolution ở cả phần encoder và decoder, cần các bộ xử lý mạnh mẽ như GPU mới có thể xử lý trong thời gian thực. Nghiên cứu này nhằm đề xuất một mô hình deep learning có khả năng tốt trong việc dự đoán đường đi cho các thiết bị tự hành mà chỉ có năng lực xử lý nhỏ, dễ dàng trong việc xây dựng dữ liệu huấn luyện.
0	Mục tiêu của nghiên cứu là xây dựng một mô hình học sâu có kích thước nhỏ nhất, đơn giản nhất mà vẫn đáp ứng được yêu cầu của bài toán xác định làn đường cho thiết bị tự hành trong nhiều điều kiện đường đi khác nhau. Đầu ra của thuật toán là vị trí tâm đường thiết bị cần hướng đến thay vì là hình ảnh phân loại các pixel như các mạng Segmentation. Việc đầu ra của thuật toán đơn giản làm giảm thiểu rất nhiều độ phức tạp tính toán của mô hình. Mô hình đề xuất được xây dựng trên kiến trúc mạng CNN (Convolution Neural Network) [6] thông thường.
0	Đối với các mạng neuron nhân tạo (Artificial Neural Network-ANN) thông thường, các đặc trưng của bức ảnh được trích xuất thông qua các thuật toán xử lý ảnh như lọc ảnh, thay đổi không gian màu, các phép biến hình, threshold... trước khi đưa vào mạng ANN. Khi sử dụng các bộ lọc số [7] để xem xét một vài đặc trưng của ảnh (Hình 5). Tuy nhiên, đối với các mô hình học sâu, các đặc trưng trên được trích xuất một cách tự động thông qua các lớp trích xuất đặc trưng, đối với đầu vào là ảnh số, các lớp tích chập (Convolution layers) thường được sử dụng và cho kết quả rất tốt.
0	Những mô hình học sâu như thế thường được gọi là mạng tích chập. Giống như các lớp ẩn khác, lớp tích chập lấy dữ liệu đầu vào, thực hiện các phép chuyển đổi để tạo ra dữ liệu đầu vào cho lớp kế tiếp (đầu ra của lớp này là đầu vào của lớp sau). Phép biến đổi được sử dụng là phép tính tích chập. Mỗi lớp tích chập chứa một hoặc nhiều bộ lọc - bộ phát hiện đặc trưng (filter - feature detector) cho phép phát hiện và trích xuất những đặc trưng khác nhau của ảnh. Mạng CNN là một tập hợp các lớp tích chập chồng lên nhau và sử dụng các hàm nonlinear activation như ReLU [8] và tanh để kích hoạt các trọng số trong các node.
0	Mỗi một lớp sau khi thông qua các hàm kích hoạt sẽ tạo ra các thông tin trừu tượng hơn cho các lớp tiếp theo. Đối với các thuật toán xử lý ảnh thông thường, các tham số của bộ lọc được chọn theo mục đích cụ thể như bộ lọc phát hiện cạnh, bộ lọc làm sắc nét, bộ lọc làm mờ... Tuy nhiên, trong mạng CNN các tham số sẽ được khởi tạo ngẫu nhiên (có thể theo một phân bố xác suất nào đó, hoặc bằng 0), trong quá trình huấn luyện, mô hình sẽ tự học được các tham số để trích xuất những đặc trưng cần thiết của bức ảnh.
0	Mô hình trong nghiên cứu này phát triển từ cấu trúc mạng CNN, đầu vào của mạng là ảnh RGB kích thước 160x240x3, mạng sử dụng hàm sigmoid hoặc hàm tanh là activation cuối cùng, trả về vị trí tâm đường chuẩn hóa. Trong mô hình CNN có 2 khía cạnh cần quan tâm là tính bất biến (Location Invariance) và tính bố cục (Compositionality). Với cùng một đối tượng, nếu đối tượng này được chiếu theo các góc độ khác nhau (translation, rotation, scaling) thì độ chính xác của thuật toán sẽ bị ảnh hưởng đáng kể. Pooling layer sẽ cho bạn tính bất biến đối với phép dịch chuyển, phép quay và phép co giãn.
0	Tính bố cục cục bộ cho ta các cấp độ biểu diễn thông tin từ mức độ thấp đến mức độ cao và trừu tượng hơn thông qua convolution từ các filter. Đó là lý do tại sao CNNs cho ra mô hình với độ chính xác rất cao, cũng giống như cách con người nhận biết các vật thể trong tự nhiên. Ngoài ra, để tăng tốc độ huấn luyện, nhóm tác giả sử dụng kỹ thuật chuẩn hóa trung gian (Batch Normalization) [9] sau các lớp tích chập và Dropout [10] để giảm hiện tượng overfiting.  Tổng tham số của mô hình là 387,297. Mô hình bao gồm 5 lớp Convolution, 2 lớp Dense, sử dụng hàm LeakyReLU làm activation (hàm phi tuyến) giữa các lớp, và hàm sigmoid ở lớp cuối cùng.
0	Dữ liệu hình ảnh (bên trái hình 7, 8) được thu thập bằng camera Astra trên xe mô hình và trên sa hình “Cuộc đua số” năm 2019 do tập đoàn FPT phối hợp Đài Truyền hình Việt Nam tổ chức (phần màu đỏ ở góc thể hiện hướng rẽ của xe tại ngã ba, ngã tư tiếp theo). Sử dụng Toolbox xây dựng trên python với opencv để gán nhãn tâm đường (màu xanh lá cây) như hình bên phải sau đó sử dụng xử lý ảnh để xác định vị trí tâm đường và chuẩn hóa theo chiều rộng của ảnh. Input data là tập ảnh bên trái với label là vị trí tâm đường chuẩn hóa tương ứng. Dữ liệu chuẩn bị gồm 15000 ảnh với các địa hình, cung đường trong các điều kiện ánh sáng khác nhau.
0	Dữ liệu được chia thành 3 tập con: tập huấn luyện (training set) để huấn luyện mô hình, tập kiểm soát (validation set) để giám sát sự overfiting của mô hình và tập đánh giá (test set) dùng để đánh giá kết quả mô hình sau khi huấn luyện. Kết quả huấn luyện của mô hình mạng Deep learning đề xuất xác định tâm đường được đưa ra trên hình 9, sai lệch chuẩn hóa (1,0 tương đương với 2,4m thực tế) của tâm đường của mô hình dự đoán so với nhãn trên các tập huấn luyện và tập kiểm soát. Sai lệch giảm qua nhiều vòng huấn luyện (epochs - mỗi vòng huấn luyện mô hình sẽ được học toàn bộ dữ liệu từ tập huấn luyện).
0	Kết quả huấn luyện ở vòng cuối cùng: sai lệch trên tập huấn luyện (training loss) là 0,0034 (0,8 cm), sai lệch trên tập kiểm soát (validation loss) là 0,0036 (0,9 cm) (hàm lỗi - loss function: mean squared error). Training loss và validation loss giảm đều, mượt cho thấy mô hình hội tụ tốt và không xảy ra hiện tượng overfiting. Trên tập kiểm tra, mô hình cho kết quả sai lệch (loss) cuối cùng là 0,0056 (1,3 cm), tương đối sát với trên tập kiểm soát (0,0036). Có thể thấy mô hình nhóm nghiên cứu đưa ra cách xác định vị trí tâm đường và đưa ra kết quả dự đoán tâm đường chính xác với vị trí tâm đường được gán nhãn.
0	Mô hình sau khi huấn luyện cũng cho kết quả dự đoán tốt khi thực nghiệm, xác định được chính xác vị trí tâm đường trong các điều kiện khác nhau. Tốc độ tính toán cao, độ trễ trung bình là 14 ms, đạt 60 fps trên Jetson Tx2, đảm bảo hoạt động trên thời gian thực đối với phần cứng có khả năng tính toán không quá mạnh. Kết quả nghiên cứu này có thể hoàn thiện ứng dụng trong bài toán điều khiển các thiết bị tự hành di chuyển trong các môi trường đô thị. Cụ thể, kết quả nghiên cứu này được áp dụng cho đội thi MTA_Race4Fun của Đại học Kỹ thuật Lê Quý Đôn trong cuộc thi “Cuộc đua số” năm 2019 do Tập đoàn FPT phối hợp Đài Truyền hình Việt Nam tổ chức.
0	Đội thi MTA_Race4Fun đã đạt chức vô địch toàn quốc. Trong quá trình thực nhiệm ở vòng bán kết (Miền Bắc) và vòng chung kết quốc gia, với điều kiện sa hình thi đấu khác nhau, điều kiện ánh sáng được thay đổi ngẫu nhiên (sử dụng các đèn led màu khác nhau tạo hiệu ứng, đi trong đường hầm, hiệu ứng bóng râm giả), địa hình thay đổi (đi trong đường hầm, trên cầu), có những đoạn đường mất line 2 bên (đoạn đường tuyết, đoạn đường không có line), kết quả mô hình đưa ra để dự đoán tâm đường đi chính xác cao, không gặp sự cố, đảm bảo thiết bị di chuyển trơn tru.
0	Tìm kiếm ảnh đa đối tượng là một bài toán quan trọng trong lĩnh vực tra cứu ảnh do sự đa dạng và tính phức tạp của hình ảnh. Trong bài báo này, một phương pháp tìm kiếm ảnh đa đối tượng dựa trên mạng R-CNN và cấu trúc KD-Tree được đề xuất nhằm phát triển những ưu điểm của mạng R-CNN trong việc xác định và phân loại từng đối tượng riêng biệt trên ảnh, đồng thời kết hợp với cấu trúc KD-Tree trong việc lưu trữ hình ảnh đã mang lại hiệu suất truy vấn cao và thời gian tìm kiếm ổn định.  Để giải quyết bài toán này, chúng tôi trích xuất và phân lớp các đối tượng trên tập dữ liệu hình ảnh bằng mô hình mạng R-CNN và lưu trữ trên cấu trúc KD-Tree.
0	Từ đó, mỗi ảnh đầu vào được phân đoạn theo từng đối tượng, trích xuất vector đặc trưng và thực hiện tìm kiếm tập ảnh tương tự dựa trên cấu trúc KD-Tree. Trên cơ sở đó, một mô hình tìm kiếm ảnh dựa trên mạng R-CNN và cấu trúc KD-Tree được đề xuất. Để minh chứng cho tính đúng đắn của cơ sở lý thuyết đã đề xuất, thực nghiệm được xây dựng trên bộ ảnh COCO với hiệu suất tìm kiếm ảnh là 0,6898. Kết quả thực nghiệm được so sánh với các công trình khác cùng trên bộ dữ liệu; điều này minh chứng cho tính khả thi và hiệu quả của phương pháp đề xuất, đồng thời có thể ứng dụng cho các bộ ảnh đa đối tượng.
0	Sự phát triển của các loại thiết bị điện tử làm cho dữ liệu đa phương tiện gia tăng nhanh theo thời gian, đặc biệt là ảnh đa đối tượng [1, 2]. Ngày nay, số lượng ảnh đa đối tượng gia tăng nhanh về số lượng và đa dạng về chủng loại thuộc nhiều lĩnh vực cũng là thách thức cho bài toán tìm kiếm ảnh đa đối tượng. Hơn nữa, việc xác định và bóc tách từng đối tượng riêng biệt trên ảnh đa đối tượng để có hiệu suất cao là một bài toán phức tạp. Sau khi phân đoạn và phân lớp từng đối tượng trên ảnh, việc lựa chọn một kỹ thuật học máy để thực hiện bài toán tìm kiếm ảnh để có hiệu suất truy vấn cao cũng là một thách thức.
0	Vì vậy, bài toán tìm kiếm ảnh đa đối tượng được nhiều nhóm nghiên cứu quan tâm cải tiến và nâng cao hiệu suất và thời gian truy vấn ổn định. Hiện nay, có nhiều phương pháp để thực hiện quá trình phát hiện và phân loại từng đối tượng trên ảnh đa đối tượng như R-CNN (Region Convolutional Neural Network), Fast R-CNN, Faster R-CNN [12, 13]. Trong bài báo này, mỗi hình ảnh đầu vào được phân đoạn thành các vùng để nhận diện đối tượng bằng mô hình mạng R-CNN đồng thời phân loại từng đối tượng trên mỗi hình ảnh đã mang lại hiệu suất cao. Hiệu suất tìm kiếm của bài toán truy vấn ảnh chịu ảnh hưởng của quá trình lưu trữ và tổ chức dữ liệu.
0	Đồng thời, một cấu trúc dữ liệu lưu trữ hình ảnh là yếu tố ảnh hưởng đến thời gian tìm kiếm. Hiện nay, có một số cấu trúc dữ liệu dạng cây được ứng dụng nhiều trong bài toán tìm kiếm ảnh như S-Tree [3] và KD-Tree [4, 8, 14]. Trên cơ sở kế thừa cấu trúc dữ liệu đa chiều, KD-Tree được sử dụng cho quá trình lưu trữ để tìm kiếm tập ảnh tương tự với ảnh đầu vào được đánh giá là khả thi và hiệu quả thông qua các công trình [4, 9]. Trong bài báo này, mỗi hình ảnh sau khi thực hiện phân đoạn theo từng đối tượng, phân loại bằng mô hình R-CNN và trích xuất vector đặc trưng được lưu trữ tại nút lá trên KD-Tree (k-Dimensional Tree).
0	Tại mỗi nút lá là tập hợp các hình ảnh có độ tương tự gần nhau nhất. Cấu trúc KD-Tree được đánh giá với khả năng mở rộng số nút lá dễ dàng, phù hợp cho những bộ ảnh có khả năng mở rộng số phân lớp, mở rộng khả năng lưu trữ và thời gian tìm kiếm ổn định do cấu trúc KD-Tree đa nhánh cân bằng [9]. Vì vậy, để phát triển những ưu điểm từ việc phân lớp ảnh bằng mô hình mạng R-CNN và phương pháp lưu trữ, tìm kiếm ảnh trên cấu trúc KD-Tree nên một phương pháp kết hợp mạng R-CNN với cấu trúc KD-Tree được đề xuất thực hiện trong bài báo này là cần thiết và đúng đắn.
0	Đóng góp của bài báo gồm: (1) Trích xuất và phân lớp từng đối tượng trên ảnh bằng mạng R-CNN; (2) Trích xuất vector đặc trưng và xây dựng cấu trúc KD-Tree để lưu trữ dữ liệu hình ảnh đã phân đoạn; (3) Đề xuất mô hình tìm kiếm ảnh; xây dựng thực nghiệm trên bộ ảnh đa đối tượng COCO [5] và so sánh với một số công trình khác trên cùng bộ dữ liệu. Kết quả thực nghiệm cho thấy hiệu suất truy vấn ảnh dựa trên mô hình đề xuất là khá cao. Phần còn lại của bài báo bao gồm: Phần 2 khảo các công trình nghiên cứu liên quan về trích xuất và phân loại đối tượng bằng mạng R-CNN, cấu trúc KD-Tree cho bài toán tìm kiếm ảnh;
0	Phần 3 trình bày mô hình mạng R-CNN để phát hiện và phân lớp đối tượng; Phần 4 xây dựng cấu trúc KD-Tree để lưu trữ dữ liệu hình ảnh; Phần 5 đề xuất mô hình truy vấn ảnh; Phần 6 xây dựng thực nghiệm và đánh giá kết quả; kết luận và hướng phát triển tiếp theo được trình bày trong Phần 7. Trong bài báo này, quá trình tìm kiếm ảnh đa đối tượng được thực hiện qua các giai đoạn gồm: (1) Trích xuất các đối tượng thị giác trên ảnh và phân lớp đối tượng; (2) xây dựng cấu trúc KD-Tree lưu trữ và tìm kiếm ảnh tương tự dựa trên cấu trúc dữ liệu đã xây dựng.
0	Vì vậy, một số công trình được khảo sát về trích xuất và phân loại đối tượng mạng R-CNN và tìm kiếm ảnh bằng cấu trúc KD-Tree nhằm phân tích ưu nhược điểm của từng phương pháp để đưa ra phương pháp kết hợp mạng R-CNN và cấu trúc KD-Tree để giải bài toán tìm kiếm ảnh đa đối tượng và nâng cao hiệu suất truy vấn. Chiao và cs. [6] đã thực hiện một phương pháp phát hiện và phân loại các khối u vú sử dụng mặt nạ R-CNN trên ảnh siêu âm. Mục đích của bài báo là xây dựng mô hình phát hiện, phân đoạn và phân loại tự động các tổn thương vú bằng hình ảnh siêu âm.
0	Dựa trên kỹ thuật học sâu, một kỹ thuật sử dụng các vùng mặt nạ với mạng lưới thần kinh phức hợp đã được phát triển để phát hiện tổn thương và phân biệt giữa lành tính và ác tính trên hình ảnh. Độ chính xác trung bình là 0,75 cho việc phát hiện và phân đoạn. Độ chính xác tổng thể của phân loại lành tính và ác tính trên hình ảnh là 0,85. Công trình này được đánh giá là khả thi và ứng dụng tốt cho lĩnh vực phát hiện sớm bệnh ung thư vú qua hình ảnh bằng mạng R-CNN. Bên cạnh đó, Kuznetsova và cs. [7] đã đề xuất một phương pháp phân tích ngữ nghĩa trực quan hình ảnh.
0	Công trình này đã trình bày một bộ sưu tập gồm 9,2 triệu hình ảnh (COCO, PASCAL VOC) được chú thích thống nhất để phân loại hình ảnh và phát hiện đối tượng bằng mô hình mạng R-CNN. Sau đó, các mối quan hệ trực quan giữa các đối tượng được xác định dựa trên ảnh đầu vào. Phương pháp đề xuất này được đánh giá là khả thi, hiệu quả và áp dụng cho nhiều bộ ảnh đa đối tượng khác nhau. Ram và cs. [10] sử dụng kỹ thuật tìm kiếm láng giềng k-NN dựa trên cấu trúc KD-Tree. Việc kết hợp này nhằm cải tiến hiệu suất tìm kiếm bằng cách xây dựng cây phân vùng không gian ngẫu nhiên để thực hiện các lược đồ tìm kiếm theo cấu trúc KD-Tree.
0	Tác giả đã chứng minh tính hiệu quả về thời gian truy vấn cũng như hiệu suất tìm kiếm. Trong công trình này, tác giả đề cập tới hai cải tiến: (1) cải thiện độ phức tạp tổng thể giải thuật tìm kiếm; (2) thực hiện đa chỉ mục trên cây KD-Tree để nâng cao hiệu quả tìm kiếm về mặt thời gian. Cùng thời điểm này, Chen và cs. [11] đã sử dụng hai kỹ thuật tìm kiếm láng giềng RNN (Range Nearest Neighbors) và NN (Nearest Neighbors) dựa trên cây KD-Tree. Kỹ thuật RNN nhằm giảm các tính toán khoảng cách không cần thiết bằng cách kiểm tra vị trí của đối tượng đang xét nằm bên trong hay bên ngoài vùng lân cận của điểm cần tìm.
0	Kỹ thuật NN được sử dụng để giảm các nút truy cập dư thừa bằng cách lưu chỉ số truy cập các điểm láng giềng. Thực nghiệm chứng minh tính hiệu quả của việc kết hợp các thuật toán tìm kiếm láng giềng RNN, NN và kNN trên cây KD-Tree là hiệu quả. Zhang [17] và cs. đã thực hiện xây dựng cấu trúc Vocabulary-KDTree nhằm thực hiện bài toán đối sánh hình ảnh. Trong công trình này, nhóm tác giả đã thực hiện hai quá trình: (1) phân cụm dữ liệu hình ảnh theo tính chất tương đồng và (2) đối sánh dữ liệu trực tuyến với một ảnh đầu vào. Cấu trúc Vocabulary-KDTree dựa trên đặc trưng SIFT (Scale-Invariant Feature Transform) bằng cách điều chỉnh trọng số tại các nút trên cây.
0	Cấu trúc Vocabulary-KDTree được chia thành hai nhóm: (1) nhóm chứa các đặc trưng hình ảnh và (2) nhóm các nút lá thực hiện điều chỉnh các trọng số liên quan đến quá trình huấn luyện để xây dựng Vocabulary-KDTree. Mô hình truy vấn ảnh được thực hiện theo hai pha. Tại pha offline, mỗi hình ảnh sau khi trích xuất đặc trưng được đối sánh và gom cụm với cấu trúc KD-Tree; từ đó xây dựng cây Vocabulary KD-Tree và thực hiện gom cụm lại trên cấu trúc này. Tại pha online, một ảnh đầu vào sau khi trích xuất đặc trưng được so sánh đặc trưng này với cấu trúc Vocabulary KD-Tree, tìm ra từ khóa làm cơ sở so sánh với đặc trưng đã trích xuất.
0	Cuối cùng, lọc bỏ những bất thường trong kết quả tìm kiếm và trả về kết quả tốt nhất. Narasimhulu và cs. [18] đã đề xuất một phương pháp tìm kiếm ảnh tượng tự dựa trên cấu trúc KD-Tree. Từ một ảnh đầu vào thực hiện tìm kiếm trên cấu trúc KD-Tree bằng thuật toán tìm kiếm theo số láng giềng nhiều nhất để làm căn cứ xác định phân lớp cho hình ảnh. Cuối cùng, tác giả dùng thang đo khoảng cách để thực hiện phân lớp các tập dữ liệu hình ảnh huấn luyện. Trong công trình này, cây KD-Tree được sử dụng trực tiếp để lưu trữ dữ liệu và phân lớp cho một ảnh đầu vào với kết quả tốt mà không mất nhiều chi phí trung gian.
0	Đây là một mô hình được đề xuất cho bài toán phân lớp và tìm kiếm ảnh dựa vào cấu trúc KD-Tree và được đánh giá là khá tốt. Những công trình nghiên cứu trên cho thấy tính khả thi cho bài toán trích xuất và phân loại đối tượng bằng R-CNN và tìm kiếm ảnh bằng cấu trúc KD-Tree. Tuy nhiên, sự kết hợp giữa kỹ thuật R-CNN và cấu trúc KD-Tree để nâng cao hiệu quả cho bài toán tìm kiếm ảnh đa đối tượng còn hạn chế về số lượng. Vì vậy, trong bài báo này, một mô hình trích xuất, phân loại đối tượng, trích xuất vector đặc trưng và lưu trữ trên cấu trúc KD-Tree được áp dụng cho bài toán tìm kiếm ảnh đa đối tượng và được thực hiện nhằm kết hợp những ưu điểm hiện có của kỹ thuật mạng R-CNN và cấu trúc KD-Tree.
0	Nâng cao hiệu suất phát hiện đối tượng là một trong những nhiệm vụ thách thức trong thị giác máy tính. Hiện nay có nhiều công trình sử dụng mạng R-CNN [1], Fast R-CNN và Faster R-CNN [12] để phát hiện các đối tượng riêng biệt trên ảnh. Mục đích của quá trình phát hiện đối tượng là phân loại đối tượng, nhận dạng đối tượng, nhận dạng mẫu, định vị đối tượng trên ảnh, tìm mối quan hệ giữa các đối tượng trên ảnh, v.v. Vì vậy, mạng R-CNN là một kỹ thuật tiên tiến, được sử dụng rộng rãi trong các công trình đã công bố trong những năm gần đây.
0	Kiến trúc của mạng R-CNN gồm ba thành phần: (1) trích xuất vùng đề xuất đối tượng (Region proposal), có tác dụng tạo và trích xuất các vùng chứa vật thể được bao bởi các bounding box; (2) trích xuất đặc trưng (Feature Extractor), giúp nhận diện hình ảnh từ các region proposal thông qua mạng CNN; (3) phân loại (classifier), dựa trên ảnh đầu vào là các đặc trưng để phân loại hình ảnh chứa trong vùng đề xuất về đúng nhãn (Hình 1) [1, 12, 13]. Mạng R-CNN được đánh giá là hiệu quả cho các bài toán phát hiện đối tượng, phân loại đối tượng trên ảnh do những ưu điểm như hiệu suất phát hiện đối tượng và phân loại đối tượng cao; một ưu điểm khác là mạng R-CNN có thể trích xuất các tính năng của hình ảnh một cách tự động.
0	Tuy nhiên, nhược điểm của phương pháp phát hiện và phân loại đối tượng trên R-CNN là nó phải vượt qua nhiều giai đoạn độc lập trong đó có trích xuất đặc trưng từ một mạng CNN trên từng vùng đề xuất tạo từ vùng chứa ảnh. Trong bài báo này, ưu điểm của mạng R-CNN được ứng dụng để phát hiện và phân loại từng đối tượng trên ảnh với hiệu suất phân loại cao. Quá trình phát hiện và phân loại đối tượng trên ảnh bằng mạng Mask R-CNN được minh họa trên Hình 1. Mỗi hình ảnh sau khi trích xuất từng đối tượng trên ảnh bằng mạng R-CNN là kết quả của quá trình trích xuất vector đặc trưng của ảnh phân đoạn.
0	Trên cơ sở này, mỗi vùng ảnh được trích xuất đặc trưng theo các nhóm đặc trưng về diện tích, chu vi, màu sắc, hình dạng và kết cấu gồm 81 thành phần cho mỗi vùng ảnh. Quá trình trích xuất vector đặc trưng có 81 thành phần được kế thừa từ công trình [4] và minh họa trên Hình 2. Mỗi hình ảnh sau khi phân đoạn đối tượng và trích xuất thành các vector đặc trưng được lưu trữ trên cấu trúc KD-Tree [14]. Mục đích xây dựng cấu trúc KD-Tree để lưu trữ hình ảnh đã phân đoạn là làm cho quá trình tìm kiếm nhanh và hiệu quả. Bên cạnh đó, cấu trúc KD-Tree có khả năng mở rộng số nhánh đã được chứng minh từ công trình [9].
0	Vì vậy, trong bài báo này, một cải tiến khác là mỗi hình ảnh được phân đoạn theo đối tượng bằng mạng R-CNN trước khi lưu trữ trên cấu trúc KD-Tree. Theo đó, nghiên cứu này đề xuất thuật toán xây dựng cấu trúc KD-Tree dựa trên tập vector đặc trưng vùng ảnh đối tượng được đề xuất. Trong thuật toán 1, hàm ExtractFeature được kế thừa từ công trình [4] còn hàm RCNN được thực hiện để phân đoạn ảnh dựa trên mạng R-CNN. Gọi n là số phần tử trong tập F để thực hiện xây dựng cây KD-Tree và h là chiều cao của cây. Khi xây dựng cây KD-Tree, thuật toán 1 cho phép thêm n phần tử vào cây có chiều cao là h.
0	Cây KD-Tree là cây cân bằng nên khi thêm phần tử vào cây, mọi phần tử đều phải được duyệt từ nút gốc đến nút lá. Vì vậy, chi phí để xây dựng cây KD-Tree chiều cao h có n phần tử là O(n × h). Vì h là hằng số, nên độ phức tạp của thuật toán 1 là O(n). Sau khi xây dựng cấu trúc KD-Tree gồm một nút gốc (Root) và các nút trong (Nodei) lưu trữ tập vector trọng số, các nút lá (Leaf) lưu trữ tập vector hình ảnh có độ tương tự gần nhau nhất. Cấu trúc KD-Tree được minh họa trên Hình 3. Ban đầu cấu trúc KD-Tree được xây dựng với bộ vector trọng số lưu trữ tại các nút trong là ngẫu nhiên nên hiệu suất phân bổ ảnh tương tự tại nút lá chưa cao.
0	Vì vậy, cần phải điều chỉnh vector tại các nút trong của KD-Tree để quá trình chèn vector đặc trưng hình ảnh vào KD-Tree sao cho nút lá chứa các hình ảnh cùng một phân lớp là nhiều nhất. Thuật toán 2 kế thừa các hàm SetLabel2Leaf và UpdateWeight từ công trình [4,9]. Gọi p là số lần điều chỉnh véc-tơ trọng số; h là chiều cao cây; m là số phần tử tham gia vào quá trình xây dựng cây theo từng Epoch i. Quá trình huấn luyện cấu trúc KD-Tree được thực hiện thông qua việc cập nhật trọng số để tạo cây và gán nhãn tại nút lá.
0	Vì vậy, chi phí để thực hiện thuật toán 2 là (p × h × m). Vì p, h là các hằng số nhỏ nên độ phức tạp của thực hiện thuật toán 2 là O(m). Trên cơ sở kết hợp mạng R-CNN và cấu trúc KD-Tree để áp dụng cho bài toán tìm kiếm ảnh đa đối tượng, chúng tôi đề xuất mô hình tìm kiếm ảnh (Hình 4). Mô hình tìm kiếm ảnh đa đối tượng dựa trên mạng R-CNN và cấu trúc KD-Tree gồm hai pha: Pha tiền xửa lý và pha truy vấn với các bước như sau: (1) Phát hiện và phân loại đối tượng trên ảnh bằng mạng R-CNN. (2) Trích xuất vector đặc trưng hình ảnh đã phân đoạn. (3) Xây dựng cấu trúc KD-Tree lưu trữ hình ảnh. (4) Ảnh đầu vào được phát hiện và phân loại bằng mạng R-CNN.
0	(5) Trích xuất vector đặc trưng cho ảnh đầu vào theo từng vùng đối tượng. (6) Tìm kiếm trên KD-Tree để trích xuất tập ảnh tương tự với ảnh đầu vào. Sau khi xây dựng cấu trúc KD-Tree, các nút lá lưu trữ tập dữ liệu hình ảnh. Vì vậy, quá trình tìm kiếm tập ảnh tương tự với một ảnh đầu vào (I) cần phải duyệt từ nút gốc đến nút lá. Nếu các vector đặc trưng của vùng ảnh phân đoạn của ảnh I thuộc về một nút lá leaf k thì trích xuất tập ảnh tương tự là tập ảnh tại nút lá leaf k.
0	Trong trường hợp ảnh I có nhiều ảnh phân đoạn I1, ..., In và các vector đặc trưng của ảnh I thuộc nhiều nút lá khác nhau thì tập ảnh tương tự với ảnh I chính là tập các ảnh thuộc tập các nút lá mà fIk tìm được. Gọi h là chiều cao của cấu trúc KD-Tree; k là số nhánh tối đa tại Node i bất kỳ, dữ liệu đầu vào là vector đặc trưng fi có n chiều. Khi truyền vector fi vào KD-Tree, thuật toán 3 duyệt qua các mức của cây. Tại mỗi mức trên KD-Tree chọn một nút tốt nhất và đi theo hướng đã chọn.
0	Do đó, tại mỗi mức có tối đa k phép so sánh để chọn nút tốt nhất. Mỗi lần so sánh thuật toán 3 duyệt qua n phần tử của vector fi. Vì vậy, tại mỗi mức số phép toán tối đa là k × n. Cây có chiều cao h, nên số phép toán tối đa để duyệt từ gốc đến lá theo một hướng được chọn là k × n × h. Vì h, k là hằng số nhỏ, nên độ phức tạp của thuật toán phụ thuộc vào n. Mặt khác, số chiều vector fi là cố định ban đầu nên n cũng là hằng số. Gọi C là giá trị hằng số và k × h × n < C nên k × h × n ≤ C × 1. Vậy độ phức tạp của thuật toán 3 là O(1).
1	Sự bùng nổ của thương mại điện tử kéo theo khối lượng lớn dữ liệu văn bản dưới dạng đánh giá, nhận xét của người tiêu dùng trên các nền tảng trực tuyến. Việc phân tích chính xác cảm xúc người dùng từ dữ liệu này đóng vai trò quan trọng trong tối ưu hóa chiến lược kinh doanh và nâng cao trải nghiệm khách hàng. Nghiên cứu này tập trung vào việc ứng dụng mô hình BERT (Bidirectional Encoder Representations from Transformers) trong bài toán phân tích cảm xúc đánh giá sản phẩm. Dữ liệu được thu thập từ hơn 120.000 bình luận tiếng Việt trên các sàn thương mại điện tử lớn trong vòng 6 tháng. Các bình luận được gán nhãn cảm xúc gồm tích cực, trung tính và tiêu cực.
1	Mục tiêu của nghiên cứu là đánh giá hiệu quả của BERT so với các mô hình truyền thống như SVM và ANN trong bối cảnh xử lý ngôn ngữ tự nhiên tiếng Việt. Mô hình BERT được tinh chỉnh (fine-tuning) trên tập dữ liệu huấn luyện gồm 96.000 mẫu, trong khi 24.000 mẫu còn lại được sử dụng cho kiểm thử. Quá trình tiền xử lý bao gồm chuẩn hóa văn bản, loại bỏ ký tự đặc biệt và xử lý từ viết tắt phổ biến. Mô hình sử dụng kiến trúc BERT-base với 12 tầng Transformer và 110 triệu tham số. Kết quả thực nghiệm cho thấy mô hình BERT đạt độ chính xác trung bình 92,4%, cao hơn đáng kể so với ANN truyền thống chỉ đạt 84,1% và SVM đạt 81,7%.
1	Thời gian huấn luyện tăng khoảng 25% so với ANN, tuy nhiên đổi lại là khả năng hiểu ngữ cảnh hai chiều giúp mô hình xử lý tốt các câu phức tạp, mỉa mai hoặc đa nghĩa. Kết quả nghiên cứu cho thấy BERT không chỉ cải thiện độ chính xác phân loại cảm xúc mà còn giúp doanh nghiệp khai thác sâu hơn insight từ khách hàng. Khi tích hợp vào hệ thống phân tích phản hồi tự động, doanh nghiệp có thể phát hiện sớm các vấn đề về chất lượng sản phẩm, dịch vụ giao hàng hoặc chăm sóc khách hàng. Thử nghiệm triển khai trên hệ thống thật cho thấy thời gian phản hồi khiếu nại giảm trung bình 32%, trong khi tỷ lệ hài lòng của khách hàng tăng từ 78% lên 89% sau ba tháng.
1	Dự báo nhu cầu tiêu thụ điện năng đóng vai trò quan trọng trong việc tối ưu hóa vận hành hệ thống điện và giảm chi phí sản xuất. Nghiên cứu này đề xuất sử dụng mạng nơ-ron nhân tạo ANN để dự đoán mức tiêu thụ điện năng của các hộ gia đình dựa trên dữ liệu lịch sử. Bộ dữ liệu được thu thập từ 1.200 hộ dân tại khu vực đô thị trong thời gian 24 tháng, bao gồm các yếu tố như công suất tiêu thụ theo giờ, nhiệt độ môi trường, độ ẩm và ngày trong tuần. Mục tiêu của nghiên cứu là xây dựng mô hình dự báo chính xác, hỗ trợ ngành điện lực trong việc điều phối nguồn cung và khuyến nghị sử dụng điện hiệu quả cho người dân.
1	Mô hình ANN được xây dựng với 3 lớp ẩn, mỗi lớp gồm 64, 128 và 64 nơ-ron, sử dụng hàm kích hoạt ReLU và thuật toán lan truyền ngược để tối ưu trọng số. Dữ liệu được chia theo tỷ lệ 70% huấn luyện, 15% xác thực và 15% kiểm thử. Sau 500 epoch huấn luyện, mô hình đạt sai số RMSE trung bình là 0,18 kWh, thấp hơn khoảng 27% so với mô hình hồi quy tuyến tính truyền thống. Kết quả cho thấy ANN có khả năng học tốt các mối quan hệ phi tuyến giữa các yếu tố đầu vào, đặc biệt là ảnh hưởng của thời tiết đến hành vi sử dụng điện của hộ gia đình.
1	Khi triển khai thử nghiệm trong hệ thống quản lý năng lượng thông minh, mô hình ANN giúp dự báo chính xác phụ tải điện theo giờ, góp phần giảm tình trạng quá tải cục bộ vào giờ cao điểm. Thống kê sau 6 tháng cho thấy tỷ lệ sự cố quá tải giảm 21%, đồng thời chi phí vận hành lưới điện giảm khoảng 15%. Ngoài ra, người dân được cung cấp khuyến nghị sử dụng điện hợp lý, giúp tiết kiệm trung bình 8–12% hóa đơn điện hàng tháng. Nghiên cứu chứng minh ANN là công cụ hiệu quả trong quản lý năng lượng, góp phần phát triển đô thị thông minh và bền vững.
1	Hồ sơ y tế điện tử chứa lượng lớn dữ liệu dạng văn bản như mô tả triệu chứng, tiền sử bệnh và ghi chú của bác sĩ. Việc khai thác hiệu quả nguồn dữ liệu này có thể hỗ trợ chẩn đoán và ra quyết định điều trị. Nghiên cứu đề xuất mô hình kết hợp BERT và ANN nhằm phân tích văn bản y khoa và dự đoán nguy cơ mắc bệnh mãn tính. Dữ liệu nghiên cứu gồm 18.000 hồ sơ bệnh nhân được ẩn danh từ một bệnh viện tuyến tỉnh trong giai đoạn 2022–2024. Mục tiêu là đánh giá khả năng hỗ trợ bác sĩ trong việc phát hiện sớm các bệnh như tiểu đường type 2 và tăng huyết áp.
1	Trong mô hình đề xuất, BERT được sử dụng để trích xuất đặc trưng ngữ nghĩa từ văn bản y khoa, sau đó các vector đặc trưng này được đưa vào mạng ANN để thực hiện phân loại nguy cơ bệnh. ANN gồm 2 lớp ẩn với 128 và 64 nơ-ron. Kết quả thử nghiệm cho thấy mô hình đạt độ chính xác 90,6%, cao hơn 14% so với ANN không sử dụng BERT. Đặc biệt, độ nhạy trong phát hiện bệnh tiểu đường đạt 93,2%, giúp giảm đáng kể tỷ lệ bỏ sót ca bệnh trong giai đoạn sớm.
1	Hệ thống được triển khai thử nghiệm tại khoa khám tổng quát cho thấy thời gian sàng lọc bệnh nhân giảm trung bình 35%, trong khi độ chính xác chẩn đoán ban đầu được cải thiện rõ rệt. Bác sĩ có thêm công cụ tham khảo đáng tin cậy, giúp giảm áp lực công việc và nâng cao chất lượng điều trị. Nghiên cứu cho thấy việc kết hợp BERT và ANN mở ra hướng đi tiềm năng cho các hệ thống hỗ trợ quyết định y tế thông minh, đặc biệt trong bối cảnh thiếu hụt nhân lực y tế tại nhiều địa phương.
1	Trong lĩnh vực giáo dục, nhu cầu tư vấn ngành học và lộ trình học tập ngày càng tăng, đặc biệt đối với học sinh trung học và sinh viên năm nhất. Nghiên cứu này đề xuất sử dụng BERT để xây dựng hệ thống tư vấn giáo dục thông minh dựa trên câu hỏi tự nhiên của người học. Dữ liệu gồm hơn 60.000 câu hỏi tư vấn thu thập từ các diễn đàn giáo dục trực tuyến trong vòng 1 năm. Mục tiêu của nghiên cứu là đánh giá khả năng hiểu ngữ cảnh và ý định người dùng của BERT trong môi trường tư vấn giáo dục tiếng Việt.
1	Mô hình BERT được huấn luyện để phân loại ý định câu hỏi thành các nhóm như chọn ngành, chọn trường, định hướng nghề nghiệp và kỹ năng cần thiết. Kết quả thử nghiệm cho thấy độ chính xác phân loại đạt 91,8%, cao hơn 18% so với mô hình ANN truyền thống. Ngoài ra, thời gian phản hồi trung bình của hệ thống chỉ khoảng 1,2 giây cho mỗi truy vấn, đáp ứng tốt nhu cầu sử dụng thực tế. BERT thể hiện ưu thế rõ rệt trong việc xử lý các câu hỏi dài, nhiều ý và mang tính suy luận.
1	Khi triển khai tại một cổng thông tin tư vấn tuyển sinh quy mô lớn, hệ thống ứng dụng BERT đã chứng minh hiệu quả rõ rệt trong việc tự động hóa quá trình tư vấn và hỗ trợ người học. Thống kê cho thấy hệ thống giúp giảm khoảng 40% khối lượng công việc thủ công của đội ngũ tư vấn viên, đặc biệt là trong các giai đoạn cao điểm tuyển sinh. Đồng thời, tỷ lệ người dùng đánh giá mức độ hài lòng tăng từ 75% lên 88% chỉ sau 4 tháng vận hành nhờ khả năng phản hồi nhanh, chính xác và phù hợp với nhu cầu cá nhân. Ngoài ra, hệ thống còn góp phần hạn chế thông tin tư vấn sai lệch, đảm bảo tính nhất quán trong nội dung cung cấp.
1	"Trong bối cảnh thương mại điện tử tại Việt Nam phát triển bùng nổ với tốc độ tăng trưởng kép hàng năm (CAGR) đạt trên 20%, khối lượng dữ liệu phản hồi từ khách hàng trên các nền tảng như Shopee hay Lazada là khổng lồ. Việc phân tích thủ công hàng triệu bình luận để hiểu thị hiếu người dùng là bất khả thi. Các phương pháp truyền thống như Bag-of-Words hay TF-IDF thường gặp hạn chế lớn trong việc nắm bắt ngữ cảnh, đặc biệt là với ngôn ngữ phức tạp, đa nghĩa và nhiều từ lóng như tiếng Việt (ví dụ: ""chất lượng chán quá"" so với ""không phải dạng vừa đâu"")."
1	"Nghiên cứu này đặt mục tiêu ứng dụng mô hình ngôn ngữ tiền huấn luyện BERT (Bidirectional Encoder Representations from Transformers), cụ thể là phiên bản PhoBERT được tối ưu cho tiếng Việt, để tự động hóa quy trình phân loại cảm xúc khách hàng, từ đó cung cấp cái nhìn sâu sắc cho các doanh nghiệp bán lẻ. Chúng tôi tiến hành thu thập dữ liệu thô gồm 50.000 bình luận từ các gian hàng điện tử và thời trang trên Shopee trong khoảng thời gian từ tháng 1/2025 đến tháng 6/2025. Dữ liệu sau đó được tiền xử lý qua các bước: loại bỏ emoji, chuẩn hóa teencode (ví dụ: ""huhu"" thành ""khóc"", ""uk"" thành ""ừ""), và tách từ sử dụng thư viện VnCoreNLP."
0	Kiến trúc chính được sử dụng là PhoBERT-base kết hợp với một lớp Fully Connected Layer ở đầu ra để phân loại cảm xúc thành 3 nhãn: Tích cực, Tiêu cực và Trung lập. Quá trình huấn luyện (Fine-tuning) được thực hiện trên GPU NVIDIA Tesla T4 với learning rate là 2e-5, batch size 32 qua 10 epochs. Để tránh hiện tượng overfitting, chúng tôi áp dụng kỹ thuật Early Stopping và Dropout với tỷ lệ 0.1, đồng thời sử dụng hàm mất mát Cross-Entropy Loss để tối ưu hóa trọng số của mạng. Kết quả thực nghiệm trên tập kiểm thử (chiếm 20% tổng dữ liệu) cho thấy mô hình PhoBERT đạt độ chính xác toàn cục (Overall Accuracy) lên tới 94.5%, vượt trội hoàn toàn so với mô hình LSTM truyền thống (chỉ đạt 82.3%) và SVM (78.9%).
1	"Cụ thể, chỉ số F1-score cho nhãn ""Tiêu cực"" đạt 0.92, chứng tỏ khả năng nhận diện các phàn nàn của khách hàng cực kỳ nhạy bén, ngay cả trong các câu có cấu trúc phủ định phức tạp như ""tưởng tốt mà hóa ra không dùng được"". Ma trận nhầm lẫn (Confusion Matrix) chỉ ra rằng tỷ lệ nhận diện sai giữa nhãn Tích cực và Tiêu cực chỉ dưới 1.5%. Ngoài ra, thời gian suy diễn trung bình cho mỗi bình luận là 0.04 giây, hoàn toàn đáp ứng được yêu cầu triển khai thời gian thực (real-time) cho các hệ thống chăm sóc khách hàng tự động (Chatbot) hiện nay."
1	Thị trường bất động sản tại các đô thị lớn như TP.HCM luôn biến động khó lường, chịu ảnh hưởng bởi hàng trăm yếu tố vĩ mô và vi mô khác nhau. Việc định giá nhà đất hiện nay chủ yếu dựa vào kinh nghiệm chủ quan của chuyên gia thẩm định hoặc phương pháp so sánh giá thị trường, dẫn đến sai số lớn và thiếu minh bạch. Nghiên cứu này đề xuất xây dựng một hệ thống định giá tự động dựa trên Mạng nơ-ron nhân tạo (Artificial Neural Network - ANN), nhằm hỗ trợ người mua và nhà đầu tư đưa ra quyết định chính xác hơn.
1	Mô hình không chỉ xem xét các yếu tố cơ bản như diện tích, số phòng ngủ, mà còn tích hợp các biến số địa lý phức tạp như khoảng cách đến trung tâm, mật độ giao thông, và quy hoạch hạ tầng trong tương lai để đưa ra mức giá sát thực tế nhất. Bộ dữ liệu được xây dựng từ 15.000 tin đăng bán nhà đất đã được xác thực giao dịch thành công trên các trang https://www.google.com/search?q=Batdongsan.com.vn và Chotot.com trong năm 2024-2025. Mỗi điểm dữ liệu bao gồm 18 đặc trưng đầu vào (input features) như: diện tích (m2), mặt tiền, lộ giới, hướng nhà, pháp lý, quận/huyện, khoảng cách tới trường học/bệnh viện gần nhất.
1	Dữ liệu được chuẩn hóa theo phương pháp Min-Max Scaling để đưa về khoảng [0,1] giúp mạng hội tụ nhanh hơn. Mô hình ANN được thiết kế với kiến trúc mạng truyền thẳng (Feedforward) gồm: 1 lớp đầu vào (18 nơ-ron), 3 lớp ẩn (Hidden Layers) với số lượng nơ-ron lần lượt là 128, 64, 32 sử dụng hàm kích hoạt ReLU để xử lý tính phi tuyến, và 1 lớp đầu ra sử dụng hàm Linear để dự đoán giá trị thực. Sau quá trình huấn luyện với 80% dữ liệu train và 20% dữ liệu test, mô hình cho kết quả sai số tuyệt đối trung bình (MAE) là 250 triệu VND trên các căn nhà có giá trị trung bình 5 tỷ VND, tương đương mức sai số khoảng 5%.
1	"Hệ số xác định R-squared đạt 0.89, cho thấy mô hình giải thích được gần 90% sự biến thiên của giá nhà dựa trên các đặc trưng đầu vào. Đặc biệt, phân tích độ nhạy (Sensitivity Analysis) cho thấy yếu tố ""vị trí hẻm xe hơi"" và ""quận trung tâm"" có trọng số tác động lớn nhất đến giá trị đầu ra. Kết quả này khả quan hơn nhiều so với phương pháp Hồi quy tuyến tính đa biến (Multiple Linear Regression) vốn chỉ đạt R-squared khoảng 0.72. Nghiên cứu này có thể được tích hợp vào các ứng dụng PropTech, giúp định giá sơ bộ chỉ trong vài giây."
1	Các bệnh lý về phổi như viêm phổi, lao phổi và gần đây là các biến chứng hậu COVID-19 đang tạo gánh nặng lớn lên hệ thống y tế, đặc biệt là tại các bệnh viện tuyến tỉnh nơi thiếu hụt bác sĩ chẩn đoán hình ảnh giàu kinh nghiệm. Việc đọc phim X-quang thủ công không chỉ tốn thời gian mà còn dễ xảy ra sai sót do sự mệt mỏi của bác sĩ khi phải xử lý hàng trăm ca mỗi ngày. Nghiên cứu này tập trung phát triển một hệ thống hỗ trợ chẩn đoán (Computer-Aided Diagnosis - CAD) sử dụng kỹ thuật Học sâu (Deep Learning), cụ thể là mạng Nơ-ron Tích chập (Convolutional Neural Network - CNN), nhằm tự động phát hiện các vùng tổn thương trên phổi, giúp sàng lọc nhanh và giảm tải áp lực cho đội ngũ y tế.
1	"Chúng tôi sử dụng bộ dữ liệu chuẩn hóa ChestX-ray14 kết hợp với 2.000 ảnh thực tế được thu thập và dán nhãn bởi các bác sĩ chuyên khoa tại bệnh viện địa phương để tăng tính thích nghi. Để giải quyết vấn đề dữ liệu không cân bằng (số ca bệnh ít hơn số ca bình thường), kỹ thuật Data Augmentation (tăng cường dữ liệu) được áp dụng: xoay ảnh, lật ngang, điều chỉnh độ sáng và độ tương phản. Mô hình được lựa chọn là ResNet-50, một kiến trúc CNN tiên tiến với 50 lớp sâu, sử dụng cơ chế ""skip connection"" để giải quyết vấn đề biến mất đạo hàm (vanishing gradient)."
1	Chúng tôi áp dụng phương pháp Transfer Learning: sử dụng trọng số đã được huấn luyện trên ImageNet và tinh chỉnh (fine-tune) lại các lớp cuối cùng để phù hợp với đặc trưng ảnh y tế đen trắng. Kết quả kiểm thử trên 500 ảnh bệnh án mới cho thấy mô hình đạt độ nhạy (Sensitivity) là 96.2% và độ đặc hiệu (Specificity) là 93.4%. Diện tích dưới đường cong ROC (AUC) đạt 0.97, một chỉ số rất cao trong chẩn đoán y khoa. Đáng chú ý, hệ thống tích hợp kỹ thuật Grad-CAM (Gradient-weighted Class Activation Mapping) cho phép trực quan hóa vùng nghi ngờ tổn thương bằng bản đồ nhiệt (heatmap), giúp bác sĩ dễ dàng đối chiếu vị trí bệnh lý.
1	Việc dự báo chính xác nhu cầu tiêu thụ điện năng (phụ tải điện) là bài toán sống còn đối với ngành điện lực, đặc biệt tại các khu công nghiệp trọng điểm nơi nhu cầu năng lượng biến động mạnh theo giờ làm việc và quy trình sản xuất. Dự báo sai lệch có thể dẫn đến lãng phí nhiên liệu phát điện (nếu dự báo thừa) hoặc gây quá tải lưới điện, cắt điện luân phiên (nếu dự báo thiếu), gây thiệt hại kinh tế nặng nề. Các mô hình thống kê truyền thống như ARIMA thường thất bại trong việc nắm bắt các chuỗi dữ liệu phi tuyến tính dài hạn và các yếu tố mùa vụ phức tạp.
1	Do đó, nghiên cứu này đề xuất sử dụng mạng Nơ-ron Hồi quy (RNN) với kiến trúc bộ nhớ dài-ngắn (LSTM) để nắm bắt các phụ thuộc chuỗi thời gian, nhằm tối ưu hóa vận hành lưới điện thông minh (Smart Grid). Dữ liệu đầu vào là chuỗi thời gian (Time-series) về công suất tiêu thụ điện được ghi nhận 30 phút/lần tại một khu công nghiệp ở Bình Dương trong suốt 3 năm (2023-2025). Bên cạnh dữ liệu lịch sử tiêu thụ, chúng tôi đưa thêm các biến ngoại sinh (exogenous variables) ảnh hưởng đến nhu cầu điện như: nhiệt độ môi trường, độ ẩm, ngày trong tuần (thứ 2-6 so với cuối tuần), và lịch nghỉ lễ.
1	Mô hình LSTM được thiết kế gồm 2 lớp LSTM xếp chồng (stacked LSTM) với 100 nơ-ron mỗi lớp, theo sau là một lớp Dropout 0.2 để giảm thiểu overfitting, và cuối cùng là lớp Dense để xuất ra giá trị dự báo cho 24 giờ tiếp theo. Hàm tối ưu Adam được sử dụng với learning rate giảm dần theo thời gian để đảm bảo hội tụ ổn định tại điểm cực tiểu toàn cục. Kết quả chạy thử nghiệm dự báo cho tháng 1/2026 cho thấy mô hình LSTM đạt sai số phần trăm tuyệt đối trung bình (MAPE) chỉ ở mức 2.8%, thấp hơn đáng kể so với mức cho phép của ngành điện là 5%.
1	Khi so sánh với mô hình ARIMA (MAPE 6.5%) và mạng nơ-ron truyền thống (MAPE 4.9%), LSTM thể hiện ưu thế vượt trội trong việc dự đoán các đỉnh phụ tải (peak load) vào giờ cao điểm sản xuất. Biểu đồ so sánh giữa giá trị thực tế và dự báo cho thấy hai đường gần như trùng khớp, ngay cả trong những ngày có thời tiết bất thường (nắng nóng đột ngột). Với độ chính xác này, đơn vị vận hành lưới điện có thể tiết kiệm ước tính khoảng 15 tỷ đồng mỗi năm nhờ tối ưu hóa kế hoạch huy động nguồn điện và giảm tổn thất truyền tải.
1	Mô hình BERT (Bidirectional Encoder Representations from Transformers), được phát triển bởi Google vào năm 2018, đã trở thành một trong các công cụ xử lý ngôn ngữ tự nhiên mạnh nhất trong lĩnh vực trí tuệ nhân tạo hiện đại. Mô hình này không chỉ dừng lại ở các bài toán ngôn ngữ truyền thống, mà còn được áp dụng rộng rãi trong y tế, đặc biệt trong việc phân tích dữ liệu lâm sàng và dự đoán kết quả điều trị bệnh nhân. Các nghiên cứu tại Bệnh Viện Johns Hopkins (Mỹ) cho thấy BERT có thể đạt độ chính xác lên đến 89,4% trong việc phân loại các chẩn đoán bệnh từ hồ sơ y tế điện tử.
1	Bài nghiên cứu này tập trung phân tích cách BERT được triển khai trong hệ thống y tế thực tế, các kết quả thực nghiệm đã đạt được, và tiềm năng phát triển trong tương lai của công nghệ này. Trong thập kỷ qua, lĩnh vực y tế đã chứng kiến một cuộc cách mạng số lớn khi các bệnh viện và cơ sở y tế chuyển sang sử dụng Hệ Thống Hồ Sơ Y Tế Điện Tử (Electronic Health Records — EHR). Theo báo cáo của Tổ Chức Y Tế Thế Giới (WHO) năm 2023, có hơn 72% các bệnh viện tại các quốc gia phát triển đã triển khai hệ thống EHR, từ đó tạo ra một lượng dữ liệu y tế khổng lồ cần được xử lý.
1	Tuy nhiên, một phần lớn dữ liệu này tồn tại ở dạng văn bản không cấu trúc, chẳng hạn như các ghi chú của bác sĩ, kết quả xét nghiệm được mô tả bằng lời, và báo cáo chẩn đoán. BERT ra mắt như một giải pháp đột phá khi mô hình này có thể hiểu ngữ nghĩa từ văn bản không cấu trúc với độ chính xác rất cao. Đặc biệt, BERT sử dụng cơ chế attention hai chiều, cho phép mô hình hiểu toàn bộ ngữ cảnh xung quanh mỗi từ, thay vì chỉ đọc một chiều như các mô hình trước đây.
1	Nghiên cứu này sử dụng bộ dữ liệu MIMIC-III (Medical Information Mart for Intensive Care III), một trong các bộ dữ liệu y tế mở lớn nhất thế giới, gồm hơn 2,2 triệu ghi chú lâm sàng từ hơn 52.722 bệnh nhân tại Bệnh Viện Beth Israel Deaconess Medical Center, Boston, Mỹ. Bộ dữ liệu này được chia thành 80% để huấn luyện mô hình (tương đương 1,76 triệu mục), 10% để kiểm tra trong quá trình huấn luyện (220.000 mục), và 10% còn lại để đánh giá kết quả cuối cùng. Quá trình tiền xử lý dữ liệu gồm bước loại bỏ các thông tin nhận dạng bệnh nhân theo quy định HIPAA, chuẩn hóa các thuật ngữ y tế, và mã hóa các chẩn đoán theo hệ thống phân loại quốc tế ICD-10.
1	Mô hình BERT được sử dụng trong nghiên cứu này là phiên bản BioBERT, một biến thể được huấn luyện riêng cho lĩnh vực y tế sinh học. BioBERT sử dụng 110 triệu tham số và được huấn luyện trên hơn 30 triệu bản tóm tắt từ cơ sở dữ liệu PubMed trong vòng 1 năm liên tục. Mô hình này xây dựng phía trên BERT gốc bằng cách bổ sung kiến thức y tế chuyên ngành, cho phép hiểu các thuật ngữ đặc thù hơn. Ngoài ra, nhóm nghiên cứu đã tích hợp một lớp phân loại (classification layer) bổ sung phía trên BioBERT để phân loại các bệnh trong 50 danh mục khác nhau, bao gồm các bệnh tim mạch, hô hấp, tiêu hóa, và ung thư.
1	Các chỉ số đánh giá chính được sử dụng trong nghiên cứu này gồm: độ chính xác tổng thể (Accuracy), độ chính xác theo lớp (Precision), độ thu hồi (Recall), và điểm F1-Score — là chỉ số tổng hợp phản ánh cân bằng giữa Precision và Recall. Ngoài ra, thời gian xử lý của mô hình cũng được ghi lại để đánh giá hiệu suất vận hành trong môi trường lâm sàng thực tế, nơi mà tốc độ phản hồi là yếu tố quan trọng hàng đầu. Kết quả cho thấy BioBERT vượt trội so với các mô hình phân loại đã được triển khai trước đây. Mô hình LSTM (Long Short-Term Memory) truyền thống chỉ đạt độ chính xác 76,3%, trong khi mô hình SVM (Support Vector Machine) đạt 71,8%.
1	Điều này cho thấy cơ chế attention hai chiều của BERT mang lại lợi thế đáng kể trong việc hiểu ngữ nghĩa của các đoạn văn y tế phức tạp. Hơn nữa, BioBERT cũng vượt mô hình GPT-2 được fine-tune cho cùng nhiệm vụ với độ chính xác 81,7%, cho thấy kiến trúc encoder-only của BERT phù hợp hơn với các bài toán phân loại văn bản so với các mô hình decoder. Từ tháng 3 đến tháng 8 năm 2024, hệ thống dựa trên BERT đã được thử nghiệm tại 3 bệnh viện lớn tại Việt Nam với tổng hơn 12.500 hồ sơ bệnh nhân được xử lý. Kết quả cho thấy mô hình đã hỗ trợ các bác sĩ phát hiện 94% các trường hợp bệnh sớm mà trước đây có thể bị bỏ qua trong quá trình sàng lọc ban đầu.
1	Thời gian chẩn đoán ban đầu giảm từ trung bình 4,2 ngày xuống còn 1,7 ngày, tương đương giảm 59,5% so với quy trình truyền thống không có hỗ trợ AI. Kết quả nghiên cứu cho thấy BERT mang lại lợi ích đáng kể trong lĩnh vực y tế. Tuy nhiên, cần nhấn mạnh rằng mô hình này không thể thay thế hoàn toàn bác sĩ, mà chỉ đóng vai trò hỗ trợ quyết định lâm sàng một cách thiết yếu. Các hạn chế hiện tại gồm: mô hình cần dữ liệu huấn luyện chất lượng cao trong mỗi lĩnh vực y tế cụ thể; chi phí tính toán khá lớn, đòi hỏi trang thiết bị GPU mạnh mẽ; và vấn đề về tính minh bạch của mô hình khi các quyết định của AI cần được giải thích cho nhân viên y tế hiểu được.
1	Trong tương lai, tích hợp BERT với các hệ thống phân tích hình ảnh y tế sẽ mở ra hướng phát triển mới, cho phép phân tích đồng thời cả dữ liệu văn bản và hình ảnh chẩn đoán. BERT và các biến thể của nó như BioBERT đã chứng minh khả năng mạnh mẽ trong lĩnh vực y tế. Với độ chính xác 89,4% và thời gian xử lý chỉ 0,34 giây mỗi hồ sơ, công nghệ này có tiềm năng phát triển lớn trong hệ thống y tế Việt Nam nói riêng và toàn thế giới nói chung. Đầu tư vào nghiên cứu phát triển BERT cho lĩnh vực y tế sẽ là một trong các chiến lược quan trọng trong kế hoạch số hóa hệ thống y tế quốc gia trong giai đoạn 2025–2030.
1	Mạng Thần Kinh Nhân Tạo (Artificial Neural Network — ANN) là một trong các nền tảng cốt lõi của trí tuệ nhân tạo hiện đại, được thiết kế để mô phỏng cách não bộ con người xử lý thông tin thông qua các nút (neuron) được kết nối với nhau. Trong lĩnh vực khí tượng học và nghiên cứu biến đổi khí hậu, ANN đã cho thấy hiệu suất đáng kể trong việc dự báo thời tiết ngắn hạn và dài hạn. Theo báo cáo của Tổ Chức Khí Tượng Thế Giới (WMO) năm 2024, các mô hình dự báo sử dụng ANN đã cải thiện độ chính xác dự báo nhiệt độ 72 giờ tới lên đến 91,3%, tăng 18,7 điểm phần trăm so với các phương pháp truyền thống.
1	Bài nghiên cứu này phân tích cách ANN được áp dụng cụ thể trong công nghệ dự báo thời tiết và đánh giá lũ lụt, cùng với các kết quả thực nghiệm đã được ghi nhận từ các dự án tại Việt Nam và khu vực Đông Nam Á. Biến đổi khí hậu đang trở thành một trong các thách thức lớn nhất mà nhân loại phải đối mặt trong thế kỷ 21. Theo báo cáo của Liên Hiệp Quốc năm 2023, nhiệt độ trung bình toàn cầu đã tăng 1,1 độ Celsius so với mức chuẩn tiền công nghiệp trong giai đoạn 1850–1900.
1	Tại Việt Nam, nhiệt độ trung bình cả nước đã tăng khoảng 0,89 độ Celsius trong 50 năm qua thuộc giai đoạn 1971–2020, theo dữ liệu chính thức của Cục Khí Tượng Thủy Việt Nam. Các hiện tượng khí hậu cực đoan như bão, lũ lụt và hạn hán đang xảy ra với tần suất cao hơn và mức độ nghiêm trọng hơn từng năm. Trong bối cảnh đó, nhu cầu có các hệ thống dự báo thời tiết chính xác và kịp thời là rất cấp thiết. ANN đã nổi lên như một công cụ đầy triển vọng khi mô hình này có thể học từ lượng dữ liệu khí tượng lịch sử khổng lồ và phát hiện các mẫu hình phức tạp mà các phương pháp toán học truyền thống không thể nắm bắt được.
1	Mô hình ANN được sử dụng trong nghiên cứu này có kiến trúc gồm 5 lớp được thiết kế phù hợp với đặc trưng của dữ liệu khí tượng: 1 lớp đầu vào (input layer) với 128 neuron, 3 lớp ẩn (hidden layers) với lần lượt 256, 512 và 256 neuron, và 1 lớp đầu ra (output layer) với 24 neuron tương ứng với 24 giờ trong một ngày cần dự báo. Hàm kích hoạch ReLU được sử dụng cho các lớp ẩn để giải quyết vấn đề vanishing gradient, trong khi hàm Linear được áp dụng cho lớp đầu ra. Tổng số tham số của mô hình là 467.248 tham số, được huấn luyện bằng thuật toán Adam optimizer với learning rate ban đầu là 0,001 và decay rate là 0,0001.
1	Mô hình được huấn luyện sử dụng dữ liệu thu thực từ 1.200 trạm quan trắc khí tượng phân rải trên toàn lãnh thổ Việt Nam, được thu thập trong giai đoạn 10 năm từ năm 2014 đến năm 2023. Các biến số đầu vào gồm: nhiệt độ không khí, độ ẩm tương đối, áp suất khí quyển, vận tốc gió, độ che phủ mây, và lượng mưa tại từng vị trí. Tổng lượng dữ liệu sau quá trình tiền xử lý và lấp đầy các giá trị còn thiếu bằng phương pháp nội suy không gian đạt hơn 105 triệu điểm dữ liệu, tạo thành một trong các bộ dữ liệu khí tượng lớn nhất tại Đông Nam Á.
1	ô hình được huấn luyện trong 500 epoch với batch size là 64 để tối ưu hóa quá trình học. Kỹ thuật early stopping được áp dụng với patience = 20 epoch nhằm tránh hiện tượng overfitting khi mô hình bắt đầu giảm hiệu suất trên dữ liệu kiểm tra. Ngoài ra, kỹ thuật dropout với tỷ lệ 0,3 cũng được áp dụng tại mỗi lớp ẩn để tăng cường khả năng tổng quát của mô hình. Quá trình huấn luyện được thực hiện trên hệ thống GPU NVIDIA A100 với tổng thời gian huấn luyện lên đến 72 giờ liên tục không gián đoạn.
1	Tại Đồng Bằng Sông Cửu Long, mô hình ANN đã được triển khai thử nghiệm dự báo lũ lụt trong suốt mùa mưa năm 2024 từ tháng 5 đến tháng 11. Kết quả cho thấy mô hình đã cảnh báo được 17 trong 19 sự kiện lũ lụt trước khi chúng thực sự xảy ra, đạt tỷ lệ cảnh báo sớm 89,5%, với thời gian cảnh báo trước trung bình là 38,6 giờ. Chỉ có 2 trường hợp lũ lụt bất ngờ không được cảnh báo do các điều kiện thời tiết đặc biệt bất thường không nằm trong mô hình huấn luyện. Đây là một thành quả đáng kể khi so sánh với hệ thống cảnh báo trước đây chỉ đạt tỷ lệ 71,2%.
1	So với các mô hình dự báo thời tiết truyền thống dựa trên các phương trình toán học (Numerical Weather Prediction — NWP), mô hình ANN cho thấy cải thiện đáng kể: độ chính xác dự báo 72 giờ tăng từ 72,6% lên 91,3% (tăng 25,8% tương đối), RMSE nhiệt độ giảm từ 2,9°C xuống còn 1,5°C. Hơn nữa, thời gian tính toán của mô hình ANN chỉ mất 12 giây cho mỗi chu kỳ dự báo, trong khi các hệ thống NWP truyền thống cần đến 45 phút để cho ra kết quả tương tự. Kết quả nghiên cứu cho thấy ANN có thể nâng cao đáng kể độ chính xác của dự báo thời tiết, đặc biệt trong bối cảnh biến đổi khí hậu ngày càng phức tạp và khó lường.
1	Tuy nhiên, một số vấn đề cần được giải quyết trong tương lai: mô hình cần được cập nhật thường xuyên với dữ liệu mới nhất từ các trạng quan trắc; phương pháp giải thích tại sao ANN đưa ra dự báo cụ thể (explainability) cần được cải thiện để tạo niềm tin với các chuyên gia khí tượng; và cần tích hợp dữ liệu vệ tinh và dữ liệu đại dương để cải thiện độ chính xác trong dự báo dài hạn hơn. Bên cạnh đó, chi phí đầu tư cơ sở hạ tầng tính toán cũng là một yếu tố cần xem xét khi triển khai mô hình này ở quy mô quốc gia.
1	ANN đã chứng minh tiềm năng vượt trội trong lĩnh vực dự báo thời tiết và giảm thiảu rủi ro lũ lụt tại Việt Nam. Với độ chính xác dự báo nhiệt độ 72 giờ đạt 91,3% và khả năng cảnh báo lũ lụt trước 38,6 giờ, công nghệ này có thể đóng góp đáng kể trong chiến lược ứng phó với biến đổi khí hậu của quốc gia. Đầu tư vào việc mở rộng mạng lưới quan trắc khí tượng và phát triển các mô hình ANN thế hệ tiếp theo sẽ là một bước đi quan trọng trong bảo vệ an toàn và tính bền vững của đất nước trong giai đoạn 2025–2030.
1	Mạng Tích Hợp Tích (Convolutional Neural Network — CNN) là một loại mạng thần kinh đặc biệt được thiết kế để xử lý dữ liệu hình ảnh, với khả năng trích xuất các đặc trưng (features) từ hình ảnh ở nhiều mức độ phức tạp khác nhau. Trong lĩnh vực bảo mật thông tin và xác minh danh tính, CNN đã được triển khai rộng rãi tại các quốc gia trên thế giới. Theo báo cáo của thị trường biometric toàn cầu năm 2024, thị trường công nghệ nhận dạng sinh học (biometrics) đạt giá trị 41,1 tỷ USD, trong đó CNN đóng góp hơn 60% trong các hệ thống nhận dạng khuôn mặt được triển khai.
1	Bài nghiên cứu này phân tích cách CNN được ứng dụng trong hệ thống xác minh giấy tờ tùy thân và nhận dạng khuôn mặt, cùng với các kết quả thực nghiệm được thu thập từ các dự án triển khai tại Việt Nam trong hai năm 2023–2024. Trong bối cảnh số hóa ngày càng sâu rộng, nhu cầu xác minh danh tính của cá nhân trong các giao dịch trực tuyến và dịch vụ công trở nên thiết yếu hơn bao giờ hết. Tại Việt Nam, chương trình Căn Cước Công Dân (CCCD) đã được triển khai với mục tiêu cấp CCCD cho 100% dân số.
1	Đến tháng 6 năm 2024, đã có hơn 82 triệu người dân được cấp CCCD gắn chip theo quy định mới. Bên cạnh đó, số lượng giao dịch tài chính trực tuyến tại Việt Nam đạt 18,2 triệu giao dịch mỗi ngày, tạo ra nhu cầu xác minh danh tính rất lớn và liên tục. CNN đã trở thành công nghệ cốt lõi trong các hệ thống này do khả năng nhận dạng hình ảnh cực kỳ chính xác và nhanh chóng. Mô hình CNN đầu tiên được áp dụng thành công trong nhận dạng hình ảnh là LeNet-5 năm 1998, nhưng sự đ突phá thực sự đến vào năm 2012 với mô hình AlexNet, đạt độ chính xác 84,6% trên bộ dữ liệu ImageNet lớn nhất thế giới.
1	Nghiên cứu này sử dụng kiến trúc CNN dựa trên mô hình ResNet-50, một trong các mô hình CNN sâu nhất và chính xác nhất trong lĩnh vực nhận dạng hình ảnh hiện tại. ResNet-50 gồm 50 lớp tích hợp tích với tổng 25,6 triệu tham số được huấn luyện. Kiến trúc này bao gồm các khối residual connection (skip connection), giải quyết triệt đầu vấn đề vanishing gradient trong các mạng sâu. Mô hình được chia thành 4 pha chính: Phase 1 trích xuất các đặc trưng cấp thấp như cạnh và góc, Phase 2–3 trích xuất đặc trưng cấp trung như nét mặt và vùng da, và Phase 4 trích xuất đặc trưng cấp cao phản ánh khuôn mặt tổng thể của mỗi cá nhân.
1	Mô hình được huấn luyện trên bộ dữ liệu gồm 3,2 triệu hình ảnh khuôn mặt của 150.000 cá nhân khác nhau, trong đó mỗi người có trung bình 21,3 hình ảnh được chụp ở các góc độ và điều kiện ánh sáng khác nhau. Bộ dữ liệu này được thu thập từ các nguồn công cộng và được gán nhãn (annotated) bởi đội chuyên gia gồm 45 người trong vòng 8 tháng liên tục. Quá trình data augmentation được áp dụng rộng rãi gồm: xoay hình ảnh (±15°), thay đổi độ sáng (±20%), và thêm nhiễu Gaussian với độ lệch chuẩn 0,05 để tăng tính đa dạng của dữ liệu huấn luyện.
1	Hệ thống được đánh giá trên 500.000 hình ảnh thử nghiệm, trong đó bao gồm các trường hợp đặc biệt như khuôn mặt bị che một phần bởi khẩu trang hoặc kính, điều kiện ánh sáng kém và các góc độ chụp không thông thường. Ngoài ra, quá trình thử nghiệm cũng bao gồm các tấn công adversarial, chẳng hạn như sử dụng ảnh in trên giấy, video deepfake được tạo bằng trí tuệ nhân tạo và mặt nạ 3D được sản xuất bằng công nghệ in 3D, nhằm kiểm tra mức độ bảo mật của hệ thống trước các mối đe dọa giả mạo ngày càng phức tạp.
1	Hệ thống CNN không chỉ nhận dạng khuôn mặt mà còn có khả năng xác minh tính xác thực của các giấy tờ tùy thân một cách tự động. Trong thử nghiệm với 50.000 giấy tờ gồm CCCD thật và các bản giả mạo, hệ thống đạt độ chính xác phân loại 98,4% với thời gian xử lý chỉ 0,18 giây mỗi giấy tờ. Mô hình đã phát hiện được các phương pháp giả mạo phổ biến, bao gồm: thay đổi ảnh thẻ (detection rate: 99,1%), sửa đổi tên hoặc số CCCD (96,7%), và sử dụng giấy tờ hợp lệ của người khác (97,8%).
1	Từ năm 2023 đến 2024, hệ thống CNN đã được triển khai tại 12 ngân hàng lớn tại Việt Nam trong quá trình xác minh khách hàng (KYC — Know Your Customer). Trong 6 tháng đầu triển khai, hệ thống đã xử lý hơn 4,8 triệu yêu cầu xác minh danh tính, trong đó phát hiện được 23.400 trường hợp giả mạo danh tính. Kết quả này đã giúp giảm tổn thất từ hoạt động gian lận xuống 67% so với giai đoạn trước khi áp dụng công nghệ CNN, tương đương tiết kiệm hàng nghìn tỷ VND cho hệ thống ngân hàng.
1	Các kết quả cho thấy CNN đạt hiệu suất rất cao trong nhận dạng khuôn mặt và xác minh giấy tờ tùy thân. Tuy nhiên, một số vấn đề cần được chú ý trong quá trình triển khai thực tế: độ chính xác giảm đáng kể khi khuôn mặt bị che hoặc hình ảnh có chất lượng thấp do điều kiện môi trường; các tấn công adversarial ngày càng tinh vi đòi hỏi mô hình phải được cập nhật liên tục; và vấn đề về bias khi các mô hình CNN có thể có độ chính xác thấp hơn đối với một số nhóm dân tộc nếu dữ liệu huấn luyện không đủ đa dạng.
1	Ngoài ra, cần thảo luận nghiêm túc về các vấn đề đạo đức liên quan đến giám sát bằng công nghệ biometric và bảo vệ quyền riêng tư của công dân trong khi vẫn đảm bảo an ninh quốc gia. CNN đã chứng minh là công nghệ không thể thiếu trong lĩnh vực bảo mật thông tin và xác minh danh tính tại Việt Nam. Với độ chính xác nhận dạng khuôn mặt lên đến 99,2% và khả năng phát hiện giả mạo giấy tờ đạt 98,4%, công nghệ này đang giúp bảo vệ hệ thống tài chính và dịch vụ công của quốc gia. Trong tương lai, tích hợp CNN với các công nghệ biometric khác như nhận dạng vân tay và đgiác mạc sẽ tạo ra các hệ thống bảo mật đa lớp, nâng cao độ tin cậy và độ an toàn hơn nữa cho nền tảng số quốc gia.
1	Mạng Thần Kinh Tái Phức Chay (Recurrent Neural Network — RNN) là một loại mạng thần kinh được thiết kế đặc biệt để xử lý dữ liệu tuần tự (sequential data), trong đó mỗi đầu ra phụ thuộc không chỉ vào đầu vào hiện tại mà còn phụ thuộc vào các trạng thái đã được xử lý trước đó. LSTM (Long Short-Term Memory), một biến thể đặc biệt của RNN, đã giải quyết vấn đề vanishing gradient, cho phép mô hình học từ dữ liệu lịch sử dài hạn một cách hiệu quả. Trong lĩnh vực tài chính, LSTM đã được áp dụng rộng rãi trong dự báo giá cổ phiếu trên toàn thế giới.
1	Theo một nghiên cứu tại Đại Học MIT năm 2023, các mô hình LSTM áp dụng cho cổ phiếu S&P 500 đạt độ chính xác dự báo giá ngày kế tiếp lên đến 78,4%, tỷ suất lợi nhuận trung bình hàng năm đạt 14,2%, trong khi chiến lược mua-giữ truyền thống chỉ đạt 10,1% trong cùng kỳ. Thị trường chứng khoán Việt Nam đã phát triển mạnh mẽ và liên tục trong hai thập kỷ qua, trở thành một trong các thị trường tài chính tăng trưởng nhanh nhất tại khu vực Đông Nam Á. Theo báo cáo của Ủy Ban Chứng Khoán Nhà Nước năm 2024, giá trị thị trường chứng khoán Việt Nam đạt 4.800 nghìn tỷ VND, với hơn 1,8 triệu tài khoản giao dịch mới được mở trong năm 2024 cho thấy sức hút ngày càng tăng. 1
1	Tại Sở Giao Dịch Chứng Khoán TP. Hồ Chí Minh (HOSE), khối lượng giao dịch trung bình hàng ngày đạt 18.500 tỷ VND, phản ánh thanh khoản thị trường đang ở mức cao. Trong bối cảnh đó, nhu cầu phân tích thị trường và dự báo giá cổ phiếu ngày càng tăng mạnh, đặc biệt từ phía các quỹ đầu tư tổ chức và nhà đầu tư chuyên nghiệp. RNN và LSTM đã nổi lên như các công cụ đầy triển vọng do khả năng xử lý dữ liệu chuỗi thời gian (time series) rất tốt và có thể nắm bắt các mẫu hình phức tạp trong dữ liệu giá.
1	Mô hình LSTM được thiết kế gồm 3 lớp LSTM chồng chất (stacked LSTM) với 128, 64, và 32 neuron tương ứng ở mỗi lớp. Mỗi neuron LSTM gồm 3 cửa kiểm soát: cửa quên (forget gate) quyết định thông tin nào cần xóa, cửa đầu vào (input gate) quyết định thông tin mới nào cần lưu trữ, và cửa đầu ra (output gate) kiểm soát thông tin được đưa ra. Phía trên các lớp LSTM, một lớp Dense (fully connected) gồm 64 neuron được thêm vào để tích hợp các đặc trưng, cuối cùng là lớp đầu ra với 1 neuron tương ứng với giá cổ phiếu được dự báo cho ngày kế tiếp. Tổng số tham số của mô hình là 89.345 tham số được tối ưu hóa trong quá trình huấn luyện.
1	Nghiên cứu này sử dụng dữ liệu của 30 cổ phiếu lớn nhất trên thị trường chứng khoán Việt Nam thuộc VN-Index Top 30, thu thập trong giai đoạn 5 năm từ tháng 1 năm 2019 đến tháng 12 năm 2023 — tổng 1.258 ngày giao dịch. Các đặc trưng đầu vào gồm: giá mở cửa, giá cao nhất, giá thấp nhất, giá đóng cửa, khối lượng giao dịch, chỉ tích RSI (Relative Strength Index), đường trung bình động MACD (Moving Average Convergence Divergence), và Bollinger Bands. Chuỗi thời gian được chia với tỷ lệ 70/15/15 cho huấn luyện, kiểm tra, và thử nghiệm cuối cùng tương ứng.
1	Dữ liệu được chuẩn hóa bằng phương pháp Min-Max Scaling về khoảng giá trị [0, 1] để tránh các cổ phiếu có giá trị lớn chi phối quá mức trong quá trình huấn luyện mô hình. Cửa sổ thời gian (lookback window) được đặt ở 60 ngày giao dịch, nghĩa là mô hình sử dụng 60 ngày lịch sử giá trước đó để dự báo giá ngày kế tiếp. Các giá trị ngoại lệ (outliers) được xử lý bằng phương pháp IQR (Interquartile Range) và loại bỏ các điểm nằm ngoài 3 lần độ lệch chuẩn so với trung bình, nhằm giảm ảnh hưởng của các biến động giá bất thường.
1	Thử nghiệm backtesting được thực hiện trên giai đoạn 6 tháng cuối năm 2023 với vốn đầu tư ban đầu là 1 tỷ VND đặt làm chuẩn. Kết quả cho thấy chiến lược đầu tư dựa trên tín hiệu của mô hình LSTM đạt tỷ suất lợi nhuận 22,8% trong 6 tháng (tương đương 45,6% hàng năm nếu duy trì), trong khi chiến lược mua-giữ VN-Index chỉ đạt 9,4% trong cùng giai đoạn. Tỷ suất thua lỗ cực đại (Maximum Drawdown) của chiến lược LSTM chỉ là -8,3%, cho thấy mô hình quản lý rủi ro tốt hơn đáng kể so với đầu tư theo cảm tâm hoặc các chỉ tích đơn lẻ.
1	Kết quả nghiên cứu cho thấy LSTM mang lại hiệu suất đầu tư đáng kể so với các phương pháp truyền thống trên thị trường chứng khoán Việt Nam. Tuy nhiên, cần nhấn mạnh một số điểm quan trọng mà các nhà đầu tư phải lưu ý: quá khứ không đảm bảo tương lai khi thị trường chứng khoán bị ảnh hưởng bởi nhiều yếu tố không lường trước được như chính sách vĩ mô, biến động toàn cầu, và sự kiện bất ngờ; mô hình LSTM cần dữ liệu huấn luyện lớn và chất lượng cao, đòi hỏi hệ thống thu thập dữ liệu mạnh mẽ; và overfitting là một rủi ro lớn khi dữ liệu thị trường Việt Nam còn tương đối hạn chế so với các thị trường phát triển.
1	Trong tương lai, tích hợp dữ liệu sentiment từ mạng xã hội (social media sentiment analysis) bằng BERT với mô hình LSTM có thể cải thiện độ chính xác dự báo hơn nữa. RNN/LSTM đã chứng minh giá trị rõ ràng trong lĩnh vực phân tích tài chính, đặc biệt trong dự báo giá cổ phiếu trên thị trường chứng khoán Việt Nam. Với độ chính xác dự báo chiều giá đạt 78,4% và tỷ suất lợi nhuận 22,8% trong 6 tháng thử nghiệm, công nghệ này mở ra hướng phát triển mới cho ngành dịch vụ tài chính trong nước.
0	Việt Nam sở hữu một hệ thống y học cổ truyền từ lâu đời với các loại cây thuốc và thảo dược đa dạng đã đóng góp lớn vào công tác phòng và chữa bệnh tại địa phương. Tuy nhiên, việc nhận biết chính xác cây thuốc đòi hỏi nhiều kiến thức và kinh nghiệm của người dùng. Do đó, nhu cầu tự động nhận dạng hình ảnh cây thuốc bằng ứng dụng di động là hữu ích với nền y học. Tập dữ liệu hình ảnh 41 loại cây thuốc và dược liệu được sử dụng trong nghiên cứu, tổng hợp từ nhiều nguồn, sau đó được huấn luyện qua các mô hình CNN.
0	Kết quả cho thấy mô hình EfficientB0 với phương pháp tăng cường dữ liệu và học chuyển giao phù hợp để triển khai trên thiết bị di động với tỷ lệ nhận dạng chính xác đạt ~94% cao hơn mô hình MobileNetV2 đạt ~90%. Ứng dụng di động nhận dạng cây thuốc là công cụ hỗ trợ các đối tượng hoạt động trong các lĩnh vực y học cổ truyền, nông nghiệp và giáo dục. Việt Nam là một quốc gia nhiệt đới với sự đa dạng về khí hậu và loại địa hình phù hợp cho sự sinh trưởng của nhiều loại dược liệu quý. Theo dữ liệu báo cáo từ World Bank Group (2023), hơn 60% dân số sống và làm việc ở nông thôn, trong đó có một số lượng lớn các nhóm dân tộc thiểu số sinh sống ở vùng miền núi và cao nguyên.
0	Người dân ở đây gặp khó khăn trong việc tiếp cận các loại thuốc hiện đại. Tuy nhiên, Việt Nam lại sở hữu một hệ thống y học cổ truyền từ lâu đời với các loại cây thuốc và thảo dược đã đóng góp lớn vào công tác phòng và chữa bệnh tại địa phương. Dựa trên kinh nghiệm truyền lại, người dân ở vùng nông thôn và miền núi từ xa xưa đã phân biệt cây thuốc dựa vào đặc điểm về mùi vị, lá, thân, rễ và hoa. Các loại cây thuốc có sẵn tại địa phương này đã được sử dụng để điều trị một số bệnh nói chung, ngoài ra một số loại còn được sử dụng như rau và gia vị hằng ngày để bảo vệ sức khỏe và cung cấp dinh dưỡng.
0	Theo thực tiễn và nghiên cứu khoa học đã chứng minh, cây thuốc và thảo dược ở Việt Nam có hiệu quả trong việc phòng và chữa các loại bệnh thông thường, thậm chí một số bệnh khó chữa. Cây thuốc và dược liệu có ý nghĩa to lớn trong lĩnh vực y tế cộng đồng, nguồn dược liệu này vừa là nguồn cung cấp thuốc cho quốc gia, vừa là mặt hàng xuất khẩu có giá trị. Theo công bố của các nhà nghiên cứu, Việt Nam có hơn 5100 loại cây thuốc và dược liệu, bên cạnh các loại dược liệu phổ biến còn có một số loại rất quý hiếm và có giá trị. Việc xác định chính xác các cây thuốc dược liệu là rất quan trọng để xác định các đặc tính dược liệu và ứng dụng chữa bệnh tiềm năng.
0	Do đó, để phục vụ công tác nghiên cứu, giảng dạy ứng dụng thực tế của cây thuốc, một ấn phẩm “Medical plant in Viet Nam” (World Health Organization Western Pacific, 1990) đã cung cấp thông tin gồm 200 cây thuốc phổ biến ở Việt Nam, kèm theo mô tả chi tiết bằng hình ảnh để nhận biết, phân loại các loại dược liệu và công dụng chữa bệnh của chúng. Một vấn đề tồn tại đối với số lượng dược liệu vô cùng phong phú như vậy là khó khăn trong việc nhận biết một cách chính xác cây thuốc dược liệu dựa trên các đặc điểm hình thái của một số dược liệu do chúng có tính tương đồng, bên cạnh đó việc thực hiện bằng mắt thường sẽ tốn nhiều thời gian, công sức và đòi hỏi nhiều kiến thức và kinh nghiệm của người dùng.
0	Trong lĩnh vực xử lý ảnh, gần đây có nhiều nghiên cứu về ứng dụng các mô hình Deep learning để nhận dạng hình ảnh trong nông nghiệp, lâm nghiệp, y học,... Cụ thể, nhóm nghiên cứu Trung Quốc đã ứng dụng Deep learning với mô hình ResNet26 để nhận dạng hình ảnh của 100 loại cây cảnh với độ chính xác đạt 91,78% (Sun et al., 2017). Một nghiên cứu khác từ Ấn Độ đã sử dụng mô hình Convolutional Neural Network (CNN) - MobileNetV2 để nhận dạng 13 loại hoa của cây thuốc với độ chính xác cao nhất đạt 98,23% (Bipin Nair et al., 2024).
0	Đặc biệt, với sự phát triển của thiết bị di động thông minh và các thiết bị nhúng, nhóm nghiên cứu Indonesia đã tiến hành thực nghiệm đánh giá độ chính xác của 3 mô hình CNN: VGG-16 (Liu & Deng, 2015), MobileNetV2 (Sandler et al., 2018) và DenseNet-121 (Huang et al., 2017) và sau đó chọn mô hình MobileNetV2 tích hợp vào ứng dụng di động với khả năng nhận dạng 24 loại cây thuốc ở Indonesia dựa trên tập hình ảnh về lá với độ chính xác đạt 97,74% (Sugiarto et al., 2023). Ở Việt Nam, Vo et al. (2019) đã sử dụng mô hình học sâu VGG16 kết hợp kỹ thuật phân loại LightGBM (Ke et al., 2017) để nhận dạng 10 loại cây dược liệu trong tự nhiên đạt độ chính xác 93,6%.
0	Bên cạnh đó, nghiên cứu của nhóm tác giả Nguyễn Quốc Trung (Nguyen & Truong, 2020) về nhận dạng cây thuốc được tiến hành trên các mô hình CNN như: VGG16, Resnet50 (He et al., 2015), InceptionV3 (Szegedy et al., 2016), DenseNet121, Xception (Chollet, 2017) và MobileNet. Kết quả thực nghiệm cho thấy mô hình Xception đạt độ chính xác cao 88,26%. Với tầm quan trọng của việc nhận dạng cây thuốc đặc thù ở Việt Nam, sự phát triển mạnh mẽ của công nghệ học sâu (Deep learning) cùng với sự phổ biến của thiết bị di động thông minh với đặc điểm giới hạn về tài nguyên và khả năng tính toán so với máy tính,
0	giải pháp nhận dạng cây thuốc trên ứng dụng thiết bị di động được đề xuất trong nghiên cứu, tích hợp mô hình mạng nơ-ron tích chập hiện đại EfficientB0 kết hợp với phương pháp tăng cường dữ liệu và học chuyển giao để tăng hiệu suất nhận dạng. Bên cạnh đó, mô hình MobileNetV2 cũng được sử dụng để so sánh kết quả dựa trên bộ dữ liệu hình ảnh của 41 loại dược liệu ở Việt Nam được thu thập từ các nguồn từ internet, cơ sở dữ liệu quốc gia và thực tiễn.MobileNetV2 là một mô hình mạng nơ-ron tích chập được giới thiệu vào năm 2018 bởi nhóm nghiên cứu Google (Sandler et al., 2018).
0	Mô hình được thiết kế để giảm thiểu số lượng phép tính và dung lượng mô hình, phù hợp để triển khai trên các thiết bị di động và hệ thống tính toán hạn chế. Mô hình này là sự cải tiến của MobileNetV1 (Howard et al., 2017), MobileNetV2 vẫn sử dụng kỹ thuật tích chập tách biệt chiều sâu (depthwise separable convolution) thành hai bước tích chập chiều sâu (depthwise convolution) và tích chập điểm (pointwise convolution) để giảm đáng kể số lượng phép tính cần thực hiện (Hình 1). Tuy nhiên, mô hình có thêm cải tiến ở khối dư đảo ngược (Inverted Residual Block). Lớp đầu tiên mở rộng số lượng kênh đầu vào thông qua một lớp pointwise convolution (1x1).
0	Sau khi mở rộng số lượng kênh, dữ liệu đi qua một lớp depthwise convolution – tích chập riêng cho từng kênh để giảm số lượng phép tính và tăng hiệu suất. Tiếp theo, số lượng kênh được thu hẹp về kích thước ban đầu thông qua một lớp pointwise convolution (1x1). Cuối cùng, kết nối tắt chỉ được sử dụng nếu kích thước của đầu vào và đầu ra tương thích (chiều rộng, chiều cao và số lượng kênh giống nhau). Ngoài ra, nút thắt tuyến tính (Linear bottleneck) là thành phần cuối của khối Inverted Residual, thay vì sử dụng hàm kích hoạt phi tuyến tại bước này, Linear bottleneck giữ tuyến tính để tránh mất mát thông tin.
0	Kiến trúc mô hình EfficientNet được đề xuất bởi nhóm nghiên cứu của Google AI (Tan & Le, 2019). Mô hình phát triển dựa trên mạng nơ-ron tích chập bằng cách mở rộng đồng thời chiều rộng, chiều cao và độ phân giải với một tỉ lệ phù hợp, giúp mô hình tăng độ chính xác mà vẫn giữ được hiệu suất tốt. EfficientNet-B0 là một trong những kiến trúc của họ EfficientNet. EfficientNetB0 sử dụng MBConv (Mobile Inverted Bottleneck Convolution) với các cải tiến thêm so với Inverted Residual Block bao gồm thêm các kỹ thuật như Squeeze-and-Excitation (SE) để tối ưu hóa hiệu suất (Hình 2).
0	Học chuyển giao có thể được triển khai dựa trên bất kỳ mô hình học máy nào, kỹ thuật được đánh giá là trở nên phổ biến hơn khi sử dụng với học sâu (Ali et al., 2023; Gupta et al., 2022). Một mạng nơ-ron tích chập (CNN) sẽ được đào tạo trên một tập dữ liệu nhất định để trích xuất các đặc điểm. Dựa trên các đặc điểm đã học, mô hình có thể dự đoán kết quả. Để cải thiện độ chính xác của CNN, cần có một lượng lớn dữ liệu để đào tạo mô hình. Trong thực tế, có một số khó khăn khi thu thập một lượng lớn dữ liệu.
0	Và một trường hợp khác – nếu một phần dữ liệu đã thu thập bị thiếu thì dữ liệu đó không đủ tin cậy để triển khai với các mô hình CNN. Giải pháp cho những vấn đề này là kỹ thuật chuyển giao học tập. Hình 3 trình bày về kỹ thuật học chuyển giao, một mô hình có thể duy trì các tham số tối ưu dựa trên quy trình đào tạo trên một tập dữ liệu phổ biến như ImageNet (Kornblith et al., 2018). Sau đó, các đặc điểm đã học được sử dụng lại cho tác vụ huấn luyện tiếp theo trong quá trình tinh chỉnh (fine-turning). Nhờ đó, độ chính xác tổng thể của mô hình được cải thiện.
0	Mục tiêu của bước này là thu thập hình ảnh dữ liệu cây thuốc và thảo dược để dùng cho huấn luyện mô hình. Đây là bước quan trọng và quyết định đến chất lượng của mô hình CNN. Nghiên cứu này đã thu thập hình ảnh của 41 loại cây thuốc và thảo dược phổ biến, việc thu thập hình ảnh và gắn nhãn loại dược liệu dựa trên các thông tin mô tả chi tiết về tên khoa học, tên dân gian, họ cây thuốc kèm hình vẽ mô tả hình thái tự nhiên của lá hoặc hoa và được đặc tả chi tiết trong ấn phẩm “Medical plant in Viet Nam” (World Health Organization Western Pacific, 1990).
0	Đối với mỗi loại dược liệu thu thập tối thiểu 15 hình ảnh, tối đa 25 hình ảnh để bảo đảm độ cân bằng giữa các lớp trong bài toán phân loại. Các nguồn để thu thập hình ảnh từ các trang web, cơ sở dữ liệu công cộng như ImageNet, Kaggle datasets, Google Images hoặc Flickr... Bên cạnh đó, nếu dữ liệu đặc thù hoặc không có sẵn trực tuyến, có thể tự chụp ảnh bằng máy ảnh kỹ thuật số hoặc điện thoại thông minh. Ngoài ra, cơ sở dữ liệu chuyên ngành, tạp chí cũng là nguồn để cung cấp các hình ảnh cây thuốc liên quan.
0	Tiêu chí dữ liệu hình ảnh: dùng kỹ thuật quét hoặc chụp rõ nét hình ảnh cây thuốc, đặc biệt phải chứa đặc trưng nhận dạng của loại cây đó, có thể lá hoặc hoa của cây thuốc trên nền đơn sắc hoặc nền tự nhiên. Tăng cường hình ảnh (Image Augmentation) là kỹ thuật được sử dụng để tăng số lượng của dữ liệu hình ảnh cho quá trình huấn luyện mà không cần thu thập thêm dữ liệu mới (Mikolajczyk & Grochowski, 2018), kỹ thuật này áp dụng các biến đổi ngẫu nhiên lên hình ảnh gốc. Điều này đặc biệt quan trọng khi dữ liệu huấn luyện ban đầu hạn chế, giúp cải thiện khả năng tổng quát hóa của mô hình học sâu, giảm nguy cơ overfitting và làm cho mô hình trở nên mạnh mẽ hơn khi gặp phải dữ liệu mới, chưa từng thấy trước đó.
0	Một số phép biến đổi hình ảnh phổ biến như: xoay hình ảnh, dịch chuyển hình ảnh, phóng to/thu nhỏ hình ảnh, lật hình ảnh, biến dạng hình học, thay đổi độ sáng, thay đổi độ tương phản, thêm nhiễu, thay đổi hình dạng, thay đổi độ bão hòa (Hình 4). Trong thực nghiệm này, phép biến đổi hình ngẫu nhiên sử dụng hàm ImageDataGenerator trong thư viện Keras với tham số (Bảng 1) để tăng số lượng dữ liệu hình ảnh lên 9 lần đối với tập dữ liệu huấn luyện (training set). Trong bài toán CNN nhận dạng hình ảnh, việc phân chia tập dữ liệu là một bước quan trọng để đảm bảo rằng mô hình được huấn luyện và học được các đặc trưng từ dữ liệu, cũng như có khả năng tổng quát hóa tốt trên các dữ liệu mới.
0	Sau khi thu thập dữ liệu hình ảnh 41 loại cây thuốc, dữ liệu này được gắn nhãn theo thư mục. Cụ thể, mỗi loại cây thuốc sẽ có một thư mục riêng với tên nhãn là tên khoa học của cây thuốc, trong thư mục chứa các hình ảnh của loại cây thuốc này. Ngoài ra, để thực hiện huấn luyện và kiểm tra, tập dữ liệu cần chia thành các tập: tập huấn luyện (training set) được áp dụng kỹ thuật tăng cường dữ liệu để huấn luyện mô hình; tập xác thực (validation set) được sử dụng để điều chỉnh các siêu tham số (hyperparameters) và kiểm tra khả năng của mô hình trên dữ liệu chưa gặp trong quá trình huấn luyện;
0	tập kiểm tra (test set) chiếm tỷ lệ ~15% bao gồm các hình ảnh chưa được mô hình huấn luyện, được sử dụng để đánh giá cuối cùng về hiệu suất của mô hình sau khi quá trình huấn luyện và tinh chỉnh đã hoàn tất (Bảng 2). Tiền xử lý dữ liệu hình ảnh là một bước thiết yếu nhằm chuẩn bị và cải thiện chất lượng dữ liệu trước khi đưa vào mô hình học máy hoặc học sâu. Quá trình này giúp làm sạch bao gồm loại bỏ các yếu tố không cần thiết, làm nổi bật các đặc trưng quan trọng, chuẩn hóa, làm giảm thiểu hiện tượng quá khớp (overfitting), tối ưu hóa thời gian huấn luyện và tài nguyên tính toán để mô hình có thể học và nhận dạng các đặc trưng một cách hiệu quả nhất.
0	Các bước tiền xử lý phổ biến trong nhận dạng hình ảnh áp dụng trong thực nghiệm gồm: Điều chỉnh kích thước hình ảnh (Resizing): Đưa tất cả các hình ảnh về cùng một kích thước chuẩn mà mô hình yêu cầu (ví dụ: 224x224 pixel). Điều này giúp giảm thiểu sự biến đổi về kích thước, giúp mô hình xử lý dữ liệu đồng nhất và tiết kiệm tài nguyên tính toán. Chuẩn hóa giá trị pixel (Normalization): Công thức chuẩn hóa (1) được áp dụng để đưa giá trị pixel về cùng một phạm vi [0, 1], bằng cách chia giá trị pixel tại vị trí i cho 255 (Koo & Cha, 2017). Mục đích nhằm giảm thiểu sự chênh lệch về giá trị và giúp mô hình hội tụ nhanh hơn trong quá trình huấn luyện.
0	Mô hình MobileNetV2 và EfficientNetB0 kết hợp học chuyển giao được sử dụng để huấn luyện mô hình nhận dạng. Thông tin về hai mô hình được mô tả ở Bảng 3. Việc chạy thực nghiệm hai mô hình trên môi trường Google Colab với ngôn ngữ lập trình Python được tiến hành trong nghiên cứu. Google Colab với ưu điểm hỗ trợ GPU Tesla T4 giúp quá trình huấn luyện các mô hình diễn ra nhanh hơn. Trước khi bắt đầu quá trình huấn luyện, mô hình cần phải khởi tạo giá trị các siêu tham số vì các tham số này không được học trực tiếp từ dữ liệu mà được sử dụng để điều khiển quá trình huấn luyện mô hình.
0	Một số siêu tham số quan trọng cần thiết lập: tốc độ học (learning rate), số vòng lặp qua toàn bộ tập dữ liệu huấn luyện (epochs), kích thước lô dữ liệu (batch size), thuật toán tối ưu hóa (Optimizer). Để đánh giá toàn diện về hiệu suất của mô hình CNN và xác định khả năng tổng quát hóa của mô hình trên dữ liệu chưa từng thấy trước đó, đồng thời so sánh hiệu suất giữa các mạng CNN, các chỉ số Accuracy, Precision, Recall, F1-score (D. M. W. Powers, 2011) được sử dụng để đánh giá hiệu suất. Độ chính xác (Accuracy) được tính bằng tỷ lệ giữa số lượng mẫu phân loại đúng (Correct prediction number) trên tổng số lượng mẫu phân loại (sample number).
0	Độ chuẩn xác (Precision): Tỷ lệ giữa số lượng dự đoán dương tính đúng so với tổng số dự đoán dương tính của mô hình. Độ chính xác của các dự đoán dương tính được tập trung (giảm thiểu dự đoán sai dương tính). Độ phủ (Recall): Tỷ lệ giữa số lượng dự đoán dương tính đúng so với tổng số lượng mẫu thực tế là dương tính, tập trung vào khả năng phát hiện tất cả các mẫu dương tính (giảm thiểu bỏ sót mẫu dương tính). F1-score: Trung bình cân bằng của độ chuẩn xác và độ phủ, kết hợp cả hai chỉ số này thành một chỉ số duy nhất.
0	F1-Score hữu ích khi quan tâm sự cân bằng giữa độ chuẩn xác và độ phủ của mô hình, đặc biệt trong các tình huống mà dữ liệu không cân bằng. Trong đó, TP-True Positive là số dự đoán chính xác, FP-False Positive và FN-False Negative là số dự đoán sai và số dự đoán bị sót. Các thiết bị di động, máy tính bảng và hệ thống nhúng IoT thường có hạn chế về phần cứng và khả năng xử lý, do đó các mô hình CNN không thể triển khai trực tiếp trên các thiết bị này. Để khắc phục, việc tối ưu mô hình, giảm kích thước để cải thiện tốc độ xử lý, đồng thời tiết kiệm bộ nhớ và năng lượng là cần thiết.
0	Google đã phát triển TensorFlow Lite, một phiên bản đơn giản hơn của TensorFlow, giúp giảm yêu cầu về tài nguyên bộ nhớ. Ứng dụng demo trên thiết bị di động được phát triển bằng ngôn ngữ Dart thông qua Flutter - một khung nguồn mở do Google phát triển và hỗ trợ. Giao diện ứng dụng được thiết kế theo như mô tả ở Bảng 4. Thư viện Google ML Kit được sử dụng để tích hợp các tính năng nhận dạng bằng mô hình CNN trên thiết bị mà không cần kết nối internet, giúp tiết kiệm tài nguyên và thời gian xử lý.
0	Dựa trên kết quả về các chỉ số hiệu suất thu được từ quá trình huấn luyện các mô hình, kết quả được thể hiện ở Bảng 5 cung cấp cái nhìn về hiệu suất của các mô hình CNN nhận dạng tập dữ liệu cây thuốc. Phương pháp tăng cường ảnh (data augmentation) có tác động lớn đến tập dữ liệu để huấn luyện mô hình CNN. Phương pháp đã cải thiện đáng kể hiệu suất nhận dạng của các mô hình, cụ thể: dữ liệu tăng cường với mô hình MobileNetV2 có các chỉ số hiệu suất trung bình đều tăng ~1-2% so với dữ liệu chưa tăng cường.
0	Tương tự, dữ liệu tăng cường với mô hình EfficientNetB0 có các chỉ số tăng ~3-4%, cải thiện đáng kể so với dữ liệu chưa tăng cường (Bảng 5). Phương pháp này có ý nghĩa trong việc tăng tính đa dạng của tập dữ liệu hình ảnh cây thuốc, đặc biệt là khó khăn trong quá trình thu thập hình ảnh của những loại cây thuốc hoặc thảo dược hiếm, ít hình ảnh. Khi bộ dữ liệu hình ảnh hạn chế, trong quá trình huấn luyện, độ chính xác và độ mất mát của dữ liệu huấn luyện và kiểm tra với mô hình EfficientNetB0 ổn định chậm hơn so với MobileNetV2 được thể hiện ở đường học tập accuracy và loss (Hình 5).
0	Nguyên nhân là do sự chênh lệch về số lượng lớp và tham số của kiến trúc mô hình. Tuy nhiên, khi dữ liệu đa dạng, mặc dù ở những epoch đầu, độ chính xác và độ mất mát của dữ liệu với mô hình EfficientNetB0 ổn định chậm hơn (Hình 5) nhưng các epoch sau đường accuracy và loss của dữ liệu hội tụ tốt và ổn định hơn so với MobileNetV2. Ngoài ra, các chỉ số hiệu suất của EfficientNetB0 cũng vượt trội hơn, cụ thể là F1-score đạt ~94% cao hơn so với MobileNetV2 ~90%. Nhìn chung, cả 2 mô hình áp dụng học chuyển giao và tăng cường dữ liệu đều đạt các chỉ số hiệu suất cao và phù hợp để triển khai trên thiết bị di động.
0	EfficientNetB0 cho kết quả cao về mặt hiệu suất, nhưng mô hình này có hạn chế là tiêu tốn tài nguyên nhiều hơn MobileNetV2 do EfficientNetB0 có kích thước mô hình lớn và tham số nhiều. Nếu thiết bị di động có khả năng xử lý tốt và có dung lượng lưu trữ đủ lớn thì EfficientNetB0 là một lựa chọn tốt để đảm bảo độ chính xác. Nhưng nếu đối với thiết bị bị giới hạn về tài nguyên thì mô hình này có thể gặp phải các vấn đề về suy luận chậm hơn và tiêu thụ nhiều năng lượng hơn. So với EfficientNetB0, MobileNetV2 đạt sự cân bằng về tốc độ và dung lượng, phù hợp để triển khai trên thiết bị di động bảo đảm hiệu suất tốt nhưng vẫn duy trì tốc độ và mức tiêu thụ tài nguyên hợp lý.
0	Trong nghiên cứu này, ứng dụng MePDetect được thiết kế và mô hình CNN EfficientB0 được tích hợp vào ứng dụng di động hoạt động trên hệ điều hành Android 9.0 Pie chạy trên các thiết bị sử dụng kiến trúc x86. Kết quả nhận dạng ban đầu của một số loại cây dược liệu đạt độ tin cậy cao (Hình 6). Ứng dụng di động sử dụng mô hình EfficientNetB0 được xây dựng để nhận dạng một số loại cây thuốc và dược liệu, kết quả cho thấy khả năng nhận diện tốt. Các mô hình CNN được đánh giá khi được áp dụng kỹ thuật học chuyển giao và tăng cường dữ liệu đã giúp cải thiện hiệu quả huấn luyện với số lượng bộ dữ liệu hạn chế.
0	Đặc biệt, nếu thiết bị di động có khả năng xử lý tốt và có dung lượng lưu trữ đủ lớn thì EfficientNetB0 là một lựa chọn tốt để đảm bảo độ chính xác mô hình. Ngược lại, nếu thiết bị bị giới hạn về tài nguyên, khả năng tính toán của vi xử lý thì MobileNetV2 là lựa chọn để cân bằng về tốc độ và dung lượng, phù hợp để triển khai trên thiết bị di động bảo đảm tốt về hiệu suất nhưng vẫn duy trì tốc độ xử lý cũng như mức tiêu thụ tài nguyên hợp lý.
0	Tuy nhiên, nghiên cứu có một số hạn chế như sau: dữ liệu hình ảnh hạn chế do khó khăn trong việc thu thập một số loại thuốc ít gặp, làm giảm khả năng mô hình nhận diện được biến thể của một số cây thuốc trong thực tế; phương pháp triển khai mô hình trực tiếp trên ứng dụng di động hạn chế về khả năng mở rộng do yêu cầu tái huấn luyện mô hình khi nhận diện thêm các loại cây mới hoặc cập nhật dữ liệu mới trên ứng dụng có thể tốn thời gian và cần tài nguyên xử lý cao hơn.
0	Đề xuất một số hướng phát triển nghiên cứu như: mở rộng tập dữ liệu đặc biệt đối với một số loại cây thuốc đã có tuy nhiên tỷ lệ dự đoán đúng thấp; cải tiến tối ưu hóa mô hình bằng cách áp dụng thêm các kiến trúc tối ưu hơn hoặc thuật toán phân loại để cải thiện tốc độ và tiết kiệm tài nguyên mà vẫn duy trì độ chính xác cao; cải thiện khả năng xử lý hình ảnh thực tế trong các môi trường không kiểm soát như ánh sáng yếu, nhiễu động và hình ảnh chất lượng thấp bằng các kỹ thuật xử lý ảnh nâng cao; triển khai hệ thống cập nhật mô hình tự động khi có dữ liệu mới hoặc khi phát hiện các loại cây mới trong hệ thống, đảm bảo mô hình luôn được cải thiện theo thời gian.
0	Trong ngành kỹ thuật y sinh (Medical engineering), đặc biệt trong việc chẩn đoán hình ảnh y khoa, các hình ảnh y khoa như X-quang, chụp cắt lớp (Computed Tomography), chụp cộng hưởng từ (Magnetic Resonance Imaging), v.v… ở các bệnh viện, cơ sở y khoa lớn ngày càng nhiều. Việc hiểu được thông tin từ ảnh y khoa sẽ giúp ích rất lớn trong việc chẩn đoán bệnh lý. Việc phân tích tỉ mỉ phát hiện đúng bệnh lý sẽ giúp đưa ra giải pháp điều trị nhanh chóng cho người bệnh. Để nâng cao hiệu quả phân tích, kỹ thuật học sâu (Deep Learning) thường được chọn bởi vì nó có khả năng huấn luyện máy tính học một lượng lớn dữ liệu được cung cấp để giải quyết những vấn đề cụ thể.
0	Trong đó, mô hình mạng nơ-ron tích chập (Convolutional Neraul Network - CNN) là một trong những mô hình học nhiều tầng có độ chính xác cao phù hợp để học và phân tích các dữ liệu hình ảnh y khoa. Nghiên cứu sử dụng CNN với mô hình Inception V3 và mô hình Inception Resnet V2 thực hiện chẩn đoán bệnh lao phổi với tập dữ liệu ảnh Xquang của Shenzhen Hospital. Kết quả tốt trong quá trình thực nghiệm đã cho thấy hai mô hình này đều khả thi trong việc chẩn đoán bệnh lao phổi trong thực tiễn. Xác suất chẩn đoán cao thể hiện sự phù hợp của mô hình đối với bài toàn đặt ra cũng như khả năng có độ chính xác cao hơn nữa nếu tiếp tục xem xét các yếu tố tác động trong quá trình huấn luyện.
0	Trong thời đại ngày nay kỹ thuật học sâu hay kỹ thuật học nhiều tầng (deep learning) đã được ứng dụng rộng rãi. Kỹ thuật này giúp hiện thực hóa một hệ thống machine learning với hiệu quả vượt trội. Các thuật toán học tập nhiều tầng rút trích các đặc trưng từ một tập dữ liệu cực lớn và đã được gán nhãn. Chẳng hạn như một tập dữ liệu hình ảnh hoặc bộ gen và sử dụng chúng để tạo ra một công cụ dự đoán. Sau khi được huấn luyện, các thuật toán có thể sử dụng để phân tích các nguồn dữ liệu khác.
0	Các thuật toán trong kỹ thuật học sâu dựa vào các mạng lưới thần kinh, trong đó các lớp của các nút giống như nơ-ron bắt chước cách não bộ của con người phân tích thông tin tìm các mối quan hệ có ý nghĩa, các lớp trong mạng nơ-ron lọc và sắp xếp thông tin, mỗi lớp nơ-ron giao tiếp và tinh chỉnh đầu ra từ lớp trước. Kỹ thuật học sâu cần có bộ dữ liệu mẫu chuẩn để huấn luyện, tạo thành số lượng lớn các giá trị cùng các tham số giúp cho thuật toán tìm và phân loại các đặc trưng tùy theo dữ liệu của bài toán thực tế.
0	Điển hình như bài toán tô màu ảnh trong Scribbler do hãng phần mềm Adobe thực hiện. Các nhà khoa học đã tạo ra một mạng nơ-ron chứa hàng chục ngàn bức ảnh được tuyển chọn kỹ để dạy Scribbler cách nhận diện khuôn mặt và tô màu cho ảnh dựa vào phỏng đoán. API Vision của Microsoft Cognitive Service cho phép nhận diện hình ảnh, các đối tượng trong ảnh, khuôn mặt và cả cảm xúc hoặc nhận dạng giọng nói và hiểu ngôn ngữ tự nhiên (Google Dosc, IBM Watson), games (Alphago), ô tô tự vận hành (Tesla, Google).
0	Kỹ thuật học nhiều tầng phát triển và cải thiện một cách hiệu quả về xử lý thông tin trong lĩnh vực máy học, như thị giác máy tính (computer vision), phân lớp ảnh (image classification), phân loại văn bản (text classification), dự đoán (prediction), xử lý ngôn ngữ tự nhiên (natural language processing) và được xem như là một bước nhảy lớn trong việc khai phá dữ liệu. Đối với dữ liệu ảnh đầu vào cần phân loại là hình ảnh, thông tin ta cần xử lý là rời rạc thì mô hình mạng nơ ron tích chập (Convolutional Neural Network - CNN) là một trong nhũng giải pháp phân loại tối ưu. Một số cấu trúc CNN cho kết quả phân loại ảnh vượt bậc được sử dụng nhiều như: LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet, ResNet.
0	Ảnh y khoa (Medical Imaging) với nghĩa tổng quát là ảnh tạo ra từ việc kết hợp một phần của hình ảnh sinh học (Biology Images) với hình ảnh y khoa (Medical Imaging) có sử dụng các công nghệ hình ảnh. Một số ảnh y khoa thông dụng như hình X-Quang, hình cộng hưởng từ (MRI), siêu âm, nội soi, chụp cắt lớp. Số lượng ảnh được tạo ra trong lĩnh vực y khoa ngày càng lớn gây khó khăn cho người xử lý, cũng như việc bỏ qua khai thác thông tin với nguồn thư viện hình ảnh hiện có này đồng nghĩa với việc bỏ qua nguồn tài nguyên quý giá.
0	Việc sử dụng kỹ thuật học sâu để hỗ trợ xử lý các ảnh y khoa hỗ trợ con người là bước đi cần thiết và quan trọng giúp cho việc nhận diện các vấn đề về bệnh được nhanh chóng và chính xác hơn. Một số bài toán y khoa hiện nay đã quan tâm đến việc ứng dụng kỹ thuật học sâu trong việc hỗ trợ phân tích hình ảnh y khoa như Jang Hyung Lee và Kwang Gi Kim thực hiện để ước tính độ tuổi của xương [2]. Từ nhu cầu ước tính độ tuổi xương tay trong độ tuổi tăng trưởng, Lee và Kim đã áp dụng kỹ thuật học sâu vào phân tích hình ảnh y khoa làm chỉ số tiên lượng tăng trưởng chiều cao của đối tượng.
0	Trong [2] các tác giả đã sử dụng mô hình hồi quy cùng Caffenet (một trong nhiều nền tảng được sử dụng trong kỹ thuật học sâu hiện nay) để huấn luyện bộ dữ liệu hình ảnh X-Quang xương tay của bệnh nhân mục đích so sánh tiên lượng tuổi của xương tay. Trong [3], nhóm tác giả trình bày về hệ thống hình ảnh siêu âm di động, ba chiều, cùng nhu cầu tái tạo hình ảnh chất lượng cao từ một số phép đo tần số vô tuyến (RF) hạn chế do mẫu thu (Rx) hoặc sự kiện truyền (Xmit) lấy mẫu phụ. Các tác giả trình bày phương pháp cải thiện tốc độ xử lý mà vẫn đạt được chất lượng cao từ dữ liệu hình ảnh siêu âm mẫu.
0	Công trình này dùng kỹ thuật học sâu để ước tính được dữ liệu tần số vô tuyến RF (radio-frequency) bị thiếu từ việc lấy mẫu Rx (receiver), xmit (transmit) mà không làm giảm chất lượng hình ảnh dựa trên mối liên kết chặt chẽ giữa mạng nơ ron sâu (deep neural network) và phân rã ma trận Hankel (Hankel matrix decomposition), nhờ đó [3] đã xây dựng thuật toán mới giúp cải thiện hiệu suất nội suy, nhờ loại bỏ các thông tin cần xử lý, giúp tăng tốc độ thực hiện. Liên quan đến việc sử dụng kỹ thuật học sâu trong việc phân tích ảnh, nhóm tác giả Jae-Hong Lee và đồng sự sử dụng mạng lưới học sâu mạng nơ ron tích chập để chẩn đoán và dự đoán răng bị tổn thương dựa trên thư viện Keras [4].
0	Trong công trình này các tác giả đã sử dụng dữ liệu đầu vào là những hình ảnh X-Quang chất lượng cao được phân loại bởi những bác sĩ nha khoa có kinh nghiệm kết hợp sử dụng CNN trên kiến trúc mạng VGG-19 cải tiến (kiến trúc mạng nơ ron tích chập cải tiến từ kiến trúc VGG) giúp tăng hiệu suất phát hiện đối tượng và phân loại hình ảnh. Việc tập huấn và phân loại ảnh X-Quang bị bệnh lao hay không được thực nghiệm bằng hai mô hình là Inception V3, Inception Resnet V2. Mô hình Inception V3 là phương pháp học máy sử dụng mạng lưới thần kinh (neural network) được huấn luyện trước, phát triển từ mô hình mạng nơ ron tích chập.
0	Mô hình Inception V3 có kiến trúc mạng sâu 48 lớp được sử dụng phân loại nhận dạng hình ảnh lớn của ImageNet (cơ sở dữ liệu trực quan lớn được thiết kế để sử dụng trong nghiên cứu nhận dạng đối tượng) sử dụng dữ liệu từ năm 2012 và có thể phân biệt giữa 1.000 loại đối tượng khác nhau. Mô hình Inception Resnet v2 có kiến trúc mạng học sâu hơn và ít tỉ lệ lỗi hơn Inception V3. Inception Resnet V2 kiến trúc mạng có thể sâu 164 lớp là sự kết hợp giữa cấu trúc Inception và kết nối Residual giúp giảm thời gian huấn luyện dữ liệu, dễ tối ưu hóa và cho độ chính xác cao trong quá trình phân loại dữ liệu ảnh.
0	Tập dữ liệu thực nghiệm được sử dụng từ nguồn ảnh X-Quang bệnh lao trong thư viện ảnh Shenzhen Hospital XRay [6] cùng với ảnh X-Quang trong tập dữ liệu ảnh được công bố [5]. Số lượng ảnh tập huấn và kiểm tra được thể hiện trong Bảng 1. Kết quả thực nghiệm tập dữ liệu trong Bảng 1 với mô hình Inception V3 và mô hình Inception Resnet V2 được thể hiện trong Bảng 2 và Bảng 3. Bài báo trình bày phương pháp học sâu phân loại tập dữ liệu ảnh X-Quang. Kết quả thực nghiệm cho thấy mạng nơ ron tích chập với mô hình Inception V3 và Inception Resnet V2 được thiết kế phù hợp trong việc giải quyết các vấn đề liên quan đến việc phát hiện đối tượng, phân loại hình ảnh dạng X-Quang.
0	Nghiên cứu hiện tại cũng cho thấy việc sử dụng phương pháp học sâu qua mô hình Inception Resnet V2 với việc thay đổi tăng số bước huấn luyện ảnh nâng cao độ chính xác khi xử lý phân loại ảnh. Mô hình Inception Resnet V2 với số bước huấn luyện tăng 5000 tốn nhiều thời gian khi xử lý ảnh nhưng cho kết quả chính xác hơn Inception V3 phù hợp cho dòng máy cấu hình cao khi phân loại ảnh X quang. Mô hình Inception V3 với số bước huấn luyện 1000 ít xảy ra lỗi cho kết quả tương đối chính xác phù hợp dùng cho các máy tính cấu hình tương đối, nhỏ gọn như Raspberry pi ví dụ Raspberry pi 3.
0	Ngoài ra việc ứng dụng được kỹ thuật học sâu trong nghiên cứu ảnh X quang sẽ giúp cho các việc chẩn đoán phát hiện sớm được bệnh lao hiệu quả hơn trong khi chi phí và thời gian để có được ảnh X quang tiết kiệm hơn rất nhiều so với việc dùng các phương pháp chụp cắt lớp (PET), chụp cắt lớp điện toán (CT), chụp cộng hưởng từ (MRI) và chụp cắt lớp phát xạ. Có thể ảnh X quang có thể cho ít thông tin hơn các phương pháp khác nhưng trong khi dùng phim X quang trong việc tầm soát phát hiện bệnh thì phim X quang sẽ có hiệu quả rất tốt mà lại tiết kiệm về thời gian hơn.
0	Hình thức đánh giá thông qua bài thi trắc nghiệm được áp dụng rộng rãi trong các kỳ thi bởi chi phí tiết kiệm, kết quả khách quan. Tuy nhiên việc chấm thi cho dạng bài thi này là một nhiệm vụ phức tạp, đặc biệt khi đề thi có nhiều các câu hỏi và nhiều mã đề. Do đó, việc xây dựng các hệ thống tự động chấm bài thi trắc nghiệm đã được chú trọng phát triển và cải tiến liên tục. Việc chấm thi trắc nghiệm được bắt đầu với việc xác định thông tin từ bài thi bao gồm câu trả lời cho từng câu hỏi, số báo danh và mã đề thi.
0	Đây là bước rất quan trọng bởi từ đây hệ thống sẽ tiến hành so sánh các câu trả lời thu được với đáp án của mã đề thi tương ứng để xác định kết quả bài làm. Hiện nay có hai phương pháp được sử dụng để thực hiện bước này: sử dụng thiết bị đọc dấu hiệu quang OMR và sử dụng các thuật toán xử lý ảnh. Phương pháp sử dụng các kỹ thuật xử lý ảnh tuy tốc độ chậm hơn so với OMR nhưng chi phí được giảm đi đáng kể do không phải sử dụng thiết bị chuyên dụng. Nội dung nghiên cứu này tập trung vào việc ứng dụng các thuật toán xử lý ảnh để có thể trích xuất các thông tin từ phiếu trả lời để phục vụ quá trình chấm thi trắc nghiệm.
0	Các dạng phiếu trả lời thường phân biệt với nhau bởi số lượng câu hỏi, phổ biến là phiếu 40 câu, phiếu 50 câu, phiếu 100 câu… Nghiên cứu này sử dụng phiếu trả lời 120 câu như mô tả như trên hình 1. Ảnh scan phiếu trả lời loại này được sử dụng làm dữ liệu đầu vào cho quy trình trích xuất dữ liệu bài thi bằng xử lý ảnh ở các bước sau. Các thông tin cần trích xuất bao gồm câu trả lời cho 120 câu hỏi, số báo danh và mã đề thi trắc nghiệm. Trong đó, phần tô câu trả lời câu hỏi nằm trong bốn khu vực ở phía dưới của phiếu, mỗi khu vực này chứa 30 câu, mỗi câu trả lời bao gồm bốn vị trí đáp án “A”, “B”, “C” và “D” được đặt trong các vòng tròn.
0	Phần số báo danh và mã đề nằm ở góc trên bên phải. Nội dung của phần này là xác định được vị trí các ô tròn và xem xét chúng có được tô hay không. Các vùng chứa các ô tròn đươc đặt trong các vùng hình chữ nhật, vì vậy cách tiếp cận là sử dụng các thuật toán phát hiện các đường biên của các vùng này. Quá trình thực hiện trải qua các bước như sau. Dữ liệu ảnh phiếu trả lời được đọc từ file và được chuyển qua dạng ảnh xám rồi được đưa qua bộ lọc Gaussian [1] để loại bỏ nhiễu. Thuật toán được sử dụng để phát hiện các cạnh, đường nét trong bức ảnh là Canny Edge Detection [1].
0	Kết quả thu được là ảnh nhị phân trong đó giữ lại các cạnh, đường nét trong bức ảnh. Từ đây xác định được các thông tin về các đường biên (contours) [1] chứa các cạnh, đường nét này. Tuy nhiên số lượng đường biên rất lớn do đó cần có các điều kiện để thể chọn ra được các vùng mong muốn. Đối với vùng chứa các câu trả lời, điều kiện được sử dụng là diện tích bao bởi đường biên là lớn nhất trong toàn bộ khu vực hình ảnh. Điều kiện tương tự cũng được áp dụng cho khu vực chứa số báo danh và mã đề thi, tuy nhiên chỉ tìm trong khu vực phía trên bên phải.
0	Các đường biên đồng thời được kiểm tra về khả năng chồng lấn lên nhau để đảm bảo các đường biên thu được là duy nhất. Kết quả xác định các vùng dữ liệu được mô tả trên hình 1. Sau khi thu được các vùng dữ liệu mong muốn, công việc cần làm tiếp theo là lấy ra được dữ liệu các ô tròn. Do các vị trí được bố trí cách đều nhau nên việc xác định dữ liệu các ô tròn này thực hiện bằng cách chia đều các vùng dữ liệu này. Với các ô tròn trong phần trả lời câu hỏi, dữ liệu tương ứng thu được từ vùng dữ liệu ban đầu (cấp i) được chia thành 6 phần nhỏ (cấp ii), tiếp tục loại bỏ khoảng trắng ở phía trên và dưới rồi chia nhỏ làm 5 phần (cấp iii),
0	mỗi phần này là phần trả lời cho một câu hỏi trong đề thi. Tiếp tục loại bỏ phần thông tin số thứ tự câu trả lời và chia làm 4 phần bằng nhau (cấp iv), đây chính là vị trí chứa các ô tròn “A”, “B”, “C” và “D”, như minh họa trên hình 2. Đối với vùng dữ liệu mã đề và số báo danh, dữ liệu các ô tròn thu được bằng cách chia đều hai vùng này do các ô này đặt liên tiếp nhau và không có khoảng trắng như trong vùng câu trả lời.
0	Sau khi thu được các dữ liệu về các ô tròn, việc cần làm tiếp theo là xác xem ô tròn này có được tô hay không. Có hai phương pháp được sử dụng để thực hiện nhiệm vụ này: sử dụng tần suất điểm ảnh [2] và sử dụng mạng mô hình nhận dạng sử dụng mạng tích chập CNN. Với phương pháp đầu tiên, vùng dữ liệu chứa ô tròn sẽ được chuyển sang thành ảnh nhị phân bằng phương pháp phân ngưỡng thresholding [1]. Khi số lượng pixel trắng trong vùng lớn hơn một ngưỡng nhất định thì có thể khẳng định ô tròn đã được tô. Phương pháp này tuy đơn giản nhưng độ chính xác không cao, khó áp dụng hàng loạt bởi phụ thuộc rất lớn vào kết quả chuyển đổi sang dạng nhị phân và ngưỡng phân loại.
0	Với phương pháp thứ hai, dữ liệu ảnh được đưa qua lớp tích chập để trích xuất các đặc trưng sau đó đưa qua lớp mạng noron từ đó tính toán ra được xác xuất được tô của ô tròn trong ảnh dữ liệu. Phương pháp này tuy phức tạp hơn so với phương pháp thứ nhất nhưng kết quả nhận dạng ổn định hơn do mô hình được huấn luyện với bộ dữ liệu khá lớn. Đây cũng là phương pháp được áp dụng triển khai trong nghiên cứu này. Mô hình mạng CNN được xây dựng dựa bởi thư viện Tensorflow trong ngôn ngữ Python. Mô hình mạng CNN nhận dạng ô tròn có được tô/ không tô có cấu trúc như hình 3.
0	Thông số của mạng CNN như sau: lớp trích xuất gồm 02 lớp tích chập kết hợp pooling liên tiếp nhau, lớp phân loại có một lớp ẩn và hai đầu ra. Mạng CNN này được huấn luyện trên tập dữ liệu được. Tập dữ liệu gồm 2000 file ảnh chứa các ô tròn thu được từ quá trình xử lý các file ảnh phiếu trả lời sử dụng để làm dữ liệu huấn luyện cho mô hình phân loại. Kết quả huấn luyện mô hình mạng CNN được thể hiển trên hình 4, 5. Hình 4 mô tả độ chính xác của mô hình theo từng chu kì huấn luyện với tập dữ liệu. Độ chính xác được cải thiện rất nhanh, chỉ trong các chu kỳ đầu thì giá trị đã hội tụ đến kết quả rất tốt (xấp xỉ 100%).
0	Hình 5 mô tả giá trị mất mát trong quá trình huấn luyện mô hình với các tập dữ liệu bên trên, quá trình cho thấy giá trị mất mát này đã giảm dần theo các chu kì huấn luyện. Hình 6 mô tả kết quả chạy mô hình nhận dạng với các dữ liệu ô tròn thực tế, trong đó các hình bên trái là dữ liệu ảnh ô tròn, còn các hình bên phải biểu kết quả nhận định ô tròn trong dữ liệu có được tô hay không. Kết quả đều cho chất lượng nhận dạng tốt, trong đó kết quả nhận định đều nghiêng hẳn về phía giá trị thực tế của dữ liệu đầu vào, kể cả trong trường hợp ô tròn bị tô lệch hay chứa thành phần nhiễu.
0	Từ đây, các thông tin về câu trả lời, số báo danh và mã đề đều được trích xuất đúng và đầy đủ để có thể tiến hành so sánh với đáp án tương ứng. Từ quá trình xử lý phiếu trả lời trắc nghiệm và kết quả huấn luyện mô hình cho thấy tính khả thi của việc xây dựng hệ thống tự động chấm thi bài thi trắc nghiệm bằng các phương pháp xử lý ảnh như trên. Kết quả thu được tốt, khả quan để có thể triển khai trên các thiết bị thực tế. Tuy nhiên, vẫn còn một số điểm hạn chế trong quá trình nghiên cứu. Đầu vào của quá trình là ảnh scan phiếu trả lời nên có chất lượng tốt, việc này chưa thể đảm bảo đạt được nếu ảnh được chụp bằng các thiết bị thông thường.
0	Quá trình xác định các vùng dữ liệu chính trong phiếu khá phức tạp và cần nhiều thời gian để chỉnh định bởi kết quả của công đoạn này ảnh hưởng lớn tới việc lấy dữ liệu ảnh của từng ô tròn. Bên cạnh đó, phương pháp xử lý nêu trên có tính linh hoạt không cao khi áp dụng sang các dạng phiếu đề thi khác bởi các vùng dữ liệu chính phụ thuộc vào số lượng các câu hỏi có trong phiếu trả lời. Như vậy, nghiên cứu cần tiếp tục theo các hướng cải tiến thuật toán xử lý ảnh để tăng độ linh hoạt, ổn định cho mô hình, giảm thời gian thực thi để đạt yêu cầu về tốc độ chấm bài trong thực tế.
0	Nhận dạng khuôn mặt là một kĩ thuật công nghệ sinh trắc học ánh xạ các đặc điểm khuôn mặt người. Tập dữ liệu ảnh Facial Expression Recognition 2013 (FER-2013) gồm có bảy loại biểu cảm khác nhau của khuôn mặt người, được tác giả dùng làm bộ dữ liệu huấn luyện trong nghiên cứu này. Hiện tại, tầm quan trọng của bảo mật hệ thống là hết sức cấp thiết, vì vậy triển khai ứng dụng xác thực nhận dạng khuôn mặt người để đăng nhập vào hệ thống, xác thực trên điện thoại thông minh, chấm công, đeo khẩu trang. Chúng tôi đề xuất mô hình học máy, học sâu với nhiều phương pháp huấn luyện khác nhau kết hợp với tập dữ liệu FER-2013,
0	được mở rộng các định dạng ảnh kích thước (32x32, 48x48, 64x64, 72x72) nhằm mở rộng mục tiêu hướng nghiên cứu và tiến hành thực nghiệm với các mô hình LDA, NB, KNN, DT, SVM. Sau đó, đánh giá sự hiệu quả của từng mô hình các tiêu chí Accuracy, Precision và F1-Score. Kết quả thực nghiệm của chúng tôi đã đóng góp được ba vấn đề chính: một là, mở rộng định dạng bộ dataset với kích thước đa dạng hơn để làm nền tảng cho kết quả nghiên cứu; hai là mô phỏng các mô hình thuật toán khác nhau trong quá trình huấn luyện nhằm đánh giá và so sánh về các tiêu chí ở trên; ba là đề xuất mô hình học sâu CNN được đánh giá hiệu quả.
0	Nhận dạng khuôn mặt người là hình thức phát hiện dùng các thiết bị máy móc liên quan đến việc thu thập thông tin dữ liệu ảnh, sau đó xử lí ảnh được thông qua các mô hình học máy, học sâu so sánh với chiết xuất đặc trưng từ bộ dữ liệu ảnh đã huấn luyện, từ đó sẽ đưa ra kết quả nhận dạng và phát hiện đối tượng ảnh. Woodrow W. Bledsoe, Helen Chan và Charles Bisson (Bledsoe, 1964; Bledsoe, 1966) đồng tác giả nghiên cứu nhận dạng khuôn mặt người từ năm 1964 đến 1966, đã nghiên cứu lập trình máy tính nhận dạng khuôn mặt người với bộ cơ sở dữ liệu lớn, vấn đề đặt ra là làm sao để so sánh sự trùng khớp giữa một ảnh và bộ dữ liệu lớn.
0	Tuy nhiên, một ứng dụng nhận dạng khuôn mặt đầy đủ chức năng đã được Kanade thực hiện vào năm 1977. Các nghiên cứu về nhận dạng khuôn mặt hai chiều (2D) đã được nghiên cứu chuyên sâu. Các nghiên cứu về khuôn mặt ba chiều (3D) bắt đầu được thực hiện sau những năm 2000. Nhiều mô hình học sâu Convolutional Neural Network (CNN) (Chauhan, Kumar, & Joshi, 2018; Bhairnallykar, Prajapati, Rajbhar, & Mujawar, 2020) đã được thực nghiệm ở nhiều bộ dữ liệu khác nhau như MNIST, CIFAR-10, cho kết quả chính xác. MNIST là tập dữ liệu gồm 70.000 hình ảnh, trong đó 60.000 hình ảnh dành cho việc huấn luyện và 10.000 dành cho thử nghiệm.
0	Kích thước của mỗi hình ảnh là 28x28 pixel và có 10 nhãn phân lớp từ 0-9. Bài viết này gồm có 4 phần, các phần còn lại được trình bày như sau: Phần 2, trình bày về đối tượng và phương pháp nghiên cứu: Mô tả tập dữ liệu chuẩn; phương pháp đánh giá; đề xuất mô hình học sâu CNN. Tiếp theo, phần 3 là kết quả và thảo luận của nghiên cứu. Cuối cùng là phần 4 kết luận cho nghiên cứu này. Để đánh giá được nhận dạng khuôn mặt với độ chính xác cao, cần có bộ dữ liệu chuẩn và được nhiều công trình nghiên cứu sử dụng.
0	Chất lượng bộ dữ liệu rất quan trọng sẽ làm ảnh hưởng kết quả cho quá trình thực nghiệm và đánh giá các phương pháp nhận dạng khuôn mặt. Bộ dữ liệu nhận dạng biểu cảm khuôn mặt năm 2013 (SAMBARE, 2020): FER-2013 là tập dữ liệu được giới thiệu tại hội nghị quốc tế về học máy (ICML) vào năm 2013 do tác giả I. J. Goodfellow và D. Erhan và các tác giả đồng nghiên cứu khác giới thiệu (Goodellow, et al., 2015). Trong tập dữ liệu này, mỗi khuôn mặt đã được phân loại dựa trên 7 loại cảm xúc (Vui mừng, tức giận, thất vọng, sợ hãi, ghê tởm, ngạc nhiên, bình thường) khác nhau, mỗi hình ảnh có kích thước 48x48 pixel, các cảm xúc được mô tả trong Hình 1.
0	Tập dữ liệu FER-2013 gồm có 35.887 ảnh, trong đó: Tập dữ liệu dùng để train là 28.709 ảnh và dùng cho việc test là 7178 ảnh, cho 7 loại biểu cảm khác nhau của khuôn mặt người, mô tả ở Bảng 1. Trong nghiên cứu này, để đánh giá hiệu suất các mô hình thực nghiệm thì cần dùng công thức như là độ chính xác (Accuracy), Precision và F1-score. Ở Bảng 2, sử dụng ma trận nhầm lẫn có các thuộc tính dương tính thật (TP), âm tính thật (TN), dương tính giả (FP) và âm tính giả (FN) (Huynh & Nguyen, 2021). Độ chính xác (Accuracy) – là mức độ gần của các phép đo với một giá trị cụ thể, số lượng dữ liệu được phân loại chính xác trên tổng số dự đoán.
0	Độ chính xác có thể không phải là thước đo tốt nếu tập dữ liệu không được cân bằng (cả hai lớp âm và dương có số lượng dữ liệu khác nhau). Công thức tính độ chính xác được định nghĩa trong công thức (1). Tỉ lệ cảnh báo giả (False Alarm Rate - FAR) – còn được gọi là False Positive Rate (tỉ lệ dương tính giả). Thước đo này được tính theo công thức (2). Tỉ lệ lý tưởng cho thước đo này càng thấp càng tốt, tức là số phân loại nhầm một phân loại bình thường sang dự đoán nhận dạng đúng (FP) càng thấp càng tốt. Độ chính xác phép đo (Precision) – là mức độ gần của các phép đo, có giá trị gần với 1 khi kết quả là một tập phân loại tốt.
0	Precision là 1 chỉ khi tử số và mẫu số bằng nhau (TP = TP + FP), điều này cũng có nghĩa là FP bằng 0. Khi FP tăng giá trị dẫn đến mẫu số lớn hơn tử số và giá trị chính xác giảm. Công thức tính Precision được định nghĩa trong công thức (3). Tỉ lệ phát hiện (Detection Rate – DR hay Recall) – Giá trị DR càng gần với 1 sẽ cho một phân loại tốt. DR là 1 chỉ khi tử số và mẫu số bằng nhau (TP = TP + FN), điều này cũng có nghĩa là FN bằng 0. Khi FN tăng giá trị dẫn đến mẫu số lớn hơn tử số và giá trị DR giảm. Chỉ số này nhằm đánh giá mức độ tổng quát hóa mô hình tìm được và được xác định theo công thức (4).
0	Nếu 2 tiêu chí Precision và DR đều tốt, nghĩa là một trong hai giá trị FP và FN phải gần bằng 0 càng tốt. Cần có một tham số đo có tính đến cả Precision và DR, đó chính là F1-Score, công thức (5). F1-Score được gọi là một trung bình điều hòa của các tiêu chí Precision và DR. Nó có xu hướng lấy giá trị gần với giá trị nào nhỏ hơn giữa 2 giá trị Precision và DR và đồng thời nó có giá trị lớn nếu cả 2 giá trị Precision và DR đều lớn. So với độ chính xác (Accuracy), F1-Score phù hợp hơn để đánh giá hiệu suất nhận dạng của các mẫu dữ liệu không cân bằng.
0	Trong phần này, trình bày liên quan đến ba vấn đề đóng góp của tác giả được nêu trong phần tóm tắt: chuyển đổi bộ dữ liệu gốc FER-2013; mô phỏng các thuật toán được hỗ trợ thư viện chính là sklearn, sau đó so sánh kết quả; đề xuất mô hình học sâu CNN được đánh giá hiệu quả, cải thiện hơn đối với các nghiên cứu liên quan khác. Từ bộ dữ liệu chuẩn FER-2013, có 35.887 ảnh với kích thước chuẩn 48x48 pixel. Sau đó, thực hiện chuyển đổi thành kích thước khác nhau 32x32, 64x64, 72x72 pixel mô tả qua trong hình 2 và sơ đồ bên dưới, sử dụng thư viện cv2,
0	OS được kết hợp chiết xuất đặc trưng dạng ảnh nhị phân lưu vào ma trận 3 chiều, sau đó gán nhãn cho từng loại theo 7 loại biểu cảm khác nhau của khuôn mặt, cuối cùng sử dụng hàm xử lí trong thư viện cv2 để tăng hoặc giảm ảnh theo kích thước đề xuất. Chiết xuất đặc trưng bộ dữ liệu ảnh FER-2013 với các kích thước (pixel) khác nhau (32x32, 64x64, 72x72) từ bộ dữ liệu gốc. Linear Discriminant Analysis là một phương pháp kĩ thuật ứng dụng dạng bài toán giảm kích thước tiền xử lí dữ liệu cho các ứng dụng máy học và phân loại (Tharwat et al., 2017).
0	Naïve Bayes là một thuật toán phân loại dữ liệu. Thuật toán phân loại hiển thị vùng tốt nhất dưới giá trị đường cong (AUC). Kết quả các nghiên cứu thuật toán có độ chính xác hơn so với thuật toán Lazy-IBK, Zero-R và cây quyết định-J48 (Wibawa et al., 2019). Ưu điểm của thuật toán Naive Bayes (Rajeswari, Juliet, & Aradhana, 2017): Dữ liệu đào tạo nhỏ, tính toán đơn giản, dễ để thực hiện, hiệu quả về thời gian. k-Nearest-Neighbours là thuật toán học có giám sát, áp dụng cho bài toán phân loại dữ liệu và hồi quy (Guo, Wang, Bell, Bi, & Greer, 2004) (Rajeswari, Juliet, & Aradhana, 2017). Thuật toán phân loại được mô tả: Gọi M là mô hình đại diện.
0	Trong đó <Cls (di), Sim (di), Num (di), Rep (di)> lần lượt đại diện cho nhãn lớp của di, độ tương đồng thấp nhất với di trong số các bộ dữ liệu được Ni bao phủ; nếu có nhiều hơn một vùng lân cận có cùng số lượng láng giềng tối đa, sẽ chọn một với giá trị tối thiểu của Sim (di). Decision trees thường được sử dụng nhiều lĩnh vực khác nhau, chẳng hạn như xử lí hình ảnh và xác định các mẫu (Taha & Mohsin, 2021). Các nút và các nhánh được cấu tạo từ mỗi cây. Mỗi nút đại diện cho các tính năng trong một danh mục được phân loại và mỗi tập hợp con xác định một giá trị có thể được nhận bởi nút.
0	Các loại thuật toán cây quyết định như là: Iterative Dichotomies, Classification and Regression Tree (CART), CHi-squared Automatic Interaction Detector (CHAID), Multivariate Adaptive Regression Splines (MARS), Conditional Inference Trees (CTREE). Support Vector Machine là một thuật toán học giám sát, ứng dụng cho bài toán thuộc phân loại dữ liệu, đệ quy (Srivastava & Bhambhu, 2010). Phát biểu bài toán: Áp dụng bài toán phân loại dữ liệu như Hình 4, tìm giải pháp tối ưu phải dựa vào tiêu chuẩn nào? Cặp dữ liệu của training set là (x1,y1),(x2,y2),...,(xN,yN) với vector Xi ∈ Rd thể hiện đầu vào của một điểm dữ liệu và yi là nhãn của điểm dữ liệu đó.
0	Mô hình mạng học sâu sử dụng đầu vào hình ảnh và biến đổi thông qua bộ lọc, để trích xuất các đặc trưng. Phương pháp huấn luyện khác kết hợp với tập dữ liệu FER-2013 với kích thước ảnh chuẩn 48x48 pixel, được mở rộng các định dạng ảnh kích thước 32x32, 64x64, 72x72 pixel để làm dữ liệu đầu vào của mô hình huấn luyện, như Hình 6. Trong mô hình này, thực nghiệm trên bộ dữ liệu chuẩn, kết quả so sánh dựa vào các tiêu chí trong phần giới thiệu đánh giá các mô hình LDA, NB, KNN, DT, SVM, CNN.
0	Số lượng ảnh được phân bổ dùng để huấn luyện và kiểm tra, bao gồm có 7 loại biểu cảm khác nhau: Vui mừng, tức giận, thất vọng, sợ hãi, ghê tởm, ngạc nhiên, bình thường, như Hình 7. Trong mỗi loại biểu cảm, sẽ đưa ra 2 bộ dữ liệu dùng để train và test riêng. Mô hình đề xuất học sâu được sử dụng train và test với các tham số đầu vào ảnh thuộc tính conv2d = kích thước ảnh 72x72 pixel, đầu ra mô hình này dense_1 = 7 loại biểu cảm khác nhau, tổng số tham số được train là 75.941.095, số vòng train Epoch = 100, thời gian cho mỗi epoch = 69s. Đồng thời, các tham số này được sử dụng để huấn luyện cho conv2d = kích thước ảnh 32x32 và 64x64 pixel.
0	Tương tự, mô hình đề xuất học sâu được sử dụng train và test với các tham số đại diện đầu vào ảnh thuộc tính conv2d = kích thước ảnh 48x48 pixel, đầu ra mô hình này dense_1 = 7 loại biểu cảm khác nhau, tổng số tham số được train là 31.900.903, số vòng train Epoch = 200, thời gian cho mỗi epoch = 34s. Tuy nhiên, khi train thì thời gian thực cao hơn rất nhiều, sử dụng card đồ họa NVIDIA® GeForce RTX™ 1650 GPU 4GB để thực hiện. Trong bài thực nghiệm này, được đánh giá trên máy tính laptop Asus Rog Strix Gaming G513IH với Windows 10 Pro 20H2 cấu hình: CPU AMD Ryzen™ 7 (8 nhân, 16 luồng) up 4.2GHz - 8MB Cache, RAM 16 Gb DDR4-3200Mhz, M.2 NVMe™ PCIe® 3.0 SSD, NVIDIA® GeForce RTX™ 1650 GPU 4GB.
0	Trong quá trình mô phỏng, đã sử dụng các thư viện như OpenCV hỗ trợ nhiều về thị giác máy tính bao gồm nhận dạng và phát hiện khuôn mặt, tìm kiếm ảnh có đặc trưng từ bộ dữ liệu lớn. Scikit-learning một thư viện hỗ trợ vấn đề bài toán phân loại, phân cụm, hồi quy, tuyến tính và quan trọng đó là mô hình đề xuất lựa chọn. Keras hỗ trợ liên quan cho mạng nơ ron học sâu, xử lí dạng bài toán về dạng hình ảnh và văn bản. TensorFlow là một thư viện phần mềm mã nguồn mở cho hiệu suất cao tính toán linh hoạt kiến trúc nhiều nền tảng (CPU, GPU, TPU), hỗ trợ mạnh về vấn đề học máy và học sâu.
0	Thực hiện cho mô hình, tham số đề xuất ở trên với kích ảnh 72x72 pixel, kết quả thu được các mô hình LDA, NB, KNN, DT, SVM kết hợp với phương pháp đánh giá các tiêu chí: Accuracy, precision, recall, F1_score, như Hình 10 và 11. Sau khi thực hiện train với mô hình đề xuất bên trên và được kết hợp thư viện chính của mô hình học sâu, kết quả thu được các mô hình dựa vào tiêu chí đánh giá Accuracy 42%, F1_score = 40% của SVM là cao nhất, Hình 11. Kết quả Bảng 3, mô tả quá trình thực nghiệm dữ liệu đầu vào với kích thước ảnh IMAGE-SIZE = 64x64, 32x32 pixel, mô hình đề xuất mạng nơ ron được đánh giá tỉ lệ độ chính xác cao nhất lần lượt là: 58.90%, 62.98%.
0	Kết quả đánh giá độ chính xác được so sánh dưới dạng biểu đồ hình cột cho 2 IMAGE-SIZE = 72x72, 32x32 pixel, được mô tả ở Hình 12, Hình 13, cao nhất là mô hình đề xuất học sâu CNN, sau đó là SVM, thấp nhất là mô hình NB và DT xấp xỉ gần bằng nhau. Kết quả Hình 12 và Hình 13, cho 2 kích thước ảnh IMAGE-SIZE = 64x64, 48x48 pixel, đánh giá số liệu qua mỗi vòng (epochs = 100) với tỉ lệ Loss và Accuracy. Với mô hình đề xuất học sâu mạng nơ ron được sử dụng train và test với các tham số đầu vào ảnh thuộc tính conv2d = kích thước ảnh 48x48 pixel, đầu ra mô hình này dense_1 = 7 loại biểu cảm khác nhau, tổng số tham số được train là 31.900.903,
0	số vòng train Epochs = 200, thời gian cho mỗi epoch = 34s. Hình 15 và 16, cho kết quả tỉ lệ chính xác (Accuracy) của hai dữ liệu huấn luyện và được kiểm tra. Kết quả khi được test với Epoch = 200 vòng, thì tỉ lệ học chính xác (accuracy) = 64.77%, Hình 17. Kết quả khi được train với Epochs = 200 vòng, thì tỉ lệ học chính xác (accuracy) = 97.57%, Hình 18. Qua khảo sát công trình nghiên cứu, tác giả các bài báo sử dụng model CNN để thực nghiệm đánh giá, theo kết quả cho thấy kết quả dữ liệu học kiểm thử của (Nishime, Endo, Yamada, Toma, & Akamine, 2016), CNN (Raghuvanshi & Choksi, 2016), CNN (Samsani & Gottala, 2020), lần lượt đạt tỉ lệ chính xác là: 58%, 48%, 61.4% và mô hình đề xuất CNN là 64.77%.
0	Theo kết quả trên Bảng 3 thì mô hình đề xuất của chúng tôi đạt hiệu quả tốt hơn. Ngoài ra, kết quả khi được train với Epochs = 200 vòng, thì tỉ lệ học đạt chính xác (accuracy) = 97.57%, Hình 18, kết quả này được đánh giá rất tốt. Qua quá trình thực nghiệm của bài nghiên cứu này, chúng tôi đã đưa ra ba vấn đề chính: một là, mở rộng bộ dữ liệu chuẩn FER-2013 với 35.887 ảnh, trong đó: để train là 28.709 ảnh và test là 7178 ảnh, kích thước ảnh gốc 48x48 pixel, được sử dụng bộ chiết xuất đặc trưng ảnh dạng nhị phân, được lưu trữ dạng matrix 3 chiều, gán nhãn theo 7 loại biểu cảm;
0	hai là, thực hiện kiểm tra mô phỏng các mô hình LDA, NB, KNN, DT, SVM được đánh giá theo tiêu chí chính xác (Accuracy) làm kết quả so sánh, cho thấy kết quả mô hình SVM tương ứng kích thước 72x72, 64x64, 32x32 có giá trị tốt nhất tương ứng: 42%, 43%, 42%, tỉ lệ học thấp nhất là mô hình NB = 29%, còn các mô hình còn lại nằm trong khoảng giữa hai mô hình trên. Ba là, mô hình đề xuất mạng nơ ron, khi huấn luyện Epochs = 100, dense = 7, kích thước tương ứng: 64x64, 32x32 pixel là 58.90%, 62.98%. Riêng kích thước ảnh gốc của tập dữ liệu chuẩn FER-2013, khi sử dụng số vòng train Epochs = 200 thì kết quả thu được trên tập dữ liệu train có độ chính xác (accuracy) = 97.57%, test = 64.77%.
0	Trong nghiên cứu này, đã so sánh với các bài nghiên cứu của (Nishime, Endo, Yamada, Toma, & Akamine, 2016), CNN (Raghuvanshi & Choksi, 2016), CNN (Samsani & Gottala, 2020) với kết quả mô hình đề xuất ở bảng 3 lần lượt là: 58%, 48%, 61.4%, 64.77%, kết quả mô hình đề xuất tỉ lệ học chính xác, hiệu quả cao nhất. Đánh giá chung về mô hình đề xuất còn hạn chế, tiếp tục khắc phục: Cần phải kết hợp các mô hình học sâu tăng cường, mô hình học sâu dạng kết hợp thì tỉ lệ độ chính xác tốt hơn cho việc nhận dạng ảnh.
0	Như vậy kết quả thực nghiệm này, chúng tôi đã nêu ra ba vấn đề chính: một là, mở rộng định dạng bộ dataset Facial Expression Recognition 2013 với kích cỡ ảnh gốc 48x48 pixel, sau đó được mở rộng nhiều kích thước ảnh khác như là 72x72, 64x64, 32x32 pixel để làm nền tảng cho đánh giá kết quả nghiên cứu thêm đa dạng; hai là, mô phỏng các mô hình thuật toán sử dụng từ các thư viện python trong quá trình huấn luyện nhằm đánh giá và so sánh về các tiêu chí Accuracy, Precision, F1-Score; ba là, đề xuất mô hình học sâu CNN được so sánh với các công trình nghiên cứu khác là đáng tin cậy và thiết thực.
0	Điều đó, đã được minh chứng rất rõ và chi tiết ở phần 3. Từ kết quả thực nghiệm với tập dữ liệu FER-2013 gồm có 35.887 ảnh, trong đó: tập dữ liệu dùng để train là 28.709 ảnh và dùng cho việc test là 7178 ảnh. Trong phần đánh giá kết quả mô hình đề xuất mạng nơ-ron CNN về xác nhận dạng khuôn mặt người được so sánh với các công trình nghiên cứu liên quan khác, cho thấy mô hình đề xuất đạt hiệu quả độ chính xác cao hơn. Qua nghiên cứu này, chúng tôi sẽ khắc phục những hạn chế của mô hình đề xuất học sâu này như đã đánh giá ở mục 3.2 thảo luận.
0	Bài báo đề xuất một phương pháp học sâu sử dụng mạng nơ-ron tích chập một chiều (1D CNN) để khôi phục dữ liệu dao động bị thiếu trong hệ thống giám sát sức khỏe kết cấu (SHM). Dữ liệu được thu thập từ mô hình cầu dây văng trong phòng thí nghiệm dưới dạng chuỗi thời gian đơn biến có các đoạn bị thiếu ngẫu nhiên. Để cải thiện khả năng học của mô hình và tính tổng quát, kỹ thuật tăng cường dữ liệu bằng nhiễu Gaussian được áp dụng trong quá trình huấn luyện. Mô hình được đánh giá bằng các chỉ số RMSE, MAE và hệ số tương quan R².
0	Kết quả nghiên cứu cho thấy, mô hình 1D CNN có khả năng trích xuất đặc trưng cục bộ vượt trội từ tín hiệu đầu vào, đồng thời có tốc độ huấn luyện nhanh, độ ổn định cao và kiến trúc gọn nhẹ, rất phù hợp với các ứng dụng trong môi trường thực tế. Đồng thời, việc bổ sung nhiễu Gaussian với độ lệch chuẩn hợp lý, giúp cải thiện đáng kể độ chính xác khôi phục so với mô hình không tăng cường dữ liệu. Phương pháp đề xuất cho thấy tiềm năng ứng dụng trong phục hồi dữ liệu bị mất hoặc hỏng trong các hệ thống SHM thực tế, góp phần nâng cao độ tin cậy của việc phân tích và chẩn đoán kết cấu.
0	Hệ thống giám sát sức khỏe công trình (SHM) đóng vai trò thiết yếu trong việc duy trì sự an toàn, độ tin cậy và hiệu quả khai thác lâu dài của các kết cấu hạ tầng quy mô lớn như cầu, đập, nhà cao tầng và các công trình dân dụng quan trọng khác [1]. Bằng cách tích hợp các cảm biến đo dao động, biến dạng, nhiệt độ hoặc nội lực, SHM cho phép thu thập và theo dõi liên tục các phản ứng của kết cấu trong suốt vòng đời vận hành. Nhờ đó, hệ thống hỗ trợ phát hiện sớm các biểu hiện hư hỏng tiềm tàng, đánh giá tuổi thọ còn lại và xây dựng kế hoạch bảo trì tối ưu để đảm bảo an toàn chịu lực, an toàn khai thác và tuổi thọ của công trình [2, 3].
0	Tuy nhiên, trong quá trình vận hành thực tiễn, dữ liệu thu thập từ hệ thống SHM thường đối mặt với các vấn đề mất mát do nhiều nguyên nhân như nhiễu tín hiệu, lỗi truyền tải hoặc hư hỏng cảm biến [4]. Những thiếu hụt này có thể làm giảm độ tin cậy trong phân tích đánh giá kết cấu và ảnh hưởng đến hiệu quả của các quyết định kỹ thuật. Do vậy, việc nghiên cứu các giải pháp phục hồi dữ liệu đo bị mất trở nên cấp thiết nhằm đảm bảo tính đầy đủ và liên tục của thông tin phục vụ cho quá trình giám sát và phân tích kết cấu. Các phương pháp truyền thống như phương pháp suy luận thống kê, phương pháp biểu diễn thưa... đã được sử dụng [5, 6].
0	Mặc dù có ưu điểm về tính đơn giản và hiệu quả tính toán, những phương pháp này thường hoặc phải giả định tính tuyến tính, hoặc sử dụng phân phối chuẩn trong dữ liệu - những giả định không phù hợp với bản chất phi tuyến, phức tạp và phụ thuộc thời gian của tín hiệu dao động trong các hệ thống SHM [7]. Để có cái nhìn tổng quan về các hướng tiếp cận đã được áp dụng cho bài toán phục hồi dữ liệu chuỗi thời gian, bảng 1 trình bày đặc điểm, dữ liệu sử dụng, phương pháp tiếp cận và hạn chế của một số phương pháp phổ biến, bao gồm cả các mô hình học máy truyền thống và mô hình học sâu.
0	Trong số các mô hình học sâu, mạng nơ-ron 1D CNN [8] nổi bật nhờ khả năng trích xuất đặc trưng cục bộ trong chuỗi thời gian và đã được áp dụng hiệu quả trong các nghiên cứu gần đây về khôi phục dữ liệu SHM. Nhiều nghiên cứu đã tập trung vào việc sử dụng CNN để phục hồi dữ liệu tại những vị trí bị mất. B.K. Oh và cs (2020) [9] cũng như P. Moeinifard và cs (2021) [10] đã đề xuất các mô hình CNN sử dụng dữ liệu từ các cảm biến ổn định để dự đoán giá trị tại các cảm biến bị lỗi.
0	Một hướng tiếp cận khác là khai thác mối quan hệ phi tuyến giữa các phản ứng kết cấu nhằm phục vụ cho mục tiêu khôi phục dữ liệu. P. Ni và cs (2022) [11] đề xuất phương pháp đo gián tiếp độ võng bằng cách sử dụng CNN để học ánh xạ giữa các phản ứng kết cấu khác và độ võng cần dự đoán. Y. Li và cs (2022) [12] phát triển một mô hình học sâu dựa trên kiến trúc mã hóa - giải mã tự động (autoencoder), kết hợp với các kết nối bỏ qua (skip connections) để tái tạo phản ứng toàn trường của kết cấu.
0	Từ góc độ tối ưu kiến trúc mạng, G. Fan và cs (2021a) [13] đề xuất mô hình CNN với kết nối dày đặc kết hợp với kỹ thuật biến đổi điểm ảnh phụ (sub-pixel transformation) và dropout, giúp nâng cao hiệu suất truyền gradient, đồng thời giảm độ phức tạp tính toán. Một số nghiên cứu khác, chẳng hạn như G. Fan và cs (2021b) [14], áp dụng mạng CNN tiến truyền hoàn toàn (fully feedforward CNN) với các kết nối dư để phục hồi dữ liệu đo bị mất. N.T.C Nhung và cs (2024) [15] đã nghiên cứu phục hồi dữ liệu cảm biến trong giám sát sức khỏe kết cấu ứng dụng học mạng nơ-ron tích chập kết hợp mạng nơ-ron hồi quy.
0	L.V. Vu và cs (2024) [16] đã nghiên cứu khôi phục dữ liệu cho hệ thống giám sát sức khoẻ công trình cầu sử dụng mô hình mạng tích chập đồ thị và mạng bộ nhớ ngắn dài hạn. Mặc dù một số nghiên cứu trước đây đã sử dụng 1D CNN để phục hồi dữ liệu đo bị thiếu trong hệ thống SHM, hầu hết các công trình này tập trung vào việc khai thác mối quan hệ giữa các cảm biến trong điều kiện dữ liệu khá sạch, ít nhiễu và thường chưa chú trọng nhiều đến khả năng tổng quát hóa của mô hình trong bối cảnh dữ liệu thực tế phức tạp.
0	Bên cạnh đó, quy trình xử lý dữ liệu đầu vào và chiến lược huấn luyện vẫn còn tương đối đơn giản, chưa khai thác triệt để các kỹ thuật hỗ trợ như tăng cường dữ liệu. Trong nghiên cứu này, một hướng tiếp cận mới được xây dựng dựa trên việc kết hợp mô hình 1D CNN với chiến lược tăng cường dữ liệu nhằm nâng cao khả năng phục hồi dữ liệu đo dao động bị thiếu trong môi trường thực tế - nơi dữ liệu có thể bị nhiễu, mất mát liên tục hoặc không đều. Khác với các nghiên cứu trước đây, phương pháp được đề xuất được áp dụng cho mô hình cầu dây văng trong phòng thí nghiệm, trong đó dữ liệu đo được chủ động làm nhiễu để mô phỏng điều kiện đo thực tế của công trình cầu thật.
0	Hướng tiếp cận này cho phép đánh giá hiệu quả phục hồi của mô hình một cách thực tiễn và có kiểm soát, góp phần nâng cao độ tin cậy của hệ thống SHM trong điều kiện vận hành thực tế, đồng thời đảm bảo khả năng áp dụng rộng rãi cho các kết cấu và môi trường đo khác nhau. Mạng nơ-ron tích chập một chiều là một trong những kiến trúc mạng học sâu mạnh mẽ và phổ biến trong việc xử lý dữ liệu chuỗi thời gian. Trong bối cảnh giám sát sức khỏe kết cấu, nơi các tín hiệu dao động được thu thập từ các cảm biến để đánh giá tình trạng của kết cấu,
0	mô hình 1D CNN đặc biệt hiệu quả nhờ khả năng trích xuất đặc trưng cục bộ từ chuỗi tín hiệu và thực hiện việc phục hồi dữ liệu bị thiếu với tốc độ huấn luyện cao. Mô hình được đề xuất (hình 1) trong nghiên cứu này được thiết kế với ba tầng 1D CNN liên tiếp, sử dụng số lượng bộ lọc giảm dần từ 128 đến 64 và 32, tất cả đều sử dụng hàm kích hoạt ReLU và đệm theo tính chất nguyên nhân (padding ‘causal’) nhằm bảo toàn cấu trúc thời gian của chuỗi tín hiệu.
0	Trong nghiên cứu này, chuẩn hóa theo lô (Batch normalization) được áp dụng giữa các tầng 1D CNN để ổn định phân phối đầu ra trung gian, kết hợp với kỹ thuật loại bỏ ngẫu nhiên (Dropout) với tỷ lệ 0,2 nhằm hạn chế quá khớp (overfitting). Cuối cùng, đầu ra được làm phẳng và đưa qua một lớp hoàn toàn kết nối (Dense) để dự đoán trực tiếp các bước tín hiệu tiếp theo. Cấu trúc tổng quát như sau: Conv1D(128 filters, kernel size=3, activation=ReLU)→ BatchNorm→ Dropout(0.2) Conv1D(64 filters, kernel size=3, activation=ReLU)→ BatchNorm → Dropout(0.2) Conv1D(32 filters, kernel size=3, activation=ReLU)→ BatchNorm→ Dropout(0.2) Flatten→ Dense(output_steps).
0	Mô hình được huấn luyện bằng thuật toán tối ưu Adam với tốc độ học (learning rate) được thiết lập là 0,001, cùng hàm mất mát là sai số bình phương trung bình (MSE). Kỹ thuật dừng sớm (early stopping) được áp dụng nhằm ngăn ngừa hiện tượng quá khớp, với tham số patience = 10. Theo đó, quá trình huấn luyện sẽ tự động dừng nếu hiệu suất trên tập xác thực không được cải thiện sau 10 vòng huấn luyện (epoch) liên tiếp, đồng thời khôi phục lại các trọng số tối ưu đạt được trước đó. Kích thước batch trong quá trình huấn luyện được đặt là 64, trong khi số epoch tối đa thiết lập là 1000, cho phép mô hình có đủ khả năng học nếu cần nhưng vẫn đảm bảo dừng sớm khi cần thiết để tránh quá khớp (overfitting).
0	Các siêu tham số được lựa chọn thông qua quá trình tinh chỉnh có kiểm soát, kết hợp giữa tham khảo các nghiên cứu liên quan trong lĩnh vực SHM và thực nghiệm trên nhiều cấu hình khác nhau. Việc lựa chọn cuối cùng dựa trên hiệu quả dự đoán của mô hình trên tập kiểm tra và tập xác thực, được đánh giá thông qua các chỉ số định lượng gồm RMSE, MAE và R². Đáng chú ý, các giá trị RMSE, MAE và R² được trình bày trong bài là giá trị trung bình thu được từ 6 lần huấn luyện độc lập cho mỗi mô hình, nhằm đảm bảo tính ổn định và độ tin cậy trong đánh giá hiệu suất của phương pháp đề xuất.
0	"Mô hình 1D CNN được xây dựng dựa trên nguyên lý tích chập, trong đó tín hiệu đầu vào được ""lọc"" thông qua các bộ lọc (kernels) có kích thước cố định. Đối với dữ liệu chuỗi thời gian một chiều, tín hiệu đầu vào x = [x1, x2, ..., xT] có thể là một dãy các giá trị dao động tại các thời điểm khác nhau và mỗi bộ lọc sẽ trượt qua chuỗi tín hiệu để tính toán đặc trưng tại các vị trí khác nhau trong chuỗi. Các lớp tích chập có thể xếp chồng để học đặc trưng ở nhiều mức độ. Để tăng độ ổn định và tốc độ hội tụ, mô hình áp dụng chuẩn hóa theo lô (Batch normalization) sau mỗi lớp tích chập một chiều (Conv1D), giúp ổn định phân phối đầu ra và giảm hiện tượng mất/bùng nổ gradient (vanishing/exploding gradient)."
0	Đồng thời, loại bỏ ngẫu nhiên (Dropout) với tỷ lệ 0,2 cũng được sử dụng nhằm giảm quá khớp (overfitting) bằng cách vô hiệu hóa ngẫu nhiên một phần các đơn vị (neurons) trong khi huấn luyện. Sự kết hợp giữa Conv1D, Batch normalization và Dropout giúp mô hình vừa học đặc trưng hiệu quả, vừa đảm bảo khả năng khái quát tốt. Trong bài toán phục hồi dữ liệu dao động bị thiếu trong hệ thống SHM, 1D CNN đóng vai trò như một công cụ trích xuất đặc trưng mạnh mẽ từ chuỗi tín hiệu không đầy đủ. Mô hình học cách nhận diện các mẫu dao động ngắn hạn và mối quan hệ lân cận trong chuỗi quan sát, từ đó dự đoán chính xác các giá trị bị mất dựa trên các đặc trưng đã học.
0	Phương pháp được đề xuất có một số ưu điểm nổi bật khi so sánh với các phương pháp truyền thống và kiến trúc học sâu hiện nay, cụ thể: So với LSTM (Long short-term memory) và GRU (Gated recurrent unit) truyền thống: Dù LSTM và GRU là các mô hình hiệu quả cho chuỗi thời gian, chúng đòi hỏi số lượng tham số lớn và dễ gặp hiện tượng suy giảm gradient khi độ dài chuỗi tăng. 1D CNN tận dụng khả năng trích xuất đặc trưng cục bộ nhanh chóng, số tham số ít hơn, huấn luyện nhanh hơn và đặc biệt phù hợp với dữ liệu SHM - nơi nhiễu và mất mát dữ liệu chủ yếu xảy ra ở quy mô cục bộ.
0	So với Autoencoder: Autoencoder có xu hướng học biểu diễn tổng thể toàn chuỗi, nên khó nắm bắt biến động ngắn hạn - vốn là đặc trưng của dữ liệu cảm biến bị thiếu cục bộ. Trong khi đó, mô hình 1D CNN trong nghiên cứu này được thiết kế để tập trung vào các vùng lân cận khu vực thiếu hụt, nhờ đó khôi phục tín hiệu một cách chi tiết và chính xác hơn. So với các kiến trúc CNN khác: Các nghiên cứu trước chủ yếu sử dụng CNN cho bài toán phân loại trạng thái kết cấu hoặc phát hiện bất thường.
0	Cho đến nay, vẫn còn hạn chế các nghiên cứu sử dụng CNN thuần 1D để xử lý trực tiếp bài toán tái tạo dữ liệu thiếu theo dạng chuỗi thời gian liên tục, đặc biệt trong bối cảnh dự báo ngắn hạn như được đề xuất trong nghiên cứu này. Về chiến lược kết hợp nhiễu Gaussian: Việc thêm nhiễu Gaussian vào dữ liệu huấn luyện giúp mô hình học được sự biến thiên tự nhiên của tín hiệu cảm biến trong môi trường thực tế, nhờ đó tăng khả năng tổng quát hóa và giảm hiện tượng quá khớp. Điều này đặc biệt quan trọng trong SHM, nơi tín hiệu thường chịu ảnh hưởng từ nhiều nguồn nhiễu không kiểm soát.
0	Độ lệch chuẩn của nhiễu được lựa chọn và điều chỉnh thông qua thực nghiệm, từ đó tối ưu hiệu quả khôi phục dữ liệu bị thiếu. Nghiên cứu này đề xuất một hướng tiếp cận hiệu quả cho bài toán phục hồi dữ liệu dao động bị thiếu trong SHM, với kiến trúc 1D CNN tối ưu cho việc khai thác đặc trưng cục bộ của tín hiệu, kết hợp với chiến lược tăng cường dữ liệu bằng nhiễu Gaussian. Phương pháp vừa đảm bảo tính hiệu quả về dự đoán, vừa phù hợp thực tiễn với đặc thù của dữ liệu SHM - vốn nhiều nhiễu và thiếu dữ liệu cục bộ.
0	Trong nghiên cứu này, mô hình 1D CNN được áp dụng để khôi phục dữ liệu đo bị thiếu từ mô hình cầu trong phòng thí nghiệm. Huấn luyện mô hình được thực hiện trên máy tính chạy Windows 10, sử dụng CPU Intel Core i7-12700F và GPU RTX 3060 (12 GB). Mô hình được xây dựng bằng Python 3.10 với các thư viện Keras và TensorFlow. Một mô hình cầu dây văng dài 3,5 m được xây dựng tại Phòng Thí nghiệm DX, Trường Đại học Giao thông Vận tải (hình 2), để đánh giá hiệu quả mô hình 1D CNN trong khôi phục dữ liệu.
0	Dầm chính của cầu có tiết diện hình chữ nhật với kích thước 0,12×0,005 m. Tháp cầu được thiết kế theo dạng chữ “A” với chiều cao đạt 1,6 m và được liên kết cứng với nền móng thông qua các bu lông có đường kính 10 mm. Hai đầu của dầm chính được gối đỡ bằng các liên kết dạng lò xo và được cố định xuống nền bằng các trụ cầu. Các trụ cầu cũng được liên kết cứng với nền móng thông qua các bu lông có đường kính 10 mm. Các cảm biến gia tốc PCB-353B34 được lắp đặt để ghi nhận dao động (hình 3), dữ liệu thu bằng hệ CompactDAQ với module NI-9234, kích thích dao động bằng búa tay và xử lý qua MATLAB/LABVIEW.
0	Nhằm tăng tính phong phú cho tập dữ liệu huấn luyện và nâng cao khả năng khái quát của mô hình, nghiên cứu áp dụng kỹ thuật tăng cường dữ liệu bằng cách bổ sung nhiễu Gaussian. Cụ thể, độ lệch chuẩn của dữ liệu gốc được sử dụng để điều chỉnh cường độ nhiễu. Nhiễu được tạo ra từ phân phối chuẩn, sau đó được cộng trực tiếp vào tín hiệu đầu vào để tạo thành dữ liệu tăng cường. Phương pháp này giúp tái hiện các điều kiện đo đạc trong thực tế, nơi dữ liệu thường bị ảnh hưởng bởi nhiễu ngẫu nhiên và sai số từ thiết bị cảm biến.
0	Việc huấn luyện mô hình với dữ liệu có nhiễu giúp hệ thống học được các đặc trưng ổn định và ít bị ảnh hưởng bởi nhiễu loạn, từ đó cải thiện độ chính xác và độ tin cậy trong các tác vụ dự báo chuỗi thời gian trong môi trường thực tế. Sau khi được tăng cường bằng cách bổ sung nhiễu, tập dữ liệu dao động được sử dụng để huấn luyện mô hình nhằm tăng khả năng thích ứng với môi trường có nhiễu và tính bất định cao. Dữ liệu này được chia thành ba phần riêng biệt: tập huấn luyện, tập xác thực và tập kiểm tra, theo tỷ lệ lần lượt là 70, 15 và 15%.
0	Trong mỗi tập, dữ liệu tiếp tục được xử lý để tách thành hai thành phần: đầu vào X và nhãn mục tiêu y, phục vụ cho bài toán khôi phục dữ liệu dao động. Để đánh giá tác động của dữ liệu huấn luyện được tăng cường bằng nhiễu đến hiệu suất của mô hình, các mô hình đã được huấn luyện với dữ liệu được bổ sung nhiễu Gaussian ở nhiều mức độ khác nhau: 0, 3, 5, 10, 15, 20 và 25% độ lệch chuẩn. Hiệu suất của các mô hình được so sánh thông qua ba chỉ số: RMSE, MAE và hệ số xác định R², được thể hiện trong các biểu đồ ở hình 4, 5 và bảng 2.
0	Hình 4 và bảng 2 cho thấy, mô hình đạt RMSE=0,00304 với dữ liệu gốc (0% nhiễu). Đáng chú ý, khi áp dụng nhiễu nhẹ (3-10%), hiệu suất được cải thiện rõ rệt, với RMSE thấp nhất là 0,00231 và MAE thấp nhất là 0,00141 tại mức nhiễu 10%. Hình 5 và bảng 2 cho thấy, chỉ số R² cũng tăng từ 0,7897 (0% nhiễu) lên 0,8798 (10% nhiễu), cho thấy khả năng khái quát hóa được cải thiện. Tuy nhiên, khi mức nhiễu vượt quá 15%, các chỉ số hiệu suất bắt đầu suy giảm. Kết quả này cho thấy, việc tăng cường dữ liệu bằng nhiễu Gaussian ở mức vừa phải giúp cải thiện độ chính xác và tính ổn định của mô hình, trong khi nhiễu quá lớn gây hiệu ứng ngược.
0	Những phát hiện này có ý nghĩa thực tiễn quan trọng đối với các hệ thống giám sát sức khỏe kết cấu cầu, nơi dữ liệu thu nhận từ cảm biến thường không tránh khỏi bị ảnh hưởng bởi nhiễu và sai số đo lường. Việc lựa chọn mức tăng cường phù hợp là cần thiết để cân bằng giữa tính kháng nhiễu và độ ổn định trong quá trình huấn luyện mô hình học sâu. Đường cong mất mát huấn luyện và xác thực, được đo bằng sai số bình phương trung bình (MSE) của hai mô hình không có và có tăng cường dữ liệu bằng nhiễu 10% độ lệch chuẩn, được trình bày trong hình 6.
0	Như thể hiện trong hình 6, mô hình đề xuất 1D CNN có tăng cường dữ liệu cho kết quả vượt trội hơn so với mô hình không có tăng cường dữ liệu ở cả giai đoạn huấn luyện và kiểm tra. Mô hình đạt giá trị mất mát thấp hơn, hội tụ nhanh hơn và duy trì độ ổn định huấn luyện cao hơn. Cần lưu ý rằng, số epochs huấn luyện khác nhau giữa các mô hình do áp dụng kỹ thuật dừng sớm (early stopping), nhằm dừng huấn luyện khi mất mát kiểm tra không còn cải thiện, từ đó đảm bảo mỗi mô hình được huấn luyện với số epochs tối ưu và tránh hiện tượng quá khớp.
0	Hình 7 và hình 8 trình bày sự so sánh kết quả khôi phục tín hiệu dao động giữa mô hình 1D CNN trong hai trường hợp: có và không áp dụng kỹ thuật tăng cường dữ liệu. Trục hoành biểu thị các bước thời gian, trong khi trục tung thể hiện giá trị gia tốc dao động (m/s²). Đường màu xanh nét liền đại diện cho tín hiệu thực tế (Actual), còn đường màu đỏ nét đứt là tín hiệu được khôi phục bởi mô hình (Reconstructed). Cùng một tập dữ liệu được sử dụng trong cả hai trường hợp nhằm đảm bảo tính nhất quán và khả năng so sánh trực quan giữa các kết quả. Kết quả cho thấy, cả hai mô hình đều có thể xấp xỉ hợp lý tín hiệu đo được.
0	Tuy nhiên, mô hình đề xuất 1D CNN có tăng cường dữ liệu thể hiện sự phù hợp vượt trội với tín hiệu thực so với mô hình không có tăng cường dữ liệu. Cụ thể, mô hình này khôi phục chính xác hơn cả biên độ lẫn dạng sóng của tín hiệu dao động. Bảng 3 trình bày kết quả đánh giá hiệu suất của mô hình 1D CNN đề xuất so với các mô hình hồi quy và học sâu, gồm: Linear Regression (hồi quy tuyến tính), XGBRegressor (thuật toán hồi quy XGBoost - Tăng cường độ dốc cực đại), LGBMRegressor (thuật toán hồi quy LightGBM - Tăng cường độ dốc nhẹ), MLP (Mạng nơ-ron truyền thẳng nhiều lớp) và RNN (Mạng nơ-ron hồi tiếp) trên tập kiểm tra.
0	Kết quả cho thấy, mô hình đề xuất 1D CNN đạt giá trị RMSE, MAE thấp nhất và hệ số xác định R² cao nhất (RMSE=0,002310, MAE=0,001414, R²=0,879782), cho thấy độ chính xác và khả năng tổng quát vượt trội. Cụ thể, so với Linear Regression, 1D CNN giúp giảm RMSE và MAE lần lượt 13,1 và 8,9%, đồng thời cải thiện R² thêm 7,8%. Mô hình cũng cho thấy, mức cải thiện đáng kể so với các thuật toán học máy như XGBRegressor và LGBMRegressor, cũng như các mô hình học sâu như MLP và RNN. Đặc biệt, khi so với RNN - một mô hình phổ biến trong xử lý chuỗi thời gian - 1D CNN vẫn cho kết quả tốt hơn với RMSE giảm 13,7% và R² tăng 4,9%.
0	Những kết quả này khẳng định tính hiệu quả của mô hình đề xuất 1D CNN và tăng cường dữ liệu trong bài toán khôi phục dữ liệu dao động trong hệ thống SHM. Nghiên cứu này đã chứng minh rằng việc áp dụng mạng nơ-ron tích chập một chiều là một hướng tiếp cận hiệu quả và phù hợp cho bài toán khôi phục dữ liệu dao động theo chuỗi thời gian trong các hệ thống giám sát sức khỏe kết cấu. Mô hình 1D CNN cho thấy, khả năng trích xuất đặc trưng cục bộ vượt trội từ tín hiệu đầu vào, đồng thời có tốc độ huấn luyện nhanh, độ ổn định cao và kiến trúc gọn nhẹ - rất phù hợp với các ứng dụng trong môi trường thực tế, nơi dữ liệu thường bị gián đoạn hoặc nhiễu loạn.
0	Về mặt học thuật, nghiên cứu đã đóng góp một quy trình xây dựng mô hình học sâu chuyên biệt cho bài toán khôi phục dữ liệu mất mát trong SHM, đồng thời làm rõ hiệu quả của việc tăng cường dữ liệu bằng nhiễu Gaussian. Cụ thể, kết quả cho thấy, việc bổ sung nhiễu với mức hợp lý có thể cải thiện đáng kể hiệu suất dự đoán và khả năng khái quát hóa của mô hình, từ đó nâng cao độ tin cậy trong môi trường dữ liệu thực. Về mặt hạn chế, nghiên cứu này mới chỉ được thực hiện trên dữ liệu dao động thu thập từ mô hình cầu trong phòng thí nghiệm và chưa xem xét trường hợp sử dụng thông tin từ nhiều cảm biến khác nhau để hỗ trợ khôi phục dữ liệu bị thiếu của một cảm biến cụ thể.
0	Trong tương lai, các hướng nghiên cứu mở rộng có thể bao gồm: (1) Triển khai và đánh giá mô hình trên dữ liệu thực địa từ các công trình ngoài thực tế; (2) Mở rộng sang các loại công trình khác như đập thủy điện, nhà cao tầng hoặc công trình công nghiệp; (3) Tích hợp thông tin từ nhiều loại cảm biến khác nhau để cải thiện độ chính xác và khả năng thích ứng của mô hình trong điều kiện giám sát đa chiều. Ngoài ra, việc kết hợp 1D CNN với các kiến trúc tiên tiến khác như encoder-decoder hoặc attention mechanism cũng là một hướng nghiên cứu hứa hẹn nhằm nâng cao hiệu quả khôi phục dữ liệu trong các hệ thống SHM phức tạp.
0	Trong những năm gần đây, nhận diện khuôn mặt đã đạt được những thành tựu cao nhờ sự phát triển của kỹ thuật học sâu. Một trong những kỹ thuật học sâu được sử dụng phổ biến là mạng nơron tích chập sâu được thiết kế, điều chỉnh cho phù hợp với mục đích sử dụng khác nhau. Trong nghiên cứu này, chúng tôi đề xuất phương pháp sử dụng các kỹ thuật HOG và mạng nơron tích chập sâu cho việc rút trích đặc trưng khuôn mặt, đồng thời sử dụng kỹ thuật SVM, mạng nơron cho việc nhận dạng.
0	Để cải tiến độ chính xác nhận dạng, chúng tôi sử dụng kỹ thuật học sâu không giám sát GAN (Generative Adversarial Networks) với hai biến thể là CGAN (Conditional Generative Adversarial Networks) và SRGAN (Super Resolution Generative Adversarial Networks) nhằm tăng tính đa dạng các trạng thái biểu cảm khuôn mặt, làm dày và tăng chất lượng tập ảnh đầu vào. Chúng tôi thực nghiệm trên các tập dữ liệu khuôn mặt Asian, LFW, VN-Celeb, Pin-faces để so sánh, đánh giá độ chính xác nhận dạng khuôn mặt. Kết quả thực nghiệm cho thấy phương pháp đề xuất đạt độ chính xác nhận diện khuôn mặt trên 90% cho hầu hết các tập dữ liệu thực nghiệm.
0	Nhận dạng khuôn mặt là một bài toán đã có lâu đời và được nghiên cứu rộng rãi trong nhiều năm trở lại đây. Bài toán nhận dạng khuôn mặt áp dụng trong nhiều lĩnh vực khác nhau như: hệ thống phát hiện tội phạm, hệ thống giám sát vào ra trong một đơn vị, hệ thống giám sát nơi công cộng, hệ thống tìm kiếm thông tin trên ảnh, video dựa trên nội dung,… Hiện nay, bài toán nhận dạng khuôn mặt gặp nhiều thách thức, ví dụ như hệ thống camera công cộng, chụp hình vui chơi thì ảnh mặt nhận được có thể bị che khuất một phần, ảnh chụp không chính diện hay chất lượng ảnh không tốt.
0	Các yếu tố này ảnh hưởng không nhỏ đến các thuật toán nhận dạng khuôn mặt. Có nhiều thuật toán khắc phục điều này, họ sử dụng một số kỹ thuật như xác định nhiều điểm chính trên khuôn mặt, lấy những chi tiết nhỏ hay sử dụng các phương pháp học sâu. Ojala và cộng sự [1] đã phát triển phương pháp Local Binary Patterns (LBP), nó là một mô tả kết cấu có thể được sử dụng để biểu diễn các khuôn mặt, vì một hình ảnh khuôn mặt có thể được xem như là một thành phần của các mô hình văn bản vi mô. Tóm lại, quy trình này bao gồm việc chia hình ảnh khuôn mặt thành nhiều vùng trong đó các tính năng LBP được trích xuất và ghép vào một vectơ đặc trưng sẽ được sử dụng làm mô tả khuôn mặt.
0	Một số nghiên cứu nhận dạng khuôn mặt khác cũng được quan tâm như SIFT của David G. Lowe [2]. Thuật toán Bag-of-visual-words của Yang và cộng sự (2007) [3] được đề xuất hiệu quả trong nhận dạng khuôn mặt. Bên cạnh đó, nhiều phương pháp nhận dạng khuôn mặt áp dụng kỹ thuật học sâu được đề xuất. Các phương pháp học sâu, chẳng hạn như mạng nơron tích chập, sử dụng nhiều tầng tích chập để trích xuất đặc trưng và nhận dạng. Năm 2014, DeepFace [14], DeepID [15], FaceNet [15] đã đạt được một bước đột phá về hiệu suất hiện đại và trọng tâm nghiên cứu đã chuyển sang các phương pháp tiếp cận dựa trên học tập sâu.
0	Các phương pháp này đạt được cải thiện độ chính xá cao từ khoảng 90% đến trên 99%. Tuy nhiên, các phương pháp này phụ thuộc nhiều vào số lượng, chất lượng, khoảng cách chụp lấy mẫu và tính đa dạng về trạng thái biểu cảm của khuôn mặt đối với tập ảnh huấn luyện. Đặc biệt, các phương pháp ứng dụng kỹ thuật học sâu đòi hỏi tập dữ liệu huấn luyện phải đủ lớn và đa dạng các trạng thái biểu cảm khuôn mặt. Các yếu tố này ảnh hưởng đến độ chính xác của nhận dạng. Để cải tiến chất lượng nhận dạng khuôn mặt, bài báo cáo này trình bày các thuật toán nhận dạng khuôn mặt
0	sử dụng kỹ thuật học sâu không giám sát GAN (Generative Adversarial Networks) với hai biến thể là CGAN (Conditional Generative Adversarial Networks) và SRGAN (Super Resolution Generative Adversarial Networks) nhằm tăng tính đa dạng các trạng thái biểu cảm khuôn mặt, làm dày và tăng chất lượng tập ảnh đầu vào. Chúng tôi thực nghiệm trên các tập dữ liệu khuôn mặt CASIA-WebFace, Asian, LFW, VN-Celeb, Pin-faces, FEI Face để so sánh, đánh giá độ chính xác nhận dạng khuôn mặt. Trong những năm gần đây, cùng với sự bùng nổ của cuộc cách mạng công nghiệp 4.0, các thuật ngữ như trí tuệ nhân tạo (AI), học máy (machine learning) và học sâu (deep learning) đang dần trở nên phổ biến.
0	Kỹ thuật Deep Learning [8] đang được ứng dụng trong nhiều lĩnh vực giao thông, kinh tế, y học, an ninh, hệ thống chấm công, ô tô tự lái, robot phẫu thuật, hệ thống dịch tự động, chatbot tự động trả lời,... Deep Learning là một kỹ thuật máy học giúp cho trí tuệ nhân tạo được sánh với trí tuệ con người. Deep Learning cấu tạo từ các mạng nơron, trong đó mạng nơron gồm nhiều tầng (thường là từ 10 đến 100 tầng). Để mạng nơron hoạt động hiệu quả khi dữ liệu học lớn, mô hình mạng lớn, khả năng tính toán của máy tính tốt [4].
0	Mạng nơron đã tạo ra một bước tiến vượt bậc, cung cấp khả năng nhận diện hình ảnh và âm thanh ở một cấp độ có thể so sánh với con người. Mạng nơron tích chập (CNN) là một trong những kỹ thuật học sâu được áp dụng nhiều cho việc thực hiện rút trích đặc trưng của ảnh. Ví dụ một bài toán về nhận dạng khuôn mặt, thì mạng CNN sẽ lấy ra đặc trưng cơ bản như mắt, mũi, miệng, … dưới dạng véc tơ hoặc huấn luyện tập dữ liệu và phân loại. Tuy nhiên CNN không chỉ được dùng để rút trích đặc trưng mà còn được sử dụng phân loại dữ liệu.
0	Người ta thường sử dụng CNN để nhận dạng và phân loại cho các bài toán như nhận diện chuyển động, nhận diện vật thể, nhận diện chữ viết và phổ biến nhất là nhận diện khuôn mặt người,… Có nhiều kiến trúc mạng CNN nổi tiếng được sử dụng rộng rãi như VGG16, ResNet50,… VGG16 [9] là một kiến trúc mạng nơron tích chập (CNN) được đề xuất bởi K. Simonyan and A. Zisserman, University of Oxford năm 2014. Model sau khi train bởi mạng VGG16 đạt độ chính xác 92.7% top-5 test trong dữ liệu ImageNet gồm 14 triệu hình ảnh thuộc 1000 lớp khác nhau.
0	Nó được coi là một trong những kiến trúc mô hình mạng nơron hiệu quả cho đến nay. Điều độc đáo nhất về VGG16 là thay vì có một số lượng lớn tham số siêu lớn, họ tập trung vào việc có các lớp chập của bộ lọc 3×3 với stride là 1 và luôn sử dụng cùng một lớp đệm (padding) và lớp maxpool của bộ lọc 2×2 có stride là 2 (hình 3). Ký hiệu 3×3 conv, 64 nghĩa là 64 kernel áp dụng trong layer đó (depth). Nó tuân theo sự sắp xếp của tích chập và các lớp pool tối đa, nhất quán trong toàn bộ kiến trúc. Cuối cùng, nó có 2 FC (tầng Fully Connected) theo sau là một hàm kích hoạt softmax cho đầu ra.
0	Mạng này là một mạng khá lớn và nó có khoảng 138 triệu tham số [7]. Mạng ResNet [6] hay còn gọi là Residual Network là một mạng CNN được thiết kế để làm việc với hàng trăm hoặc hàng nghìn lớp chập. Khi huấn luyện các mô hình (số lượng layers lớn, số lượng param lớn,…) ta thường gặp phải vấn đề về vanishing gradient hoặc exploding gradient. Thực tế cho thấy khi số lượng layer trong CNN model tăng, độ chính xác của mô hình cũng tăng theo, tuy nhiên khi tăng số layers quá lớn (> 50 layers) thì độ chính xác lại bị giảm đi [6].
0	Thực tế Gradient thường sẽ có giá trị nhỏ dần khi đi xuống các layer thấp hơn. Kết quả là các cập nhật thực hiện bởi Gradients Descent không làm thay đổi nhiều trọng số của các layer đó, làm chúng không thể hội tụ và mạng sẽ không thu được kết quả tốt. Hiện tượng như vậy gọi là Vanishing Gradients [5]. Lấy một mạng đơn giản (mạng loại 18 lớp VGG) (Mạng-1) và biến thể sâu hơn của nó (34 lớp, Mạng-2) và thêm các Residual block vào Mạng-2 (34 lớp với residual connections còn lại, Mạng-3) [4].
0	GAN là một thuật toán học không giám sát (Unsupersived Learning) [10] được giới thiệu vào năm 2014. GAN được sinh ra với kỳ vọng tạo ra được những hệ thống có độ chính xác cao mà cần ít hoạt động của con người trong khâu huấn luyện. Nó là mô hình mạng có khả năng sinh ra dữ liệu mới giống với dữ liệu trong dataset có sẵn sau quá trình học. GAN có thể tự sinh ra một khuôn mặt mới, một con người, một đoạn văn, chữ viết, bản nhạc giao hưởng hay những thứ tương tự thế. GAN cấu tạo gồm 2 mạng là Generator (G) và Discriminator (D) được biểu diễn trong hình 4.
0	Thành phần mạng Generator là để tạo một hình ảnh giả hay còn có một tên gọi khác là một hình ảnh nhiễu. Trong khi Generator sinh ra các dữ liệu giống như thật thì Discriminator cố gắng phân biệt đâu là dữ liệu được sinh ra từ Generator và đâu là dữ liệu thật. Đầu vào cho G là một nhiễu z, được sinh ngẫu nhiên từ một phân phối xác suất (phổ biến nhất là Gaussian). GAN có nhiều biến thể khác nhau, trong nghiên cứu này chúng tôi giới thiệu hai biến thể để đó là: CGAN (Conditional Generative Adversarial Networks) và SRGAN (Super Resolution Generative Adversarial Networks).
0	Để làm tăng độ chính xác nhận dạng khuôn mặt, chúng tôi sử dụng mạng CGAN sinh ra các ảnh giống thật nhất để làm dày tập ảnh huấn luyện và SRGAN cho giai đoạn tiền xử lý ảnh nhằm làm rõ nét tập ảnh huấn luyện và các ảnh đầu vào kiểm thử. Khi huấn luyện GAN xong, chúng ta dùng generator để sinh ảnh mới giống trong tập dữ liệu. Tuy nhiên, chúng ta không kiểm soát được là ảnh sinh ra giống người nào trong tập dữ liệu. Bài toán trong nghiên cứu này là muốn kiểm soát được generator sinh ra ảnh theo một người nhất định.
0	Mô hình này gọi là Conditional GAN (CGAN). Trong bài báo này, biến thể CGAN (Conditional Generative Adversarial Networks) [12] dùng để sinh tập ảnh mới với mục đích để làm dày tập dữ liệu. Trong CGAN, các nhãn hoạt động như là một phần mở rộng tới không gian tiềm ẩn (latent space) z để sinh và phân biệt đối nghịch các hình ảnh tốt hơn. Nhiệm vụ của CGAN là nối các nhãn với các hình ảnh đã được định danh bằng các nhãn tương ứng lại với nhau. Mô hình Generator sẽ sinh ra ảnh dựa trên không gian tiềm ẩn z và nhãn tương ứng của ảnh đó.
0	Mô hình Discriminator có nhiệm vụ phân biệt những ảnh mà Generator sinh ra có là ảnh thật hay không, có đúng với nhãn của ảnh thật hay không [12]. SRGAN [9] là mạng cho kết quả hình ảnh siêu phân giải hấp dẫn hơn đối với góc nhìn của con người. Mục đích của SRGAN là tạo ra những hình ảnh I^{SR} sắc nét có độ phân giải cao từ những bức ảnh tương ứng có độ phân giải thấp I^{LR}. I^{LR} là ảnh có độ phân giải thấp được trích từ một phần I^{HR} có độ phân giải cao. Hình ảnh có độ phân giải cao có sẵn trong khi huấn luyện.
0	Trong khi đang huấn luyện, I^{LR} thu được bởi áp dụng bộ lọc Gaussian từ I^{HR}. Hình 6 mô tả kiến trúc mạng SRGAN. Trong nghiên cứu này, chúng tôi sử dụng SRGAN cho bước tiền xử lý tập ảnh đầu vào cho việc huấn luyện và nhận dạng vì khi sử dụng camera có độ phân giải thấp, khoảng cách đặt giám sát khác nhau sẽ làm ảnh hưởng đến chất lượng video. Điều này sẽ ảnh hưởng đến độ chính xác khi nhận dạng. Chính vì vậy, chúng tôi đề xuất giải pháp sử dụng SRGAN để giải quyết vấn đề này và tạo ra ảnh rõ nét, tăng cường chất lượng ảnh khi nhận dạng.
0	Bài toán nhận dạng được chia thành ba bước chính: Tiền xử lý, rút trích đặc trưng và phân loại. Từ một ảnh đầu vào ta đưa vào bài toán nhận dạng, đầu tiên đi qua bước tiền xử lý ảnh, chúng tôi sử dụng 2 phương pháp tiền xử lý ảnh là Data Augmentation và Data Synthesis (CGAN) cho pha huấn luyện và Data Synthesis (SRGAN) cho pha nhận dạng, bước này thực hiện làm dày tập ảnh đầu vào trước khi vào giai đoạn huấn luyện, sau khi tăng cường làm dày tập dữ liệu dữ liệu ảnh, chúng tôi tiến hành thực hiện bước rút trích đặc trưng ảnh.
0	Một số thuật toán rút trích đặc trưng ảnh được sử dụng do tính phổ biến như: HOG, CNN. Sau khi rút trích đặc trưng của tập ảnh, chúng tôi huấn luyện và phân loại. Hình 7 minh hoạ mô hình tổng quát cho các bước nhận dạng khuôn mặt trong phương pháp đề xuất. Để cải tiến độ chính xác nhận dạng, trước khi đưa tập dữ liệu khuôn mặt vào mô hình huấn luyện, chúng tôi làm dày tập dữ liệu bằng phương pháp tiền xử lý ảnh. Trong bước tiền xử lý ảnh này, chúng tôi đề xuất sử dụng phương pháp Data Synthesis (CGAN).
0	Chúng tôi đề xuất sử dụng thuật toán học không giám sát CGAN (Conditional Generative Adversarial Networks), một biến thể của GAN, để thực hiện tiền xử lý ảnh. Trong giai đoạn này, thuật toán học CGAN được sử dụng để làm dày tập ảnh huấn luyện bằng cách tạo mới và thêm vào tập huấn luyện các ảnh chứa các trạng thái biểu hiện khác nhau trên khuôn mặt dựa trên tập dữ liệu huấn luyện. Kết quả tập dữ liệu huấn luyện được làm dày bằng cách bổ sung 9 ảnh cho mỗi ảnh dữ liệu huấn luyện tương ứng. Như vậy, sau khi tiền xử lý ảnh, tập dữ liệu ảnh huấn luyện sẽ tăng gấp 10 lần so với tập ảnh gốc.
0	Chúng tôi đề xuất sử dụng hai kỹ thuật rút trích đặc trưng là HOG và mạng nơron tích chập. Phương pháp rút trích đặc trưng bằng HOG (Histograms of Oriented Gradients) [13]: Phương pháp rút trích đặc trưng hình ảnh bởi HOG được đề xuất bởi tác giả là Dalal và Triggs. Phương pháp này được sử dụng phổ biến trong các bài toán nhận dạng đối tượng đặc biệt nhận dạng khuôn mặt. Với hình ảnh khuôn mặt có độ phân giải là 224×224, chúng tôi sẽ sử dụng số pixel mỗi cell là 12. Phương pháp rút trích đặc trưng bằng kỹ thuật mạng nơron tích chập: Chúng tôi sử dụng kiến trúc mạng nơron ResNet50 để rút trích đặc trưng ảnh. Chúng tôi chuẩn hoá ảnh đầu vào với kích thước là 224×224.
0	Chúng tôi đề xuất hai phương pháp huấn luyện bằng SVM và mạng nơron tích chập. Phương pháp huấn luyện bằng mô hình SVM: Phương pháp này sử dụng các tham số cơ bản như là C, gamma, kernel. Chúng tôi sẽ đề cập các giá trị cụ thể trong phần kết quả thực nghiệm. Phương pháp huấn luyện bằng mô hình mạng nơron tích chập: Mạng nơron tích chập mà chúng tôi sử dụng cho bài toán nhận dạng khuôn mặt là mạng ResNet50. Đây là một dạng pre-trained model và cho độ chính xác khi huấn luyện là rất cao.
0	Để tiến hành huấn luyện mô hình, chúng tôi đề xuất kỹ thuật transfer learning, kỹ thuật này sử dụng các pre-trained model để huấn luyện tập dữ liệu. Giai đoạn nhận dạng được chia thành 3 bước chính: Tiền xử lý ảnh, trích chọn đặc trưng và nhận dạng. Chúng tôi sử dụng phương pháp Data Synthesis bằng cách sử dụng mạng SRGAN nhằm nâng cao chất lượng tập ảnh dữ liệu đầu vào. Bước này tạo ra tập ảnh dữ liệu kiểm thử được cải tiến chất lượng với độ phân giải cao và rõ nét. Tương tự trích chọn đặc trưng của giai đoạn huấn luyện, chúng tôi cũng đề xuất sử dụng hai phương pháp đó là phương pháp sử dụng thuật toán mô tả đặc trưng HOG và phương pháp mạng nơron tích chập (VGG16, ResNet50).
0	Chúng tôi sử dụng các mạng nơron tích chập và mô hình SVM cho quá trình nhận dạng. Sau đây chúng tôi trình bày danh sách các phương pháp nhận dạng trong nghiên cứu của chúng tôi được mô tả trong bảng 1. Từ bảng 1, phương pháp NHS (None Image Process – Hog – SVM) là phương pháp chúng tôi bỏ qua bước tiền xử lý ảnh, rút trích đặc trưng bởi HOG và phân loại ảnh bằng phương pháp SVM. Phương pháp NRR (None Image Process – ResNet50 – ResNet50): bỏ qua bước tiền xử lý ảnh, sử dụng mạng nơron tích chập để rút trích đặc trưng bằng kiến trúc mạng ResNet50 và phân loại ảnh bằng kiến trúc mạng ResNet50. Mô tả tương tự đối với các phương pháp AHS, ARR.
0	Chúng tôi thực nghiệm các phương pháp đề xuất trên các tập dữ liệu được trình bày trong bảng 2. Chúng tôi chia tập dữ liệu ảnh đầu vào ngẫu nhiên thành hai tập: tập huấn luyện và tập kiểm thử tỉ lệ 8:2. Bảng 3 trình bày các giá trị tham số thực nghiệm huấn luyện của mô hình mạng CGAN. Các phương pháp đề xuất trường hợp không tiền xử lý ảnh với phương pháp NRR có tham số epoch là 60 và learning-rate là 0.001, phương pháp NHS có hệ số gamma = 0.001. Phương pháp NRR và ARR có tham số epoch là 60 và learning-rate là 0.001. Phương pháp NHS và AHS có hệ số C = 10 và hệ số gamma = 0.001.
0	Kết quả độ chính xác phân loại với các phương pháp NHS, NRR, AHS và ARR được biểu diễn bằng đồ thị trình bày trong hình 8. Phương pháp ARR có ba tập dữ liệu đạt độ chính xác ở ngưỡng cao nhất là tập LFW, Pin-Faces và Asian-Faces với độ chính xác là 99% và độ chính xác thấp hơn là của tập Vn-Celeb là 97%. Khi thực hiện phương pháp AHS tập dữ liệu có độ chính xác cao nhất là tập Asian-Faces với độ chính xác là 98% và tập có độ chính xác nhỏ hơn là Vn-Celeb là 79%. Độ chính xác của phương pháp ARR cao hơn phương pháp AHS là 1% khi thực hiện đánh giá trên tập Asian-Faces và cao hơn 16% khi thực hiện trên tập Vn-Celeb.
0	Từ hình 8, khi thực hiện huấn luyện và phân loại dữ liệu bằng mô hình SVM chúng ta có hai phương pháp là NHS và AHS. Phương pháp AHS cho độ chính xác cao hơn phương pháp NHS, cụ thể là cao hơn 7% trên tập Asian-Faces, 44% trên tập Pin-Faces, 13% trên tập LFW, 38% trên tập Vn-Celeb. Khi thực hiện huấn luyện bằng mạng nơron tích chập, mạng ResNet50, ta có hai phương pháp là NRR và ARR. Ta thấy được phương pháp ARR cho độ chính xác cao hơn phương pháp NRR, cụ thể cao hơn 1% trên tập Asian-Faces, 6% trên tập Pin-Faces, 5% trên tập LFW, 4% trên tập Vn-Celeb.
0	Như vậy, từ hình 8 cho ta thấy các phương pháp có áp dụng tiền xử lý ảnh trên các tập dữ liệu đầu vào cho kết quả độ chính xác được cải thiện rõ rệt, cao hơn so với các phương pháp không áp dụng tiền xử lý ảnh. Đặc biệt, khi áp dụng mạng CGAN để thực hiện làm dày tập ảnh huấn luyện và SRGAN để tiền xử lý ảnh nâng cao chất lượng tập ảnh huấn luyện và kiểm thử thì độ chính xác được cải thiện rõ rệt đạt trên 90% cho tất cả tập dữ liệu thực nghiệm nêu trong bảng 2.
0	Khi thực nghiệm, chúng tôi theo dõi thời gian thực hiện của các phương pháp đề xuất được biểu diễn trong hình 9, 10, 11. Rõ ràng, phương pháp ARR có thời gian huấn luyện lớn hơn rất nhiều so với các phương pháp còn lại nhưng việc phân nhận dạng cho độ chính xác cao và ổn định hơn đối với hầu hết các tập dữ liệu. Vì vậy, phương pháp ARR thích hợp cho việc nhận dạng khuôn mặt đối với các tập dữ liệu lớn và đa dạng. Trong nghiên cứu này, chúng tôi đề xuất một số phương pháp sử dụng mạng nơron tích chập trong nhận dạng khuôn mặt. Từ kết quả thực nghiệm, chúng ta có thể thấy rằng phương pháp ARR cho độ chính xác cao hơn các phương pháp NHS, NRR, SHS, SVV, AHS.
0	Để cải tiến độ chính xác nhận dạng khuôn mặt, chúng tôi đề xuất sử dụng mô hình mạng học sâu không giám sát GAN (Generative Adversarial Networks) với hai biến thể là CGAN (Conditional Generative Adversarial Networks) để tăng tính đa dạng các trạng thái biểu cảm khuôn mặt, làm dày tập ảnh huấn luyện và SRGAN (Super Resolution Generative Adversarial Networks) để nâng cao chất lượng ảnh, tăng độ phân giải tập dữ ảnh huấn luyện và kiểm thử. Kết quả khi áp dụng bước tiền xử lý ảnh với CGAN và SRGAN trên các tập dữ liệu cho thấy độ chính xác được cải thiện rõ rệt với độ chính xác nhận dạng trên 90%.
1	Mạng sinh đối kháng (Generative Adversarial Network – GAN) là một trong những mô hình học sâu nổi bật, được đề xuất nhằm tạo ra dữ liệu mới có phân bố tương tự dữ liệu gốc. Trong lĩnh vực y tế, việc thu thập ảnh chẩn đoán như X-quang, CT hoặc MRI gặp nhiều hạn chế do chi phí cao và yêu cầu bảo mật nghiêm ngặt. Theo thống kê từ một bệnh viện tuyến trung ương, số lượng ảnh MRI não phục vụ nghiên cứu chỉ đạt khoảng 2.500 mẫu mỗi năm, không đủ lớn để huấn luyện mô hình học sâu hiệu quả. GAN được áp dụng để sinh thêm dữ liệu tổng hợp, giúp mở rộng tập huấn luyện lên gấp 3–5 lần.
1	Điều này góp phần giảm hiện tượng quá khớp và nâng cao độ tin cậy của các hệ thống hỗ trợ chẩn đoán tự động trong thực tế lâm sàng. Trong nghiên cứu này, mô hình DCGAN được sử dụng để sinh ảnh X-quang phổi phục vụ phát hiện bệnh viêm phổi. Tập dữ liệu ban đầu gồm 6.000 ảnh, trong đó 3.000 ảnh dương tính và 3.000 ảnh âm tính. Sau khi áp dụng GAN, số lượng ảnh huấn luyện tăng lên 18.000 ảnh, giúp cải thiện đáng kể hiệu suất mô hình phân loại CNN. Kết quả cho thấy độ chính xác tăng từ 86,4% lên 92,1%, trong khi chỉ số F1-score tăng từ 0,84 lên 0,91.
1	Đặc biệt, tỷ lệ bỏ sót ca bệnh giảm khoảng 35%, một yếu tố quan trọng trong chẩn đoán y tế. Các ảnh sinh ra được đánh giá bởi bác sĩ chuyên khoa và đạt mức tương đồng hơn 90% so với ảnh thật. Việc ứng dụng GAN trong đời sống không chỉ giới hạn ở y tế mà còn mở rộng sang các lĩnh vực như an ninh, giao thông và giáo dục. Trong y tế, GAN có thể hỗ trợ đào tạo bác sĩ thông qua dữ liệu mô phỏng, giảm phụ thuộc vào dữ liệu bệnh nhân thật.Ngoài ra, GAN còn được sử dụng để khử nhiễu ảnh chụp liều thấp, giúp giảm liều phóng xạ cho bệnh nhân tới 40%.
1	Mạng nơ-ron nhân tạo (Artificial Neural Network – ANN) được sử dụng rộng rãi trong các bài toán dự báo chuỗi thời gian nhờ khả năng học mối quan hệ phi tuyến phức tạp. Trong lĩnh vực năng lượng, dự báo nhu cầu tiêu thụ điện đóng vai trò quan trọng trong việc cân bằng cung cầu và tối ưu vận hành lưới điện. Theo báo cáo của ngành điện lực, sai số dự báo chỉ cần giảm 1% có thể tiết kiệm hàng chục tỷ đồng chi phí vận hành mỗi năm. Tuy nhiên, các phương pháp thống kê truyền thống thường gặp khó khăn khi xử lý dữ liệu lớn và biến động mạnh theo thời tiết, mùa vụ và hành vi người dùng. ANN được lựa chọn như một giải pháp hiệu quả để cải thiện độ chính xác trong các hệ thống dự báo hiện đại.
1	Nghiên cứu triển khai mô hình ANN nhiều lớp với 3 lớp ẩn để dự báo nhu cầu điện theo giờ tại một thành phố lớn, sử dụng dữ liệu trong 5 năm liên tiếp, tương đương hơn 43.000 mẫu. Các biến đầu vào bao gồm nhiệt độ, độ ẩm, ngày trong tuần và dữ liệu tiêu thụ lịch sử. Kết quả cho thấy mô hình ANN đạt sai số trung bình tuyệt đối (MAE) là 2,8%, thấp hơn đáng kể so với phương pháp hồi quy tuyến tính với MAE 5,1%. Trong các đợt cao điểm mùa hè, ANN duy trì độ ổn định tốt, giúp giảm nguy cơ quá tải lưới điện khoảng 18%. Điều này chứng minh khả năng thích nghi của ANN với dữ liệu biến động mạnh trong điều kiện thực tế.
1	Ứng dụng ANN trong dự báo điện năng mang lại lợi ích rõ rệt cho cả nhà cung cấp và người tiêu dùng. Các công ty điện lực có thể tối ưu kế hoạch phát điện, giảm chi phí nhiên liệu và hạn chế phát thải CO₂ lên tới 12%. Đối với người dân, hệ thống dự báo chính xác hỗ trợ triển khai biểu giá điện linh hoạt, khuyến khích sử dụng điện vào giờ thấp điểm. Trong tương lai, ANN có thể được kết hợp với dữ liệu IoT từ công tơ thông minh để dự báo ở cấp hộ gia đình. Tuy nhiên, vấn đề bảo mật dữ liệu và khả năng mở rộng mô hình vẫn là thách thức cần được giải quyết trong các nghiên cứu tiếp theo.
1	Mạng nơ-ron tích chập (Convolutional Neural Network – CNN) là nền tảng cốt lõi của các hệ thống thị giác máy tính hiện đại. Trong bối cảnh đô thị hóa nhanh, ùn tắc giao thông gây thiệt hại kinh tế lớn, ước tính chiếm khoảng 3–5% GDP tại các thành phố lớn. Việc giám sát giao thông thủ công bằng con người gặp nhiều hạn chế về chi phí và độ chính xác. CNN được ứng dụng để tự động nhận dạng phương tiện, phát hiện vi phạm và phân tích mật độ giao thông theo thời gian thực. Nhờ khả năng trích xuất đặc trưng không gian hiệu quả, CNN trở thành giải pháp then chốt cho các hệ thống giao thông thông minh trong đời sống hiện nay.
1	Nghiên cứu sử dụng mô hình YOLOv5 dựa trên CNN để nhận dạng phương tiện từ camera giao thông. Tập dữ liệu gồm 120.000 khung hình được thu thập trong 6 tháng tại các nút giao lớn. Kết quả cho thấy hệ thống đạt độ chính xác nhận dạng 94,6% đối với ô tô và 92,3% đối với xe máy. Thời gian xử lý trung bình chỉ 0,03 giây mỗi khung hình, đáp ứng yêu cầu thời gian thực. Sau khi triển khai thử nghiệm, tỷ lệ phát hiện vi phạm vượt đèn đỏ tăng 27%, trong khi thời gian xử lý ùn tắc giảm trung bình 15%. Điều này chứng minh hiệu quả rõ rệt của CNN trong ứng dụng giao thông thực tế.
1	CNN trong giao thông không chỉ hỗ trợ quản lý mà còn góp phần nâng cao an toàn cho người dân. Hệ thống có thể cảnh báo tai nạn sớm, giúp lực lượng chức năng phản ứng nhanh hơn khoảng 20%. Tuy nhiên, các yếu tố như thời tiết xấu, ánh sáng kém và góc quay phức tạp vẫn ảnh hưởng đến độ chính xác. Ngoài ra, chi phí đầu tư hạ tầng và bảo mật dữ liệu hình ảnh là những vấn đề cần cân nhắc. Trong tương lai, việc kết hợp CNN với học tăng cường và dữ liệu cảm biến sẽ mở ra hướng phát triển mới cho giao thông thông minh bền vững.
1	Thuật toán K-Nearest Neighbors (KNN) là một phương pháp học máy đơn giản nhưng hiệu quả, dựa trên độ tương đồng giữa các đối tượng. Trong thương mại điện tử, việc cá nhân hóa trải nghiệm người dùng đóng vai trò quan trọng trong việc tăng doanh thu. Theo thống kê, hệ thống gợi ý hiệu quả có thể tăng tỷ lệ chuyển đổi mua hàng từ 10% đến 30%. KNN được áp dụng để phân tích hành vi mua sắm dựa trên lịch sử truy cập và đánh giá sản phẩm. Nhờ tính trực quan và dễ triển khai, KNN phù hợp cho các doanh nghiệp vừa và nhỏ trong giai đoạn đầu chuyển đổi số.
1	Nghiên cứu xây dựng hệ thống gợi ý sử dụng KNN với dữ liệu từ 50.000 người dùng và 10.000 sản phẩm. Khoảng cách cosine được sử dụng để đo độ tương đồng hành vi. Kết quả thử nghiệm cho thấy tỷ lệ click vào sản phẩm gợi ý tăng từ 18% lên 26% so với hệ thống không cá nhân hóa. Doanh thu trung bình mỗi người dùng tăng khoảng 12% sau 3 tháng triển khai. Thời gian phản hồi của hệ thống duy trì dưới 0,1 giây cho mỗi truy vấn, đáp ứng yêu cầu thực tế. Điều này cho thấy KNN vẫn giữ được giá trị ứng dụng cao dù là thuật toán cổ điển.
1	Mặc dù KNN mang lại hiệu quả ban đầu, thuật toán này gặp hạn chế về khả năng mở rộng khi dữ liệu tăng lớn. Chi phí tính toán tăng theo số lượng người dùng và sản phẩm, gây áp lực lên hệ thống. Để khắc phục, các nghiên cứu đề xuất kết hợp KNN với phương pháp giảm chiều dữ liệu hoặc chuyển sang mô hình lai như KNN-CNN. Trong tương lai, việc tích hợp KNN với dữ liệu hành vi thời gian thực và AI giải thích được sẽ giúp hệ thống gợi ý minh bạch hơn, từ đó nâng cao niềm tin của người tiêu dùng trong môi trường thương mại điện tử hiện đại.
1	Nghiên cứu này tập trung vào việc ứng dụng kiến trúc Super-Resolution Generative Adversarial Network (SRGAN) để nâng cao chất lượng hình ảnh chụp cộng hưởng từ (MRI) có độ phân giải thấp. Trong thực tế lâm sàng, việc thu thập dữ liệu hình ảnh độ phân giải cao thường tốn nhiều thời gian và gây khó chịu cho bệnh nhân. Chúng tôi đề xuất một mô hình GAN gồm bộ sinh (Generator) dựa trên các khối Residual và bộ phân biệt (Discriminator) để tái tạo các chi tiết giải phẫu bị mất. Kết quả thực nghiệm trên tập dữ liệu 5,000 ảnh MRI cho thấy sự cải thiện đáng kể về chỉ số PSNR và SSIM, giúp các bác sĩ chẩn đoán chính xác hơn các tổn thương nhỏ mà trước đây khó có thể quan sát bằng mắt thường.
1	Mô hình nghiên cứu sử dụng cấu trúc SRGAN với hàm mất mát kết hợp giữa Content Loss và Adversarial Loss. Bộ sinh được thiết kế với 16 khối Deep Residual giúp bảo toàn các đặc trưng kết cấu của mô não, trong khi bộ phân biệt được huấn luyện để phân loại giữa ảnh siêu phân giải do AI tạo ra và ảnh thực tế. Quá trình huấn luyện được thực hiện trên hệ thống máy chủ sử dụng GPU NVIDIA A100 trong vòng 150 epochs. Chúng tôi áp dụng kỹ thuật Data Augmentation để tăng cường tính đa dạng cho dữ liệu đầu vào, bao gồm xoay ảnh, lật ảnh và thay đổi độ sáng, nhằm đảm bảo mô hình có khả năng thích nghi tốt với các thiết bị chụp MRI của nhiều hãng sản xuất khác nhau.
1	Kết quả nghiên cứu đạt được vô cùng khả quan với chỉ số PSNR (Peak Signal-to-Noise Ratio) trung bình tăng từ 24.5 dB lên 31.2 dB đối với ảnh được phóng đại 4 lần. Đặc biệt, chỉ số tương đồng cấu trúc SSIM (Structural Similarity Index) đạt mức 0.92, cao hơn hẳn so với các phương pháp nội suy truyền thống (chỉ đạt 0.78). Qua khảo sát mù đối với 10 bác sĩ chẩn đoán hình ảnh, có đến 85% ý kiến cho rằng ảnh sau khi xử lý bằng GAN cung cấp thông tin chi tiết tốt hơn cho việc đánh giá khối u. Điều này chứng minh rằng GAN không chỉ là công cụ tạo ảnh nghệ thuật mà còn là trợ thủ đắc lực trong y tế, giúp giảm thời gian chụp MRI xuống 30% mà vẫn đảm bảo chất lượng hình ảnh đầu ra.
1	Bài báo trình bày giải pháp ứng dụng mạng nơ-ron nhân tạo đa lớp (Multilayer Perceptron - MLP) để dự báo lưu lượng xe tại các nút giao thông trọng điểm của TP. Hồ Chí Minh. Bài toán đặt ra là dự đoán số lượng phương tiện theo từng khung giờ dựa trên các yếu tố đầu vào như lịch sử giao thông, điều kiện thời tiết và các sự kiện đặc biệt. Bằng cách huấn luyện trên bộ dữ liệu thu thập từ các camera giám sát trong vòng 12 tháng, mô hình ANN của chúng tôi có khả năng đưa ra dự báo chính xác cao trước 30 phút. Nghiên cứu này đóng góp một công cụ quan trọng cho hệ thống quản lý giao thông thông minh, giúp giảm thiểu tình trạng ùn tắc vào giờ cao điểm một cách hiệu quả.
1	Cấu trúc mạng ANN được đề xuất bao gồm 1 lớp đầu vào (12 đặc trưng), 3 lớp ẩn (với số lượng nơ-ron lần lượt là 64, 32, 16) và 1 lớp đầu ra dự báo lưu lượng. Chúng tôi sử dụng hàm kích hoạt ReLU cho các lớp ẩn để khắc phục hiện tượng triệt tiêu đạo hàm và hàm Linear cho lớp đầu ra. Thuật toán tối ưu hóa Adam được lựa chọn với tốc độ học (learning rate) ban đầu là 0.001. Dữ liệu được chia theo tỷ lệ 70:15:15 tương ứng cho huấn luyện, kiểm tra và đánh giá độc lập. Để tăng độ chính xác, chúng tôi thực hiện chuẩn hóa dữ liệu về khoảng [0, 1] trước khi đưa vào mạng, điều này giúp quá trình hội tụ diễn ra nhanh hơn và giảm sai số dự báo đáng kể.
1	Sau khi thử nghiệm trên tuyến đường Điện Biên Phủ, mô hình đạt sai số tuyệt đối trung bình (MAE) là 12.4 xe/phút, thấp hơn nhiều so với mô hình hồi quy tuyến tính (28.9 xe/phút). Chỉ số tương quan bình phương R-squared đạt mức 0.89, cho thấy mô hình giải thích được phần lớn sự biến động của lưu lượng giao thông. Trong các điều kiện thời tiết xấu như mưa lớn, ANN vẫn duy trì độ chính xác trên 80%, trong khi các phương pháp thống kê cũ thường bị sai lệch lớn. Kết quả nghiên cứu đề xuất rằng nếu tích hợp mô hình này vào hệ thống điều khiển đèn tín hiệu, thời gian chờ tại các nút giao có thể giảm trung bình 15-20%, góp phần cải thiện môi trường và giảm chi phí nhiên liệu cho người dân.
1	Nghiên cứu này giới thiệu một mô hình học sâu dựa trên kiến trúc MobileNetV2 – một dạng mạng CNN tối ưu cho thiết bị di động – để phân loại các loại bệnh trên lá lúa. Ngành nông nghiệp Việt Nam thường xuyên đối mặt với các dịch bệnh như đạo ôn, bạc lá hay rầy nâu, gây thiệt hại hàng nghìn tỷ đồng mỗi năm. Chúng tôi đã xây dựng một bộ dữ liệu gồm 10,000 hình ảnh lá lúa được nhãn hóa bởi các chuyên gia nông nghiệp. Mô hình CNN được huấn luyện để tự động nhận diện các đốm bệnh và đưa ra cảnh báo sớm. Với kích thước mô hình nhẹ, ứng dụng có thể chạy trực tiếp trên smartphone của nông dân mà không cần kết nối internet liên tục, mang lại giá trị thực tiễn cực kỳ cao.
1	Sức mạnh của CNN nằm ở các lớp tích chập (Convolutional Layers) có khả năng tự động học các đặc trưng hình học từ đơn giản như cạnh, góc đến phức tạp như cấu trúc đốm nấm trên mặt lá. Chúng tôi áp dụng kỹ thuật Transfer Learning (Học chuyển đổi) từ trọng số của ImageNet để rút ngắn thời gian huấn luyện và cải thiện độ chính xác trên tập dữ liệu nhỏ. Lớp Global Average Pooling được sử dụng thay cho các lớp kết nối đầy đủ (Dense) truyền thống để giảm số lượng tham số, tránh tình trạng quá khớp (Overfitting). Toàn bộ quy trình từ xử lý ảnh đầu vào đến khi đưa ra kết luận phân loại chỉ mất chưa đầy 0.5 giây trên cấu hình điện thoại tầm trung, đáp ứng tốt yêu cầu thực địa.
1	Thực nghiệm cho thấy mô hình đạt độ chính xác tổng thể (Accuracy) là 94.5% trên 10 loại bệnh phổ biến. Đối với bệnh đạo ôn – loại bệnh nguy hiểm nhất – chỉ số F1-score đạt tới 0.96, đảm bảo tỷ lệ bỏ sót bệnh ở mức cực thấp. So với việc giám sát thủ công bởi nông dân (độ chính xác trung bình khoảng 70-75%), hệ thống CNN cung cấp một phương thức sàng lọc tin cậy hơn hẳn. Theo tính toán mô phỏng, việc phát hiện sớm bệnh lý thông qua ứng dụng CNN có thể giúp người nông dân giảm lượng thuốc bảo vệ thực vật sử dụng tới 20% và tăng năng suất thu hoạch thêm 10% nhờ can thiệp đúng lúc, đúng bệnh, bảo vệ sức khỏe người tiêu dùng và hệ sinh thái.
1	"Trong kỷ nguyên thương mại điện tử, việc cá nhân hóa trải nghiệm người dùng là yếu tố then chốt. Nghiên cứu này ứng dụng thuật toán K-Nearest Neighbors (KNN) để xây dựng hệ thống gợi ý sản phẩm dựa trên hành vi mua sắm tương đồng của khách hàng (Collaborative Filtering). Thuật toán hoạt động dựa trên nguyên lý tìm kiếm những người dùng có ""khoảng cách"" sở thích gần nhất để đưa ra đề xuất sản phẩm. Chúng tôi đã thử nghiệm hệ thống trên cơ sở dữ liệu của một trang web bán lẻ điện tử với hơn 50,000 người dùng và 5,000 mặt hàng. Kết quả cho thấy tỷ lệ chuyển đổi (Conversion Rate) tăng trưởng rõ rệt khi các gợi ý được đưa ra một cách chính xác dựa trên dữ liệu lịch sử."
1	Để triển khai KNN hiệu quả, việc lựa chọn tham số K và thước đo khoảng cách là vô cùng quan trọng. Nghiên cứu này sử dụng khoảng cách Cosine Similarity thay vì khoảng cách Euclidean để đánh giá sự tương đồng giữa các vector sở thích của khách hàng, vì nó hiệu quả hơn trong không gian dữ liệu thưa thớt (sparse data). Qua quá trình thử nghiệm Cross-validation, chúng tôi xác định giá trị K = 20 mang lại sự cân bằng tốt nhất giữa độ chính xác và tính đa dạng của gợi ý. Để tối ưu tốc độ tính toán, chúng tôi áp dụng cấu trúc dữ liệu BallTree, giúp giảm độ phức tạp thời gian khi tìm kiếm láng giềng trong tập dữ liệu lớn, đảm bảo hệ thống phản hồi trong thời gian thực.
1	Số liệu sau 3 tháng triển khai cho thấy tỷ lệ nhấp chuột (CTR - Click-Through Rate) vào danh mục gợi ý tăng từ 4.2% lên 9.8%. Quan trọng hơn, giá trị đơn hàng trung bình của nhóm khách hàng nhận được gợi ý từ KNN cao hơn 18% so với nhóm không sử dụng. Thuật toán KNN chứng minh ưu điểm nổi bật là tính đơn giản, dễ triển khai và không cần giai đoạn huấn luyện phức tạp như các mạng nơ-ron sâu, nhưng vẫn mang lại hiệu quả kinh tế lớn cho doanh nghiệp. Tuy nhiên, nghiên cứu cũng chỉ ra hạn chế của KNN là tiêu tốn bộ nhớ khi tập dữ liệu mở rộng quá lớn, từ đó đề xuất các giải pháp nén dữ liệu hoặc kết hợp với các kỹ thuật giảm chiều như PCA trong tương lai.
0	Trong nghiên cứu này, chúng tôi thực hiện tối ưu hóa mô hình BERT (Bidirectional Encoder Representations from Transformers) để tự động phân loại cảm xúc từ các đánh giá của khách hàng trên các sàn thương mại điện tử lớn tại Việt Nam. Khác với các mô hình truyền thống chỉ đọc văn bản theo một chiều, BERT có khả năng hiểu ngữ cảnh hai chiều, giúp nhận diện chính xác các sắc thái biểu cảm phức tạp như mỉa mai hoặc khen ngợi gián tiếp. Chúng tôi xây dựng bộ dữ liệu gồm 20,000 bình luận về sản phẩm công nghệ và tiến hành tinh chỉnh (Fine-tuning) mô hình. Kết quả cho thấy BERT vượt trội hoàn toàn so với các kiến trúc cũ, giúp doanh nghiệp nắm bắt phản hồi thị trường một cách nhanh chóng và chính xác nhất.
1	Dữ liệu văn bản tiếng Việt sau khi thu thập được đưa qua bước tiền xử lý bao gồm tách từ (Tokenization) bằng công cụ VinAI-BERT và loại bỏ các ký tự đặc biệt nhưng vẫn giữ lại các biểu tượng cảm xúc (emojis) vì chúng mang giá trị ngữ nghĩa lớn. Chúng tôi sử dụng kiến trúc bert-base-multilingual-cased làm mô hình gốc và thêm một lớp phân loại (Classification Layer) ở phía trên. Quá trình huấn luyện sử dụng hàm tối ưu AdamW với tốc độ học cực nhỏ (2x10^{-5}) để tránh làm hỏng các trọng số đã được học trước đó. Việc sử dụng kỹ thuật Batch Normalization và Dropout (0.1) giúp mô hình ổn định hơn và hạn chế tối đa hiện tượng quá khớp khi đối mặt với các từ lóng hoặc ngôn ngữ mạng của người dùng trẻ.
1	Mô hình BERT đạt độ chính xác (Accuracy) ấn tượng lên tới 92.8%, cao hơn hẳn so với mô hình LSTM truyền thống (chỉ đạt 84.5%). Đặc biệt, chỉ số Recall cho các đánh giá tiêu cực đạt 0.91, điều này cực kỳ quan trọng vì nó giúp doanh nghiệp không bỏ sót các khiếu nại của khách hàng để xử lý kịp thời. Qua thực tế triển khai, hệ thống có khả năng xử lý hơn 1,000 bình luận mỗi phút, giúp bộ phận chăm sóc khách hàng giảm tải được 60% khối lượng công việc kiểm soát thủ công. Nghiên cứu khẳng định rằng việc áp dụng xử lý ngôn ngữ tự nhiên (NLP) hiện đại là chìa khóa để xây dựng hệ thống quản trị quan hệ khách hàng thông minh trong thời đại số.
1	Dự báo thị trường tài chính luôn là một thách thức lớn do tính biến động cao và dữ liệu có độ nhiễu lớn. Nghiên cứu này đề xuất sử dụng mạng Long Short-Term Memory (LSTM) – một biến thể của mạng nơ-ron hồi quy (RNN) có khả năng ghi nhớ các phụ thuộc dài hạn – để dự báo giá đóng cửa của các mã cổ phiếu thuộc nhóm VN30. Chúng tôi thu thập dữ liệu lịch sử trong 5 năm (từ 2021 đến 2026) bao gồm các chỉ số: giá mở cửa, giá cao nhất, thấp nhất, khối lượng giao dịch và các chỉ báo kỹ thuật như RSI, MACD. Mô hình được kỳ vọng sẽ hỗ trợ các nhà đầu tư cá nhân trong việc đưa ra quyết định mua/bán dựa trên các dự đoán có cơ sở khoa học thay vì cảm tính.
1	Kiến trúc LSTM của chúng tôi gồm 2 lớp ẩn LSTM (mỗi lớp 50 đơn vị) kết hợp với các lớp Dense để trích xuất đặc trưng từ chuỗi thời gian. Dữ liệu được xử lý bằng kỹ thuật Sliding Window với độ dài cửa sổ là 60 ngày (tức dùng dữ liệu 2 tháng trước để dự báo ngày tiếp theo). Chúng tôi áp dụng phương pháp chuẩn hóa Min-Max Scaling để đưa dữ liệu về khoảng [0, 1], giúp thuật toán gradient descent hội tụ nhanh hơn. Một điểm đặc biệt trong nghiên cứu này là việc tích hợp thêm cơ chế Early Stopping, tự động dừng huấn luyện khi sai số trên tập kiểm tra không còn giảm, giúp mô hình giữ được tính tổng quát hóa cao khi đối mặt với những biến động bất ngờ của thị trường thực tế.
1	Kết quả thực nghiệm trên mã cổ phiếu đầu ngành cho thấy sai số bình quân phương chân phương (RMSE) chỉ ở mức 1.45, một con số rất thấp so với giá trị thực tế của cổ phiếu. Chỉ số sai số tuyệt đối phần trăm trung bình (MAPE) đạt 1.8%, cho thấy dự báo của LSTM sát với thực tế đến kinh ngạc trong các giai đoạn thị trường ổn định. Mặc dù trong những phiên thị trường có biến động cực mạnh (Black Swan events), sai số có tăng lên nhưng mô hình vẫn giữ được xu hướng chung (trend) của giá. Nghiên cứu kết luận rằng LSTM là công cụ mạnh mẽ trong phân tích kỹ thuật hiện đại, giúp tăng tỷ lệ lợi nhuận kỳ vọng cho nhà đầu tư thêm khoảng 12% mỗi năm so với chiến lược mua và nắm giữ truyền thống.
1	Với sự gia tăng của các cuộc tấn công mạng, việc phát hiện sớm các phần mềm độc hại là nhiệm vụ cấp bách. Bài báo này trình bày phương pháp sử dụng thuật toán Support Vector Machine (SVM) để phân loại các tệp tin thực thi thành hai nhóm: lành tính và mã độc. Thay vì dựa trên các dấu hiệu nhận dạng (signatures) vốn dễ bị các hacker qua mặt bằng kỹ thuật làm rối mã, chúng tôi tập trung vào phân tích tĩnh các đặc trưng của tệp tin như các lệnh gọi hàm API, quyền truy cập hệ thống và cấu trúc các phân đoạn trong tệp PE. Bằng cách tìm ra siêu phẳng tối ưu (Optimal Hyperplane) để phân tách dữ liệu trong không gian đa chiều, mô hình SVM cung cấp khả năng bảo vệ chủ động trước các loại mã độc mới chưa từng xuất hiện.
1	Chúng tôi sử dụng kỹ thuật trích xuất đặc trưng từ 15,000 mẫu mã độc thu thập từ các nguồn uy tín. Mỗi tệp tin được biểu diễn thành một vector đặc trưng gồm 512 chiều. Do dữ liệu mã độc thường không thể phân tách tuyến tính trong không gian ban đầu, chúng tôi áp dụng Kernel RBF (Radial Basis Function) để đưa dữ liệu lên không gian cao chiều hơn. Quá trình tối ưu hóa các tham số C (mức độ phạt sai số) và gamma (độ ảnh hưởng của từng điểm dữ liệu) được thực hiện thông qua kỹ thuật Grid Search kết hợp với kiểm chứng chéo 5 lớp (5-fold cross-validation). Việc lựa chọn các đặc trưng quan trọng nhất thông qua chỉ số Information Gain giúp giảm chiều dữ liệu, từ đó tăng tốc độ phân loại mà vẫn giữ được độ chính xác cần thiết.
1	Hệ thống đạt tỷ lệ phát hiện chính xác (Precision) lên đến 96.2% và tỷ lệ báo động giả (False Positive Rate) cực thấp, chỉ dưới 0.8%. Đây là một yếu tố sống còn vì tỷ lệ báo động giả cao sẽ gây phiền hà cho người dùng và làm giảm hiệu suất hệ thống. Thời gian để SVM phân tích và đưa ra kết luận cho một tệp tin mới chỉ mất trung bình 15ms, cho phép tích hợp trực tiếp vào các phần mềm diệt virus chạy thời gian thực hoặc các cổng kiểm soát dữ liệu (Gateway). Nghiên cứu cho thấy SVM đặc biệt hiệu quả trong việc phát hiện các biến thể của mã độc tống tiền (Ransomware), giúp ngăn chặn kịp thời các hành vi mã hóa dữ liệu trái phép và bảo vệ an toàn thông tin cho các tổ chức doanh nghiệp.
1	Xe tự hành yêu cầu lượng dữ liệu hình ảnh và cảm biến cực lớn để huấn luyện mô hình nhận thức môi trường. Tuy nhiên, các tình huống hiếm gặp như tai nạn, thời tiết xấu hoặc hành vi giao thông bất thường rất khó thu thập trong thực tế. Theo báo cáo thử nghiệm, hơn 70% dữ liệu huấn luyện xe tự hành đến từ các tình huống thông thường, dẫn đến nguy cơ mô hình phản ứng kém trong điều kiện bất thường. GAN được đề xuất như một giải pháp tạo dữ liệu tổng hợp, giúp mô phỏng các kịch bản giao thông phức tạp. Việc bổ sung dữ liệu này giúp cân bằng tập huấn luyện và nâng cao khả năng tổng quát hóa của hệ thống lái xe tự động trong môi trường đời sống thực.
1	Nghiên cứu sử dụng Conditional GAN để sinh ảnh giao thông trong các điều kiện mưa lớn, sương mù và ban đêm. Tập dữ liệu gốc gồm 80.000 ảnh thu từ camera gắn trên xe thử nghiệm. Sau khi áp dụng GAN, số lượng ảnh tăng lên 240.000 ảnh, trong đó các tình huống hiếm chiếm khoảng 35%. Mô hình nhận dạng làn đường dựa trên CNN sau huấn luyện đạt độ chính xác 93,8%, tăng từ mức 88,6% ban đầu. Đặc biệt, khả năng phát hiện vật cản trong điều kiện ánh sáng yếu cải thiện khoảng 22%. Các kết quả cho thấy dữ liệu tổng hợp từ GAN đóng vai trò quan trọng trong việc nâng cao độ an toàn cho xe tự hành.
1	Trong đời sống, việc ứng dụng GAN giúp giảm chi phí thử nghiệm xe tự hành lên tới 40%, do hạn chế nhu cầu thu thập dữ liệu thực tế tốn kém. Ngoài ra, dữ liệu tổng hợp cho phép mô phỏng các tình huống nguy hiểm mà con người khó hoặc không thể tái hiện an toàn. Tuy nhiên, thách thức chính là đảm bảo dữ liệu sinh ra phản ánh đúng quy luật vật lý và hành vi giao thông thực. Trong tương lai, các mô hình GAN kết hợp mô phỏng vật lý và dữ liệu cảm biến đa nguồn được kỳ vọng sẽ trở thành nền tảng quan trọng cho ngành giao thông thông minh và xe tự hành thương mại.
1	Hồ sơ sức khỏe điện tử (EHR) chứa lượng lớn dữ liệu như kết quả xét nghiệm, chỉ số sinh tồn và lịch sử điều trị. Việc khai thác hiệu quả nguồn dữ liệu này có thể hỗ trợ bác sĩ trong chẩn đoán sớm bệnh lý. ANN và KNN là hai thuật toán được sử dụng phổ biến nhờ khả năng xử lý dữ liệu đa chiều và dễ triển khai. Theo thống kê, hơn 60% sai sót chẩn đoán đến từ việc bỏ sót thông tin trong hồ sơ bệnh án. Việc áp dụng các mô hình học máy giúp tự động phát hiện các mẫu bất thường, từ đó hỗ trợ quyết định lâm sàng và nâng cao chất lượng chăm sóc sức khỏe trong đời sống hàng ngày.
1	Nghiên cứu sử dụng ANN để phân loại nguy cơ mắc bệnh tim mạch dựa trên dữ liệu của 45.000 bệnh nhân, bao gồm 18 thuộc tính đầu vào. Kết quả cho thấy mô hình ANN đạt độ chính xác 91,2% trong dự đoán nguy cơ cao. Song song đó, KNN được áp dụng để tìm các bệnh nhân có hồ sơ tương tự nhằm hỗ trợ so sánh lâm sàng. Khi kết hợp hai phương pháp, độ nhạy của hệ thống tăng thêm 8% so với chỉ sử dụng ANN đơn lẻ. Thời gian xử lý trung bình mỗi hồ sơ dưới 0,05 giây, đáp ứng yêu cầu sử dụng trong môi trường bệnh viện thực tế.
1	Trong ứng dụng đời sống, hệ thống hỗ trợ chẩn đoán dựa trên ANN và KNN giúp bác sĩ giảm tải công việc và tăng độ chính xác trong quyết định điều trị. Các thử nghiệm cho thấy thời gian chẩn đoán ban đầu giảm khoảng 25%, trong khi tỷ lệ phát hiện sớm bệnh tăng gần 20%. Tuy nhiên, vấn đề bảo mật dữ liệu y tế và khả năng giải thích mô hình vẫn là thách thức lớn. Hướng nghiên cứu tương lai tập trung vào các mô hình ANN giải thích được và hệ thống lai giữa học máy và tri thức y khoa, nhằm đảm bảo độ tin cậy và tính chấp nhận trong thực tế lâm sàng.